<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.CV](#cs.CV) [Total: 64]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.RO](#cs.RO) [Total: 5]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
*William Bruns*

Main category: cs.CL

TLDR: 论文通过RASP语言证明Transformer编码器-解码器可以系统化、组合化地完成ReCOGS_pos任务，并在大多数测试集上达到100%准确率。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在组合泛化任务（如COGS基准）中的表现，尤其是针对其结构泛化能力不足的问题。

Method: 使用RASP语言构建Transformer等效模型，通过19个注意力头兼容的扁平模式匹配规则处理ReCOGS_pos任务，无需层次化或树状结构。

Result: 模型在ReCOGS测试集上达到100%语义精确匹配，除obj_pp_to_subj_pp外所有泛化分集均为100%。

Conclusion: ReCOGS_pos任务无需树状结构解决方案，扁平模式匹配规则即可实现高精度。

Abstract: Humans understand new combinations of words encountered if they are
combinations of words recognized from different contexts, an ability called
Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020)
arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural
generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted
Access Sequence Processing (RASP), a Transformer-equivalent programming
language, to prove by construction that a Transformer encoder-decoder can
perform the semantically equivalent ReCOGS_pos (Wu et al., 2024)
arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP
model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on
all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,
our RASP model shows the ReCOGS_pos task does not require a hierarchical or
tree-structured solution: we use word-level tokens with an "embedding" layer
that tags with possible parts of speech, applying just once per encoder pass 19
attention-head compatible flat pattern-matching rules, shown using grammar
coverage (Zeller et al., 2023) to be learnable from the training data, plus
general prepositional phrase (pp) handling and sentential complement (cp)
handling logic, and output the next logical form (LF) token (repeating until
the LF is complete). The model does not apply recursive, tree-structured rules
like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact
match on pp recursion, cp recursion using the decoder loop.

</details>

### [2] [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
*Myrthe Reuver,Indira Sen,Matteo Melis,Gabriella Lapesa*

Main category: cs.CL

TLDR: 论文研究了性别歧视研究者与大型语言模型（LLMs）的协作，通过四步流程探索了专家与LLMs在性别歧视定义与分类中的互动效果。


<details>
  <summary>Details</summary>
Motivation: 探讨性别歧视研究者与LLMs的协作潜力，评估LLMs在性别歧视研究中的适用性。

Method: 1. 九位性别歧视研究者回答问题；2. 进行两项交互实验（评估LLMs知识与定义性别歧视）；3. 使用三种定义进行零样本分类实验。

Result: LLMs生成的性别歧视定义更长且复杂，专家定义平均表现较差，但部分专家通过协作定义提升了分类性能。

Conclusion: LLMs在性别歧视研究中具有一定潜力，但专家与LLMs协作可能更有效。

Abstract: This paper investigates hybrid intelligence and collaboration between
researchers of sexism and Large Language Models (LLMs), with a four-component
pipeline. First, nine sexism researchers answer questions about their knowledge
of sexism and of LLMs. They then participate in two interactive experiments
involving an LLM (GPT3.5). The first experiment has experts assessing the
model's knowledge about sexism and suitability for use in research. The second
experiment tasks them with creating three different definitions of sexism: an
expert-written definition, an LLM-written one, and a co-created definition.
Lastly, zero-shot classification experiments use the three definitions from
each expert in a prompt template for sexism detection, evaluating GPT4o on
2.500 texts sampled from five sexism benchmarks. We then analyze the resulting
67.500 classification decisions. The LLM interactions lead to longer and more
complex definitions of sexism. Expert-written definitions on average perform
poorly compared to LLM-generated definitions. However, some experts do improve
classification performance with their co-created definitions of sexism, also
experts who are inexperienced in using LLMs.

</details>

### [3] [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
*Sungjun Han,Juyoung Suk,Suyeong An,Hyungguk Kim,Kyuseok Kim,Wonsuk Yang,Seungtaek Choi,Jamin Shin*

Main category: cs.CL

TLDR: Trillion-7B是一种高效的韩语为中心的多语言大语言模型，通过XLDA机制和优化数据策略实现低成本高性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言模型中知识从英语到目标语言（如韩语和日语）的高效迁移问题。

Method: 采用Cross-lingual Document Attention (XLDA)机制，结合优化数据混合、语言特定过滤和定制分词器。

Result: 在27个基准测试中表现优异，仅用10%的多语言数据和59.4K GPU小时完成训练。

Conclusion: Trillion-7B在多语言性能和跨语言一致性上表现卓越，成本效益高。

Abstract: We introduce Trillion-7B, the most token-efficient Korean-centric
multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)
mechanism enables highly efficient and effective knowledge transfer from
English to target languages like Korean and Japanese. Combined with optimized
data mixtures, language-specific filtering, and tailored tokenizer
construction, Trillion-7B achieves competitive performance while dedicating
only 10\% of its 2T training tokens to multilingual data and requiring just
59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations
across 27 benchmarks in four languages demonstrate Trillion-7B's robust
multilingual performance and exceptional cross-lingual consistency.

</details>

### [4] [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
*Yucheng Lu,Kazimier Smith*

Main category: cs.CL

TLDR: 研究表明，使用LLM生成的标签微调小型编码器模型在文本分类中存在性能下降、不稳定性和过早性能停滞等问题，需谨慎应用于高风险任务。


<details>
  <summary>Details</summary>
Motivation: 探讨在文本分类中使用LLM生成标签微调小型模型的可靠性，尤其是在高风险应用中。

Method: 通过实证分析比较使用LLM生成标签和黄金标签训练的模型，评估性能差异和稳定性。

Result: 发现LLM生成标签导致性能下降、训练不稳定和过早性能停滞，并提出了部分缓解策略。

Conclusion: LLM生成标签在高风险文本分类任务中存在固有风险，需谨慎使用并进一步研究缓解方法。

Abstract: Using LLM-generated labels to fine-tune smaller encoder-only models for text
classification has gained popularity in various settings. While this approach
may be justified in simple and low-stakes applications, we conduct empirical
analysis to demonstrate how the perennial curse of training on synthetic data
manifests itself in this specific setup. Compared to models trained on gold
labels, we observe not only the expected performance degradation in accuracy
and F1 score, but also increased instability across training runs and premature
performance plateaus. These findings cast doubts on the reliability of such
approaches in real-world applications. We contextualize the observed phenomena
through the lens of error propagation and offer several practical mitigation
strategies, including entropy-based filtering and ensemble techniques. Although
these heuristics offer partial relief, they do not fully resolve the inherent
risks of propagating non-random errors from LLM annotations to smaller
classifiers, underscoring the need for caution when applying this workflow in
high-stakes text classification tasks.

</details>

### [5] [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
*Tyler A. Chang,Benjamin K. Bergen*

Main category: cs.CL

TLDR: 研究发现Transformer语言模型中存在一种称为bigram子网络的最小化结构，能够仅基于当前token预测下一个token。这些子网络在训练好的模型中仅占不到0.2%的参数，但对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型中最小化的预测机制，以理解模型如何从当前token转换为下一个token的预测。

Method: 识别并分析语言模型中的bigram子网络，这些子网络仅基于当前token预测下一个token。

Result: bigram子网络在1B参数的模型中存在，且集中在第一层Transformer MLP中，对模型性能至关重要。它们与最优剪枝子网络有显著重叠。

Conclusion: bigram子网络是语言模型中基本预测的最小必要结构，为研究语言模型电路提供了新方法。

Abstract: In Transformer language models, activation vectors transform from current
token embeddings to next token predictions as they pass through the model. To
isolate a minimal form of this transformation, we identify language model
subnetworks that make bigram predictions, naive next token predictions based
only on the current token. We find that bigram subnetworks can be found in
fully trained language models up to 1B parameters, and these subnetworks are
critical for model performance even when they consist of less than 0.2% of
model parameters. Bigram subnetworks are concentrated in the first Transformer
MLP layer, and they overlap significantly with subnetworks trained to optimally
prune a given model. Mechanistically, the bigram subnetworks often recreate a
pattern from the full models where the first layer induces a sharp change that
aligns activations with next token predictions rather than current token
representations. Our results demonstrate that bigram subnetworks comprise a
minimal subset of parameters that are both necessary and sufficient for basic
next token predictions in language models, and they help drive the
transformation from current to next token activations in the residual stream.
These subnetworks can lay a foundation for studying language model circuits by
building up from a minimal circuit rather than the traditional approach of
ablating circuits from a full model.

</details>

### [6] [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
*Szymon Kobus,Deniz Gündüz*

Main category: cs.CL

TLDR: 论文通过建立推测解码与信道模拟的联系，提出了一种信息论分析框架，推导了生成加速与草稿模型生成令牌数的显式关系，并提出了一种新的推测解码方法ERSD。


<details>
  <summary>Details</summary>
Motivation: 探索推测解码与信道模拟之间的联系，以信息论为基础分析推测解码的加速潜力。

Method: 通过信息论分析推测解码的加速上限，并提出基于指数竞赛的ERSD方法。

Result: 推导了生成加速与草稿模型生成令牌数的显式关系，ERSD方法达到了最先进性能。

Conclusion: 论文通过信息论分析为推测解码提供了理论支持，并提出了一种高效的ERSD方法。

Abstract: Speculative decoding accelerates large language model inference using a
smaller draft model. In this paper, we establish a surprising connection
between speculative decoding and channel simulation, which aims at simulating a
noisy channel using as few bits as possible. This connection allows us to
provide an information-theoretic analysis of the speed up that can be achieved
by speculative decoding. Leveraging this link, we derive an explicit relation
between generation speed-up and the number of tokens $k$ generated by the draft
model for large $k$, which serves as an upper bound for all $k$. We also
propose a novel speculative decoding method via exponential race ERSD that
matches state-of-the-art performance.

</details>

### [7] [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
*Keqi Deng,Wenxi Chen,Xie Chen,Philip C. Woodland*

Main category: cs.CL

TLDR: 论文提出SimulS2S-LLM方法，通过离线训练语音大语言模型（LLM）并结合测试时策略，实现流式语音翻译，提升翻译质量与延迟的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决流式语音翻译中语音作为提示输入与生成过程不匹配的问题，释放LLM的流式处理能力。

Method: 离线训练语音LLM，提取边界感知的语音提示以匹配文本输入，设计增量束搜索预测离散语音标记，并使用预训练声码器合成输出语音。

Result: 在CVSS语音数据上，SimulS2S-LLM在相同训练数据下优于现有方法，如ASR-BLEU分数提升3分，延迟相近。

Conclusion: SimulS2S-LLM通过优化训练与推理的匹配，实现了流式语音翻译的高质量与低延迟平衡。

Abstract: Simultaneous speech translation (SST) outputs translations in parallel with
streaming speech input, balancing translation quality and latency. While large
language models (LLMs) have been extended to handle the speech modality,
streaming remains challenging as speech is prepended as a prompt for the entire
generation process. To unlock LLM streaming capability, this paper proposes
SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy
to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between
training and inference by extracting boundary-aware speech prompts that allows
it to be better matched with text input data. SimulS2S-LLM achieves
simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete
output speech tokens and then synthesising output speech using a pre-trained
vocoder. An incremental beam search is designed to expand the search space of
speech token prediction without increasing latency. Experiments on the CVSS
speech data show that SimulS2S-LLM offers a better translation quality-latency
trade-off than existing methods that use the same training data, such as
improving ASR-BLEU scores by 3 points at similar latency.

</details>

### [8] [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
*Minghao Wu,Weixuan Wang,Sinuo Liu,Huifeng Yin,Xintong Wang,Yu Zhao,Chenyang Lyu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TLDR: 该立场论文分析了2000多个多语言基准，发现英语仍占主导地位，且翻译基准与本地化基准相比表现较差，强调了定制化多语言基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 推动多语言评估的公平性，揭示当前多语言基准的不足，促进更有效的多语言技术进步。

Method: 分析了来自148个国家的2000多个多语言基准（2021-2024年），比较了基准表现与人类判断的相关性。

Result: 英语在多语言基准中占比过高，本地化基准比翻译基准更符合本地人类判断（0.68 vs 0.47）。STEM任务与人类评价相关性高（0.70-0.85），而传统NLP任务相关性低（0.11-0.30）。

Conclusion: 提出六项当前多语言评估的局限性，制定有效多语言基准的指导原则，并呼吁全球合作开发符合实际应用的基准。

Abstract: As large language models (LLMs) continue to advance in linguistic
capabilities, robust multilingual evaluation has become essential for promoting
equitable technological progress. This position paper examines over 2,000
multilingual (non-English) benchmarks from 148 countries, published between
2021 and 2024, to evaluate past, present, and future practices in multilingual
benchmarking. Our findings reveal that, despite significant investments
amounting to tens of millions of dollars, English remains significantly
overrepresented in these benchmarks. Additionally, most benchmarks rely on
original language content rather than translations, with the majority sourced
from high-resource countries such as China, India, Germany, the UK, and the
USA. Furthermore, a comparison of benchmark performance with human judgments
highlights notable disparities. STEM-related tasks exhibit strong correlations
with human evaluations (0.70 to 0.85), while traditional NLP tasks like
question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).
Moreover, translating English benchmarks into other languages proves
insufficient, as localized benchmarks demonstrate significantly higher
alignment with local human judgments (0.68) than their translated counterparts
(0.47). This underscores the importance of creating culturally and
linguistically tailored benchmarks rather than relying solely on translations.
Through this comprehensive analysis, we highlight six key limitations in
current multilingual evaluation practices, propose the guiding principles
accordingly for effective multilingual benchmarking, and outline five critical
research directions to drive progress in the field. Finally, we call for a
global collaborative effort to develop human-aligned benchmarks that prioritize
real-world applications.

</details>

### [9] [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
*Qiyao Wang,Guhong Chen,Hongbo Wang,Huaren Liu,Minghui Zhu,Zhifei Qin,Linwei Li,Yilin Yue,Shiqiang Wang,Jiayan Li,Yihang Wu,Ziqiang Liu,Longze Chen,Run Luo,Liyang Fan,Jiaming Li,Lei Zhang,Kan Xu,Hongfei Lin,Hamid Alinejad-Rokny,Shiwen Ni,Yuan Lin,Min Yang*

Main category: cs.CL

TLDR: 该论文提出了首个全面的IP任务分类法和双语基准IPBench，用于评估大语言模型在知识产权领域的实际应用表现。


<details>
  <summary>Details</summary>
Motivation: 知识产权领域复杂且知识密集，现有数据集和基准未能全面覆盖真实场景，需要更全面的评估工具。

Method: 引入IPBench基准，涵盖8种IP机制和20项任务，评估16种大语言模型的表现。

Result: 最佳模型准确率仅为75.8%，开源模型表现落后于闭源通用模型。

Conclusion: IPBench填补了知识产权领域评估工具的空白，未来将持续更新以反映实际挑战。

Abstract: Intellectual Property (IP) is a unique domain that integrates technical and
legal knowledge, making it inherently complex and knowledge-intensive. As large
language models (LLMs) continue to advance, they show great potential for
processing IP tasks, enabling more efficient analysis, understanding, and
generation of IP-related content. However, existing datasets and benchmarks
either focus narrowly on patents or cover limited aspects of the IP field,
lacking alignment with real-world scenarios. To bridge this gap, we introduce
the first comprehensive IP task taxonomy and a large, diverse bilingual
benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is
designed to evaluate LLMs in real-world intellectual property applications,
encompassing both understanding and generation. We benchmark 16 LLMs, ranging
from general-purpose to domain-specific models, and find that even the
best-performing model achieves only 75.8% accuracy, revealing substantial room
for improvement. Notably, open-source IP and law-oriented models lag behind
closed-source general-purpose models. We publicly release all data and code of
IPBench and will continue to update it with additional IP-related tasks to
better reflect real-world challenges in the intellectual property domain.

</details>

### [10] [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
*Sophia Maria*

Main category: cs.CL

TLDR: Compass-v2是一个轻量级的MoE模型，专为东南亚语言和电子商务应用设计，通过混合专家模块和高质量数据集实现高性能和低成本推理。


<details>
  <summary>Details</summary>
Motivation: 解决主流LLMs对低资源语言（尤其是东南亚语言）和电子商务领域的覆盖不足问题。

Method: 设计30B总参数和5B活跃参数的MoE模型，结合细粒度和共享专家模块；构建高质量东南亚语言数据集和电子商务数据集；提出混合推理模型支持快速和深度思考。

Result: 在30B以下模型中，Compass-v2在东南亚多语言和电子商务任务上表现最佳，同时推理成本显著降低。

Conclusion: Compass-v2通过创新设计和数据集构建，成功填补了低资源语言和电子商务领域的空白，同时兼顾性能和成本。

Abstract: Predominant LLMs focus on high-resource languages while leaving low-resource
languages, particularly those in Southeast Asia (SEA), underrepresented. In
addition, those models are general-purpose and pay limited attention to the
e-commerce domain. To overcome these limitations, we introduce Compass-v2, a
lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast
Asian languages and e-commerce applications. To balance model performance and
inference cost, the model is designed with 30B total parameters and 5B active
parameters, incorporating both fine-grained and shared expert modules. To
enhance multilingual performance, we curated and constructed a high-quality,
industry-leading SEA dataset, to the best of our knowledge. To boost
performance in the e-commerce domain, we built a dataset comprising hundreds of
billions of tokens, sourced through external data mining and internal platform
collection. Besides, we pioneered a hybrid reasoning model that supports both
fast thinking and deep thinking within a unified framework to enhance the
reasoning capabilities, diverging from the conventional industry practice of
deploying two separate models. Through extensive experimental evaluations, our
model demonstrates state-of-the-art SEA multilingual and e-commerce performance
among sub-30B models, while maintaining significantly lower inference cost.

</details>

### [11] [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
*Issa Sugiura,Kouta Nakayama,Yusuke Oda*

Main category: cs.CL

TLDR: 论文介绍了llm-jp-modernbert，一种基于大规模日语语料库训练的ModernBERT模型，支持8192 tokens的长上下文。虽然在下游任务中未超越现有基线，但在填充掩码测试中表现良好，并分析了上下文长度扩展的影响。


<details>
  <summary>Details</summary>
Motivation: 探索在大规模语料库和长上下文条件下预训练编码器模型（如BERT）的性能，填补与解码器模型相比的研究空白。

Method: 使用公开的大规模日语语料库训练ModernBERT模型，支持8192 tokens的上下文长度，并通过填充掩码测试和伪困惑度实验评估性能。

Result: 模型在填充掩码测试中表现良好，但在下游任务中未超越现有基线。通过伪困惑度实验验证了上下文长度扩展的效果。

Conclusion: llm-jp-modernbert为长上下文BERT的发展提供了可复现的模型和代码，支持进一步研究。

Abstract: Encoder-only transformer models like BERT are widely adopted as a pre-trained
backbone for tasks like sentence classification and retrieval. However,
pretraining of encoder models with large-scale corpora and long contexts has
been relatively underexplored compared to decoder-only transformers. In this
work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly
available, massive Japanese corpus with a context length of 8192 tokens. While
our model does not surpass existing baselines on downstream tasks, it achieves
good results on fill-mask test evaluations. We also analyze the effect of
context length expansion through pseudo-perplexity experiments. Furthermore, we
investigate sentence embeddings in detail, analyzing their transitions during
training and comparing them with those from other existing models, confirming
similar trends with models sharing the same architecture. To support
reproducibility and foster the development of long-context BERT, we release our
model, along with the training and evaluation code.

</details>

### [12] [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
*Elyas Meguellati,Assaad Zeghina,Shazia Sadiq,Gianluca Demartini*

Main category: cs.CL

TLDR: 论文提出了一种利用LLM进行文本预处理和语义增强的方法，以提升复杂社交媒体分类任务的性能，效果接近人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM生成合成训练数据，而忽视了其在文本预处理和语义增强中的潜力，尤其是在复杂社交媒体任务中LLM的零样本分类表现不佳。

Method: 通过提示LLM清理噪声文本并提供上下文丰富的解释，增强训练集，而不大幅增加数据量。

Result: 在多个数据集上的评估显示，LLM零样本分类表现较差，但结合语义增强后性能接近人工标注数据。

Conclusion: 策略性地将LLM整合到机器学习流程中，可高效提升社交媒体分类任务性能，对打击有害内容有广泛意义。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
performance on simple text classification tasks, frequently under zero-shot
settings. However, their efficacy declines when tackling complex social media
challenges such as propaganda detection, hateful meme classification, and
toxicity identification. Much of the existing work has focused on using LLMs to
generate synthetic training data, overlooking the potential of LLM-based text
preprocessing and semantic augmentation. In this paper, we introduce an
approach that prompts LLMs to clean noisy text and provide context-rich
explanations, thereby enhancing training sets without substantial increases in
data volume. We systematically evaluate on the SemEval 2024 multi-label
Persuasive Meme dataset and further validate on the Google Jigsaw toxic
comments and Facebook hateful memes datasets to assess generalizability. Our
results reveal that zero-shot LLM classification underperforms on these
high-context tasks compared to supervised models. In contrast, integrating
LLM-based semantic augmentation yields performance on par with approaches that
rely on human-annotated data, at a fraction of the cost. These findings
underscore the importance of strategically incorporating LLMs into machine
learning (ML) pipeline for social media classification tasks, offering broad
implications for combating harmful content online.

</details>

### [13] [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
*Yuxin Jiang,Yufei Wang,Chuhan Wu,Xinyi Dai,Yan Xu,Weinan Gan,Yasheng Wang,Xin Jiang,Lifeng Shang,Ruiming Tang,Wei Wang*

Main category: cs.CL

TLDR: WebR是一种自动化框架，直接从原始网页文档合成高质量的指令调优数据，无需依赖种子数据或强假设，显著提升LLM的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动数据合成方法依赖种子数据质量或对网页结构的强假设，限制了高质量指令-响应对的生成。

Method: 提出WebR框架，采用双视角范式（网页作为指令或响应），从原始网页内容合成数据。

Result: WebR生成的数据在四个指令跟随基准上比现有方法最高提升16.65%，并表现出更好的兼容性、数据效率和可扩展性。

Conclusion: WebR为指令调优数据合成提供了高效、可扩展的解决方案，显著提升了LLM的性能。

Abstract: The improvement of LLMs' instruction-following capabilities depends
critically on the availability of high-quality instruction-response pairs.
While existing automatic data synthetic methods alleviate the burden of manual
curation, they often rely heavily on either the quality of seed data or strong
assumptions about the structure and content of web documents. To tackle these
challenges, we propose Web Reconstruction (WebR), a fully automated framework
for synthesizing high-quality instruction-tuning (IT) data directly from raw
web documents with minimal assumptions. Leveraging the inherent diversity of
raw web content, we conceptualize web reconstruction as an instruction-tuning
data synthesis task via a novel dual-perspective paradigm--Web as Instruction
and Web as Response--where each web document is designated as either an
instruction or a response to trigger the reconstruction process. Comprehensive
experiments show that datasets generated by WebR outperform state-of-the-art
baselines by up to 16.65% across four instruction-following benchmarks.
Notably, WebR demonstrates superior compatibility, data efficiency, and
scalability, enabling enhanced domain adaptation with minimal effort. The data
and code are publicly available at https://github.com/YJiangcm/WebR.

</details>

### [14] [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
*Pavan Yadav,Nikhil Khandalkar,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.CL

TLDR: 该研究比较了GPT-2和Llama-2-7b-chat-hf在心理理论任务中的表现，发现Llama-2在低温度设置下表现更优，而上下文复杂度增加会降低预测准确性。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在心理理论任务中的表现，探究上下文复杂度和温度设置对模型预测能力的影响。

Method: 构建包含不同复杂度上下文的短故事数据集，测试模型在四种温度设置下的表现，分析零阶、一阶和二阶推理任务中的预测准确性。

Result: Llama-2在低温度下表现优于GPT-2，上下文复杂度增加会降低准确性，模型在高阶推理任务中表现差异更大。

Conclusion: 模型架构、温度和上下文复杂度显著影响预测能力，研究揭示了当前语言模型的优势和局限性。

Abstract: Language models have made significant progress in generating coherent text
and predicting next tokens based on input prompts. This study compares the
next-token prediction performance of two well-known models: OpenAI's GPT-2 and
Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their
capabilities, we built a dataset from 10 short stories sourced from the Explore
ToM Dataset. We enhanced these stories by programmatically inserting additional
sentences (infills) using GPT-4, creating variations that introduce different
levels of contextual complexity. This setup enables analysis of how increasing
context affects model performance. We tested both models under four temperature
settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next
token across three reasoning levels. Zero-order reasoning involves tracking the
state, either current (ground truth) or past (memory). First-order reasoning
concerns understanding another's mental state (e.g., "Does Anne know the apple
is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think
that Charles knows the apple is salted?").
  Our results show that adding more infill sentences slightly reduces
prediction accuracy, as added context increases complexity and ambiguity.
Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at
lower temperatures, demonstrating greater confidence in selecting the most
probable token. As reasoning complexity rises, model responses diverge more.
Notably, GPT-2 and Llama-2 display greater variability in predictions during
first- and second-order reasoning tasks. These findings illustrate how model
architecture, temperature, and contextual complexity influence next-token
prediction, contributing to a better understanding of the strengths and
limitations of current language models.

</details>

### [15] [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
*Xiaowei Yuan,Zhao Yang,Ziyang Huang,Yequan Wang,Siqi Fan,Yiming Ju,Jun Zhao,Kang Liu*

Main category: cs.CL

TLDR: 论文提出了一种名为CaLE的新方法，通过增强LLMs内部表示中的上下文知识利用，改进了上下文忠实生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注解码策略，忽略了LLMs内部状态中上下文信息处理的基本机制，导致LLMs无法充分利用上下文知识。

Method: 提出Context-aware Layer Enhancement (CaLE)，通过V-usable信息分析策略性地在最优层增强上下文信息，从而丰富最终层的表示。

Result: 实验表明，CaLE在问答任务中有效提升了上下文忠实生成能力，尤其是在涉及未知或冲突上下文知识的场景中。

Conclusion: CaLE通过优化LLMs内部表示中的上下文信息处理，显著提升了上下文忠实生成的能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, yet they often struggle with context-faithfulness generations
that properly reflect contextual knowledge. While existing approaches focus on
enhancing the decoding strategies, they ignore the fundamental mechanism of how
contextual information is processed within LLMs' internal states. As a result,
LLMs remain limited in their ability to fully leverage contextual knowledge. In
this paper, we propose Context-aware Layer Enhancement (CaLE), a novel
intervention method that enhances the utilization of contextual knowledge
within LLMs' internal representations. By employing V-usable information
analysis, CaLE strategically amplifies the growth of contextual information at
an optimal layer, thereby enriching representations in the final layer. Our
experiments demonstrate that CaLE effectively improves context-faithful
generation in Question-Answering tasks, particularly in scenarios involving
unknown or conflicting contextual knowledge.

</details>

### [16] [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
*Hongtao Wang,Taiyan Zhang,Renchi Yang,Jianliang Xu*

Main category: cs.CL

TLDR: 本文提出了一种名为TECL的成本效益框架，通过有限查询预算利用LLMs反馈实现高效文本聚类。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖LLMs的高成本查询或推理，导致计算和财务负担。TECL旨在在有限预算内优化聚类效果。

Method: TECL采用EdgeLLM或TriangleLLM构建文本对的约束条件，并通过加权约束聚类生成聚类结果。

Result: 实验表明，TECL在相同查询成本下显著优于现有无监督文本聚类方法。

Conclusion: TECL为LLMs在文本聚类中的高效应用提供了可行方案。

Abstract: Text clustering aims to automatically partition a collection of text
documents into distinct clusters based on linguistic features. In the
literature, this task is usually framed as metric clustering based on text
embeddings from pre-trained encoders or a graph clustering problem upon
pairwise similarities from an oracle, e.g., a large ML model. Recently, large
language models (LLMs) bring significant advancement in this field by offering
contextualized text embeddings and highly accurate similarity scores, but
meanwhile, present grand challenges to cope with substantial computational
and/or financial overhead caused by numerous API-based queries or inference
calls to the models.
  In response, this paper proposes TECL, a cost-effective framework that taps
into the feedback from LLMs for accurate text clustering within a limited
budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or
TriangleLLM to construct must-link/cannot-link constraints for text pairs, and
further leverages such constraints as supervision signals input to our weighted
constrained clustering approach to generate clusters. Particularly, EdgeLLM
(resp. TriangleLLM) enables the identification of informative text pairs (resp.
triplets) for querying LLMs via well-thought-out greedy algorithms and accurate
extraction of pairwise constraints through carefully-crafted prompting
techniques. Our experiments on multiple benchmark datasets exhibit that TECL
consistently and considerably outperforms existing solutions in unsupervised
text clustering under the same query cost for LLMs.

</details>

### [17] [Computational Typology](https://arxiv.org/abs/2504.15642)
*Gerhard Jäger*

Main category: cs.CL

TLDR: 本文介绍了计算统计模型在语言类型学研究中的应用及其优势。


<details>
  <summary>Details</summary>
Motivation: 语言类型学旨在通过结构特征研究语言的多样性，计算方法的引入为大规模语言数据分析提供了新工具。

Method: 采用计算统计模型分析大规模语言数据，验证语言结构和演化的假设。

Result: 计算统计模型在类型学研究中展现出显著优势，支持了对语言普遍性的探索。

Conclusion: 计算方法的引入为语言类型学研究提供了更高效和精确的分析手段。

Abstract: Typology is a subfield of linguistics that focuses on the study and
classification of languages based on their structural features. Unlike
genealogical classification, which examines the historical relationships
between languages, typology seeks to understand the diversity of human
languages by identifying common properties and patterns, known as universals.
In recent years, computational methods have played an increasingly important
role in typological research, enabling the analysis of large-scale linguistic
data and the testing of hypotheses about language structure and evolution. This
article provides an illustration of the benefits of computational statistical
modeling in typology.

</details>

### [18] [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
*Simon Jehnen,Joaquín Ordieres-Meré,Javier Villalba-Díez*

Main category: cs.CL

TLDR: 论文研究了BERTopic与FinTextSim结合在分析10-K文件中的有效性，FinTextSim显著提升了主题模型的性能。


<details>
  <summary>Details</summary>
Motivation: 信息可用性和计算能力的进步促使需要更高效的方法分析财务文本数据，以提取有价值的见解。

Method: 使用BERTopic和FinTextSim（一种优化的句子转换模型）分析S&P 500公司的10-K文件（2016-2022）。

Result: FinTextSim显著提高了主题内相似性（81%）并降低了主题间相似性（100%），而BERTopic仅在与FinTextSim结合时才能形成清晰的经济主题簇。

Conclusion: FinTextSim对财务文本分析的提升至关重要，未来研究和财务信息质量将因此受益，为利益相关者提供竞争优势。

Abstract: Recent advancements in information availability and computational
capabilities have transformed the analysis of annual reports, integrating
traditional financial metrics with insights from textual data. To extract
valuable insights from this wealth of textual data, automated review processes,
such as topic modeling, are crucial. This study examines the effectiveness of
BERTopic, a state-of-the-art topic model relying on contextual embeddings, for
analyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies
(2016-2022). Moreover, we introduce FinTextSim, a finetuned
sentence-transformer model optimized for clustering and semantic search in
financial contexts. Compared to all-MiniLM-L6-v2, the most widely used
sentence-transformer, FinTextSim increases intratopic similarity by 81% and
reduces intertopic similarity by 100%, significantly enhancing organizational
clarity. We assess BERTopic's performance using embeddings from both FinTextSim
and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and
distinct economic topic clusters when paired with FinTextSim's embeddings.
Without FinTextSim, BERTopic struggles with misclassification and overlapping
topics. Thus, FinTextSim is pivotal for advancing financial text analysis.
FinTextSim's enhanced contextual embeddings, tailored for the financial domain,
elevate the quality of future research and financial information. This improved
quality of financial information will enable stakeholders to gain a competitive
advantage, streamlining resource allocation and decision-making processes.
Moreover, the improved insights have the potential to leverage business
valuation and stock price prediction models.

</details>

### [19] [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
*Mandy Cartner,Matthew Kogan,Nikolas Webster,Matthew Wagers,Ivy Sichel*

Main category: cs.CL

TLDR: 论文探讨了语言学中的“岛屿”现象，特别是主语作为岛屿的原因，并通过实验验证了不同结构中主语岛屿效应的普遍性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证主语岛屿效应是否仅与信息结构冲突（如wh-疑问句）相关，还是普遍存在于其他涉及移动的句法结构中。

Method: 通过三个大规模可接受性研究，使用超加性设计，测试wh-疑问句、关系从句和话题化结构中的主语岛屿效应。

Result: 实验结果显示，主语岛屿效应在所有测试结构中均存在，而不仅限于wh-疑问句。

Conclusion: 研究支持主语岛屿效应的抽象句法解释，而非仅与信息结构冲突相关。

Abstract: The term islands in linguistics refers to phrases from which extracting an
element results in ungrammaticality (Ross, 1967). Grammatical subjects are
considered islands because extracting a sub-part of a subject results in an
ill-formed sentence, despite having a clear intended meaning (e.g., "Which
topic did the article about inspire you?"). The generative tradition, which
views syntax as autonomous of meaning and function, attributes this
ungrammaticality to the abstract movement dependency between the wh-phrase and
the subject-internal position with which it is associated for interpretation.
However, research on language that emphasizes its communicative function
suggests instead that syntactic constraints, including islands, can be
explained based on the way different constructions package information.
Accordingly, Abeill\'e et al. (2020) suggest that the islandhood of subjects is
specific to the information structure of wh-questions, and propose that
subjects are not islands for movement, but for focusing, due to their
discourse-backgroundedness. This predicts that other constructions that differ
in their information structure from wh-questions, but still involve movement,
should not create a subject island effect. We test this prediction in three
large-scale acceptability studies, using a super-additive design that singles
out subject island violations, in three different constructions: wh-questions,
relative clauses, and topicalization. We report evidence for a subject island
effect in each construction type, despite only wh-questions introducing what
Abeill\'e et al. (2020) call "a clash in information structure." We argue that
this motivates an account of islands in terms of abstract, syntactic
representations, independent of the communicative function associated with the
constructions.

</details>

### [20] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TLDR: Tina是一种高效的小型推理模型，通过低秩适应（LoRA）在强化学习中实现高性能推理，成本极低。


<details>
  <summary>Details</summary>
Motivation: 探索如何在语言模型中高效实现强推理能力。

Method: 使用LoRA对1.5B参数的基模型进行参数高效更新，结合强化学习。

Result: Tina在推理性能上媲美甚至超越SOTA模型，成本仅为260分之一。

Conclusion: LoRA能高效适应推理结构，同时保留基模型知识，为低成本高性能推理提供了新思路。

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>

### [21] [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
*Ruizhe Li,Chiwei Zhu,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TLDR: 本文提出了一种基于Torrance创造性写作测试（TTCW）的自动化评估方法，用于评估大型语言模型（LLMs）生成的创意文本的创造力，显著提高了与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 评估机器生成文本的创造力是一个挑战，现有方法依赖昂贵的人工标注或与人类评估不一致。

Method: 采用基于参考的Likert式评分方法，将生成的创意文本与高质量参考文本进行对比评分。

Result: 实验结果显示，该方法显著提升了LLM评估与人类评估的一致性，配对准确率达到0.75（提升15%）。

Conclusion: 该方法为自动化评估创意文本的创造力提供了有效解决方案，具有实际应用潜力。

Abstract: Creative writing is a key capability of Large Language Models (LLMs), with
potential applications in literature, storytelling, and various creative
domains. However, evaluating the creativity of machine-generated texts remains
a significant challenge, as existing methods either rely on costly manual
annotations or fail to align closely with human assessments. In this paper, we
propose an effective automated evaluation method based on the Torrance Test of
Creative Writing (TTCW), which evaluates creativity as product. Our method
employs a reference-based Likert-style approach, scoring generated creative
texts relative to high-quality reference texts across various tests.
Experimental results demonstrate that our method significantly improves the
alignment between LLM evaluations and human assessments, achieving a pairwise
accuracy of 0.75 (+15\%).

</details>

### [22] [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
*Valeria Lerman,Yaniv Dover*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLMs）如何基于人类的可信度维度（能力、善意、正直）和人口统计变量形成对人类的信任，发现其模式与人类相似，但也存在偏见。


<details>
  <summary>Details</summary>
Motivation: 理解LLM与人类在决策互动中的信任动态，尤其是LLM如何形成对人类的信任，目前研究较少。

Method: 基于行为理论，通过43,200次模拟实验，分析五种流行语言模型在五种场景下的信任形成机制。

Result: LLM的信任形成与人类相似，但受可信度和人口统计变量（如年龄、宗教、性别）影响，尤其在金融场景中。不同模型表现有差异。

Conclusion: 需进一步研究AI对人类的信任动态，并监控偏见，以避免信任敏感应用中潜在的有害结果。

Abstract: As large language models (LLMs) and LLM-based agents increasingly interact
with humans in decision-making contexts, understanding the trust dynamics
between humans and AI agents becomes a central concern. While considerable
literature studies how humans trust AI agents, it is much less understood how
LLM-based agents develop effective trust in humans. LLM-based agents likely
rely on some sort of implicit effective trust in trust-related contexts (e.g.,
evaluating individual loan applications) to assist and affect decision making.
Using established behavioral theories, we develop an approach that studies
whether LLMs trust depends on the three major trustworthiness dimensions:
competence, benevolence and integrity of the human subject. We also study how
demographic variables affect effective trust. Across 43,200 simulated
experiments, for five popular language models, across five different scenarios
we find that LLM trust development shows an overall similarity to human trust
development. We find that in most, but not all cases, LLM trust is strongly
predicted by trustworthiness, and in some cases also biased by age, religion
and gender, especially in financial scenarios. This is particularly true for
scenarios common in the literature and for newer models. While the overall
patterns align with human-like mechanisms of effective trust formation,
different models exhibit variation in how they estimate trust; in some cases,
trustworthiness and demographic factors are weak predictors of effective trust.
These findings call for a better understanding of AI-to-human trust dynamics
and monitoring of biases and trust development patterns to prevent unintended
and potentially harmful outcomes in trust-sensitive applications of AI.

</details>

### [23] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
*Michael A. Hedderich,Anyi Wang,Raoyuan Zhao,Florian Eichin,Barbara Plank*

Main category: cs.CL

TLDR: Spotlight结合自动化与人工分析，通过数据挖掘技术区分语言模型输出的随机变化与系统性差异，提供标记模式以高效分析提示和模型变化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（自动指标或人工评估）存在局限性，无法全面或高效地分析提示和模型变化对输出的影响。

Method: 基于数据挖掘技术自动区分输出中的随机变化与系统性差异，生成标记模式指导人工分析。

Result: 创建三个基准测试验证标记模式提取方法的可靠性，并通过用户研究证明该方法能帮助理解输出差异。

Conclusion: Spotlight为提示工程和模型行为研究提供了新视角，支持发现由提示和模型变化引起的相关差异。

Abstract: Prompt engineering for large language models is challenging, as even small
prompt perturbations or model changes can significantly impact the generated
output texts. Existing evaluation methods, either automated metrics or human
evaluation, have limitations, such as providing limited insights or being
labor-intensive. We propose Spotlight, a new approach that combines both
automation and human analysis. Based on data mining techniques, we
automatically distinguish between random (decoding) variations and systematic
differences in language model outputs. This process provides token patterns
that describe the systematic differences and guide the user in manually
analyzing the effects of their prompt and model changes efficiently. We create
three benchmarks to quantitatively test the reliability of token pattern
extraction methods and demonstrate that our approach provides new insights into
established prompt data. From a human-centric perspective, through
demonstration studies and a user study, we show that our token pattern approach
helps users understand the systematic differences of language model outputs,
and we are able to discover relevant differences caused by prompt and model
changes (e.g. related to gender or culture), thus supporting the prompt
engineering process and human-centric model behavior research.

</details>

### [24] [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
*Junshu Pan,Wei Shen,Shulin Huang,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TLDR: Pre-DPO是一种基于DPO的训练范式，通过引入指导性参考模型提升偏好优化性能，无需依赖外部模型或额外数据。


<details>
  <summary>Details</summary>
Motivation: DPO和SimPO在训练中存在数据利用效率低、性能上限和训练鲁棒性不足的问题，Pre-DPO旨在解决这些问题。

Method: Pre-DPO利用指导性参考模型，动态调整样本权重，优化偏好数据训练。

Result: 在AlpacaEval 2.0和Arena-Hard v0.1基准测试中，Pre-DPO显著提升了DPO和SimPO的性能。

Conclusion: Pre-DPO是一种简单有效的训练范式，能显著提升偏好优化性能。

Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from
human feedback (RLHF) for large language models (LLMs) by directly optimizing
human preferences without an explicit reward model. We find that during DPO
training, the reference model plays the role of a data weight adjuster.
However, the common practice of initializing the policy and reference models
identically in DPO can lead to inefficient data utilization and impose a
performance ceiling. Meanwhile, the lack of a reference model in Simple
Preference Optimization (SimPO) reduces training robustness and necessitates
stricter conditions to prevent catastrophic forgetting. In this work, we
propose Pre-DPO, a simple yet effective DPO-based training paradigm that
enhances preference optimization performance by leveraging a guiding reference
model. This reference model provides foresight into the optimal policy state
achievable through the training preference data, serving as a guiding mechanism
that adaptively assigns higher weights to samples more suitable for the model
and lower weights to those less suitable. Extensive experiments on AlpacaEval
2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently
improves the performance of both DPO and SimPO, without relying on external
models or additional data.

</details>

### [25] [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
*Luwei Xiao,Rui Mao,Shuai Zhao,Qika Lin,Yanhao Jia,Liang He,Erik Cambria*

Main category: cs.CL

TLDR: 本文提出了一种名为Chimera的多模态情感分类框架，结合了认知和美学因素，通过细粒度视觉特征和情感认知共振提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在理解细粒度视觉内容和情感认知机制方面存在不足，需要更全面的框架来捕捉语义内容和情感表达的驱动因素。

Method: Chimera框架整合了视觉块特征、粗粒度与细粒度视觉特征，并通过大语言模型生成情感原因和印象，增强模型对情感线索的感知。

Result: 实验结果表明，该模型在标准数据集上表现优异，且比GPT-4o等大语言模型更具灵活性。

Conclusion: Chimera通过结合认知和美学因素，显著提升了多模态情感分类的性能，并公开了实现和数据集。

Abstract: Multimodal aspect-based sentiment classification (MASC) is an emerging task
due to an increase in user-generated multimodal content on social platforms,
aimed at predicting sentiment polarity toward specific aspect targets (i.e.,
entities or attributes explicitly mentioned in text-image pairs). Despite
extensive efforts and significant achievements in existing MASC, substantial
gaps remain in understanding fine-grained visual content and the cognitive
rationales derived from semantic content and impressions (cognitive
interpretations of emotions evoked by image content). In this study, we present
Chimera: a cognitive and aesthetic sentiment causality understanding framework
to derive fine-grained holistic features of aspects and infer the fundamental
drivers of sentiment expression from both semantic perspectives and
affective-cognitive resonance (the synergistic effect between emotional
responses and cognitive interpretations). Specifically, this framework first
incorporates visual patch features for patch-word alignment. Meanwhile, it
extracts coarse-grained visual features (e.g., overall image representation)
and fine-grained visual regions (e.g., aspect-related regions) and translates
them into corresponding textual descriptions (e.g., facial, aesthetic).
Finally, we leverage the sentimental causes and impressions generated by a
large language model (LLM) to enhance the model's awareness of sentimental cues
evoked by semantic content and affective-cognitive resonance. Experimental
results on standard MASC datasets demonstrate the effectiveness of the proposed
model, which also exhibits greater flexibility to MASC compared to LLMs such as
GPT-4o. We have publicly released the complete implementation and dataset at
https://github.com/Xillv/Chimera

</details>

### [26] [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
*Chenxu Yang,Qingyi Si,Yongjie Duan,Zheliang Zhu,Chenyu Zhu,Zheng Lin,Li Cao,Weiping Wang*

Main category: cs.CL

TLDR: 提出一种自截断链式思维（CoT）的方法，通过动态终止冗余推理步骤，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 长链式思维（CoT）可能导致效率低下和准确性损失，需要一种动态截断方法。

Method: 在推理过渡点（如“Wait”标记）监测模型行为，动态终止冗余推理链的生成。

Result: 在多个基准测试中，CoT序列长度减少31%-43%，准确性提高1.7%-5.7%。

Conclusion: 该方法无需额外训练，可无缝集成到现有推理模型中，显著提升效率和准确性。

Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time
scaling, which extends long chain-of-thought (CoT) generation to solve complex
tasks. However, overthinking in long CoT not only slows down the efficiency of
problem solving, but also risks accuracy loss due to the extremely detailed or
redundant reasoning steps. We propose a simple yet effective method that allows
LLMs to self-truncate CoT sequences by early exit during generation. Instead of
relying on fixed heuristics, the proposed method monitors model behavior at
potential reasoning transition points (e.g.,"Wait" tokens) and dynamically
terminates the next reasoning chain's generation when the model exhibits high
confidence in a trial answer. Our method requires no additional training and
can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments
on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024
show that the proposed method is consistently effective on deepseek-series
reasoning LLMs, reducing the length of CoT sequences by an average of 31% to
43% while improving accuracy by 1.7% to 5.7%.

</details>

### [27] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
*Cheng Wen,Tingwei Guo,Shuaijiang Zhao,Wei Zou,Xiangang Li*

Main category: cs.CL

TLDR: 论文通过强化学习（RL）提升大型音频-语言模型（LALM）的推理能力，提出结构化音频推理模型SARI，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在音频-语言推理中的迁移效果，填补现有研究的空白。

Method: 采用两阶段训练：监督微调（SFT）和课程引导的GRPO强化学习，比较结构化与非结构化推理。

Result: SARI模型在基准模型上平均准确率提升16.35%，并在MMAU测试中达到67.08%的SOTA性能。

Conclusion: 结构化推理和课程学习显著增强音频-语言理解能力。

Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.

</details>

### [28] [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
*Fanny Jourdan,Yannick Chevalier,Cécile Favre*

Main category: cs.CL

TLDR: FairTranslate是一个新数据集，用于评估LLM在英语到法语翻译中处理非二元性别偏见的能力，结果显示现有模型存在显著偏见。


<details>
  <summary>Details</summary>
Motivation: LLM在翻译包容性语言（如使用单数'they'代词）时表现不佳，需系统评估其公平性。

Method: 创建FairTranslate数据集（2418句对），评估四个LLM在不同提示下的表现。

Result: LLM在性别表示上存在显著偏见，公平翻译仍具挑战性。

Conclusion: 需针对性策略确保LLM翻译系统的公平性，数据集和代码已公开。

Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks
but often fall short when translating inclusive language -- such as texts
containing the singular 'they' pronoun or otherwise reflecting fair linguistic
protocols. Because these challenges span both computational and societal
domains, it is imperative to critically evaluate how well LLMs handle inclusive
translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset
designed to evaluate non-binary gender biases in machine translation systems
from English to French. FairTranslate includes 2418 English-French sentence
pairs related to occupations, annotated with rich metadata such as the
stereotypical alignment of the occupation, grammatical gender indicator
ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,
Llama3.3-70B) on this dataset under different prompting procedures. Our results
reveal substantial biases in gender representation across LLMs, highlighting
persistent challenges in achieving equitable outcomes in machine translation.
These findings underscore the need for focused strategies and interventions
aimed at ensuring fair and inclusive language usage in LLM-based translation
systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and
disclose the code for all experiments on GitHub.

</details>

### [29] [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
*Shang Wang*

Main category: cs.CL

TLDR: 提出了一种名为W-PCA的新型零样本神经架构搜索方法，专注于轻量级语言模型，通过参数计数和主成分分析优化评估效率。


<details>
  <summary>Details</summary>
Motivation: 现有零样本NAS方法存在评估指标偏差和计算效率低的问题，需要一种更高效的方法来设计和评估轻量级语言模型。

Method: 采用W-PCA方法，结合参数计数和FFN层主成分分析作为评估代理，无需梯度计算，显著提升效率。

Result: 在GLUE和SQuAD数据集上验证，W-PCA显著减少训练时间，测试分数优于现有方法；在FlexiBERT搜索空间上排名相关性更高，解决时间更短。

Conclusion: W-PCA是一种高效且准确的零样本NAS方法，适用于轻量级语言模型的设计与评估。

Abstract: The demand for efficient natural language processing (NLP) systems has led to
the development of lightweight language models. Previous work in this area has
primarily focused on manual design or training-based neural architecture search
(NAS) methods. Recently, zero-shot NAS methods have been proposed for
evaluating language models without the need for training. However, prevailing
approaches to zero-shot NAS often face challenges such as biased evaluation
metrics and computational inefficiencies. In this paper, we introduce
weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored
for lightweight language models. Our approach utilizes two evaluation proxies:
the parameter count and the number of principal components with cumulative
contribution exceeding $\eta$ in the feed-forward neural (FFN) layer.
Additionally, by eliminating the need for gradient computations, we optimize
the evaluation time, thus enhancing the efficiency of designing and evaluating
lightweight language models. We conduct a comparative analysis on the GLUE and
SQuAD datasets to evaluate our approach. The results demonstrate that our
method significantly reduces training time compared to one-shot NAS methods and
achieves higher scores in the testing phase compared to previous
state-of-the-art training-based methods. Furthermore, we perform ranking
evaluations on a dataset sampled from the FlexiBERT search space. Our approach
exhibits superior ranking correlation and further reduces solving time compared
to other zero-shot NAS methods that require gradient computation.

</details>

### [30] [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
*Zhenkai Qin,Dongze Wu,Yuxin Liu,Guifang Yang*

Main category: cs.CL

TLDR: 论文提出了一种基于提示增强的神经网络框架MS-FSLHate，用于少样本仇恨言论检测，结合了可学习提示嵌入、CNN-BiLSTM主干网络和对抗数据增强，在性能上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上仇恨言论的泛滥对在线社区构成威胁，现有深度学习模型在少样本或低资源场景下性能下降，需要更有效的检测系统。

Method: 提出MS-FSLHate框架，整合可学习提示嵌入、CNN-BiLSTM主干网络与注意力池化，以及基于同义词的对抗数据增强。

Result: 在两个基准数据集（HateXplain和HSOL）上，模型在精确率、召回率和F1分数上优于基线方法，且具有高效性和可扩展性。

Conclusion: 结合提示学习和对抗增强的方法在少样本仇恨言论检测中表现出鲁棒性和适应性，适合资源受限环境部署。

Abstract: The proliferation of hate speech on social media poses a significant threat
to online communities, requiring effective detection systems. While deep
learning models have shown promise, their performance often deteriorates in
few-shot or low-resource settings due to reliance on large annotated corpora.
To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for
few-shot hate speech detection implemented on the MindSpore deep learning
platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM
backbone with attention pooling, and synonym-based adversarial data
augmentation to improve generalization. Experimental results on two benchmark
datasets-HateXplain and HSOL-demonstrate that our approach outperforms
competitive baselines in precision, recall, and F1-score. Additionally, the
framework shows high efficiency and scalability, suggesting its suitability for
deployment in resource-constrained environments. These findings highlight the
potential of combining prompt-based learning with adversarial augmentation for
robust and adaptable hate speech detection in few-shot scenarios.

</details>

### [31] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
*Tom Zehle,Moritz Schlager,Timo Heiß,Matthias Feurer*

Main category: cs.CL

TLDR: CAPO是一种基于AutoML技术的成本感知提示优化算法，通过进化方法和多目标优化提升效率，减少LLM调用次数和输入标记，在多数情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的性能对提示词高度敏感，但现有提示优化方法成本高昂，需要大量LLM调用和输入标记。

Method: CAPO结合进化算法和AutoML技术，利用LLM作为操作符，通过竞赛机制减少评估次数，多目标优化平衡性能和提示长度。

Result: 在11/15的案例中，CAPO优于现有方法，性能提升高达21%，且在小预算下表现更好，提示长度更短。

Conclusion: CAPO通过提高成本效率，使提示优化更强大和易用，是迈向高效提示优化的重要一步。

Abstract: Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

</details>

### [32] [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
*Igor Rozhkov,Natalia Loukachevitch*

Main category: cs.CL

TLDR: 本文介绍了在RuTermEval竞赛中应用Binder模型提取嵌套术语的研究，取得了最佳成绩，并探索了从非嵌套标注数据中提取嵌套术语的新任务。


<details>
  <summary>Details</summary>
Motivation: 研究目的是验证Binder模型在提取嵌套术语中的有效性，并探索从非嵌套标注数据中提取嵌套术语的可行性。

Method: 采用Binder模型，该模型此前在嵌套命名实体识别中表现优异，应用于嵌套术语提取任务。

Result: 在RuTermEval竞赛的三个赛道中均取得了最佳术语识别成绩。

Conclusion: 提出的方法能够有效提取嵌套术语，且无需嵌套标注数据。

Abstract: In this paper, we describe our participation in the RuTermEval competition
devoted to extracting nested terms. We apply the Binder model, which was
previously successfully applied to the recognition of nested named entities, to
extract nested terms. We obtained the best results of term recognition in all
three tracks of the RuTermEval competition. In addition, we study the new task
of recognition of nested terms from flat training data annotated with terms
without nestedness. We can conclude that several approaches we proposed in this
work are viable enough to retrieve nested terms effectively without nested
labeling of them.

</details>

### [33] [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
*Jingyu Zhang,Jiacan Yu,Marc Marone,Benjamin Van Durme,Daniel Khashabi*

Main category: cs.CL

TLDR: 论文提出了一种名为BloomScrub的轻量级推理时方法，用于解决大语言模型在预训练中接触受版权保护内容后可能导致的侵权风险。该方法通过结合引用检测和重写技术，提供可扩展的版权筛查，并在必要时选择不响应，从而降低侵权风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预训练阶段可能接触受版权保护的内容，导致部署后无意中生成侵权内容。现有方法对平均风险有效，但忽略了最坏情况下的侵权风险（如长段直接引用）。

Method: 提出BloomScrub方法，结合引用检测和重写技术，利用高效的数据草图（Bloom过滤器）进行可扩展的版权筛查。对于无法移除的长引用，系统可选择不响应。

Result: 实验表明，BloomScrub能有效降低侵权风险，同时保持模型实用性，并适应不同严格程度的执行要求。

Conclusion: 轻量级的推理时方法（如BloomScrub）对版权预防具有显著效果。

Abstract: The exposure of large language models (LLMs) to copyrighted material during
pre-training raises concerns about unintentional copyright infringement post
deployment. This has driven the development of "copyright takedown" methods,
post-training approaches aimed at preventing models from generating content
substantially similar to copyrighted ones. While current mitigation approaches
are somewhat effective for average-case risks, we demonstrate that they
overlook worst-case copyright risks exhibits by the existence of long, verbatim
quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet
highly effective inference-time approach that provides certified copyright
takedown. Our method repeatedly interleaves quote detection with rewriting
techniques to transform potentially infringing segments. By leveraging
efficient data sketches (Bloom filters), our approach enables scalable
copyright screening even for large-scale real-world corpora. When quotes beyond
a length threshold cannot be removed, the system can abstain from responding,
offering certified risk reduction. Experimental results show that BloomScrub
reduces infringement risk, preserves utility, and accommodates different levels
of enforcement stringency with adaptive abstention. Our results suggest that
lightweight, inference-time methods can be surprisingly effective for copyright
prevention.

</details>

### [34] [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
*Zhifan Ye,Kejing Xia,Yonggan Fu,Xin Dong,Jihoon Hong,Xiangchi Yuan,Shizhe Diao,Jan Kautz,Pavlo Molchanov,Yingyan Celine Lin*

Main category: cs.CL

TLDR: LongMamba是一种无需训练的技术，通过识别和过滤关键令牌，显著提升Mamba模型的长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型（SSMs）如Mamba在长上下文处理中效率高，但其性能仍不及Transformer。LongMamba旨在解决这一不足，实现高效且准确的长上下文理解。

Method: LongMamba通过将Mamba的隐藏通道分为局部和全局通道，并识别全局通道中的关键令牌，过滤不重要令牌，以缓解隐藏状态记忆衰减问题。

Result: 在合成和真实长上下文场景的广泛测试中，LongMamba显著提升了Mamba的长上下文性能，无需额外训练。

Conclusion: LongMamba为Mamba模型的长上下文性能设定了新标准，扩展了其操作范围，且无需额外训练。

Abstract: State space models (SSMs) have emerged as an efficient alternative to
Transformer models for language modeling, offering linear computational
complexity and constant memory usage as context length increases. However,
despite their efficiency in handling long contexts, recent studies have shown
that SSMs, such as Mamba models, generally underperform compared to
Transformers in long-context understanding tasks. To address this significant
shortfall and achieve both efficient and accurate long-context understanding,
we propose LongMamba, a training-free technique that significantly enhances the
long-context capabilities of Mamba models. LongMamba builds on our discovery
that the hidden channels in Mamba can be categorized into local and global
channels based on their receptive field lengths, with global channels primarily
responsible for long-context capability. These global channels can become the
key bottleneck as the input context lengthens. Specifically, when input lengths
largely exceed the training sequence length, global channels exhibit
limitations in adaptively extend their receptive fields, leading to Mamba's
poor long-context performance. The key idea of LongMamba is to mitigate the
hidden state memory decay in these global channels by preventing the
accumulation of unimportant tokens in their memory. This is achieved by first
identifying critical tokens in the global channels and then applying token
filtering to accumulate only those critical tokens. Through extensive
benchmarking across synthetic and real-world long-context scenarios, LongMamba
sets a new standard for Mamba's long-context performance, significantly
extending its operational range without requiring additional training. Our code
is available at https://github.com/GATECH-EIC/LongMamba.

</details>

### [35] [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
*Daniel Hendriks,Philipp Spitzer,Niklas Kühl,Gerhard Satzger*

Main category: cs.CL

TLDR: 论文探讨了知识蒸馏在大型语言模型（LLMs）中的应用，提出新的蒸馏方法并比较其性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在资源受限环境中的部署问题，通过知识蒸馏训练小型学生模型。

Method: 应用critique-revision prompting生成训练数据，并综合现有方法训练学生模型。

Result: 在Commonsense Question-Answering数据集上系统比较了性能和可解释性。

Conclusion: 新蒸馏方法及其比较推动了小型语言模型的蒸馏技术，有助于LLM技术的广泛应用。

Abstract: Artificial Intelligence (AI) has increasingly influenced modern society,
recently in particular through significant advancements in Large Language
Models (LLMs). However, high computational and storage demands of LLMs still
limit their deployment in resource-constrained environments. Knowledge
distillation addresses this challenge by training a small student model from a
larger teacher model. Previous research has introduced several distillation
methods for both generating training data and for training the student model.
Despite their relevance, the effects of state-of-the-art distillation methods
on model performance and explainability have not been thoroughly investigated
and compared. In this work, we enlarge the set of available methods by applying
critique-revision prompting to distillation for data generation and by
synthesizing existing methods for training. For these methods, we provide a
systematic comparison based on the widely used Commonsense Question-Answering
(CQA) dataset. While we measure performance via student model accuracy, we
employ a human-grounded study to evaluate explainability. We contribute new
distillation methods and their comparison in terms of both performance and
explainability. This should further advance the distillation of small language
models and, thus, contribute to broader applicability and faster diffusion of
LLM technology.

</details>

### [36] [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
*Ziqiao Ma,Jing Ding,Xuejun Zhang,Dezhi Luo,Jiahe Ding,Sihan Xu,Yuchen Huang,Run Peng,Joyce Chai*

Main category: cs.CL

TLDR: 论文从语用学角度重新审视指代表达生成（REG），提出新数据集RefOI，并揭示当前视觉语言模型在语用能力上的三大失败。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）的评估常忽视语用维度，将REG简化为基于区域的描述任务，忽略了Gricean准则。

Method: 引入包含1.5k图像的RefOI数据集，系统评估现有VLMs的语用能力。

Result: 发现三大语用能力失败：未能唯一识别指代对象、包含冗余信息、与人类语用偏好不一致。自动评估无法捕捉这些语用问题。

Conclusion: 呼吁开发更符合人类实际交流的语用模型和评估框架。

Abstract: Referring Expression Generation (REG) is a core task for evaluating the
pragmatic competence of vision-language systems, requiring not only accurate
semantic grounding but also adherence to principles of cooperative
communication (Grice, 1975). However, current evaluations of vision-language
models (VLMs) often overlook the pragmatic dimension, reducing REG to a
region-based captioning task and neglecting Gricean maxims. In this work, we
revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of
1.5k images annotated with both written and spoken referring expressions.
Through a systematic evaluation of state-of-the-art VLMs, we identify three key
failures of pragmatic competence: (1) failure to uniquely identify the
referent, (2) inclusion of excessive or irrelevant information, and (3)
misalignment with human pragmatic preference, such as the underuse of minimal
spatial cues. We also show that standard automatic evaluations fail to capture
these pragmatic violations, reinforcing superficial cues rather than genuine
referential success. Our findings call for a renewed focus on pragmatically
informed models and evaluation frameworks that align with real human
communication.

</details>

### [37] [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
*A. Fronzetti Colladon,R. Vestrelli*

Main category: cs.CL

TLDR: 本文提出了一种利用GDELT数据集低成本获取全文新闻文章的方法，解决了现有新闻数据集的访问限制问题。


<details>
  <summary>Details</summary>
Motivation: 新闻数据在多个学科中至关重要，但现有数据源成本高或数据不完整，限制了研究。

Method: 利用GDELT Web News NGrams 3.0数据集，通过Python代码从n-grams重建全文新闻文章。

Result: 该方法实现了低成本获取大规模结构化新闻数据，适用于文本分析。

Conclusion: 该方法提升了新闻数据的可访问性，支持经济预测、计算社会科学和自然语言处理等应用。

Abstract: News data have become an essential resource across various disciplines,
including economics, finance, management, social sciences, and computer
science. Researchers leverage newspaper articles to study economic trends,
market dynamics, corporate strategies, public perception, political discourse,
and the evolution of public opinion. Additionally, news datasets have been
instrumental in training large-scale language models, with applications in
sentiment analysis, fake news detection, and automated news summarization.
Despite their significance, access to comprehensive news corpora remains a key
challenge. Many full-text news providers, such as Factiva and LexisNexis,
require costly subscriptions, while free alternatives often suffer from
incomplete data and transparency issues. This paper presents a novel approach
to obtaining full-text newspaper articles at near-zero cost by leveraging data
from the Global Database of Events, Language, and Tone (GDELT). Specifically,
we focus on the GDELT Web News NGrams 3.0 dataset, which provides
high-frequency updates of n-grams extracted from global online news sources. We
provide Python code to reconstruct full-text articles from these n-grams by
identifying overlapping textual fragments and intelligently merging them. Our
method enables researchers to access structured, large-scale newspaper data for
text analysis while overcoming the limitations of existing proprietary
datasets. The proposed approach enhances the accessibility of news data for
empirical research, facilitating applications in economic forecasting,
computational social science, and natural language processing.

</details>

### [38] [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
*Zhiyuan Hu,Shiyun Xiong,Yifan Zhang,See-Kiong Ng,Anh Tuan Luu,Bo An,Shuicheng Yan,Bryan Hooi*

Main category: cs.CL

TLDR: 提出了一种通过过程监督和奖励模型引导视觉语言模型（VLM）在GUI导航任务中优化动作的方法，显著提升了静态和动态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在复杂GUI任务中表现不足，且商业模型为黑箱，开源模型微调成本高，现有评估方法存在延迟反馈和局部优化问题。

Method: 在推理时通过奖励模型对VLM代理进行过程监督，优化每一步动作，并结合轨迹反思和重试机制。

Result: 静态环境中单步动作准确率提升3.4%，动态环境中任务成功率提升约33%，进一步整合机制后效果更显著。

Conclusion: 该方法有效解决了GUI任务中的动作优化问题，显著提升了模型性能。

Abstract: Recent advancements in visual language models (VLMs) have notably enhanced
their capabilities in handling complex Graphical User Interface (GUI)
interaction tasks. Despite these improvements, current frameworks often
struggle to generate correct actions in challenging GUI environments.
State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source
VLMs for GUI tasks requires significant resources. Additionally, existing
trajectory-level evaluation and refinement techniques frequently fall short due
to delayed feedback and local optimization issues. To address these challenges,
we propose an approach that guides VLM agents with process supervision by a
reward model during GUI navigation and control at inference time. This guidance
allows the VLM agent to optimize actions at each inference step, thereby
improving performance in both static and dynamic environments. In particular,
our method demonstrates significant performance gains in three GUI navigation
tasks, achieving a 3.4% improvement in single step action accuracy for static
environments, along with a around 33% increase in task success rate in one
dynamic environment. With further integration of trajectory reflection and
retry mechanisms, we also demonstrate even greater enhancement in task success.

</details>

### [39] [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
*Shi Qiu,Shaoyang Guo,Zhuo-Yang Song,Yunbo Sun,Zeyu Cai,Jiashen Wei,Tianyu Luo,Yixuan Yin,Haoxu Zhang,Yi Hu,Chenyang Wang,Chencheng Tang,Haoling Chang,Qi Liu,Ziheng Zhou,Tianyu Zhang,Jingtian Zhang,Zhangyi Liu,Minghao Li,Yuku Zhang,Boxuan Jing,Xianqi Yin,Yutong Ren,Zizhuo Fu,Weike Wang,Xudong Tian,Anqi Lv,Laifu Man,Jianxiang Li,Feiyu Tao,Qihua Sun,Zhou Liang,Yushu Mu,Zhongxuan Li,Jing-Jun Zhang,Shutao Zhang,Xiaotian Li,Xingqi Xia,Jiawei Lin,Zheyu Shen,Jiahang Chen,Qiuhao Xiong,Binran Wang,Fengyuan Wang,Ziyang Ni,Bohan Zhang,Fan Cui,Changkun Shao,Qing-Hong Cao,Ming-xing Luo,Muhan Zhang,Hua Xing Zhu*

Main category: cs.CL

TLDR: PHYBench是一个用于评估大语言模型在物理场景中推理能力的高质量基准，包含500个精心设计的物理问题，并提出新的评估指标EED Score。实验表明，当前最优模型在复杂物理推理上仍显著落后于人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在物理推理任务上的表现尚不明确，需要一种高质量基准来评估其能力，并揭示其与人类专家的差距。

Method: 构建PHYBench基准，包含500个基于真实物理场景的问题，覆盖多个物理领域和难度级别，并提出EED Score作为评估指标。

Result: 实验显示，即使是最先进的推理模型，在复杂物理推理任务上也显著落后于人类专家。

Conclusion: PHYBench揭示了当前大语言模型在物理推理上的局限性，为未来改进提供了方向，基准和数据集已公开。

Abstract: We introduce PHYBench, a novel, high-quality benchmark designed for
evaluating reasoning capabilities of large language models (LLMs) in physical
contexts. PHYBench consists of 500 meticulously curated physics problems based
on real-world physical scenarios, designed to assess the ability of models to
understand and reason about realistic physical processes. Covering mechanics,
electromagnetism, thermodynamics, optics, modern physics, and advanced physics,
the benchmark spans difficulty levels from high school exercises to
undergraduate problems and Physics Olympiad challenges. Additionally, we
propose the Expression Edit Distance (EED) Score, a novel evaluation metric
based on the edit distance between mathematical expressions, which effectively
captures differences in model reasoning processes and results beyond
traditional binary scoring methods. We evaluate various LLMs on PHYBench and
compare their performance with human experts. Our results reveal that even
state-of-the-art reasoning models significantly lag behind human experts,
highlighting their limitations and the need for improvement in complex physical
reasoning scenarios. Our benchmark results and dataset are publicly available
at https://phybench-official.github.io/phybench-demo/.

</details>

### [40] [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
*Yuxin Zuo,Kaiyan Zhang,Shang Qu,Li Sheng,Xuekai Zhu,Biqing Qi,Youbang Sun,Ganqu Cui,Ning Ding,Bowen Zhou*

Main category: cs.CL

TLDR: 论文提出了一种名为TTRL的新方法，利用无标签数据通过强化学习训练大语言模型，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究在无标签数据上使用强化学习的挑战，尤其是推理时的奖励估计问题。

Method: 提出TTRL方法，利用预训练模型的先验知识，通过多数投票等测试时缩放技术生成有效奖励信号。

Result: TTRL在多项任务中表现优异，如Qwen-2.5-Math-7B在AIME 2024上的pass@1性能提升约159%。

Conclusion: TTRL在无标签数据上表现优异，接近有标签数据的模型性能，具有广泛的应用潜力。

Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model, and
approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks, and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation](https://arxiv.org/abs/2504.15309)
*Anran Yu,Wei Feng,Yaochen Zhang,Xiang Li,Lei Meng,Lei Wu,Xiangxu Meng*

Main category: cs.CV

TLDR: 提出了一种结合风格优化和内容保留策略的个性化文本到图像生成方法，解决了现有方法在风格化和内容准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于标识符微调的方法在风格化和文本控制性上表现不佳，需要改进。

Method: 采用风格优化策略（利用视觉推理提示和参考图像优化风格嵌入）和内容保留策略（保持模型泛化能力）。

Result: 实验证明该方法能生成更一致且个性化的文本到图像输出。

Conclusion: 提出的策略有效提升了风格化和内容准确性，同时增强了文本控制性。

Abstract: The personalized text-to-image generation has rapidly advanced with the
emergence of Stable Diffusion. Existing methods, which typically fine-tune
models using embedded identifiers, often struggle with insufficient stylization
and inaccurate image content due to reduced textual controllability. In this
paper, we propose style refinement and content preservation strategies. The
style refinement strategy leverages the semantic information of visual
reasoning prompts and reference images to optimize style embeddings, allowing a
more precise and consistent representation of style information. The content
preservation strategy addresses the content bias problem by preserving the
model's generalization capabilities, ensuring enhanced textual controllability
without compromising stylization. Experimental results verify that our approach
achieves superior performance in generating consistent and personalized
text-to-image outputs.

</details>

### [42] [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
*Yuan-Hong Liao,Sven Elflein,Liu He,Laura Leal-Taixé,Yejin Choi,Sanja Fidler,David Acuna*

Main category: cs.CV

TLDR: 论文提出LongPerceptualThoughts数据集，通过三阶段合成框架生成长思维链，显著提升视觉和文本推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探索长思维链在感知任务中的作用，弥补现有模型在系统2推理中的不足。

Method: 提出三阶段数据合成框架：1) 从密集图像描述生成可验证选择题；2) 从视觉语言模型提取简单思维链；3) 通过前沿推理模型扩展为长思维链。

Result: 在7B模型上，视觉推理任务平均提升3.4分，V$^*$ Bench提升11.8分，文本推理任务MMLU-Pro提升2分。

Conclusion: 长思维链对感知任务有效，且能泛化到文本推理，为多模态推理提供新思路。

Abstract: Recent reasoning models through test-time scaling have demonstrated that long
chain-of-thoughts can unlock substantial performance boosts in hard reasoning
tasks such as math and code. However, the benefit of such long thoughts for
system-2 reasoning is relatively less explored in other domains such as
perceptual tasks where shallower, system-1 reasoning seems sufficient. In this
paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K
long-thought traces for perceptual tasks. The key challenges in synthesizing
elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models
are not yet equipped with such thinking behavior and that it is not
straightforward to build a reliable process verifier for perceptual tasks.
Thus, we propose a novel three-stage data synthesis framework that first
synthesizes verifiable multiple-choice questions from dense image descriptions,
then extracts simple CoTs from VLMs for those verifiable problems, and finally
expands those simple thoughts to elaborate long thoughts via frontier reasoning
models. In controlled experiments with a strong instruction-tuned 7B model, we
demonstrate notable improvements over existing visual reasoning data-generation
methods. Our model, trained on the generated dataset, achieves an average +3.4
points improvement over 5 vision-centric benchmarks, including +11.8 points on
V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves
performance on the text reasoning benchmark, MMLU-Pro, by +2 points.

</details>

### [43] [Event2Vec: Processing neuromorphic events directly by representations in vector space](https://arxiv.org/abs/2504.15371)
*Wei Fang,Priyadarshini Panda*

Main category: cs.CV

TLDR: 论文提出了一种名为event2vec的事件表示方法，将事件数据转换为向量形式，解决了事件相机数据与传统计算机视觉方法不兼容的问题，并在ASL-DVS数据集上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据具有异步、稀疏和不规则的特点，与传统计算机视觉方法不兼容，现有方法存在预处理复杂、损失时间分辨率或并行计算不兼容的问题。

Method: 受word2vec启发，提出event2vec方法，将事件数据转换为向量表示。

Result: 在ASL-DVS数据集分类任务中，event2vec在参数效率、准确性和速度上优于之前的图/图像/体素表示方法。

Conclusion: event2vec不仅性能优越，还能将事件数据与自然语言处理领域对齐，为事件数据融入大语言和多模态模型提供了可能。

Abstract: The neuromorphic event cameras have overwhelming advantages in temporal
resolution, power efficiency, and dynamic range compared to traditional
cameras. However, the event cameras output asynchronous, sparse, and irregular
events, which are not compatible with mainstream computer vision and deep
learning methods. Various methods have been proposed to solve this issue but at
the cost of long preprocessing procedures, losing temporal resolutions, or
being incompatible with massively parallel computation. Inspired by the great
success of the word to vector, we summarize the similarities between words and
events, then propose the first event to vector (event2vec) representation. We
validate event2vec on classifying the ASL-DVS dataset, showing impressive
parameter efficiency, accuracy, and speed than previous graph/image/voxel-based
representations. Beyond task performance, the most attractive advantage of
event2vec is that it aligns events to the domain of natural language
processing, showing the promising prospect of integrating events into large
language and multimodal models. Our codes, models, and training logs are
available at https://github.com/fangwei123456/event2vec.

</details>

### [44] [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
*Zhiqiu Lin,Siyuan Cen,Daniel Jiang,Jay Karhade,Hewei Wang,Chancharik Mitra,Tiffany Ling,Yuhan Huang,Sifan Liu,Mingyu Chen,Rushikesh Zawar,Xue Bai,Yilun Du,Chuang Gan,Deva Ramanan*

Main category: cs.CV

TLDR: CameraBench是一个用于评估和改进相机运动理解的大规模数据集和基准测试，包含约3000个多样化视频，标注了相机运动基元，并揭示了专家与新手在标注上的差异。


<details>
  <summary>Details</summary>
Motivation: 当前对相机运动的理解缺乏标准化的评估工具和数据集，CameraBench旨在填补这一空白，推动相机运动理解的进一步发展。

Method: 通过专家标注和多阶段质量控制构建数据集，设计相机运动基元分类法，并进行大规模人类研究评估标注性能。

Result: 发现SfM模型难以捕捉依赖场景内容的语义基元，而VLMs难以捕捉几何基元；通过微调生成式VLM，实现了两者的优势结合。

Conclusion: CameraBench为相机运动理解提供了标准化工具，未来有望推动该领域的进一步发展。

Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to
assess and improve camera motion understanding. CameraBench consists of ~3,000
diverse internet videos, annotated by experts through a rigorous multi-stage
quality control process. One of our contributions is a taxonomy of camera
motion primitives, designed in collaboration with cinematographers. We find,
for example, that some motions like "follow" (or tracking) require
understanding scene content like moving subjects. We conduct a large-scale
human study to quantify human annotation performance, revealing that domain
expertise and tutorial-based training can significantly enhance accuracy. For
example, a novice may confuse zoom-in (a change of intrinsics) with translating
forward (a change of extrinsics), but can be trained to differentiate the two.
Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language
Models (VLMs), finding that SfM models struggle to capture semantic primitives
that depend on scene content, while VLMs struggle to capture geometric
primitives that require precise estimation of trajectories. We then fine-tune a
generative VLM on CameraBench to achieve the best of both worlds and showcase
its applications, including motion-augmented captioning, video question
answering, and video-text retrieval. We hope our taxonomy, benchmark, and
tutorials will drive future efforts towards the ultimate goal of understanding
camera motions in any video.

</details>

### [45] [Physics Driven Image Simulation from Commercial Satellite Imagery](https://arxiv.org/abs/2504.15378)
*Scott Sorensen,Wayne Treible,Robert Wagner,Andrew D. Gilliam,Todd Rovito,Joseph L. Mundy*

Main category: cs.CV

TLDR: 利用卫星图像自动生成物理真实的3D场景，无需激光雷达，提高模拟效率和保真度。


<details>
  <summary>Details</summary>
Motivation: 通过物理驱动的图像模拟，超越传统渲染管道的限制，自动生成真实场景，减少人工干预。

Method: 基于数字表面模型（DSM）构建场景几何，利用卫星图像估计材质并填充动态元素（如植被、车辆）。

Result: 实现了无需激光雷达的高保真场景模拟，适用于从紫外到长波红外的多种波段。

Conclusion: 该方法显著提升了场景构建的自动化程度和保真度，适用于算法开发和图像处理。

Abstract: Physics driven image simulation allows for the modeling and creation of
realistic imagery beyond what is afforded by typical rendering pipelines. We
aim to automatically generate a physically realistic scene for simulation of a
given region using satellite imagery to model the scene geometry, drive
material estimates, and populate the scene with dynamic elements. We present
automated techniques to utilize satellite imagery throughout the simulated
scene to expedite scene construction and decrease manual overhead. Our
technique does not use lidar, enabling simulations that could not be
constructed previously. To develop a 3D scene, we model the various components
of the real location, addressing the terrain, modelling man-made structures,
and populating the scene with smaller elements such as vegetation and vehicles.
To create the scene we begin with a Digital Surface Model, which serves as the
basis for scene geometry, and allows us to reason about the real location in a
common 3D frame of reference. These simulated scenes can provide increased
fidelity with less manual intervention for novel locations on earth, and can
facilitate algorithm development, and processing pipelines for imagery ranging
from UV to LWIR $(200nm-20\mu m)$.

</details>

### [46] [Plug-and-Play Versatile Compressed Video Enhancement](https://arxiv.org/abs/2504.15380)
*Huimin Zeng,Jiacheng Li,Zhiwei Xiong*

Main category: cs.CV

TLDR: 提出了一种基于编解码信息的视频增强框架，通过自适应增强压缩视频，提升下游视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 视频压缩虽减少文件大小，但会降低视觉质量，影响下游视觉模型的鲁棒性。

Method: 框架包含压缩感知适应网络（CAA）和比特流感知增强网络（BAE），利用编解码信息自适应增强视频。

Result: 实验表明该框架在质量增强和多任务辅助上优于现有方法。

Conclusion: 该框架作为即插即用模块，能有效提升压缩视频的质量和下游任务性能。

Abstract: As a widely adopted technique in data transmission, video compression
effectively reduces the size of files, making it possible for real-time cloud
computing. However, it comes at the cost of visual quality, posing challenges
to the robustness of downstream vision models. In this work, we present a
versatile codec-aware enhancement framework that reuses codec information to
adaptively enhance videos under different compression settings, assisting
various downstream vision tasks without introducing computation bottleneck.
Specifically, the proposed codec-aware framework consists of a
compression-aware adaptation (CAA) network that employs a hierarchical
adaptation mechanism to estimate parameters of the frame-wise enhancement
network, namely the bitstream-aware enhancement (BAE) network. The BAE network
further leverages temporal and spatial priors embedded in the bitstream to
effectively improve the quality of compressed input frames. Extensive
experimental results demonstrate the superior quality enhancement performance
of our framework over existing enhancement methods, as well as its versatility
in assisting multiple downstream tasks on compressed videos as a plug-and-play
module. Code and models are available at
https://huimin-zeng.github.io/PnP-VCVE/.

</details>

### [47] [ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images](https://arxiv.org/abs/2504.15384)
*Chen Zhao,Anjum Shaik,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Qiuying Sha,Hui Shen,Hong-Wen Deng,Weihua Zhou*

Main category: cs.CV

TLDR: 提出了一种基于双能X射线吸收测量（DXA）图像的髋部骨折风险预测方法ICGM-FRAX，通过迭代比较测试图与模板图的相似性来评估风险，实验结果显示高灵敏度。


<details>
  <summary>Details</summary>
Motivation: 髋部骨折对老年人健康影响重大，早期准确识别高风险个体对有效干预至关重要。

Method: 将DXA图像分割为多个感兴趣区域（RoIs），提取放射组学特征并构建图结构，通过迭代交叉图匹配评估风险。

Result: 在547名受试者中，ICGM-FRAX的灵敏度达到0.9869，显示出高预测准确性。

Conclusion: ICGM-FRAX是一种有效的髋部骨折风险预测方法，具有高灵敏度和准确性。

Abstract: Hip fractures represent a major health concern, particularly among the
elderly, often leading decreased mobility and increased mortality. Early and
accurate detection of at risk individuals is crucial for effective
intervention. In this study, we propose Iterative Cross Graph Matching for Hip
Fracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip
fractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX
involves iteratively comparing a test (subject) graph with multiple template
graphs representing the characteristics of hip fracture subjects to assess the
similarity and accurately to predict hip fracture risk. These graphs are
obtained as follows. The DXA images are separated into multiple regions of
interest (RoIs), such as the femoral head, shaft, and lesser trochanter.
Radiomic features are then calculated for each RoI, with the central
coordinates used as nodes in a graph. The connectivity between nodes is
established according to the Euclidean distance between these coordinates. This
process transforms each DXA image into a graph, where each node represents a
RoI, and edges derived by the centroids of RoIs capture the spatial
relationships between them. If the test graph closely matches a set of template
graphs representing subjects with incident hip fractures, it is classified as
indicating high hip fracture risk. We evaluated our method using 547 subjects
from the UK Biobank dataset, and experimental results show that ICGM-FRAX
achieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip
fractures.

</details>

### [48] [MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World](https://arxiv.org/abs/2504.15397)
*Ankit Dhiman,Manan Shah,R Venkatesh Babu*

Main category: cs.CV

TLDR: 本文提出了一种改进扩散模型生成逼真镜像反射的方法，通过合成数据增强和分阶段训练提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成镜像反射时难以完全遵循物理规律，尤其是在物体位置和姿态变化时表现不佳。

Method: 引入合成数据增强（随机位置、旋转和物体配对）和三阶段训练课程，开发MirrorFusion 2.0模型。

Result: 模型在复杂场景中表现出更好的泛化能力，支持了方法的有效性。

Conclusion: 通过数据增强和分阶段训练，显著提升了镜像反射生成的逼真度和泛化能力。

Abstract: Diffusion models have become central to various image editing tasks, yet they
often fail to fully adhere to physical laws, particularly with effects like
shadows, reflections, and occlusions. In this work, we address the challenge of
generating photorealistic mirror reflections using diffusion-based generative
models. Despite extensive training data, existing diffusion models frequently
overlook the nuanced details crucial to authentic mirror reflections. Recent
approaches have attempted to resolve this by creating synhetic datasets and
framing reflection generation as an inpainting task; however, they struggle to
generalize across different object orientations and positions relative to the
mirror. Our method overcomes these limitations by introducing key augmentations
into the synthetic data pipeline: (1) random object positioning, (2) randomized
rotations, and (3) grounding of objects, significantly enhancing generalization
across poses and placements. To further address spatial relationships and
occlusions in scenes with multiple objects, we implement a strategy to pair
objects during dataset generation, resulting in a dataset robust enough to
handle these complex scenarios. Achieving generalization to real-world scenes
remains a challenge, so we introduce a three-stage training curriculum to
develop the MirrorFusion 2.0 model to improve real-world performance. We
provide extensive qualitative and quantitative evaluations to support our
approach. The project page is available at: https://mirror-verse.github.io/.

</details>

### [49] [Context Aware Grounded Teacher for Source Free Object Detection](https://arxiv.org/abs/2504.15404)
*Tajamul Ashraf,Rajes Manna,Partha Sarathi Purkayastha,Tavaheed Tariq,Janibul Bashir*

Main category: cs.CV

TLDR: 论文提出了一种名为Grounded Teacher（GT）的框架，用于解决源数据不可用时的目标域适应问题，特别是在医学影像中，通过关系上下文模块和专家分支来减少上下文偏差和提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，源数据不可用时，传统的半监督师生架构容易因上下文不平衡和领域偏移导致伪标签不准确，进而影响学生模型性能。本文旨在解决这一问题。

Method: 提出Grounded Teacher框架，利用关系上下文模块建模上下文关系，并通过专家分支监督学生模型，以减少偏差并提升性能。

Result: 在三个医学数据集上的实验验证了GT框架在减少上下文偏差和提升模型性能方面的有效性。

Conclusion: GT框架通过关系上下文模块和专家分支，显著减少了上下文偏差，提升了模型在源数据不可用时的适应能力。

Abstract: We focus on the Source Free Object Detection (SFOD) problem, when source data
is unavailable during adaptation, and the model must adapt to the unlabeled
target domain. In medical imaging, several approaches have leveraged a
semi-supervised student-teacher architecture to bridge domain discrepancy.
Context imbalance in labeled training data and significant domain shifts
between domains can lead to biased teacher models that produce inaccurate
pseudolabels, degrading the student model's performance and causing a mode
collapse. Class imbalance, particularly when one class significantly outnumbers
another, leads to contextual bias. To tackle the problem of context bias and
the significant performance drop of the student model in the SFOD setting, we
introduce Grounded Teacher (GT) as a standard framework. In this study, we
model contextual relationships using a dedicated relational context module and
leverage it to mitigate inherent biases in the model. This approach enables us
to apply augmentations to closely related classes, across and within domains,
enhancing the performance of underrepresented classes while keeping the effect
on dominant classes minimal. We further improve the quality of predictions by
implementing an expert foundational branch to supervise the student model. We
validate the effectiveness of our approach in mitigating context bias under the
SFOD setting through experiments on three medical datasets supported by
comprehensive ablation studies. All relevant resources, including preprocessed
data, trained model weights, and code, are publicly available at this
https://github.com/Tajamul21/Grounded_Teacher.

</details>

### [50] [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
*David Ma,Yuanxing Zhang,Jincheng Ren,Jarvis Guo,Yifan Yao,Zhenlin Wei,Zhenzhu Yang,Zhongyuan Peng,Boyu Feng,Jun Ma,Xiao Gu,Zhoufutu Wen,King Zhu,Yancheng He,Meng Cao,Shiwen Ni,Jiaheng Liu,Wenhao Huang,Ge Zhang,Xiaojie Jin*

Main category: cs.CV

TLDR: IV-Bench是一个新的多模态大语言模型（MLLMs）评估基准，专注于图像背景在视频理解中的作用，填补了现有评估框架的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要关注图像推理或通用视频理解任务，忽视了图像背景在视频理解中的重要性。

Method: 提出IV-Bench，包含967个视频和2,585个标注的图像-文本查询，覆盖13个任务和5个类别。评估了开源和闭源MLLMs的性能。

Result: 当前模型在图像背景视频感知和推理任务中表现不佳，最高准确率仅为28.9%。关键影响因素包括推理模式、帧数和分辨率。

Conclusion: IV-Bench揭示了当前模型的局限性，并提供了未来研究的方向，包括数据合成和训练格式对齐的挑战。

Abstract: Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)
primarily focus on image reasoning or general video understanding tasks,
largely overlooking the significant role of image context in video
comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive
benchmark for evaluating Image-Grounded Video Perception and Reasoning.
IV-Bench consists of 967 videos paired with 2,585 meticulously annotated
image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5
representative categories. Extensive evaluations of state-of-the-art
open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,
Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models
substantially underperform in image-grounded video Perception and Reasoning,
merely achieving at most 28.9% accuracy. Further analysis reveals key factors
influencing model performance on IV-Bench, including inference pattern, frame
number, and resolution. Additionally, through a simple data synthesis approach,
we demonstratethe challenges of IV- Bench extend beyond merely aligning the
data format in the training proecss. These findings collectively provide
valuable insights for future research. Our codes and data are released in
https://github.com/multimodal-art-projection/IV-Bench.

</details>

### [51] [Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images](https://arxiv.org/abs/2504.15470)
*Jonathan Brokman,Amit Giloni,Omer Hofman,Roman Vainshtein,Hisashi Kojima,Guy Gilboa*

Main category: cs.CV

TLDR: 该论文提出了一种基于概率流形分析的零样本和少样本图像检测方法，通过量化生成内容的偏向来区分真实与AI生成图像，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本和少样本图像检测方法缺乏理论支持和性能不足的问题。

Method: 利用预训练扩散模型捕获的隐式概率流形偏向来建立检测标准，并通过分数函数分析近似流形的曲率、梯度和偏向。在少样本设置中采用混合专家方法。

Result: 在20种生成模型上的实验表明，该方法在零样本和少样本设置中均优于现有方法。

Conclusion: 通过流形分析，该方法在理论和实践上推动了生成内容偏见的理解与应用。

Abstract: Distinguishing between real and AI-generated images, commonly referred to as
'image detection', presents a timely and significant challenge. Despite
extensive research in the (semi-)supervised regime, zero-shot and few-shot
solutions have only recently emerged as promising alternatives. Their main
advantage is in alleviating the ongoing data maintenance, which quickly becomes
outdated due to advances in generative technologies. We identify two main gaps:
(1) a lack of theoretical grounding for the methods, and (2) significant room
for performance improvements in zero-shot and few-shot regimes. Our approach is
founded on understanding and quantifying the biases inherent in generated
content, where we use these quantities as criteria for characterizing generated
images. Specifically, we explore the biases of the implicit probability
manifold, captured by a pre-trained diffusion model. Through score-function
analysis, we approximate the curvature, gradient, and bias towards points on
the probability manifold, establishing criteria for detection in the zero-shot
regime. We further extend our contribution to the few-shot setting by employing
a mixture-of-experts methodology. Empirical results across 20 generative models
demonstrate that our method outperforms current approaches in both zero-shot
and few-shot settings. This work advances the theoretical understanding and
practical usage of generated content biases through the lens of manifold
analysis.

</details>

### [52] [Emergence and Evolution of Interpretable Concepts in Diffusion Models](https://arxiv.org/abs/2504.15473)
*Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TLDR: 本文利用稀疏自编码器（SAEs）研究文本到图像扩散模型的内在机制，揭示了其激活中的人可解释概念，并展示了如何通过这些概念干预生成过程。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但其内部机制仍不明确。希望通过机械可解释性（MI）技术，如SAEs，揭示其工作原理。

Method: 使用SAEs框架分析流行的文本到图像扩散模型的激活，发现人可解释概念，并设计干预技术控制生成过程。

Result: 发现早期阶段可有效控制图像构图，中期阶段构图固定但风格可干预，后期阶段仅能调整纹理细节。

Conclusion: SAEs能有效揭示扩散模型的内部机制，并可用于干预生成过程，为理解和控制扩散模型提供了新方法。

Abstract: Diffusion models have become the go-to method for text-to-image generation,
producing high-quality images from noise through a process called reverse
diffusion. Understanding the dynamics of the reverse diffusion process is
crucial in steering the generation and achieving high sample quality. However,
the inner workings of diffusion models is still largely a mystery due to their
black-box nature and complex, multi-step generation process. Mechanistic
Interpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at
uncovering the operating principles of models through granular analysis of
their internal representations. These MI techniques have been successful in
understanding and steering the behavior of large language models at scale.
However, the great potential of SAEs has not yet been applied toward gaining
insight into the intricate generative process of diffusion models. In this
work, we leverage the SAE framework to probe the inner workings of a popular
text-to-image diffusion model, and uncover a variety of human-interpretable
concepts in its activations. Interestingly, we find that even before the first
reverse diffusion step is completed, the final composition of the scene can be
predicted surprisingly well by looking at the spatial distribution of activated
concepts. Moreover, going beyond correlational analysis, we show that the
discovered concepts have a causal effect on the model output and can be
leveraged to steer the generative process. We design intervention techniques
aimed at manipulating image composition and style, and demonstrate that (1) in
early stages of diffusion image composition can be effectively controlled, (2)
in the middle stages of diffusion image composition is finalized, however
stylistic interventions are effective, and (3) in the final stages of diffusion
only minor textural details are subject to change.

</details>

### [53] [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
*Atin Pothiraj,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TLDR: 论文提出了一种新任务CAPTURe，用于测试视觉语言模型（VLMs）对遮挡物体的计数和推理能力，发现现有模型在遮挡情况下表现较差，而人类表现优异。


<details>
  <summary>Details</summary>
Motivation: 遮挡物体在现实场景中常见，但现有模型对遮挡物体的理解和推理能力有限，需要新的测试方法。

Method: 设计了CAPTURe任务，包含真实图像（CAPTURe-real）和合成图像（CAPTURe-synthetic）两部分，评估了四种VLMs在遮挡和非遮挡情况下的表现。

Result: 模型在遮挡情况下表现更差，尤其是GPT-4o等强模型也未能有效计数；人类表现优异；提供遮挡物体位置信息可提升模型性能。

Conclusion: VLMs在遮挡推理和计数能力上存在不足，需进一步改进。

Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects
is vital to understanding visual scenes, as occlusions frequently occur in
real-world environments and act as obstacles for spatial comprehension. To test
models' ability to reason about multiple occluded objects, we introduce a novel
task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which
requires a model to count objects arranged in a pattern by inferring how the
pattern continues behind an occluder (an object which blocks parts of the
scene). CAPTURe requires both recognizing visual patterns and reasoning, making
it a useful testbed for evaluating vision-language models (VLMs) on whether
they understand occluded patterns and possess spatial understanding skills. By
requiring models to reason about occluded objects, CAPTURe also tests VLMs'
ability to form world models that would allow them to fill in missing
information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually
filtered images of real objects in patterns and (2) CAPTURe-synthetic, a
controlled diagnostic with generated patterned images. We evaluate four strong
VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models
struggle to count on both occluded and unoccluded patterns. Crucially, we find
that models perform worse with occlusion, suggesting that VLMs are also
deficient in inferring unseen spatial relationships: even the strongest VLMs
like GPT-4o fail to count with occlusion. In contrast, we find that humans
achieve very little error on CAPTURe. We also find that providing auxiliary
information of occluded object locations increases performance, underscoring
that the model error comes both from an inability to handle occlusion as well
as difficulty counting in images.

</details>

### [54] [InstaRevive: One-Step Image Enhancement via Dynamic Score Matching](https://arxiv.org/abs/2504.15513)
*Yixuan Zhu,Haolin Wang,Ao Li,Wenliang Zhao,Yansong Tang,Jingxuan Niu,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TLDR: InstaRevive是一种基于扩散蒸馏的图像增强框架，通过动态控制和文本提示减少采样步骤，提升生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 复杂环境和成像设备限制使得图像增强需求广泛，但现有扩散方法计算成本高。

Method: 采用基于分数的扩散蒸馏和动态控制策略，结合文本提示辅助条件。

Result: 在多种任务和数据集上验证了高效性和高质量结果。

Conclusion: InstaRevive在图像增强中表现出色，兼具高效性和视觉吸引力。

Abstract: Image enhancement finds wide-ranging applications in real-world scenarios due
to complex environments and the inherent limitations of imaging devices. Recent
diffusion-based methods yield promising outcomes but necessitate prolonged and
computationally intensive iterative sampling. In response, we propose
InstaRevive, a straightforward yet powerful image enhancement framework that
employs score-based diffusion distillation to harness potent generative
capability and minimize the sampling steps. To fully exploit the potential of
the pre-trained diffusion model, we devise a practical and effective diffusion
distillation pipeline using dynamic control to address inaccuracies in updating
direction during score matching. Our control strategy enables a dynamic
diffusing scope, facilitating precise learning of denoising trajectories within
the diffusion model and ensuring accurate distribution matching gradients
during training. Additionally, to enrich guidance for the generative power, we
incorporate textual prompts via image captioning as auxiliary conditions,
fostering further exploration of the diffusion model. Extensive experiments
substantiate the efficacy of our framework across a diverse array of
challenging tasks and datasets, unveiling the compelling efficacy and
efficiency of InstaRevive in delivering high-quality and visually appealing
results. Code is available at https://github.com/EternalEvan/InstaRevive.

</details>

### [55] [Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness](https://arxiv.org/abs/2504.15599)
*Shichen Li,Chenhui Shao*

Main category: cs.CV

TLDR: 提出了一种多模态数据融合框架，用于实时预测食品干燥状态，显著提高了预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 食品干燥的实时预测对节能、生产效率和产品质量至关重要，但现有方法因数据有限和动态性难以满足需求。

Method: 采用端到端多模态数据融合框架，结合视频数据和过程参数，使用编码器-解码器架构和基于Transformer的解码器。

Result: 模型在糖饼干干燥实验中平均预测误差仅15秒，优于现有方法65.69%和纯视频模型11.30%。

Conclusion: 该模型在精度、规模和计算效率间取得平衡，适用于工业多模态融合任务，具有广泛扩展性。

Abstract: Food drying is essential for food production, extending shelf life, and
reducing transportation costs. Accurate real-time forecasting of drying
readiness is crucial for minimizing energy consumption, improving productivity,
and ensuring product quality. However, this remains challenging due to the
dynamic nature of drying, limited data availability, and the lack of effective
predictive analytical methods. To address this gap, we propose an end-to-end
multi-modal data fusion framework that integrates in-situ video data with
process parameters for real-time food drying readiness forecasting. Our
approach leverages a new encoder-decoder architecture with modality-specific
encoders and a transformer-based decoder to effectively extract features while
preserving the unique structure of each modality. We apply our approach to
sugar cookie drying, where time-to-ready is predicted at each timestamp.
Experimental results demonstrate that our model achieves an average prediction
error of only 15 seconds, outperforming state-of-the-art data fusion methods by
65.69% and a video-only model by 11.30%. Additionally, our model balances
prediction accuracy, model size, and computational efficiency, making it
well-suited for heterogenous industrial datasets. The proposed model is
extensible to various other industrial modality fusion tasks for online
decision-making.

</details>

### [56] [SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking](https://arxiv.org/abs/2504.15609)
*Yunfeng Li,Bo Wang,Jiahao Wan,Xueyi Wu,Ye Li*

Main category: cs.CV

TLDR: 论文提出了首个大规模水下声学目标跟踪（UAOT）基准SonarT165，并提出了高效框架STFTrack，包含多视角模板融合模块和最优轨迹校正模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 水下能见度不足时，仅声纳系统能提供稳定数据，但缺乏统一评估基准限制了现有方法的实用性。

Method: 提出SonarT165基准，包含165个方形序列和165个扇形序列及205K标注；提出STFTrack框架，含多视角模板融合模块（MTFM）和最优轨迹校正模块（OTCM），并引入声学图像增强和频率增强模块（FEM）。

Result: 实验表明SonarT165揭示了当前SOT跟踪器的局限性，STFTrack在基准上达到最优性能。

Conclusion: SonarT165和STFTrack为水下声学目标跟踪提供了标准化评估和高效解决方案。

Abstract: Underwater observation systems typically integrate optical cameras and
imaging sonar systems. When underwater visibility is insufficient, only sonar
systems can provide stable data, which necessitates exploration of the
underwater acoustic object tracking (UAOT) task. Previous studies have explored
traditional methods and Siamese networks for UAOT. However, the absence of a
unified evaluation benchmark has significantly constrained the value of these
methods. To alleviate this limitation, we propose the first large-scale UAOT
benchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and
205K high-quality annotations. Experimental results demonstrate that SonarT165
reveals limitations in current state-of-the-art SOT trackers. To address these
limitations, we propose STFTrack, an efficient framework for acoustic object
tracking. It includes two novel modules, a multi-view template fusion module
(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module
integrates multi-view feature of both the original image and the binary image
of the dynamic template, and introduces a cross-attention-like layer to fuse
the spatio-temporal target representations. The OTCM module introduces the
acoustic-response-equivalent pixel property and proposes normalized pixel
brightness response scores, thereby suppressing suboptimal matches caused by
inaccurate Kalman filter prediction boxes. To further improve the model
feature, STFTrack introduces a acoustic image enhancement method and a
Frequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive
experiments show the proposed STFTrack achieves state-of-the-art performance on
the proposed benchmark. The code is available at
https://github.com/LiYunfengLYF/SonarT165.

</details>

### [57] [HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15612)
*Hongxing Peng,Kang Lin,Huanai Liu*

Main category: cs.CV

TLDR: 提出了一种基于Mamba架构的HS-Mamba框架，用于高光谱图像分类，结合局部和全局特征，显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像的高维度和特征内联特性对Mamba架构的应用提出了挑战，需要一种新的方法结合局部和全局特征。

Method: HS-Mamba采用双通道空间-光谱编码器（DCSS-encoder）和轻量级全局内联注意力（LGI-Att）分支，分别处理局部和全局特征。

Result: 在四个基准HSI数据集上，HS-Mamba优于现有最先进方法。

Conclusion: HS-Mamba通过融合局部和全局特征，实现了高精度的高光谱图像分类。

Abstract: Hyperspectral image (HSI) classification has been one of the hot topics in
remote sensing fields. Recently, the Mamba architecture based on selective
state-space models (S6) has demonstrated great advantages in long sequence
modeling. However, the unique properties of hyperspectral data, such as high
dimensionality and feature inlining, pose challenges to the application of
Mamba to HSI classification. To compensate for these shortcomings, we propose
an full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts
a strategy different from pixel-patch based or whole-image based, but combines
the advantages of both. The patches cut from the whole image are sent to
multi-groups Mamba, combined with positional information to perceive local
inline features in the spatial and spectral domains, and the whole image is
sent to a lightweight attention module to enhance the global feature
representation ability. Specifically, HS-Mamba consists of a dual-channel
spatial-spectral encoder (DCSS-encoder) module and a lightweight global inline
attention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of
Mamba to decouple and model the local features of dual-channel sequences with
non-overlapping patches. The LGI-Att branch uses a lightweight compressed and
extended attention module to perceive the global features of the spatial and
spectral domains of the unsegmented whole image. By fusing local and global
features, high-precision classification of hyperspectral images is achieved.
Extensive experiments demonstrate the superiority of the proposed HS-Mamba,
outperforming state-of-the-art methods on four benchmark HSI datasets.

</details>

### [58] [AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization](https://arxiv.org/abs/2504.15619)
*Jinda Lu,Jinghan Li,Yuan Gao,Junkang Wu,Jiancan Wu,Xiang Wang,Xiangnan He*

Main category: cs.CV

TLDR: AdaViP通过视觉增强的偏好优化方法，结合视觉和语言偏好，显著提升了多模态大语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注语言偏好，忽视了视觉上下文的重要性，导致模型对视觉细节的敏感性不足。

Method: 提出AdaViP方法，包括基于视觉的偏好对构建和自适应偏好优化，动态平衡视觉与语言偏好。

Result: AdaViP-7B在Object HalBench上显著减少了幻觉现象，响应级和提及级幻觉分别降低了93.7%和96.4%。

Conclusion: AdaViP通过视觉增强和自适应优化，有效提升了多模态大语言模型的偏好对齐能力。

Abstract: Preference alignment through Direct Preference Optimization (DPO) has
demonstrated significant effectiveness in aligning multimodal large language
models (MLLMs) with human preferences. However, existing methods focus
primarily on language preferences while neglecting the critical visual context.
In this paper, we propose an Adaptive Vision-enhanced Preference optimization
(AdaViP) that addresses these limitations through two key innovations: (1)
vision-based preference pair construction, which integrates multiple visual
foundation models to strategically remove key visual elements from the image,
enhancing MLLMs' sensitivity to visual details; and (2) adaptive preference
optimization that dynamically balances vision- and language-based preferences
for more accurate alignment. Extensive evaluations across different benchmarks
demonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%
reductions in response-level and mentioned-level hallucination respectively on
the Object HalBench, significantly outperforming current state-of-the-art
methods.

</details>

### [59] [FaceInsight: A Multimodal Large Language Model for Face Perception](https://arxiv.org/abs/2504.15624)
*Jingzhi Li,Changjiang Luo,Ruoyu Chen,Hua Zhang,Wenqi Ren,Jianhou Gan,Xiaochun Cao*

Main category: cs.CV

TLDR: FaceInsight是一个针对面部感知任务的多模态大语言模型，通过视觉-文本对齐和面部分割图提升了面部信息的细粒度理解，显著优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态大语言模型在面部感知任务中表现不佳，导致不准确或误导性回答，需要专门解决方案。

Method: 引入视觉-文本对齐以建模面部知识的依赖关系，并加入面部分割图作为辅助感知模态，增强语义理解。

Result: 在三种面部感知任务中，FaceInsight在无需训练和微调设置下均优于九种对比模型。

Conclusion: FaceInsight通过结合视觉-文本对齐和辅助模态，有效提升了面部感知任务的性能。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
strong capabilities in understanding general visual content. However, these
general-domain MLLMs perform poorly in face perception tasks, often producing
inaccurate or misleading responses to face-specific queries. To address this
gap, we propose FaceInsight, the versatile face perception MLLM that provides
fine-grained facial information. Our approach introduces visual-textual
alignment of facial knowledge to model both uncertain dependencies and
deterministic relationships among facial information, mitigating the
limitations of language-driven reasoning. Additionally, we incorporate face
segmentation maps as an auxiliary perceptual modality, enriching the visual
input with localized structural cues to enhance semantic understanding.
Comprehensive experiments and analyses across three face perception tasks
demonstrate that FaceInsight consistently outperforms nine compared MLLMs under
both training-free and fine-tuned settings.

</details>

### [60] [ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?](https://arxiv.org/abs/2504.15627)
*Doanh C. Bui,Hoai Luan Pham,Vu Trung Duong Le,Tuan Hai Vu,Van Duy Tran,Yasuhiko Nakashima*

Main category: cs.CV

TLDR: 比较传统持续学习方法与视觉语言零样本分类在WSI终身学习中的效果。


<details>
  <summary>Details</summary>
Motivation: 解决WSI（全切片图像）终身学习中多任务统一模型的训练问题，避免每次新任务都需训练新模型。

Method: 应用正则化和基于排练的方法，并与视觉语言基础模型的零样本分类进行比较。

Result: 首次比较了传统持续学习策略与视觉语言零样本分类在WSI任务中的表现。

Conclusion: 需进一步研究持续学习策略是否优于零样本分类，以提升WSI终身学习性能。

Abstract: Lifelong learning for whole slide images (WSIs) poses the challenge of
training a unified model to perform multiple WSI-related tasks, such as cancer
subtyping and tumor classification, in a distributed, continual fashion. This
is a practical and applicable problem in clinics and hospitals, as WSIs are
large, require storage, processing, and transfer time. Training new models
whenever new tasks are defined is time-consuming. Recent work has applied
regularization- and rehearsal-based methods to this setting. However, the rise
of vision-language foundation models that align diagnostic text with pathology
images raises the question: are these models alone sufficient for lifelong WSI
learning using zero-shot classification, or is further investigation into
continual learning strategies needed to improve performance? To our knowledge,
this is the first study to compare conventional continual-learning approaches
with vision-language zero-shot classification for WSIs. Our source code and
experimental results will be available soon.

</details>

### [61] [AffordanceSAM: Segment Anything Once More in Affordance Grounding](https://arxiv.org/abs/2504.15650)
*Dengyang Jiang,Mengmeng Wang,Teli Ma,Hengzhuang Li,Yong liu,Guang Dai,Lei Zhang*

Main category: cs.CV

TLDR: AffordanceSAM通过扩展SAM的分割能力到功能区域识别，提升了模型对未见物体和功能的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型在功能区域识别上的泛化能力不足，限制了实际应用。

Method: 提出AffordanceSAM，包含功能适配模块和由粗到细的训练策略。

Result: 在AGD20K基准测试中超越现有方法，并能处理新物体和功能。

Conclusion: AffordanceSAM展示了强大的泛化能力，适用于复杂场景。

Abstract: Improving the generalization ability of an affordance grounding model to
recognize regions for unseen objects and affordance functions is crucial for
real-world application. However, current models are still far away from such
standards. To address this problem, we introduce AffordanceSAM, an effective
approach that extends SAM's generalization capacity to the domain of affordance
grounding. For the purpose of thoroughly transferring SAM's robust performance
in segmentation to affordance, we initially propose an affordance-adaption
module in order to help modify SAM's segmentation output to be adapted to the
specific functional regions required for affordance grounding. We concurrently
make a coarse-to-fine training recipe to make SAM first be aware of affordance
objects and actions coarsely, and then be able to generate affordance heatmaps
finely. Both quantitative and qualitative experiments show the strong
generalization capacity of our AffordanceSAM, which not only surpasses previous
methods under AGD20K benchmark but also shows evidence to handle the task with
novel objects and affordance functions.

</details>

### [62] [DiTPainter: Efficient Video Inpainting with Diffusion Transformers](https://arxiv.org/abs/2504.15661)
*Xian Wu,Chang Liu*

Main category: cs.CV

TLDR: DiTPainter是一种基于扩散变换器（DiT）的视频修复模型，通过高效设计的Transformer网络解决现有方法中因光流不准确或大掩码导致的模糊和不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复算法依赖光流传播像素，但面对不准确光流或大掩码时效果不佳；预训练的DiT模型参数过多，难以直接应用于视频修复。

Method: 提出DiTPainter，一种端到端的视频修复模型，采用专为视频修复设计的高效Transformer网络，从头训练而非依赖预训练模型。

Result: 实验表明，DiTPainter在质量和时空一致性上优于现有算法，并能处理任意长度视频，适用于视频去字幕和补全任务。

Conclusion: DiTPainter通过高效Transformer设计解决了现有方法的局限性，为视频修复任务提供了更优解决方案。

Abstract: Many existing video inpainting algorithms utilize optical flows to construct
the corresponding maps and then propagate pixels from adjacent frames to
missing areas by mapping. Despite the effectiveness of the propagation
mechanism, they might encounter blurry and inconsistencies when dealing with
inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)
has emerged as a revolutionary technique for video generation tasks. However,
pretrained DiT models for video generation all contain a large amount of
parameters, which makes it very time consuming to apply to video inpainting
tasks. In this paper, we present DiTPainter, an end-to-end video inpainting
model based on Diffusion Transformer (DiT). DiTPainter uses an efficient
transformer network designed for video inpainting, which is trained from
scratch instead of initializing from any large pretrained models. DiTPainter
can address videos with arbitrary lengths and can be applied to video
decaptioning and video completion tasks with an acceptable time cost.
Experiments show that DiTPainter outperforms existing video inpainting
algorithms with higher quality and better spatial-temporal consistency.

</details>

### [63] [Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection](https://arxiv.org/abs/2504.15665)
*Pei Liu,Yisi Luo,Wenzhen Wang,Xiangyong Cao*

Main category: cs.CV

TLDR: 提出了一种基于运动增强和非局部相似性的隐式神经表示（INR）框架，用于红外弱小目标检测，解决了动态背景和目标丢失问题。


<details>
  <summary>Details</summary>
Motivation: 传统低秩加稀疏模型难以捕捉动态背景和全局时空相关性，导致背景泄漏或目标丢失。

Method: 结合光流进行运动估计，利用非局部相似性构建低秩性质的块张量，并提出基于张量分解的INR模型。

Result: 实验表明，该方法能有效分离弱小目标与复杂背景，检测精度和鲁棒性优于现有方法。

Conclusion: 提出的框架通过神经表示编码非局部低秩性和时空相关性，显著提升了红外弱小目标检测性能。

Abstract: Infrared dim and small target detection presents a significant challenge due
to dynamic multi-frame scenarios and weak target signatures in the infrared
modality. Traditional low-rank plus sparse models often fail to capture dynamic
backgrounds and global spatial-temporal correlations, which results in
background leakage or target loss. In this paper, we propose a novel
motion-enhanced nonlocal similarity implicit neural representation (INR)
framework to address these challenges. We first integrate motion estimation via
optical flow to capture subtle target movements, and propose multi-frame fusion
to enhance motion saliency. Second, we leverage nonlocal similarity to
construct patch tensors with strong low-rank properties, and propose an
innovative tensor decomposition-based INR model to represent the nonlocal patch
tensor, effectively encoding both the nonlocal low-rankness and
spatial-temporal correlations of background through continuous neural
representations. An alternating direction method of multipliers is developed
for the nonlocal INR model, which enjoys theoretical fixed-point convergence.
Experimental results show that our approach robustly separates dim targets from
complex infrared backgrounds, outperforming state-of-the-art methods in
detection accuracy and robustness.

</details>

### [64] [DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining](https://arxiv.org/abs/2504.15669)
*Wei Zhuo,Zhiyue Tang,Wufeng Xue,Hao Ding,Linlin Shen*

Main category: cs.CV

TLDR: FS-DINO是一个基于DINOv2和SAM的统一模型，通过轻量级分割器和跨模型蒸馏实现少样本语义分割。


<details>
  <summary>Details</summary>
Motivation: 解决少样本语义分割中数据稀缺问题，探索如何结合DINOv2和SAM的优势构建统一模型。

Method: 提出FS-DINO，仅使用DINOv2编码器和轻量级分割器，通过瓶颈适配器、元视觉提示生成器和解码器实现分割，并结合SAM知识进行跨模型蒸馏。

Result: 在COCO-20i、PASCAL-5i和FSS-1000数据集上验证了方法的有效性和优越性。

Conclusion: FS-DINO成功整合了DINOv2和SAM的知识，为少样本语义分割提供了高效解决方案。

Abstract: Few-shot semantic segmentation has gained increasing interest due to its
generalization capability, i.e., segmenting pixels of novel classes requiring
only a few annotated images. Prior work has focused on meta-learning for
support-query matching, with extensive development in both prototype-based and
aggregation-based methods. To address data scarcity, recent approaches have
turned to foundation models to enhance representation transferability for novel
class segmentation. Among them, a hybrid dual-modal framework including both
DINOv2 and SAM has garnered attention due to their complementary capabilities.
We wonder "can we build a unified model with knowledge from both foundation
models?" To this end, we propose FS-DINO, with only DINOv2's encoder and a
lightweight segmenter. The segmenter features a bottleneck adapter, a
meta-visual prompt generator based on dense similarities and semantic
embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we
effectively integrate SAM's knowledge into our lightweight segmenter, which can
be further enhanced by 4D correlation mining on support-query pairs. Extensive
experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness
and superiority of our method.

</details>

### [65] [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681)
*Vidi Team,Celong Liu,Chia-Wen Kuo,Dawei Du,Fan Chen,Guang Chen,Jiamin Yuan,Lingxi Zhang,Lu Guo,Lusha Li,Longyin Wen,Qingyu Chen,Rachel Deng,Sijie Zhu,Stuart Siew,Tong Jin,Wei Lu,Wen Zhong,Xiaohui Shen,Xin Gu,Xing Mei,Xueqiong Qu*

Main category: cs.CV

TLDR: Vidi是一种大型多模态模型（LMM），专注于视频编辑中的时间检索任务，能够处理长视频并显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 视频已成为互联网上主要的交流媒介，但传统模型在处理多模态和长视频时面临挑战。

Method: Vidi模型专注于时间检索任务，支持处理长视频和多模态输入（如视觉、音频、文本）。

Result: Vidi在时间检索任务上显著优于GPT-4o和Gemini等领先专有模型。

Conclusion: Vidi在视频编辑场景中表现出色，为大规模高质量视频内容创作提供了有效支持。

Abstract: Humans naturally share information with those they are connected to, and
video has become one of the dominant mediums for communication and expression
on the Internet. To support the creation of high-quality large-scale video
content, a modern pipeline requires a comprehensive understanding of both the
raw input materials (e.g., the unedited footage captured by cameras) and the
editing components (e.g., visual effects). In video editing scenarios, models
must process multiple modalities (e.g., vision, audio, text) with strong
background knowledge and handle flexible input lengths (e.g., hour-long raw
videos), which poses significant challenges for traditional models. In this
report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a
wide range of video understand editing scenarios. The first release focuses on
temporal retrieval, i.e., identifying the time ranges within the input videos
corresponding to a given text query, which plays a critical role in intelligent
editing. The model is capable of processing hour-long videos with strong
temporal understanding capability, e.g., retrieve time ranges for certain
queries. To support a comprehensive evaluation in real-world scenarios, we also
present the VUE-TR benchmark, which introduces five key advancements. 1) Video
duration: significantly longer than existing temporal retrival datasets, 2)
Audio support: includes audio-based queries, 3) Query format: diverse query
lengths/formats, 4) Annotation quality: ground-truth time ranges are manually
annotated. 5) Evaluation metric: a refined IoU metric to support evaluation
over multiple time ranges. Remarkably, Vidi significantly outperforms leading
proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,
indicating its superiority in video editing scenarios.

</details>

### [66] [You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection](https://arxiv.org/abs/2504.15694)
*Jun Dong,Wenli Wu,Jintao Cheng,Xiaoyu Tang*

Main category: cs.CV

TLDR: 提出了一种超轻量实时水下目标检测框架YSOOB，通过多频谱小波编码和动态卷积优化，显著提升了模型性能和效率。


<details>
  <summary>Details</summary>
Motivation: 水下环境图像质量低且计算资源有限，现有目标检测模型的精度和效率仍需改进。

Method: 采用多频谱小波编码（MSWE）减少水下光学颜色失真，动态选择关键信息，并通过通道压缩和重构大核卷积（RLKC）实现模型轻量化。

Result: YSOOB仅120万参数，在URPC2020和DUO数据集上mAP50分别达83.1%和82.9%，推理速度显著优于YOLOv12-N。

Conclusion: YSOOB在性能和效率上均优于现有方法，适用于水下目标检测。

Abstract: Despite the remarkable achievements in object detection, the model's accuracy
and efficiency still require further improvement under challenging underwater
conditions, such as low image quality and limited computational resources. To
address this, we propose an Ultra-Light Real-Time Underwater Object Detection
framework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a
Multi-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on
the input image, minimizing the semantic loss caused by underwater optical
color distortion. Furthermore, we revisit the unique characteristics of
even-sized and transposed convolutions, allowing the model to dynamically
select and enhance key information during the resampling process, thereby
improving its generalization ability. Finally, we eliminate model redundancy
through a simple yet effective channel compression and reconstructed large
kernel convolution (RLKC) to achieve model lightweight. As a result, forms a
high-performance underwater object detector YSOOB with only 1.2 million
parameters. Extensive experimental results demonstrate that, with the fewest
parameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO
datasets, respectively, comparable to the current SOTA detectors. The inference
speed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge
computing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by
28.1% and 22.5%, respectively.

</details>

### [67] [RePOPE: Impact of Annotation Errors on the POPE Benchmark](https://arxiv.org/abs/2504.15707)
*Yannic Neuhaus,Matthias Hein*

Main category: cs.CV

TLDR: 论文研究了MSCOCO数据集中标签错误对POPE基准的影响，重新标注后发现标签质量显著影响模型排名。


<details>
  <summary>Details</summary>
Motivation: 由于数据标注成本高，基准数据集常使用现有图像数据集的标签，但标签错误可能影响评估结果。

Method: 重新标注POPE基准图像，识别标签错误的不平衡分布，并基于修正后的标签（RePOPE）评估多个模型。

Result: 修正标签后，模型排名发生显著变化，表明标签质量对评估结果有重要影响。

Conclusion: 标签质量对基准测试的可靠性至关重要，RePOPE提供了更准确的评估标准。

Abstract: Since data annotation is costly, benchmark datasets often incorporate labels
from established image datasets. In this work, we assess the impact of label
errors in MSCOCO on the frequently used object hallucination benchmark POPE. We
re-annotate the benchmark images and identify an imbalance in annotation errors
across different subsets. Evaluating multiple models on the revised labels,
which we denote as RePOPE, we observe notable shifts in model rankings,
highlighting the impact of label quality. Code and data are available at
https://github.com/YanNeu/RePOPE .

</details>

### [68] [Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models](https://arxiv.org/abs/2504.15723)
*Dasol Jeong,Donggoo Kang,Jiwon Park,Hyebean Lee,Joonki Paik*

Main category: cs.CV

TLDR: 提出了一种基于扩散的零样本图像编辑框架，统一了文本引导和参考引导方法，无需微调。


<details>
  <summary>Details</summary>
Motivation: 旨在实现无需微调的图像编辑，同时保持源图像的结构完整性。

Method: 利用扩散反演和时间步特定的空文本嵌入，结合分阶段潜在注入策略（早期形状注入，后期属性注入）。

Result: 在表情迁移、纹理变换和风格注入等任务中表现出色，验证了方法的可扩展性和适应性。

Conclusion: 该方法在多样化的图像编辑场景中实现了高精度和全局一致性。

Abstract: We propose a diffusion-based framework for zero-shot image editing that
unifies text-guided and reference-guided approaches without requiring
fine-tuning. Our method leverages diffusion inversion and timestep-specific
null-text embeddings to preserve the structural integrity of the source image.
By introducing a stage-wise latent injection strategy-shape injection in early
steps and attribute injection in later steps-we enable precise, fine-grained
modifications while maintaining global consistency. Cross-attention with
reference latents facilitates semantic alignment between the source and
reference. Extensive experiments across expression transfer, texture
transformation, and style infusion demonstrate state-of-the-art performance,
confirming the method's scalability and adaptability to diverse image editing
scenarios.

</details>

### [69] [SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems](https://arxiv.org/abs/2504.15728)
*Manjunath D,Aniruddh Sikdar,Prajwal Gurunath,Sumanth Udupa,Suresh Sundaram*

Main category: cs.CV

TLDR: 论文提出了一种名为SAGA的新策略，用于解决RGB到IR图像域适应中的颜色偏差问题，并引入了一个多传感器数据集IndraEye以验证其效果。


<details>
  <summary>Details</summary>
Motivation: 由于IR图像缺乏颜色和纹理信息，RGB训练的模型在IR图像上表现不佳，导致高误报率和低质量伪标签。

Method: 提出Semantic-Aware Gray color Augmentation (SAGA)，通过提取与IR图像相关的对象级特征来减少颜色偏差并缩小域差距。

Result: 实验表明，SAGA在RGB到IR的域适应中显著提升性能，mAP提高了0.4%至7.6%。

Conclusion: SAGA和IndraEye数据集为多模态学习和域适应提供了有效工具，尤其在无人机图像处理中表现优异。

Abstract: Domain-adaptive thermal object detection plays a key role in facilitating
visible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered
image pairs and minimizing reliance on large annotated IR datasets. However,
inherent limitations of IR images, such as the lack of color and texture cues,
pose challenges for RGB-trained models, leading to increased false positives
and poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray
color Augmentation (SAGA), a novel strategy for mitigating color bias and
bridging the domain gap by extracting object-level features relevant to IR
images. Additionally, to validate the proposed SAGA for drone imagery, we
introduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse
applications. The dataset contains 5,612 images with 145,666 instances,
captured from diverse angles, altitudes, backgrounds, and times of day,
offering valuable opportunities for multimodal learning, domain adaptation for
object detection and segmentation, and exploration of sensor-specific strengths
and weaknesses. IndraEye aims to enhance the development of more robust and
accurate aerial perception systems, especially in challenging environments.
Experimental results show that SAGA significantly improves RGB-to-IR adaptation
for autonomous driving and IndraEye dataset, achieving consistent performance
gains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain
adaptation techniques. The dataset and codes are available at
https://github.com/airliisc/IndraEye.

</details>

### [70] [GADS: A Super Lightweight Model for Head Pose Estimation](https://arxiv.org/abs/2504.15751)
*Menan Velayuthan,Asiri Gawesha,Purushoth Velayuthan,Nuwan Kodagoda,Dharshana Kasthurirathna,Pradeepa Samarasinghe*

Main category: cs.CV

TLDR: 提出了一种基于Deep Set框架的新架构GADS，通过分组地标和使用小型Deep Set层降低计算复杂度，显著减小模型大小并提高速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于地标的方法过于注重精度而忽略了简单性和模型大小，限制了在边缘设备和计算资源有限环境中的部署。

Method: 提出Grouped Attention Deep Sets (GADS)，通过分组地标和使用多头部注意力机制提取和组合组间信息。

Result: 模型比当前最轻量级模型小7.5倍、快25倍，比性能最佳模型小4321倍。

Conclusion: GADS为资源受限的头姿态估计方法提供了强大的基线。

Abstract: In human-computer interaction, head pose estimation profoundly influences
application functionality. Although utilizing facial landmarks is valuable for
this purpose, existing landmark-based methods prioritize precision over
simplicity and model size, limiting their deployment on edge devices and in
compute-poor environments. To bridge this gap, we propose \textbf{Grouped
Attention Deep Sets (GADS)}, a novel architecture based on the Deep Set
framework. By grouping landmarks into regions and employing small Deep Set
layers, we reduce computational complexity. Our multihead attention mechanism
extracts and combines inter-group information, resulting in a model that is
$7.5\times$ smaller and executes $25\times$ faster than the current lightest
state-of-the-art model. Notably, our method achieves an impressive reduction,
being $4321\times$ smaller than the best-performing model. We introduce vanilla
GADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three
benchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture
as a robust baseline for resource-constrained head pose estimation methods.

</details>

### [71] [DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy](https://arxiv.org/abs/2504.15756)
*Qirui Yang,Fangpu Zhang,Yeying Jin,Qihua Cheng,Pengtao Jiang,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TLDR: 提出了一种单阶段原始域去摩尔纹框架DSDNet，通过结合原始和YCbCr图像的优势，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 移动成像中摩尔纹问题严重，现有方法存在信息丢失或效率低下问题。

Method: 设计了双流网络DSDNet，结合原始和YCbCr图像，并引入SADM模块和LCAT模块。

Result: DSDNet在视觉质量和定量评估上优于现有方法，推理速度提升2.4倍。

Conclusion: DSDNet在去摩尔纹任务中表现出色，具有实际应用优势。

Abstract: With the rapid advancement of mobile imaging, capturing screens using
smartphones has become a prevalent practice in distance learning and conference
recording. However, moir\'e artifacts, caused by frequency aliasing between
display screens and camera sensors, are further amplified by the image signal
processing pipeline, leading to severe visual degradation. Existing sRGB domain
demoir\'eing methods struggle with irreversible information loss, while recent
two-stage raw domain approaches suffer from information bottlenecks and
inference inefficiency. To address these limitations, we propose a single-stage
raw domain demoir\'eing framework, Dual-Stream Demoir\'eing Network (DSDNet),
which leverages the synergy of raw and YCbCr images to remove moir\'e while
preserving luminance and color fidelity. Specifically, to guide luminance
correction and moir\'e removal, we design a raw-to-YCbCr mapping pipeline and
introduce the Synergic Attention with Dynamic Modulation (SADM) module. This
module enriches the raw-to-sRGB conversion with cross-domain contextual
features. Furthermore, to better guide color fidelity, we develop a
Luminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance
and chrominance representations. Extensive experiments demonstrate that DSDNet
outperforms state-of-the-art methods in both visual quality and quantitative
evaluation, and achieves an inference speed $\mathrm{\textbf{2.4x}}$ faster
than the second-best method, highlighting its practical advantages. We provide
an anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.

</details>

### [72] [Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection](https://arxiv.org/abs/2504.15770)
*Lei Xu,Mehmet Yamac,Mete Ahishali,Moncef Gabbouj*

Main category: cs.CV

TLDR: 提出了一种基于MTS-DR模块的新型神经网络MTS-DR-Net，用于边缘检测任务，通过减少冗余信息并聚焦相关子空间，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 边缘检测在计算机视觉任务中具有重要作用，但传统方法需要大感受野，导致网络结构过深。MTS因子化操作能实现大感受野，但需进一步优化以减少冗余信息。

Method: 提出MTS-DR模块作为新主干网络，结合MTS层和MTS-DR块，通过降维去除冗余信息，并引入U形细化模块进一步优化。

Result: 在BSDS500和BIPEDv2数据集上的实验验证了MTS-DR-Net的有效性。

Conclusion: MTS-DR-Net通过降维和细化模块显著提升了边缘检测性能，为相关任务提供了新思路。

Abstract: Edge detection has attracted considerable attention thanks to its exceptional
ability to enhance performance in downstream computer vision tasks. In recent
years, various deep learning methods have been explored for edge detection
tasks resulting in a significant performance improvement compared to
conventional computer vision algorithms. In neural networks, edge detection
tasks require considerably large receptive fields to provide satisfactory
performance. In a typical convolutional operation, such a large receptive field
can be achieved by utilizing a significant number of consecutive layers, which
yields deep network structures. Recently, a Multi-scale Tensorial Summation
(MTS) factorization operator was presented, which can achieve very large
receptive fields even from the initial layers. In this paper, we propose a
novel MTS Dimensional Reduction (MTS-DR) module guided neural network,
MTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and
corresponding MTS-DR blocks as a new backbone to remove redundant information
initially. Such a dimensional reduction module enables the neural network to
focus specifically on relevant information (i.e., necessary subspaces).
Finally, a weight U-shaped refinement module follows MTS-DR blocks in the
MTS-DR-Net. We conducted extensive experiments on two benchmark edge detection
datasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The
implementation of the proposed MTS-DR-Net can be found at
https://github.com/LeiXuAI/MTS-DR-Net.git.

</details>

### [73] [Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models](https://arxiv.org/abs/2504.15776)
*Quentin Herau,Nathan Piasco,Moussab Bennehar,Luis Rolado,Dzmitry Tsishkou,Bingbing Liu,Cyrille Migniot,Pascal Vasseur,Cédric Demonceaux*

Main category: cs.CV

TLDR: 提出了一种基于NeRF的优化方法，用于改进自动驾驶数据集中传感器位姿和校准参数，提升数据集的准确性。


<details>
  <summary>Details</summary>
Motivation: 公共数据集在自动驾驶研究中至关重要，但传感器校准和车辆位姿的不准确性可能导致下游任务评估错误，影响系统可靠性。

Method: 采用NeRF的鲁棒优化方法，通过重投影指标、新视角合成渲染质量和几何对齐验证优化效果。

Result: 方法显著提升了传感器位姿的准确性，优化后的数据集为研究社区提供了更可靠的资源。

Conclusion: 该方法不仅提升了现有数据集的实用性，还为更可靠的自动驾驶模型奠定了基础。

Abstract: Autonomous driving systems rely on accurate perception and localization of
the ego car to ensure safety and reliability in challenging real-world driving
scenarios. Public datasets play a vital role in benchmarking and guiding
advancement in research by providing standardized resources for model
development and evaluation. However, potential inaccuracies in sensor
calibration and vehicle poses within these datasets can lead to erroneous
evaluations of downstream tasks, adversely impacting the reliability and
performance of the autonomous systems. To address this challenge, we propose a
robust optimization method based on Neural Radiance Fields (NeRF) to refine
sensor poses and calibration parameters, enhancing the integrity of dataset
benchmarks. To validate improvement in accuracy of our optimized poses without
ground truth, we present a thorough evaluation process, relying on reprojection
metrics, Novel View Synthesis rendering quality, and geometric alignment. We
demonstrate that our method achieves significant improvements in sensor pose
accuracy. By optimizing these critical parameters, our approach not only
improves the utility of existing datasets but also paves the way for more
reliable autonomous driving models. To foster continued progress in this field,
we make the optimized sensor poses publicly available, providing a valuable
resource for the research community.

</details>

### [74] [Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos](https://arxiv.org/abs/2504.15782)
*Daniele Baieri,Riccardo Cicciarella,Michael Krützen,Emanuele Rodolà,Silvia Zuffi*

Main category: cs.CV

TLDR: 提出了一种基于模型的单目视频方法，用于估计野生海豚的3D形状和运动，以评估其身体状况。


<details>
  <summary>Details</summary>
Motivation: 水生动物在自然水下环境中的观测困难，导致其3D重建研究较少，而海豚的身体状况评估需要更精确的方法。

Method: 采用基于模型的方法，结合传输模型以解决水下遮挡问题，应用于不同海况下的视频数据。

Result: 估计了海豚的质量和体积，并与基于手动2D测量的方法进行了比较。

Conclusion: 该方法为水生动物3D重建提供了新思路，尤其在复杂水下环境中表现良好。

Abstract: We address the problem of estimating the metric 3D shape and motion of wild
dolphins from monocular video, with the aim of assessing their body condition.
While considerable progress has been made in reconstructing 3D models of
terrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty
of observing them in their natural underwater environment. To address this, we
propose a model-based approach that incorporates a transmission model to
account for water-induced occlusion. We apply our method to video captured
under different sea conditions. We estimate mass and volume, and compare our
results to a manual 2D measurements-based method.

</details>

### [75] [Towards prediction of morphological heart age from computed tomography angiography](https://arxiv.org/abs/2504.15783)
*Johan Öfverstedt,Elin Lundström,Håkan Ahlström,Joel Kullberg*

Main category: cs.CV

TLDR: 该研究通过CTA图像预测年龄，开发了一种新的心脏形态年龄生物标志物，并分析了心脏形态与衰老的关系。


<details>
  <summary>Details</summary>
Motivation: 研究心脏形态与年龄的关系，并开发一种新的生物标志物以量化心脏形态年龄。

Method: 使用图像配准方法标准化图像，提取密度和局部体积的稳健特征，并用机器学习模型拟合回归。

Result: 在SCAPIS数据集中，女性和男性的平均绝对误差分别为2.74年和2.77年。预测结果与形态高度一致。

Conclusion: 该方法成功预测了心脏形态年龄，并通过显著性分析提高了模型的可解释性。

Abstract: Age prediction from medical images or other health-related non-imaging data
is an important approach to data-driven aging research, providing knowledge of
how much information a specific tissue or organ carries about the chronological
age of the individual. In this work, we studied the prediction of age from
computed tomography angiography (CTA) images, which provide detailed
representations of the heart morphology, with the goals of (i) studying the
relationship between morphology and aging, and (ii) developing a novel
\emph{morphological heart age} biomarker. We applied an image
registration-based method that standardizes the images from the whole cohort
into a single space. We then extracted supervoxels (using unsupervised
segmentation), and corresponding robust features of density and local volume,
which provide a detailed representation of the heart morphology while being
robust to registration errors. Machine learning models are then trained to fit
regression models from these features to the chronological age. We applied the
method to a subset of the images from the Swedish CArdioPulomonary bioImage
Study (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a
mean absolute error of $2.74$ years for females and $2.77$ years for males. The
predictions from different sub-regions of interest were observed to be more
highly correlated with the predictions from the whole heart, compared to the
chronological age, revealing a high consistency in the predictions from
morphology. Saliency analysis was also performed on the prediction models to
study what regions are associated positively and negatively with the predicted
age. This resulted in detailed association maps where the density and volume of
known, as well as some novel sub-regions of interest, are determined to be
important. The saliency analysis aids in the interpretability of the models and
their predictions.

</details>

### [76] [Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views](https://arxiv.org/abs/2504.15786)
*Ningli Xu,Rongjun Qin*

Main category: cs.CV

TLDR: 提出了一种新的跨视角合成方法，通过固定潜在扩散模型生成一致的地面视图图像，解决了卫星与地面视角差异大的问题。


<details>
  <summary>Details</summary>
Motivation: 卫星与地面视角的差异导致生成的地面视图图像不一致，现有方法多关注单视图生成，缺乏多视图一致性。

Method: 采用固定潜在扩散模型，引入卫星引导去噪和卫星-时间去噪两个条件模块，确保多视图生成的一致性。

Result: 实验表明，该方法在感知和时间指标上优于现有方法，实现了高真实感和多视图一致性。

Conclusion: 该方法有效解决了跨视角图像生成的一致性问题，为大规模地面场景或视频生成提供了新思路。

Abstract: Generating consistent ground-view images from satellite imagery is
challenging, primarily due to the large discrepancies in viewing angles and
resolution between satellite and ground-level domains. Previous efforts mainly
concentrated on single-view generation, often resulting in inconsistencies
across neighboring ground views. In this work, we propose a novel cross-view
synthesis approach designed to overcome these challenges by ensuring
consistency across ground-view images generated from satellite views. Our
method, based on a fixed latent diffusion model, introduces two conditioning
modules: satellite-guided denoising, which extracts high-level scene layout to
guide the denoising process, and satellite-temporal denoising, which captures
camera motion to maintain consistency across multiple generated views. We
further contribute a large-scale satellite-ground dataset containing over
100,000 perspective pairs to facilitate extensive ground scene or video
generation. Experimental results demonstrate that our approach outperforms
existing methods on perceptual and temporal metrics, achieving high
photorealism and consistency in multi-view outputs.

</details>

### [77] [Development and evaluation of a deep learning algorithm for German word recognition from lip movements](https://arxiv.org/abs/2504.15792)
*Dinh Nam Pham,Torsten Rahne*

Main category: cs.CV

TLDR: 该论文开发了一种基于神经网络的德语唇读算法，通过3D CNN和GRU模型结合，显著提高了识别准确率，最高达87%（已知说话者）和63%（未知说话者）。


<details>
  <summary>Details</summary>
Motivation: 现有唇读算法多针对英语，缺乏德语支持，且传统方法错误率高。

Method: 使用1806个德语视频片段，训练3D CNN、GRU及组合模型（GRUConv），比较不同图像区域和色彩空间。

Result: GRUConv模型表现最佳，已知说话者准确率87%，未知说话者63%；唇部区域裁剪显著提升准确率（70% vs 34%）。

Conclusion: 该德语唇读算法准确率高，可推广至更多词汇类别，适用于未知说话者。

Abstract: When reading lips, many people benefit from additional visual information
from the lip movements of the speaker, which is, however, very error prone.
Algorithms for lip reading with artificial intelligence based on artificial
neural networks significantly improve word recognition but are not available
for the German language. A total of 1806 video clips with only one
German-speaking person each were selected, split into word segments, and
assigned to word classes using speech-recognition software. In 38,391 video
segments with 32 speakers, 18 polysyllabic, visually distinguishable words were
used to train and validate a neural network. The 3D Convolutional Neural
Network and Gated Recurrent Units models and a combination of both models
(GRUConv) were compared, as were different image sections and color spaces of
the videos. The accuracy was determined in 5000 training epochs. Comparison of
the color spaces did not reveal any relevant different correct classification
rates in the range from 69% to 72%. With a cut to the lips, a significantly
higher accuracy of 70% was achieved than when cut to the entire speaker's face
(34%). With the GRUConv model, the maximum accuracies were 87% with known
speakers and 63% in the validation with unknown speakers. The neural network
for lip reading, which was first developed for the German language, shows a
very high level of accuracy, comparable to English-language algorithms. It
works with unknown speakers as well and can be generalized with more word
classes.

</details>

### [78] [Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness](https://arxiv.org/abs/2504.15796)
*Jiaqi Tang,Yinsong Xu,Qingchao Chen*

Main category: cs.CV

TLDR: 提出了一种基于显著性图的样本选择方法（SM-DSB），用于解决点云无监督域适应中自监督任务梯度冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在点云无监督域适应中采用多任务学习框架，但部分自监督任务的梯度可能对分类性能产生负面影响。

Method: 设计了一种基于3D显著性图偏度的评分机制，动态筛选对分类有益的自监督样本。

Result: 实验表明，该方法优于现有技术，且计算开销小。

Conclusion: 提供了一种通过反向传播分析理解无监督域适应问题的新视角。

Abstract: Object classification models utilizing point cloud data are fundamental for
3D media understanding, yet they often struggle with unseen or
out-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain
adaptation (UDA) methods typically employ a multi-task learning (MTL) framework
that combines primary classification tasks with auxiliary self-supervision
tasks to bridge the gap between cross-domain feature distributions. However,
our further experiments demonstrate that not all gradients from
self-supervision tasks are beneficial and some may negatively impact the
classification performance. In this paper, we propose a novel solution, termed
Saliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient
conflicts. Specifically, our method designs a new scoring mechanism based on
the skewness of 3D saliency maps to estimate gradient conflicts without
requiring target labels. Leveraging this, we develop a sample selection
strategy that dynamically filters out samples whose self-supervision gradients
are not beneficial for the classification. Our approach is scalable,
introducing modest computational overhead, and can be integrated into all the
point cloud UDA MTL frameworks. Extensive evaluations demonstrate that our
method outperforms state-of-the-art approaches. In addition, we provide a new
perspective on understanding the UDA problem through back-propagation analysis.

</details>

### [79] [Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models](https://arxiv.org/abs/2504.15823)
*Songyan Xie,Jinghang Wen,Encheng Su,Qiucheng Yu*

Main category: cs.CV

TLDR: 提出了一种新型的、隐蔽且实用的对抗性补丁，用于在黑盒设置下攻击近红外（NIR）人脸识别系统，通过红外吸收墨水生成数字优化的补丁，并利用光反射模型减少数字与真实NIR成像的差异。


<details>
  <summary>Details</summary>
Motivation: 展示近红外人脸识别系统在物理对抗攻击下的潜在风险，尤其是在低光或化妆条件下的实际应用场景。

Method: 使用人眼不可见的红外吸收墨水生成补丁，并通过光反射模型模拟NIR光反射以减少数字与真实成像的差异。

Result: 在数字和物理领域中均提高了攻击成功率，平均物理领域攻击成功率达82.46%，优于现有方法的64.18%。

Conclusion: 该方法在隐蔽性和攻击效果上优于现有技术，突显了近红外人脸识别系统的安全漏洞。

Abstract: Near-infrared (NIR) face recognition systems, which can operate effectively
in low-light conditions or in the presence of makeup, exhibit vulnerabilities
when subjected to physical adversarial attacks. To further demonstrate the
potential risks in real-world applications, we design a novel, stealthy, and
practical adversarial patch to attack NIR face recognition systems in a
black-box setting. We achieved this by utilizing human-imperceptible
infrared-absorbing ink to generate multiple patches with digitally optimized
shapes and positions for infrared images. To address the optimization mismatch
between digital and real-world NIR imaging, we develop a light reflection model
for human skin to minimize pixel-level discrepancies by simulating NIR light
reflection.
  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition
systems, the experimental results show that our method improves the attack
success rate in both digital and physical domains, particularly maintaining
effectiveness across various face postures. Notably, the proposed approach
outperforms SOTA methods, achieving an average attack success rate of 82.46% in
the physical domain across different models, compared to 64.18% for existing
methods. The artifact is available at
https://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.

</details>

### [80] [Text-based Animatable 3D Avatars with Morphable Model Alignment](https://arxiv.org/abs/2504.15835)
*Yiqian Wu,Malte Prinzler,Xiaogang Jin,Siyu Tang*

Main category: cs.CV

TLDR: 论文提出AnimPortrait3D框架，解决文本生成3D头像时外观与几何模糊及语义对齐问题，通过预训练模型初始化与控制网络优化，显著提升生成质量和动画效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成3D头像方法在细节真实性和动画自然性上表现不佳，主要因2D扩散预测的模糊性和语义对齐不足。

Method: 结合预训练文本到3D模型初始化头像，并利用ControlNet基于变形模型的语义和法线图优化动态表情。

Result: 实验表明，该方法在生成质量、对齐性和动画保真度上优于现有技术。

Conclusion: AnimPortrait3D框架在文本驱动的3D头像生成领域取得了显著进展。

Abstract: The generation of high-quality, animatable 3D head avatars from text has
enormous potential in content creation applications such as games, movies, and
embodied virtual assistants. Current text-to-3D generation methods typically
combine parametric head models with 2D diffusion models using score
distillation sampling to produce 3D-consistent results. However, they struggle
to synthesize realistic details and suffer from misalignments between the
appearance and the driving parametric model, resulting in unnatural animation
results. We discovered that these limitations stem from ambiguities in the 2D
diffusion predictions during 3D avatar distillation, specifically: i) the
avatar's appearance and geometry is underconstrained by the text input, and ii)
the semantic alignment between the predictions and the parametric head model is
insufficient because the diffusion model alone cannot incorporate information
from the parametric model. In this work, we propose a novel framework,
AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with
morphable model alignment, and introduce two key strategies to address these
challenges. First, we tackle appearance and geometry ambiguities by utilizing
prior information from a pretrained text-to-3D model to initialize a 3D avatar
with robust appearance, geometry, and rigging relationships to the morphable
model. Second, we refine the initial 3D avatar for dynamic expressions using a
ControlNet that is conditioned on semantic and normal maps of the morphable
model to ensure accurate alignment. As a result, our method outperforms
existing approaches in terms of synthesis quality, alignment, and animation
fidelity. Our experiments show that the proposed method advances the state of
the art in text-based, animatable 3D head avatar generation.

</details>

### [81] [DERD-Net: Learning Depth from Event-based Ray Densities](https://arxiv.org/abs/2504.15863)
*Diego de Oliveira Hitzges,Suman Ghosh,Guillermo Gallego*

Main category: cs.CV

TLDR: 提出了一种基于事件相机的深度估计框架，通过处理异步事件数据，实现了在单目和双目设置下的高效深度预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习框架难以处理事件相机的异步数据流，因此需要一种适应事件数据特性的新方法。

Method: 将3D场景结构编码为视差空间图像（DSI），利用3D卷积和循环结构处理局部子区域，实现并行化快速推理。

Result: 在标准数据集上表现优异：单目数据媲美现有双目方法，双目数据误差降低至少42%，深度完整性提升3倍以上。

Conclusion: 该框架在事件相机深度估计和SLAM领域具有成为标准方法的潜力。

Abstract: Event cameras offer a promising avenue for multi-view stereo depth estimation
and Simultaneous Localization And Mapping (SLAM) due to their ability to detect
blur-free 3D edges at high-speed and over broad illumination conditions.
However, traditional deep learning frameworks designed for conventional cameras
struggle with the asynchronous, stream-like nature of event data, as their
architectures are optimized for discrete, image-like inputs. We propose a
scalable, flexible and adaptable framework for pixel-wise depth estimation with
event cameras in both monocular and stereo setups. The 3D scene structure is
encoded into disparity space images (DSIs), representing spatial densities of
rays obtained by back-projecting events into space via known camera poses. Our
neural network processes local subregions of the DSIs combining 3D convolutions
and a recurrent structure to recognize valuable patterns for depth prediction.
Local processing enables fast inference with full parallelization and ensures
constant ultra-low model complexity and memory costs, regardless of camera
resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)
demonstrate unprecedented effectiveness: (i) using purely monocular data, our
method achieves comparable results to existing stereo methods; (ii) when
applied to stereo data, it strongly outperforms all state-of-the-art (SOTA)
approaches, reducing the mean absolute error by at least 42%; (iii) our method
also allows for increases in depth completeness by more than 3-fold while still
yielding a reduction in median absolute error of at least 30%. Given its
remarkable performance and effective processing of event-data, our framework
holds strong potential to become a standard approach for using deep learning
for event-based depth estimation and SLAM. Project page:
https://github.com/tub-rip/DERD-Net

</details>

### [82] [MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search](https://arxiv.org/abs/2504.15865)
*Lotfi Abdelkrim Mecharbat,Ibrahim Elmakky,Martin Takac,Mohammed Yaqub*

Main category: cs.CV

TLDR: MedNNS是一个针对医学影像的神经架构搜索框架，通过联合优化架构选择和权重初始化，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像任务中，深度学习模型的架构选择和权重初始化是关键挑战，传统方法如ImageNet迁移学习效果有限。

Method: 提出MedNNS框架，构建元空间联合优化架构和权重，引入rank loss和FID loss捕捉模型和数据集关系。

Result: 实验表明，MedNNS在多个数据集上优于ImageNet预训练模型和SOTA NAS方法，平均准确率提升1.7%。

Conclusion: MedNNS为医学影像任务提供了一种高效的神经架构搜索解决方案。

Abstract: Deep learning (DL) has achieved remarkable progress in the field of medical
imaging. However, adapting DL models to medical tasks remains a significant
challenge, primarily due to two key factors: (1) architecture selection, as
different tasks necessitate specialized model designs, and (2) weight
initialization, which directly impacts the convergence speed and final
performance of the models. Although transfer learning from ImageNet is a widely
adopted strategy, its effectiveness is constrained by the substantial
differences between natural and medical images. To address these challenges, we
introduce Medical Neural Network Search (MedNNS), the first Neural Network
Search framework for medical imaging applications. MedNNS jointly optimizes
architecture selection and weight initialization by constructing a meta-space
that encodes datasets and models based on how well they perform together. We
build this space using a Supernetwork-based approach, expanding the model zoo
size by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we
introduce rank loss and Fr\'echet Inception Distance (FID) loss into the
construction of the space to capture inter-model and inter-dataset
relationships, thereby achieving more accurate alignment in the meta-space.
Experimental results across multiple datasets demonstrate that MedNNS
significantly outperforms both ImageNet pre-trained DL models and SOTA Neural
Architecture Search (NAS) methods, achieving an average accuracy improvement of
1.7% across datasets while converging substantially faster. The code and the
processed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.

</details>

### [83] [Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.15883)
*Farida Mohsen,Samir Belhaouari,Zubair Shah*

Main category: cs.CV

TLDR: RadFuse是一种多表征深度学习框架，结合非线性RadEx变换的sinogram图像与传统眼底图像，显著提升了糖尿病视网膜病变的检测和分级性能。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变的早期检测和准确分级对预防视力丧失至关重要，但现有方法难以捕捉眼底图像中复杂、不规则的病变模式。

Method: 提出RadEx变换（Radon变换的非线性扩展），生成sinogram表征以捕捉复杂病变模式，并结合传统眼底图像，通过多表征深度学习框架RadFuse进行分级。

Result: 在APTOS-2019和DDR数据集上，RadFuse在三种CNN架构中均优于仅使用眼底图像的模型，并在五级分级和二元分类任务中达到高精度（如kappa值93.24%，二元分类准确率99.09%）。

Conclusion: RadFuse通过整合非线性特征和先进数学变换，显著提升了糖尿病视网膜病变的分类性能，推动了医学图像分析的发展。

Abstract: Diabetic retinopathy is a serious ocular complication that poses a
significant threat to patients' vision and overall health. Early detection and
accurate grading are essential to prevent vision loss. Current automatic
grading methods rely heavily on deep learning applied to retinal fundus images,
but the complex, irregular patterns of lesions in these images, which vary in
shape and distribution, make it difficult to capture subtle changes. This study
introduces RadFuse, a multi-representation deep learning framework that
integrates non-linear RadEx-transformed sinogram images with traditional fundus
images to enhance diabetic retinopathy detection and grading. Our RadEx
transformation, an optimized non-linear extension of the Radon transform,
generates sinogram representations to capture complex retinal lesion patterns.
By leveraging both spatial and transformed domain information, RadFuse enriches
the feature set available to deep learning models, improving the
differentiation of severity levels. We conducted extensive experiments on two
benchmark datasets, APTOS-2019 and DDR, using three convolutional neural
networks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant
improvements over fundus-image-only models across all three CNN architectures
and outperformed state-of-the-art methods on both datasets. For severity
grading across five stages, RadFuse achieved a quadratic weighted kappa of
93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary
classification between healthy and diabetic retinopathy cases, the method
reached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,
surpassing previously established models. These results demonstrate RadFuse's
capacity to capture complex non-linear features, advancing diabetic retinopathy
classification and promoting the integration of advanced mathematical
transforms in medical image analysis.

</details>

### [84] [MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2504.15888)
*Zhiqiang Wei,Lianqing Zheng,Jianan Liu,Tao Huang,Qing-Long Han,Wenwen Zhang,Fengdeng Zhang*

Main category: cs.CV

TLDR: MS-Occ提出了一种新颖的多阶段LiDAR-相机融合框架，通过中间和后期融合结合几何和语义信息，显著提升了3D语义占用感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉方法几何不准确和LiDAR方法语义信息不足的问题，提升自动驾驶在复杂环境中的感知能力。

Method: 采用多阶段融合框架，包括中间阶段的Gaussian-Geo模块和Semantic-Aware模块，以及后期阶段的Adaptive Fusion模块和HCCVF模块。

Result: 在nuScenes-OpenOccupancy基准测试中，IoU达到32.1%，mIoU达到25.3%，优于现有方法。

Conclusion: MS-Occ通过多阶段融合显著提升了3D语义占用感知性能，尤其在小型物体感知方面表现突出，适用于安全关键的自动驾驶场景。

Abstract: Accurate 3D semantic occupancy perception is essential for autonomous driving
in complex environments with diverse and irregular objects. While
vision-centric methods suffer from geometric inaccuracies, LiDAR-based
approaches often lack rich semantic information. To address these limitations,
MS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes
middle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's
geometric fidelity with camera-based semantic richness via hierarchical
cross-modal fusion. The framework introduces innovations at two critical
stages: (1) In the middle-stage feature fusion, the Gaussian-Geo module
leverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D
image features with dense geometric priors, and the Semantic-Aware module
enriches LiDAR voxels with semantic context via deformable cross-attention; (2)
In the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically
balances voxel features across modalities, while the High Classification
Confidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using
self-attention-based refinement. Experiments on the nuScenes-OpenOccupancy
benchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%
and a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU
and +2.4% mIoU. Ablation studies further validate the contribution of each
module, with substantial improvements in small-object perception, demonstrating
the practical value of MS-Occ for safety-critical autonomous driving scenarios.

</details>

### [85] [Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions](https://arxiv.org/abs/2504.15918)
*Chang Zong,Bin Li,Shoujun Zhou,Jian Wan,Lei Zhang*

Main category: cs.CV

TLDR: 论文提出了一种新任务In-VAL，模拟人与视频的多轮交互以定位视觉答案，并提出了Ask2Loc框架来解决语义差距问题。


<details>
  <summary>Details</summary>
Motivation: 用户在获取视频片段时需多次交互以理解内容，现有方法无法有效模拟这一过程。

Method: 提出Ask2Loc框架，包含聊天、重写和搜索三个模块，分别解决意图模糊、语言不完整和内容碎片化问题。

Result: 在三个重构数据集上，Ask2Loc比传统方法性能提升最高达14.91（mIoU）。

Conclusion: Ask2Loc通过多轮交互有效解决了In-VAL任务中的语义差距问题，性能显著提升。

Abstract: Locating specific segments within an instructional video is an efficient way
to acquire guiding knowledge. Generally, the task of obtaining video segments
for both verbal explanations and visual demonstrations is known as visual
answer localization (VAL). However, users often need multiple interactions to
obtain answers that align with their expectations when using the system. During
these interactions, humans deepen their understanding of the video content by
asking themselves questions, thereby accurately identifying the location.
Therefore, we propose a new task, named In-VAL, to simulate the multiple
interactions between humans and videos in the procedure of obtaining visual
answers. The In-VAL task requires interactively addressing several semantic gap
issues, including 1) the ambiguity of user intent in the input questions, 2)
the incompleteness of language in video subtitles, and 3) the fragmentation of
content in video segments. To address these issues, we propose Ask2Loc, a
framework for resolving In-VAL by asking questions. It includes three key
modules: 1) a chatting module to refine initial questions and uncover clear
intentions, 2) a rewriting module to generate fluent language and create
complete descriptions, and 3) a searching module to broaden local context and
provide integrated content. We conduct extensive experiments on three
reconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage
methods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on
the In-VAL task. Our code and datasets can be accessed at
https://github.com/changzong/Ask2Loc.

</details>

### [86] [ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting](https://arxiv.org/abs/2504.15921)
*Jian Hu,Dimitrios Korkinof,Shaogang Gong,Mariano Beguerisse-Diaz*

Main category: cs.CV

TLDR: ViSMaP是一种无监督视频摘要系统，利用元提示技术生成长视频摘要，无需昂贵的人工标注。


<details>
  <summary>Details</summary>
Motivation: 解决长视频摘要中相关事件稀疏且未分段的问题，同时避免依赖高成本的有监督分层训练。

Method: 通过LLMs生成优化的伪摘要，采用元提示策略迭代生成和优化摘要，利用短视频模型的片段描述作为指导。

Result: ViSMaP在多个数据集上表现优异，性能接近全监督的先进模型，且能跨领域泛化。

Conclusion: ViSMaP为长视频摘要提供了一种高效的无监督解决方案，性能与有监督方法相当。

Abstract: We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a
system to summarise hour long videos with no-supervision. Most existing video
understanding models work well on short videos of pre-segmented events, yet
they struggle to summarise longer videos where relevant events are sparsely
distributed and not pre-segmented. Moreover, long-form video understanding
often relies on supervised hierarchical training that needs extensive
annotations which are costly, slow and prone to inconsistency. With ViSMaP we
bridge the gap between short videos (where annotated data is plentiful) and
long ones (where it's not). We rely on LLMs to create optimised
pseudo-summaries of long videos using segment descriptions from short ones.
These pseudo-summaries are used as training data for a model that generates
long-form video summaries, bypassing the need for expensive annotations of long
videos. Specifically, we adopt a meta-prompting strategy to iteratively
generate and refine creating pseudo-summaries of long videos. The strategy
leverages short clip descriptions obtained from a supervised short video model
to guide the summary. Each iteration uses three LLMs working in sequence: one
to generate the pseudo-summary from clip descriptions, another to evaluate it,
and a third to optimise the prompt of the generator. This iteration is
necessary because the quality of the pseudo-summaries is highly dependent on
the generator prompt, and varies widely among videos. We evaluate our summaries
extensively on multiple datasets; our results show that ViSMaP achieves
performance comparable to fully supervised state-of-the-art models while
generalising across domains without sacrificing performance. Code will be
released upon publication.

</details>

### [87] [A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers](https://arxiv.org/abs/2504.15928)
*Meng Wang,Tian Lin,Qingshan Hou,Aidi Lin,Jingcheng Wang,Qingsheng Peng,Truong X. Nguyen,Danqi Fang,Ke Zou,Ting Xu,Cancan Xue,Ten Cheer Quek,Qinkai Yu,Minxin Liu,Hui Zhou,Zixuan Xiao,Guiqin He,Huiyu Liang,Tingkun Shi,Man Chen,Linna Liu,Yuanyuan Peng,Lianyu Wang,Qiuming Hu,Junhong Chen,Zhenhua Zhang,Cheng Chen,Yitian Zhao,Dianbo Liu,Jianhua Wu,Xinjian Chen,Changqing Zhang,Triet Thanh Nguyen,Yanda Meng,Yalin Zheng,Yih Chung Tham,Carol Y. Cheung,Huazhu Fu,Haoyu Chen,Ching-Yu Cheng*

Main category: cs.CV

TLDR: GlobeReady是一个无需重新训练即可在不同临床中心部署的AI平台，用于眼科疾病诊断，具有高准确性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在跨中心部署时需要重新训练，限制了其广泛应用。GlobeReady旨在解决这一问题，提供无需技术支持的诊断工具。

Method: 通过免训练局部特征增强技术，应对不同中心和人群的领域偏移，并结合置信度量化方法提升诊断准确性。

Result: 在多个国家和数据集上表现优异，准确率高达93.9-99.4%，临床评价平均4.6/5分。

Conclusion: GlobeReady展示了强大的跨中心诊断能力，有望推动眼科AI的广泛应用。

Abstract: Artificial intelligence (AI) shows remarkable potential in medical imaging
diagnostics, but current models typically require retraining when deployed
across different clinical centers, limiting their widespread adoption. We
introduce GlobeReady, a clinician-friendly AI platform that enables ocular
disease diagnosis without retraining/fine-tuning or technical expertise.
GlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an
11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.
Through training-free local feature augmentation, it addresses domain shifts
across centers and populations, reaching an average accuracy of 88.9% across
five centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in
confidence-quantifiable diagnostic approach further boosted accuracy to
94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution
cases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians
from multiple countries rated GlobeReady highly (average 4.6 out of 5) for its
usability and clinical relevance. These results demonstrate GlobeReady's
robust, scalable diagnostic capability and potential to support ophthalmic care
without technical barriers.

</details>

### [88] [Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models](https://arxiv.org/abs/2504.15929)
*Saban Ozturk,Melih B. Yilmaz,Muti Kara,M. Talat Yavuz,Aykut Koç,Tolga Çukur*

Main category: cs.CV

TLDR: MedTrim是一种通过多模态三元组学习增强图像-文本对齐的新方法，利用疾病类别和病理描述符优化医学视觉语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据量大，现有对齐方法过于关注疾病类别分离，忽略了细粒度病理属性（如位置、大小、严重程度），导致表现不佳。

Method: 提出MedTrim方法，包括基于本体的实体识别模块、新型评分函数用于三元组挖掘，以及多模态三元组对齐目标。

Result: MedTrim在下游检索和分类任务中表现优于现有对齐方法。

Conclusion: MedTrim通过细粒度病理属性对齐，显著提升了医学视觉语言模型的性能。

Abstract: Diagnostic imaging relies on interpreting both images and radiology reports,
but the growing data volumes place significant pressure on medical experts,
yielding increased errors and workflow backlogs. Medical vision-language models
(med-VLMs) have emerged as a powerful framework to efficiently process
multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit
their performance hinges on how well image and text representations are
aligned. Existing alignment methods, predominantly based on contrastive
learning, prioritize separation between disease classes over segregation of
fine-grained pathology attributes like location, size or severity, leading to
suboptimal representations. Here, we propose MedTrim (Meta-entity-driven
Triplet mining), a novel method that enhances image-text alignment through
multimodal triplet learning synergistically guided by disease class as well as
adjectival and directional pathology descriptors. Unlike common alignment
methods that separate broad disease classes, MedTrim leverages structured
meta-entity information to preserve subtle but clinically significant
intra-class variations. For this purpose, we first introduce an ontology-based
entity recognition module that extracts pathology-specific meta-entities from
CXR reports, as annotations on pathology attributes are rare in public
datasets. For refined sample selection in triplet mining, we then introduce a
novel score function that captures an aggregate measure of inter-sample
similarity based on disease classes and adjectival/directional descriptors.
Lastly, we introduce a multimodal triplet alignment objective for explicit
within- and cross-modal alignment between samples sharing detailed pathology
characteristics. Our demonstrations indicate that MedTrim improves performance
in downstream retrieval and classification tasks compared to state-of-the-art
alignment methods.

</details>

### [89] [Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time](https://arxiv.org/abs/2504.15931)
*Ekaterina Kondrateva,Sandzhi Barg,Mikhail Vasiliev*

Main category: cs.CV

TLDR: 论文研究了两种现代分割流程（FastSurfer和SynthSeg）在脑部MRI图像分割中的准确性和可重复性，发现小脑区存在显著体积变化，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 脑部MRI的准确和可重复形态测量对监测神经解剖变化至关重要，但扫描仪引起的变异性限制了其可靠性。

Method: 使用两个数据集（SIMON和SRPBS），通过Dice系数、Surface Dice、Hausdorff距离等指标量化分割变异性，并分析注册模板和插值模式的影响。

Result: 研究发现小脑区（如杏仁核）在测试-重测条件下体积变化可达7-8%，并提出了基于表面的质量过滤方法以提高可靠性。

Conclusion: 研究为形态测量可重复性提供了基准，并强调了在真实神经影像研究中需要协调策略。

Abstract: Accurate and reproducible brain morphometry from structural MRI is critical
for monitoring neuroanatomical changes across time and across imaging domains.
Although deep learning has accelerated segmentation workflows, scanner-induced
variability and reproducibility limitations remain-especially in longitudinal
and multi-site settings. In this study, we benchmark two modern segmentation
pipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the
most widely adopted tools in neuroimaging.
  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and
a 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation
variability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),
and Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume
variation in small subcortical structures such as the amygdala and ventral
diencephalon, even under controlled test-retest conditions. This raises a key
question: is it feasible to detect subtle longitudinal changes on the order of
5-10% in pea-sized brain regions, given the magnitude of domain-induced
morphometric noise?
  We further analyze the effects of registration templates and interpolation
modes, and propose surface-based quality filtering to improve segmentation
reliability. This study provides a reproducible benchmark for morphometric
reproducibility and emphasizes the need for harmonization strategies in
real-world neuroimaging studies.
  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation

</details>

### [90] [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
*Yimu Wang,Xuye Liu,Wei Pang,Li Ma,Shuai Yuan,Paul Debevec,Ning Yu*

Main category: cs.CV

TLDR: 这篇综述全面回顾了基于扩散模型的视频生成技术，涵盖其发展、技术基础、应用及挑战，并提供了更广泛和细致的视角。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成中展现出显著优势，但面临运动一致性、计算效率和伦理问题等挑战，需要系统性的综述以指导研究和实践。

Method: 通过系统分类现有方法，分析架构创新和优化策略，并探讨其在低层视觉任务及相关领域的应用。

Result: 提供了更全面和更新的视角，包括评估指标、行业解决方案和训练工程技术，为研究者和从业者提供资源。

Conclusion: 该综述为扩散模型与视频生成交叉领域的研究和实践奠定了基础，推动了这一快速发展领域的理论和实践进展。

Abstract: Recent advances in diffusion models have revolutionized video generation,
offering superior temporal consistency and visual quality compared to
traditional generative adversarial networks-based approaches. While this
emerging field shows tremendous promise in applications, it faces significant
challenges in motion consistency, computational efficiency, and ethical
considerations. This survey provides a comprehensive review of diffusion-based
video generation, examining its evolution, technical foundations, and practical
applications. We present a systematic taxonomy of current methodologies,
analyze architectural innovations and optimization strategies, and investigate
applications across low-level vision tasks such as denoising and
super-resolution. Additionally, we explore the synergies between diffusionbased
video generation and related domains, including video representation learning,
question answering, and retrieval. Compared to the existing surveys (Lei et
al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which
focus on specific aspects of video generation, such as human video synthesis
(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on
diffusion-based approaches with a special section for evaluation metrics,
industry solutions, and training engineering techniques in video generation.
This survey serves as a foundational resource for researchers and practitioners
working at the intersection of diffusion models and video generation, providing
insights into both the theoretical frameworks and practical implementations
that drive this rapidly evolving field. A structured list of related works
involved in this survey is also available on
https://github.com/Eyeline-Research/Survey-Video-Diffusion.

</details>

### [91] [Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning](https://arxiv.org/abs/2504.15932)
*Wang Lin,Liyu Jia,Wentao Hu,Kaihang Pan,Zhongqi Yue,Wei Zhao,Jingyuan Chen,Fei Wu,Hanwang Zhang*

Main category: cs.CV

TLDR: 论文提出了一种结合符号推理和强化学习的方法（Phys-AR框架），通过Diffusion Timestep Tokenizer（DDT）和两阶段训练，确保视频生成符合物理规律。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的方法难以处理未见过的物理条件（如速度），因为它们依赖数据驱动的近似方法。为了解决这一问题，论文提出结合符号推理和强化学习来增强视频生成的物理一致性。

Method: 1. 提出DDT，通过学习离散、递归的视觉标记恢复扩散过程中丢失的视觉属性；2. 设计Phys-AR框架，包括两阶段训练：监督微调传递符号知识，强化学习优化推理能力。

Result: 实验结果表明，Phys-AR能够生成物理一致的视频。

Conclusion: 通过结合符号推理和强化学习，Phys-AR框架有效解决了视频生成中的物理一致性问题。

Abstract: Despite recent progress in video generation, producing videos that adhere to
physical laws remains a significant challenge. Traditional diffusion-based
methods struggle to extrapolate to unseen physical conditions (eg, velocity)
due to their reliance on data-driven approximations. To address this, we
propose to integrate symbolic reasoning and reinforcement learning to enforce
physical consistency in video generation. We first introduce the Diffusion
Timestep Tokenizer (DDT), which learns discrete, recursive visual tokens by
recovering visual attributes lost during the diffusion process. The recursive
visual tokens enable symbolic reasoning by a large language model. Based on it,
we propose the Phys-AR framework, which consists of two stages: The first stage
uses supervised fine-tuning to transfer symbolic knowledge, while the second
stage applies reinforcement learning to optimize the model's reasoning
abilities through reward functions based on physical conditions. Our approach
allows the model to dynamically adjust and improve the physical properties of
generated videos, ensuring adherence to physical laws. Experimental results
demonstrate that PhysAR can generate videos that are physically consistent.

</details>

### [92] [FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation](https://arxiv.org/abs/2504.15958)
*Zebin Yao,Lei Ren,Huixing Jiang,Chen Wei,Xiaojie Wang,Ruifan Li,Fangxiang Feng*

Main category: cs.CV

TLDR: FreeGraftor是一种无需训练的框架，通过跨图像特征嫁接解决主题驱动图像生成中保真度与效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在主题保真度和效率之间存在矛盾，调优方法耗时且资源密集，而零样本方法无法保持主题一致性。

Method: FreeGraftor采用语义匹配和位置约束注意力融合，将视觉细节从参考主题转移到生成图像，并结合噪声初始化策略保留几何先验。

Result: 实验表明，FreeGraftor在主题保真度和文本对齐方面显著优于现有方法，且支持多主题生成。

Conclusion: FreeGraftor无需微调或额外训练，提供了一种高效且高保真的主题驱动图像生成解决方案。

Abstract: Subject-driven image generation aims to synthesize novel scenes that
faithfully preserve subject identity from reference images while adhering to
textual guidance, yet existing methods struggle with a critical trade-off
between fidelity and efficiency. Tuning-based approaches rely on time-consuming
and resource-intensive subject-specific optimization, while zero-shot methods
fail to maintain adequate subject consistency. In this work, we propose
FreeGraftor, a training-free framework that addresses these limitations through
cross-image feature grafting. Specifically, FreeGraftor employs semantic
matching and position-constrained attention fusion to transfer visual details
from reference subjects to the generated image. Additionally, our framework
incorporates a novel noise initialization strategy to preserve geometry priors
of reference subjects for robust feature matching. Extensive qualitative and
quantitative experiments demonstrate that our method enables precise subject
identity transfer while maintaining text-aligned scene synthesis. Without
requiring model fine-tuning or additional training, FreeGraftor significantly
outperforms existing zero-shot and training-free approaches in both subject
fidelity and text alignment. Furthermore, our framework can seamlessly extend
to multi-subject generation, making it practical for real-world deployment. Our
code is available at https://github.com/Nihukat/FreeGraftor.

</details>

### [93] [Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications](https://arxiv.org/abs/2504.15991)
*Leonardo Olivi,Edoardo Santero Mormile,Enzo Tartaglione*

Main category: cs.CV

TLDR: 该论文探讨了在月球和火星地形中，通过适配器实现高效迁移学习用于岩石分割的可行性，提出了减少带宽和内存需求的方法。


<details>
  <summary>Details</summary>
Motivation: 解决地外环境中标记数据稀缺的问题，推动深度学习在地外探索中的应用。

Method: 采用适配器策略，结合预训练模型，提出层融合和适配器排名两种内存节省方法。

Result: 适配器策略成功减少了带宽和内存需求，同时评估了任务性能、内存和计算资源的权衡。

Conclusion: 研究为地外设备的高效迁移学习提供了新思路，并指出了未来研究方向。

Abstract: In recent years, the application of Deep Learning techniques has shown
remarkable success in various computer vision tasks, paving the way for their
deployment in extraterrestrial exploration. Transfer learning has emerged as a
powerful strategy for addressing the scarcity of labeled data in these novel
environments. This paper represents one of the first efforts in evaluating the
feasibility of employing adapters toward efficient transfer learning for rock
segmentation in extraterrestrial landscapes, mainly focusing on lunar and
martian terrains. Our work suggests that the use of adapters, strategically
integrated into a pre-trained backbone model, can be successful in reducing
both bandwidth and memory requirements for the target extraterrestrial device.
In this study, we considered two memory-saving strategies: layer fusion (to
reduce to zero the inference overhead) and an ``adapter ranking'' (to also
reduce the transmission cost). Finally, we evaluate these results in terms of
task performance, memory, and computation on embedded devices, evidencing
trade-offs that open the road to more research in the field.

</details>

### [94] [MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment](https://arxiv.org/abs/2504.16003)
*Yachun Mi,Yu Li,Weicheng Meng,Chaofeng Chen,Chen Hui,Shaohui Liu*

Main category: cs.CV

TLDR: MVQA结合Mamba模型和USDS采样方法，高效实现视频质量评估，性能接近SOTA，速度提升2倍，GPU内存仅需1/5。


<details>
  <summary>Details</summary>
Motivation: 长时长高清视频的增长使得高效视频质量评估（VQA）成为关键挑战，现有方法难以平衡效率与性能。

Method: 提出MVQA模型，基于Mamba结构，结合USDS采样方法（语义和失真采样融合），通过预定义掩码减少计算负担。

Result: MVQA性能接近SOTA方法，速度提升2倍，GPU内存仅需1/5。

Conclusion: MVQA和USDS为高效VQA提供了新思路，平衡了性能与效率。

Abstract: The rapid growth of long-duration, high-definition videos has made efficient
video quality assessment (VQA) a critical challenge. Existing research
typically tackles this problem through two main strategies: reducing model
parameters and resampling inputs. However, light-weight Convolution Neural
Networks (CNN) and Transformers often struggle to balance efficiency with high
performance due to the requirement of long-range modeling capabilities.
Recently, the state-space model, particularly Mamba, has emerged as a promising
alternative, offering linear complexity with respect to sequence length.
Meanwhile, efficient VQA heavily depends on resampling long sequences to
minimize computational costs, yet current resampling methods are often weak in
preserving essential semantic information. In this work, we present MVQA, a
Mamba-based model designed for efficient VQA along with a novel Unified
Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch
sampling from low-resolution videos and distortion patch sampling from
original-resolution videos. The former captures semantically dense regions,
while the latter retains critical distortion details. To prevent computation
increase from dual inputs, we propose a fusion mechanism using pre-defined
masks, enabling a unified sampling strategy that captures both semantic and
quality information without additional computational burden. Experiments show
that the proposed MVQA, equipped with USDS, achieve comparable performance to
state-of-the-art methods while being $2\times$ as fast and requiring only $1/5$
GPU memory.

</details>

### [95] [Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework](https://arxiv.org/abs/2504.16016)
*Xinyuan Song,Yangfan He,Sida Li,Jianhui Wang,Hongyang He,Xinhang Yuan,Ruoyu Wang,Jiaqi Chen,Keqin Li,Kuan Lu,Menghao Huo,Binxu Li,Pei Liu*

Main category: cs.CV

TLDR: 论文提出了一种通用的理论框架，用于在DDIM模型中通过适配器保持帧一致性，并证明了时序一致性目标的可微性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 增强扩散模型在视频编辑任务中的帧间一致性，同时减少训练成本。

Method: 插入小型可学习模块到预训练扩散模型中，结合提示学习和时序一致性损失。

Result: 证明了时序一致性目标的可微性、梯度的Lipschitz边界，以及梯度下降的收敛性。

Conclusion: 理论发现支持了基于适配器的扩散视频编辑方法的可靠性，并为视频生成任务提供了理论依据。

Abstract: Adapter-based methods are commonly used to enhance model performance with
minimal additional complexity, especially in video editing tasks that require
frame-to-frame consistency. By inserting small, learnable modules into
pretrained diffusion models, these adapters can maintain temporal coherence
without extensive retraining. Approaches that incorporate prompt learning with
both shared and frame-specific tokens are particularly effective in preserving
continuity across frames at low training cost. In this work, we want to provide
a general theoretical framework for adapters that maintain frame consistency in
DDIM-based models under a temporal consistency loss. First, we prove that the
temporal consistency objective is differentiable under bounded feature norms,
and we establish a Lipschitz bound on its gradient. Second, we show that
gradient descent on this objective decreases the loss monotonically and
converges to a local minimum if the learning rate is within an appropriate
range. Finally, we analyze the stability of modules in the DDIM inversion
procedure, showing that the associated error remains controlled. These
theoretical findings will reinforce the reliability of diffusion-based video
editing methods that rely on adapter strategies and provide theoretical
insights in video generation tasks.

</details>

### [96] [PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning](https://arxiv.org/abs/2504.16023)
*Song Wang,Xiaolu Liu,Lingdong Kong,Jianyun Xu,Chunyong Hu,Gongfan Fang,Wentong Li,Jianke Zhu,Xinchao Wang*

Main category: cs.CV

TLDR: PointLoRA是一种结合低秩适应（LoRA）和多尺度令牌选择的高效点云模型微调方法，显著减少可调参数数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型复杂度增加，完全微调需要大量计算和存储资源，而现有参数高效微调方法通常依赖复杂机制，增加了可调参数。

Method: 在点云变换器中嵌入LoRA层以减少可调参数，并结合多尺度令牌选择提取关键局部信息作为下游微调的提示。

Result: 在多个预训练模型和公开数据集上，仅需3.43%的可调参数即达到竞争性性能。

Conclusion: PointLoRA是一种简单高效的方法，适用于资源受限的应用场景。

Abstract: Self-supervised representation learning for point cloud has demonstrated
effectiveness in improving pre-trained model performance across diverse tasks.
However, as pre-trained models grow in complexity, fully fine-tuning them for
downstream applications demands substantial computational and storage
resources. Parameter-efficient fine-tuning (PEFT) methods offer a promising
solution to mitigate these resource requirements, yet most current approaches
rely on complex adapter and prompt mechanisms that increase tunable parameters.
In this paper, we propose PointLoRA, a simple yet effective method that
combines low-rank adaptation (LoRA) with multi-scale token selection to
efficiently fine-tune point cloud models. Our approach embeds LoRA layers
within the most parameter-intensive components of point cloud transformers,
reducing the need for tunable parameters while enhancing global feature
capture. Additionally, multi-scale token selection extracts critical local
information to serve as prompts for downstream fine-tuning, effectively
complementing the global context captured by LoRA. The experimental results
across various pre-trained models and three challenging public datasets
demonstrate that our approach achieves competitive performance with only 3.43%
of the trainable parameters, making it highly effective for
resource-constrained applications. Source code is available at:
https://github.com/songw-zju/PointLoRA.

</details>

### [97] [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030)
*Joya Chen,Ziyun Zeng,Yiqi Lin,Wei Li,Zejun Ma,Mike Zheng Shou*

Main category: cs.CV

TLDR: 本文提出了一种利用自动语音识别（ASR）转录的低成本方法训练视频大语言模型（Video LLMs），通过流式训练方法实现细粒度视觉语言对齐，并发布了相关数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型依赖昂贵的人工标注或专有模型API，限制了大规模训练。本文探索利用ASR转录的低成本替代方案。

Method: 提出流式训练方法，将ASR词与视频帧按时间戳密集交错，并构建数据生产流程生成预训练和微调数据集。

Result: ASR预训练的模型在视频问答和实时视频评论任务中表现优异，甚至超越更大规模的模型。

Conclusion: 该方法展示了低成本ASR数据在大规模视频语言模型训练中的潜力，并实现了广泛通用性。

Abstract: Recent video large language models (Video LLMs) often depend on costly human
annotations or proprietary model APIs (e.g., GPT-4o) to produce training data,
which limits their training at scale. In this paper, we explore large-scale
training for Video LLM with cheap automatic speech recognition (ASR)
transcripts. Specifically, we propose a novel streaming training approach that
densely interleaves the ASR words and video frames according to their
timestamps. Compared to previous studies in vision-language representation with
ASR, our method naturally fits the streaming characteristics of ASR, thus
enabling the model to learn temporally-aligned, fine-grained vision-language
modeling. To support the training algorithm, we introduce a data production
pipeline to process YouTube videos and their closed captions (CC, same as ASR),
resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset
for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,
the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general
video QA performance and exhibits a new capability in real-time video
commentary. To evaluate this, we carefully design a new LiveSports-3K
benchmark, using LLM-as-a-judge to measure the free-form commentary.
Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B
models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even
working in a real-time mode. Meanwhile, it achieves state-of-the-art results at
the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,
demonstrating the broad generalizability of our approach. All resources of this
paper have been released at https://showlab.github.io/livecc.

</details>

### [98] [Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis](https://arxiv.org/abs/2504.16047)
*Frank Li,Hari Trivedi,Bardia Khosravi,Theo Dapamede,Mohammadreza Chavoshi,Abdulhameed Dere,Rohan Satya Isaac,Aawez Mansuri,Janice Newsome,Saptarshi Purkayastha,Judy Gichoya*

Main category: cs.CV

TLDR: 该研究评估了三种视觉语言基础模型（RAD-DINO、CheXagent和BiomedCLIP）在放射学任务中的表现，发现预训练方法对下游任务性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在医学AI应用中的潜力，特别是在放射学任务中捕捉细粒度成像特征的能力。

Method: 评估三种模型在胸部X光片的分类、分割和回归任务（气胸和心脏肥大）中的表现，并设计了一个结合全局和局部特征的自定义分割模型。

Result: RAD-DINO在分割任务中表现最佳，CheXagent在分类任务中表现优异，BiomedCLIP表现不一致。自定义分割模型显著提升了性能。

Conclusion: 预训练方法对任务性能有显著影响，无文本监督的模型更适合细粒度分割任务，而文本监督模型在分类和可解释性上更有优势。

Abstract: Foundation models, trained on vast amounts of data using self-supervised
techniques, have emerged as a promising frontier for advancing artificial
intelligence (AI) applications in medicine. This study evaluates three
different vision-language foundation models (RAD-DINO, CheXagent, and
BiomedCLIP) on their ability to capture fine-grained imaging features for
radiology tasks. The models were assessed across classification, segmentation,
and regression tasks for pneumothorax and cardiomegaly on chest radiographs.
Self-supervised RAD-DINO consistently excelled in segmentation tasks, while
text-supervised CheXagent demonstrated superior classification performance.
BiomedCLIP showed inconsistent performance across tasks. A custom segmentation
model that integrates global and local features substantially improved
performance for all foundation models, particularly for challenging
pneumothorax segmentation. The findings highlight that pre-training methodology
significantly influences model performance on specific downstream tasks. For
fine-grained segmentation tasks, models trained without text supervision
performed better, while text-supervised models offered advantages in
classification and interpretability. These insights provide guidance for
selecting foundation models based on specific clinical applications in
radiology.

</details>

### [99] [Vision language models are unreliable at trivial spatial cognition](https://arxiv.org/abs/2504.16061)
*Sangeet Khemlani,Tyler Tran,Nathaniel Gyory,Anthony M. Harrison,Wallace E. Lawson,Ravenna Thielstrom,Hunter Thompson,Taaren Singh,J. Gregory Trafton*

Main category: cs.CV

TLDR: 论文研究了视觉语言模型（VLMs）在空间认知任务中的可靠性，发现其性能受提示词微小变化的影响，揭示了其在空间关系推理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 测试VLMs在简单空间认知任务（如判断物体相对位置）中的可靠性，以评估其在实际应用中的表现。

Method: 开发了一个基准数据集TableTest，包含3D场景图像，用于评估当前最先进的VLMs。

Result: VLMs的性能会因提示词的微小变化而下降，表明其在空间关系推理中存在局限性。

Conclusion: 研究揭示了VLMs在空间关系推理中的不足，并提出了改进图像描述语料库以优化训练和测试的机会。

Abstract: Vision language models (VLMs) are designed to extract relevant visuospatial
information from images. Some research suggests that VLMs can exhibit humanlike
scene understanding, while other investigations reveal difficulties in their
ability to process relational information. To achieve widespread applicability,
VLMs must perform reliably, yielding comparable competence across a wide
variety of related tasks. We sought to test how reliable these architectures
are at engaging in trivial spatial cognition, e.g., recognizing whether one
object is left of another in an uncluttered scene. We developed a benchmark
dataset -- TableTest -- whose images depict 3D scenes of objects arranged on a
table, and used it to evaluate state-of-the-art VLMs. Results show that
performance could be degraded by minor variations of prompts that use logically
equivalent descriptions. These analyses suggest limitations in how VLMs may
reason about spatial relations in real-world applications. They also reveal
novel opportunities for bolstering image caption corpora for more efficient
training and testing.

</details>

### [100] [Boosting Generative Image Modeling via Joint Image-Feature Synthesis](https://arxiv.org/abs/2504.16064)
*Theodoros Kouzelis,Efstathios Karypidis,Ioannis Kakogeorgiou,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CV

TLDR: 论文提出了一种新的生成图像建模框架，通过结合低层次图像潜在表示和高层次语义特征，显著提升了生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型（LDMs）在高质量图像生成中占主导地位，但将表示学习与生成建模结合仍具挑战性。

Method: 利用扩散模型联合建模变分自编码器的低层次图像潜在表示和预训练自监督编码器（如DINO）的高层次语义特征。

Result: 在条件和非条件设置下，该方法显著提升了图像质量和训练收敛速度。

Conclusion: 该方法为表示感知生成建模开辟了新方向，简化了训练并解锁了新的推理策略——表示引导。

Abstract: Latent diffusion models (LDMs) dominate high-quality image generation, yet
integrating representation learning with generative modeling remains a
challenge. We introduce a novel generative image modeling framework that
seamlessly bridges this gap by leveraging a diffusion model to jointly model
low-level image latents (from a variational autoencoder) and high-level
semantic features (from a pretrained self-supervised encoder like DINO). Our
latent-semantic diffusion approach learns to generate coherent image-feature
pairs from pure noise, significantly enhancing both generative quality and
training efficiency, all while requiring only minimal modifications to standard
Diffusion Transformer architectures. By eliminating the need for complex
distillation objectives, our unified design simplifies training and unlocks a
powerful new inference strategy: Representation Guidance, which leverages
learned semantics to steer and refine image generation. Evaluated in both
conditional and unconditional settings, our method delivers substantial
improvements in image quality and training convergence speed, establishing a
new direction for representation-aware generative modeling.

</details>

### [101] [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072)
*Long Lian,Yifan Ding,Yunhao Ge,Sifei Liu,Hanzi Mao,Boyi Li,Marco Pavone,Ming-Yu Liu,Trevor Darrell,Adam Yala,Yin Cui*

Main category: cs.CV

TLDR: DAM模型通过焦点提示和局部视觉骨干网络实现高分辨率区域描述，结合半监督学习数据管道解决数据稀缺问题，并在多个基准测试中取得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在图像和视频中特定区域生成详细准确描述的挑战。

Method: 提出DAM模型，采用焦点提示和局部视觉骨干网络，结合半监督学习数据管道（DLC-SDP）扩展数据集。

Result: DAM在7个基准测试中取得最优表现，涵盖关键词、短语和多句子级别的区域描述任务。

Conclusion: DAM通过创新方法解决了局部描述问题，并在多个任务中验证了其有效性。

Abstract: Generating detailed and accurate descriptions for specific regions in images
and videos remains a fundamental challenge for vision-language models. We
introduce the Describe Anything Model (DAM), a model designed for detailed
localized captioning (DLC). DAM preserves both local details and global context
through two key innovations: a focal prompt, which ensures high-resolution
encoding of targeted regions, and a localized vision backbone, which integrates
precise localization with its broader context. To tackle the scarcity of
high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data
Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and
expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark
designed to evaluate DLC without relying on reference captions. DAM sets new
state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and
detailed multi-sentence localized image and video captioning.

</details>

### [102] [From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning](https://arxiv.org/abs/2504.16080)
*Le Zhuo,Liangbing Zhao,Sayak Paul,Yue Liao,Renrui Zhang,Yi Xin,Peng Gao,Mohamed Elhoseiny,Hongsheng Li*

Main category: cs.CV

TLDR: ReflectionFlow是一个推理时框架，通过噪声级、提示级和反射级三个互补的缩放轴，帮助扩散模型迭代优化输出。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在复杂场景和细粒度细节上表现不佳，受大型语言模型自我反思能力的启发，提出改进方法。

Method: 引入ReflectionFlow框架，构建GenRef数据集，通过联合建模多模态输入进行反射调优。

Result: ReflectionFlow显著优于简单的噪声级缩放方法，提供高质量图像合成的可扩展解决方案。

Conclusion: ReflectionFlow为复杂任务下的高质量图像合成提供了高效且可扩展的方法。

Abstract: Recent text-to-image diffusion models achieve impressive visual quality
through extensive scaling of training data and model parameters, yet they often
struggle with complex scenes and fine-grained details. Inspired by the
self-reflection capabilities emergent in large language models, we propose
ReflectionFlow, an inference-time framework enabling diffusion models to
iteratively reflect upon and refine their outputs. ReflectionFlow introduces
three complementary inference-time scaling axes: (1) noise-level scaling to
optimize latent initialization; (2) prompt-level scaling for precise semantic
guidance; and most notably, (3) reflection-level scaling, which explicitly
provides actionable reflections to iteratively assess and correct previous
generations. To facilitate reflection-level scaling, we construct GenRef, a
large-scale dataset comprising 1 million triplets, each containing a
reflection, a flawed image, and an enhanced image. Leveraging this dataset, we
efficiently perform reflection tuning on state-of-the-art diffusion
transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified
framework. Experimental results show that ReflectionFlow significantly
outperforms naive noise-level scaling methods, offering a scalable and
compute-efficient solution toward higher-quality image synthesis on challenging
tasks.

</details>

### [103] [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082)
*Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TLDR: MR. Video是一个基于MapReduce原则的长视频理解框架，通过独立感知短片段（Map）和联合聚合信息（Reduce）提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有序列到序列视觉语言模型（VLMs）和视频代理在长视频理解中的局限性，如上下文长度限制和关键片段选择的复杂性。

Method: 采用两阶段MapReduce：1) 字幕生成（Map生成短片段字幕，Reduce标准化共享名称）；2) 分析（Map提取相关信息，Reduce整合答案）。

Result: 在LVBench上比现有VLMs和视频代理准确率提升超过10%。

Conclusion: MapReduce原则在长视频理解中简单有效，适用于VLMs和视频代理，并通过LLM代理验证其有效性。

Abstract: We propose MR. Video, an agentic long video understanding framework that
demonstrates the simple yet effective MapReduce principle for processing long
videos: (1) Map: independently and densely perceiving short video clips, and
(2) Reduce: jointly aggregating information from all clips. Compared with
sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed
short video perception without being limited by context length. Compared with
existing video agents that typically rely on sequential key segment selection,
the Map operation enables simpler and more scalable sequence parallel
perception of short video segments. Its Reduce step allows for more
comprehensive context aggregation and reasoning, surpassing explicit key
segment retrieval. This MapReduce principle is applicable to both VLMs and
video agents, and we use LLM agents to validate its effectiveness.
  In practice, MR. Video employs two MapReduce stages: (A) Captioning:
generating captions for short video clips (map), then standardizing repeated
characters and objects into shared names (reduce); (B) Analysis: for each user
question, analyzing relevant information from individual short videos (map),
and integrating them into a final answer (reduce). MR. Video achieves over 10%
accuracy improvement on the challenging LVBench compared to state-of-the-art
VLMs and video agents.
  Code is available at: https://github.com/ziqipang/MR-Video

</details>

### [104] [MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](https://arxiv.org/abs/2504.16083)
*Yucheng Li,Huiqiang Jiang,Chengruidong Zhang,Qianhui Wu,Xufang Luo,Surin Ahn,Amir H. Abdi,Dongsheng Li,Jianfeng Gao,Yuqing Yang,Lili Qiu*

Main category: cs.CV

TLDR: MMInference通过动态稀疏注意力方法加速长上下文多模态输入的预填充阶段，提升Vision Language Models的效率。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文多模态输入中二次注意力复杂度对实际部署的阻碍。

Method: 引入动态稀疏注意力方法MMInference，利用视频输入的时空局部性（Grid模式）和模态边界处理，动态构建稀疏分布。

Result: 在1M tokens下加速预填充阶段达8.3倍，同时保持准确性。

Conclusion: MMInference无需修改模型即可无缝集成到现有VLM流程中，显著提升效率。

Abstract: The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
sparse attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique sparse pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different sparse distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal sparse patterns for each head, MMInference constructs the sparse
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient sparse computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [105] [Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence](https://arxiv.org/abs/2504.15970)
*Baichuan Zeng*

Main category: cs.HC

TLDR: 本文综述了扩展现实（XR）技术的发展，包括硬件和软件的演进，分析了当前最先进的XR产品，并探讨了未来方向，如多模态AI和物联网驱动的数字孪生技术。


<details>
  <summary>Details</summary>
Motivation: XR技术作为连接物理和虚拟世界的桥梁，具有广泛的应用潜力，未来将无处不在。本文旨在通过综述其发展历程和现状，为未来XR技术的创新提供方向。

Method: 通过分析XR的硬件（如显示器、传感器）和软件（如视觉任务、用户界面）框架，对比和评估当前最先进的XR产品性能。

Result: 研究发现，商业XR设备在空间智能方面仍需提升，未来需结合多模态AI和数字孪生技术以实现自适应XR系统。

Conclusion: 未来XR技术应通过空间智能和多模态AI的结合，创造更真实的数字空间，成为人机交互的新前沿。

Abstract: Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality
(VR) and Mixed Reality (MR), is a transformative technology bridging the
physical and virtual world and it has diverse potential which will be
ubiquitous in the future. This review examines XR's evolution through
foundational framework - hardware ranging from monitors to sensors and software
ranging from visual tasks to user interface; highlights state of the art (SOTA)
XR products with the comparison and analysis of performance based on their
foundational framework; discusses how commercial XR devices can support the
demand of high-quality performance focusing on spatial intelligence. For future
directions, attention should be given to the integration of multi-modal AI and
IoT-driven digital twins to enable adaptive XR systems. With the concept of
spatial intelligence, future XR should establish a new digital space with
realistic experience that benefits humanity. This review underscores the
pivotal role of AI in unlocking XR as the next frontier in human-computer
interaction.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [106] [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
*Jiayi Pan,Xiuyu Li,Long Lian,Charlie Snell,Yifei Zhou,Adam Yala,Trevor Darrell,Kurt Keutzer,Alane Suhr*

Main category: cs.AI

TLDR: 论文提出了一种名为自适应并行推理（APR）的新框架，通过结合串行和并行计算优化语言模型的推理能力，解决了现有方法的延迟和冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法（如串行链式思维和并行自一致性）存在输出过长、延迟高或协调不足的问题，限制了语言模型的推理性能。

Method: APR框架通过自适应多线程推理（使用spawn()和join()操作）和端到端强化学习策略，优化推理线程以提高任务成功率。

Result: 实验表明，APR在相同上下文窗口下性能更高（83.4% vs. 60.0%），计算量增加时扩展性更好（80.1% vs. 66.6%），且在相同延迟下准确率更高（75.2% vs. 57.3%）。

Conclusion: APR为语言模型通过自适应计算分配自主优化推理过程迈出了重要一步。

Abstract: Scaling inference-time computation has substantially improved the reasoning
capabilities of language models. However, existing methods have significant
limitations: serialized chain-of-thought approaches generate overly long
outputs, leading to increased latency and exhausted context windows, while
parallel methods such as self-consistency suffer from insufficient
coordination, resulting in redundant computations and limited performance
gains. To address these shortcomings, we propose Adaptive Parallel Reasoning
(APR), a novel reasoning framework that enables language models to orchestrate
both serialized and parallel computations end-to-end. APR generalizes existing
reasoning methods by enabling adaptive multi-threaded inference using spawn()
and join() operations. A key innovation is our end-to-end reinforcement
learning strategy, optimizing both parent and child inference threads to
enhance task success rate without requiring predefined reasoning structures.
Experiments on the Countdown reasoning task demonstrate significant benefits of
APR: (1) higher performance within the same context window (83.4% vs. 60.0% at
4k context); (2) superior scalability with increased computation (80.1% vs.
66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%
vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling
language models to autonomously optimize their reasoning processes through
adaptive allocation of computation.

</details>

### [107] [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
*Daocheng Fu,Zijun Chen,Renqiu Xia,Qi Liu,Yuan Feng,Hongbin Zhou,Renrui Zhang,Shiyang Feng,Peng Gao,Junchi Yan,Botian Shi,Bo Zhang,Yu Qiao*

Main category: cs.AI

TLDR: 论文提出了一种名为TrustGeoGen的数据引擎，用于生成几何问题，并通过形式化验证提供基准，解决了现有几何问题生成中的噪声和自相矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在几何问题解决中缺乏有效的方法和基准，且合成数据常包含噪声和自相矛盾信息。

Method: TrustGeoGen通过多模态对齐生成、形式化验证、自举机制和GeoExplore算法，生成具有模态完整性的几何问题数据集。

Result: 生成的GeoTrust-200K数据集在实验中显示，现有模型的准确率仅为49.17%，且训练后的模型在GeoQA上表现出更好的泛化能力。

Conclusion: TrustGeoGen为几何问题解决提供了可靠的数据集和基准，显著减少了逻辑不一致性。

Abstract: Mathematical geometric problem solving (GPS) often requires effective
integration of multimodal information and verifiable logical coherence. Despite
the fast development of large language models in general problem solving, it
remains unresolved regarding with both methodology and benchmarks, especially
given the fact that exiting synthetic GPS benchmarks are often not
self-verified and contain noise and self-contradicted information due to the
illusion of LLMs. In this paper, we propose a scalable data engine called
TrustGeoGen for problem generation, with formal verification to provide a
principled benchmark, which we believe lays the foundation for the further
development of methods for GPS. The engine synthesizes geometric data through
four key innovations: 1) multimodal-aligned generation of diagrams, textual
descriptions, and stepwise solutions; 2) formal verification ensuring
rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling
complexity escalation via recursive state generation and 4) our devised
GeoExplore series algorithms simultaneously produce multi-solution variants and
self-reflective backtracking traces. By formal logical verification,
TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,
along with GeoTrust-test testset. Experiments reveal the state-of-the-art
models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its
evaluation stringency. Crucially, models trained on GeoTrust achieve OOD
generalization on GeoQA, significantly reducing logical inconsistencies
relative to pseudo-label annotated by OpenAI-o1. Our code is available at
https://github.com/Alpha-Innovator/TrustGeoGen

</details>

### [108] [AGI Is Coming... Right After AI Learns to Play Wordle](https://arxiv.org/abs/2504.15434)
*Sarath Shekkizhar,Romain Cosentino*

Main category: cs.AI

TLDR: 论文研究了OpenAI的计算机用户代理（CUA）在完成纽约时报Wordle游戏任务时的表现，发现其在颜色识别上存在显著差异，成功率仅为5.36%。


<details>
  <summary>Details</summary>
Motivation: 探索多模态代理在简单任务中的表现，揭示当前前沿AI模型的局限性。

Method: 通过让CUA完成Wordle游戏任务，评估其行为和识别能力。

Result: 模型在颜色识别上表现不佳，成功率低，表明简单任务对AI仍是挑战。

Conclusion: 讨论了潜在原因、未来发展方向及改进AI系统的研究路径。

Abstract: This paper investigates multimodal agents, in particular, OpenAI's
Computer-User Agent (CUA), trained to control and complete tasks through a
standard computer interface, similar to humans. We evaluated the agent's
performance on the New York Times Wordle game to elicit model behaviors and
identify shortcomings. Our findings revealed a significant discrepancy in the
model's ability to recognize colors correctly depending on the context. The
model had a $5.36\%$ success rate over several hundred runs across a week of
Wordle. Despite the immense enthusiasm surrounding AI agents and their
potential to usher in Artificial General Intelligence (AGI), our findings
reinforce the fact that even simple tasks present substantial challenges for
today's frontier AI models. We conclude with a discussion of the potential
underlying causes, implications for future development, and research directions
to improve these AI systems.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [109] [SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems](https://arxiv.org/abs/2504.15305)
*Abhishek Tyagi,Charu Gaur*

Main category: cs.RO

TLDR: Veg是一个自主空中监视平台，具备故障容忍能力，集成了视觉SLAM、高级控制架构和嵌入式视觉模块，支持GPS独立导航、实时物体和人脸识别。


<details>
  <summary>Details</summary>
Motivation: 设计一个适用于受限环境的无人机平台，整合实时定位、故障恢复和嵌入式AI功能。

Method: 采用LQR内环和PD外环轨迹控制的级联控制设计，利用ORB-SLAM3进行6自由度定位，支持基于Dijkstra路径规划的导航，并配备实时故障检测与识别系统。

Result: 平台在模拟和实际测试中验证了其性能，能够实现高精度的物体检测和人脸识别，并具备故障恢复能力。

Conclusion: Veg成功整合了实时定位、故障恢复和嵌入式AI功能，适用于复杂环境中的自主监视任务。

Abstract: We present an autonomous aerial surveillance platform, Veg, designed as a
fault-tolerant quadcopter system that integrates visual SLAM for
GPS-independent navigation, advanced control architecture for dynamic
stability, and embedded vision modules for real-time object and face
recognition. The platform features a cascaded control design with an LQR
inner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for
6-DoF localization and loop closure, and supports waypoint-based navigation
through Dijkstra path planning over SLAM-derived maps. A real-time Failure
Detection and Identification (FDI) system detects rotor faults and executes
emergency landing through re-routing. The embedded vision system, based on a
lightweight CNN and PCA, enables onboard object detection and face recognition
with high precision. The drone operates fully onboard using a Raspberry Pi 4
and Arduino Nano, validated through simulations and real-world testing. This
work consolidates real-time localization, fault recovery, and embedded AI on a
single platform suitable for constrained environments.

</details>

### [110] [A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities](https://arxiv.org/abs/2504.15654)
*Md Abdul Baset Sarker,Art Nguyen,Sigmond Kukla,Kevin Fite,Masudul H. Imtiaz*

Main category: cs.RO

TLDR: 本文介绍了一种新型AI视觉辅助儿童假手，专为10-12岁上肢残疾儿童设计，具有仿生外观、多关节功能和轻量化设计，结合3D打印技术和先进机器视觉，提供低成本、可定制解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前肌电假肢存在局限性，尤其是对低收入家庭儿童的可及性和负担能力不足。本文旨在通过AI视觉和嵌入式计算技术，开发一种低成本、高性能的假肢解决方案。

Method: 采用3D打印技术，集成机器视觉、传感和嵌入式计算，通过腕部微型摄像头实现实时物体检测和精确抓取，结合深度学习模型进行物体检测和抓取分类。

Result: 物体检测和抓取分类模型的准确率分别为96%和100%，力预测的平均绝对误差为0.018，实现了高性能和低功耗。

Conclusion: 该假肢通过AI视觉和嵌入式技术，解决了当前假肢的局限性，为儿童提供了一种低成本、高性能的辅助工具。

Abstract: This paper introduces a novel AI vision-enabled pediatric prosthetic hand
designed to assist children aged 10-12 with upper limb disabilities. The
prosthesis features an anthropomorphic appearance, multi-articulating
functionality, and a lightweight design that mimics a natural hand, making it
both accessible and affordable for low-income families. Using 3D printing
technology and integrating advanced machine vision, sensing, and embedded
computing, the prosthetic hand offers a low-cost, customizable solution that
addresses the limitations of current myoelectric prostheses. A micro camera is
interfaced with a low-power FPGA for real-time object detection and assists
with precise grasping. The onboard DL-based object detection and grasp
classification models achieved accuracies of 96% and 100% respectively. In the
force prediction, the mean absolute error was found to be 0.018. The features
of the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted
micro camera for artificial sensing, enabling a wide range of hand-based tasks;
b) real-time object detection and distance estimation for precise grasping; and
c) ultra-low-power operation that delivers high performance within constrained
power and resource limits.

</details>

### [111] [RaSCL: Radar to Satellite Crossview Localization](https://arxiv.org/abs/2504.15899)
*Blerim Abdullai,Tony Wang,Xinyuan Qiao,Florian Shkurti,Timothy D. Barfoot*

Main category: cs.RO

TLDR: 提出了一种不依赖GNSS的全局定位方法，通过将地面雷达图像与高空RGB图像配准，结合里程计和全局位姿优化，实现了高效定位。


<details>
  <summary>Details</summary>
Motivation: GNSS在许多实时自主应用中不可靠、不准确且不足，需要一种替代方案。

Method: 通过地面雷达与高空RGB图像配准，结合里程计和全局位姿优化，提取关键特征进行定位。

Result: 在多种地理条件和机器人平台上验证了方法的有效性，包括无人水面艇和城市/郊区驾驶数据集。

Conclusion: 该方法为GNSS不可靠的场景提供了一种有效的全局定位解决方案。

Abstract: GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous
field applications. In this work, we present a GNSS-free global localization
solution that contains a method of registering imaging radar on the ground with
overhead RGB imagery, with joint optimization of relative poses from odometry
and global poses from our overhead registration. Previous works have used
various combinations of ground sensors and overhead imagery, and different
feature extraction and matching methods. These include various handcrafted and
deep-learning-based methods for extracting features from overhead imagery. Our
work presents insights on extracting essential features from RGB overhead
images for effective global localization against overhead imagery using only
ground radar and a single georeferenced initial guess. We motivate our method
by evaluating it on datasets in diverse geographic conditions and robotic
platforms, including on an Unmanned Surface Vessel (USV) as well as urban and
suburban driving datasets.

</details>

### [112] [Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping](https://arxiv.org/abs/2504.15953)
*Chance J. Hamilton,Alfredo Weitzenfeld*

Main category: cs.RO

TLDR: VPCE模型通过视觉输入模拟位置细胞激活，利用高维外观特征聚类生成空间表征，验证其与生物位置细胞特性的相关性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉地标在空间编码中的核心作用，验证仅凭视觉输入是否足以生成类似生物位置细胞的空间表征。

Method: 提取机器人摄像头图像的高维外观特征，通过聚类定义感受野，基于径向基函数计算激活。

Result: VPCE能区分视觉相似但空间不同的位置，适应环境变化（如墙体增减），生成类似生物位置细胞的激活模式。

Conclusion: 结构化视觉输入无需运动或奖励学习即可支持生物启发的认知地图构建。

Abstract: This paper presents the Visual Place Cell Encoding (VPCE) model, a
biologically inspired computational framework for simulating place cell-like
activation using visual input. Drawing on evidence that visual landmarks play a
central role in spatial encoding, the proposed VPCE model activates visual
place cells by clustering high-dimensional appearance features extracted from
images captured by a robot-mounted camera. Each cluster center defines a
receptive field, and activation is computed based on visual similarity using a
radial basis function. We evaluate whether the resulting activation patterns
correlate with key properties of biological place cells, including spatial
proximity, orientation alignment, and boundary differentiation. Experiments
demonstrate that the VPCE can distinguish between visually similar yet
spatially distinct locations and adapt to environment changes such as the
insertion or removal of walls. These results suggest that structured visual
input, even in the absence of motion cues or reward-driven learning, is
sufficient to generate place-cell-like spatial representations and support
biologically inspired cognitive mapping.

</details>

### [113] [ForesightNav: Learning Scene Imagination for Efficient Exploration](https://arxiv.org/abs/2504.16062)
*Hardik Shah,Jiaxu Xing,Nico Messikommer,Boyang Sun,Marc Pollefeys,Davide Scaramuzza*

Main category: cs.RO

TLDR: ForesightNav是一种受人类想象和推理启发的探索策略，通过预测未探索区域的上下文信息（如占用和语义细节），提升机器人在未知环境中的导航效率。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何利用先验知识在未知环境中导航和探索，以开发具备类似能力的自主机器人。

Method: 提出ForesightNav，为机器人提供预测未探索区域上下文信息的能力，从而选择长期导航目标。

Result: 在Structured3D数据集上验证，展示了准确的占用预测和优越的探索性能，PointNav完成率100%，ObjectNav SPL达67%。

Conclusion: 想象驱动的推理能显著提升自主系统的通用性和探索效率。

Abstract: Understanding how humans leverage prior knowledge to navigate unseen
environments while making exploratory decisions is essential for developing
autonomous robots with similar abilities. In this work, we propose
ForesightNav, a novel exploration strategy inspired by human imagination and
reasoning. Our approach equips robotic agents with the capability to predict
contextual information, such as occupancy and semantic details, for unexplored
regions. These predictions enable the robot to efficiently select meaningful
long-term navigation goals, significantly enhancing exploration in unseen
environments. We validate our imagination-based approach using the Structured3D
dataset, demonstrating accurate occupancy prediction and superior performance
in anticipating unseen scene geometry. Our experiments show that the
imagination module improves exploration efficiency in unseen environments,
achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav
on the Structured3D Validation split. These contributions demonstrate the power
of imagination-driven reasoning for autonomous systems to enhance generalizable
and efficient exploration.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [114] [Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data](https://arxiv.org/abs/2504.15448)
*Yanampally Abhiram Reddy,Siddhi Agarwal,Vikram Parashar,Arshiya Arora*

Main category: econ.GN

TLDR: 本文提出了一种结合NLP和机器学习的实时企业声誉监控系统，用于分析社交媒体上的公众情绪。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体时代，了解公众对企业的情绪对投资者、政策制定者和研究人员至关重要。

Method: 采用混合情感检测框架（VADER和DistilBERT），结合预处理和集成分类方法。

Result: 分析显示不同企业情绪差异显著，如亚马逊（81.2）和三星（45.8）情绪积极，微软（21.7）和沃尔玛（21.9）情绪消极。

Conclusion: 该系统能为利益相关者提供基于全面情绪分析的可操作见解，支持战略决策。

Abstract: In the age of social media, understanding public sentiment toward major
corporations is crucial for investors, policymakers, and researchers. This
paper presents a comprehensive sentiment analysis system tailored for corporate
reputation monitoring, combining Natural Language Processing (NLP) and machine
learning techniques to accurately interpret public opinion in real time. The
methodology integrates a hybrid sentiment detection framework leveraging both
rule-based models (VADER) and transformer-based deep learning models
(DistilBERT), applied to social media data from multiple platforms. The system
begins with robust preprocessing involving noise removal and text
normalization, followed by sentiment classification using an ensemble approach
to ensure both interpretability and contextual accuracy. Results are visualized
through sentiment distribution plots, comparative analyses, and temporal
sentiment trends for enhanced interpretability. Our analysis reveals
significant disparities in public sentiment across major corporations, with
companies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment
scores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment
profiles. These findings demonstrate the utility of our multi-source sentiment
framework in providing actionable insights regarding corporate public
perception, enabling stakeholders to make informed strategic decisions based on
comprehensive sentiment analysis.

</details>

<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [115] [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
*Kun Wang,Guibin Zhang,Zhenhong Zhou,Jiahao Wu,Miao Yu,Shiqian Zhao,Chenlong Yin,Jinhu Fu,Yibo Yan,Hanjun Luo,Liang Lin,Zhihao Xu,Haolang Lu,Xinye Cao,Xinyun Zhou,Weifei Jin,Fanci Meng,Junyuan Mao,Hao Wu,Minghe Wang,Fan Zhang,Junfeng Fang,Chengwei Liu,Yifan Zhang,Qiankun Li,Chongye Guo,Yalan Qin,Yi Ding,Donghai Hong,Jiaming Ji,Xinfeng Li,Yifan Jiang,Dongxia Wang,Yihao Huang,Yufei Guo,Jen-tse Huang,Yanwei Yue,Wenke Huang,Guancheng Wan,Tianlin Li,Lei Bai,Jie Zhang,Qing Guo,Jingyi Wang,Tianlong Chen,Joey Tianyi Zhou,Xiaojun Jia,Weisong Sun,Cong Wu,Jing Chen,Xuming Hu,Yiming Li,Xiao Wang,Ningyu Zhang,Luu Anh Tuan,Guowen Xu,Tianwei Zhang,Xingjun Ma,Xiang Wang,Bo An,Jun Sun,Mohit Bansal,Shirui Pan,Yuval Elovici,Bhavya Kailkhura,Bo Li,Yaodong Yang,Hongwei Li,Wenyuan Xu,Yizhou Sun,Wei Wang,Qing Li,Ke Tang,Yu-Gang Jiang,Felix Juefei-Xu,Hui Xiong,Xiaofeng Wang,Shuicheng Yan,Dacheng Tao,Philip S. Yu,Qingsong Wen,Yang Liu*

Main category: cs.CR

TLDR: 本文提出了“全栈安全”概念，系统性地探讨了大型语言模型（LLM）从训练到商业化的全生命周期安全问题，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在学术和工业界的广泛应用，其安全性和安全性问题日益突出，但现有研究多聚焦于特定阶段，缺乏对全生命周期的系统性分析。

Method: 通过定义LLM的完整生命周期（数据准备、预训练、后训练、部署和商业化），并基于800多篇文献的系统性综述，提出全栈安全框架。

Result: 研究提供了全面的安全视角、广泛的文献支持以及独特的见解，包括数据生成安全、对齐技术、模型编辑和基于LLM的代理系统等研究方向。

Conclusion: 本文为LLM安全研究提供了系统性框架和未来研究方向，对学术界和工业界具有重要指导意义。

Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a
promising pathway toward achieving Artificial General Intelligence for both
academic and industrial communities, owing to their unprecedented performance
across various applications. As LLMs continue to gain prominence in both
research and commercial domains, their security and safety implications have
become a growing concern, not only for researchers and corporations but also
for every nation. Currently, existing surveys on LLM safety primarily focus on
specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning
phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs.
To address this gap, this paper introduces, for the first time, the concept of
"full-stack" safety to systematically consider safety issues throughout the
entire process of LLM training, deployment, and eventual commercialization.
Compared to the off-the-shelf LLM safety surveys, our work demonstrates several
distinctive advantages: (I) Comprehensive Perspective. We define the complete
LLM lifecycle as encompassing data preparation, pre-training, post-training,
deployment and final commercialization. To our knowledge, this represents the
first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive
Literature Support. Our research is grounded in an exhaustive review of over
800+ papers, ensuring comprehensive coverage and systematic organization of
security issues within a more holistic understanding. (III) Unique Insights.
Through systematic literature analysis, we have developed reliable roadmaps and
perspectives for each chapter. Our work identifies promising research
directions, including safety in data generation, alignment techniques, model
editing, and LLM-based agent systems. These insights provide valuable guidance
for researchers pursuing future work in this field.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [116] [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)
*Soham Bonnerjee,Zhen Wei,Yeon,Anna Asch,Sagnik Nandy,Promit Ghosal*

Main category: stat.ML

TLDR: 本文提出了一种差分隐私预训练算法，用于线性注意力头，并首次对线性回归中的上下文学习（ICL）进行了隐私-准确性权衡的理论分析。


<details>
  <summary>Details</summary>
Motivation: 研究上下文学习（ICL）在形式隐私约束下的可行性，填补现有研究的空白。

Method: 提出差分隐私预训练算法，分析线性回归中ICL的隐私-准确性权衡。

Result: 揭示了优化与隐私引入噪声之间的基本矛盾，并证明该方法对训练提示的对抗性扰动具有鲁棒性。

Conclusion: 理论分析和实验验证表明，该方法在隐私保护下仍能有效实现上下文学习。

Abstract: In-context learning (ICL)-the ability of transformer-based models to perform
new tasks from examples provided at inference time-has emerged as a hallmark of
modern language models. While recent works have investigated the mechanisms
underlying ICL, its feasibility under formal privacy constraints remains
largely unexplored. In this paper, we propose a differentially private
pretraining algorithm for linear attention heads and present the first
theoretical analysis of the privacy-accuracy trade-off for ICL in linear
regression. Our results characterize the fundamental tension between
optimization and privacy-induced noise, formally capturing behaviors observed
in private training via iterative methods. Additionally, we show that our
method is robust to adversarial perturbations of training prompts, unlike
standard ridge regression. All theoretical findings are supported by extensive
simulations across diverse settings.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [117] [Enhancing DR Classification with Swin Transformer and Shifted Window Attention](https://arxiv.org/abs/2504.15317)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Zied Bouraoui*

Main category: eess.IV

TLDR: 提出了一种结合预处理和Swin Transformer的方法，用于糖尿病视网膜病变（DR）分类，在Aptos和IDRiD数据集上分别达到89.65%和97.40%的准确率。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球致盲的主要原因，早期检测对治疗至关重要，但现有方法因图像质量、类别不平衡和像素级相似性等问题面临挑战。

Method: 采用图像裁剪、CLAHE增强和针对性数据增强的预处理流程，结合Swin Transformer的层次化令牌处理和窗口注意力机制。

Result: 在Aptos和IDRiD数据集上的准确率分别为89.65%和97.40%，尤其在早期DR检测中表现突出。

Conclusion: 该方法在自动视网膜筛查中具有临床应用潜力，特别是在早期DR检测方面。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide,
underscoring the importance of early detection for effective treatment.
However, automated DR classification remains challenging due to variations in
image quality, class imbalance, and pixel-level similarities that hinder model
training. To address these issues, we propose a robust preprocessing pipeline
incorporating image cropping, Contrast-Limited Adaptive Histogram Equalization
(CLAHE), and targeted data augmentation to improve model generalization and
resilience. Our approach leverages the Swin Transformer, which utilizes
hierarchical token processing and shifted window attention to efficiently
capture fine-grained features while maintaining linear computational
complexity. We validate our method on the Aptos and IDRiD datasets for
multi-class DR classification, achieving accuracy rates of 89.65% and 97.40%,
respectively. These results demonstrate the effectiveness of our model,
particularly in detecting early-stage DR, highlighting its potential for
improving automated retinal screening in clinical settings.

</details>

### [118] [Split-quaternions for perceptual white balance](https://arxiv.org/abs/2504.15481)
*Michel Berthier,Nicoletta Prencipe,Edoardo Provenzi*

Main category: eess.IV

TLDR: 提出了一种基于分裂四元数的感知色适应变换方法，用于白平衡处理。


<details>
  <summary>Details</summary>
Motivation: 受近期开发的量子化颜色感知模型启发，强调该模型中代数结构与分裂四元数子代数之间的联系。

Method: 通过分裂四元数乘法实现色适应变换，并与传统von Kries方法进行定量比较。

Result: 展示了该方法在彩色图像处理中的潜力。

Conclusion: 分裂四元数方法为色适应变换提供了新的可能性。

Abstract: We propose a perceptual chromatic adaptation transform for white balance that
makes use of split-quaternions. The novelty of the present work, which is
motivated by a recently developed quantum-like model of color perception,
consists at stressing the link between the algebraic structures appearing in
this model and a certain sub-algebra of the split-quaternions. We show the
potentiality of this approach for color image processing applications by
proposing a chromatic adaptation transform, implemented via an appropriate use
of the split-quaternion multiplication. Moreover, quantitative comparisons with
the widely used state-of-the art von Kries chromatic adaptation transform are
provided.

</details>

### [119] [VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining](https://arxiv.org/abs/2504.15545)
*Zizhi Chen,Xinyu Zhang,Minghao Han,Yizhou Liu,Ziyun Qian,Weifeng Zhang,Xukun Zhang,Jingwei Wei,Lihua Zhang*

Main category: eess.IV

TLDR: 本文提出了一种基于病理视觉语言大模型（VLM）的虚拟染色方法，通过结合对比学习提示和组织结构锚点，解决了传统虚拟染色中忽略病理知识和物理特性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟染色方法仅实现风格迁移，忽略了病理知识和染色物理特性，导致结果不理想。本文旨在利用VLM的丰富知识提升虚拟染色的准确性和实用性。

Method: 引入病理VLM作为辅助工具，结合对比学习提示、组织结构锚点和染色特异性锚点，开发基于VLM约束的数据增强方法。

Result: 在公开多域非配对染色数据集上验证，生成高真实感图像，并提升下游任务（如肾小球检测和分割）的准确性。

Conclusion: 该方法通过整合病理知识和物理特性，显著提升了虚拟染色的质量和下游任务的性能。

Abstract: In histopathology, tissue sections are typically stained using common H&E
staining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific
tissue structures. The rapid advancement of deep learning offers an effective
solution for generating virtually stained images, significantly reducing the
time and labor costs associated with traditional histochemical staining.
However, a new challenge arises in separating the fundamental visual
characteristics of tissue sections from the visual differences induced by
staining agents. Additionally, virtual staining often overlooks essential
pathological knowledge and the physical properties of staining, resulting in
only style-level transfer. To address these issues, we introduce, for the first
time in virtual staining tasks, a pathological vision-language large model
(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,
foundational concept anchors for tissue sections, and staining-specific concept
anchors to leverage the extensive knowledge of the pathological VLM. This
approach is designed to describe, frame, and enhance the direction of virtual
staining. Furthermore, we have developed a data augmentation method based on
the constraints of the VLM. This method utilizes the VLM's powerful image
interpretation capabilities to further integrate image style and structural
information, proving beneficial in high-precision pathological diagnostics.
Extensive evaluations on publicly available multi-domain unpaired staining
datasets demonstrate that our method can generate highly realistic images and
enhance the accuracy of downstream tasks, such as glomerular detection and
segmentation. Our code is available at:
https://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR

</details>

### [120] [RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2504.15649)
*Biao Wu,Diankai Zhang,Shaoli Liu,Si Gao,Chengjian Zheng,Ning Wang*

Main category: eess.IV

TLDR: 提出了一种名为RepNet-VSR的可重参数化架构，用于实时4倍视频超分辨率，在资源受限的边缘设备上实现了高质量与高效部署的平衡。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率在资源受限的边缘设备上部署时面临计算密集和实时性挑战，需要一种高效且高质量的方法。

Method: 采用可重参数化架构（RepNet-VSR），优化计算效率，适用于实时处理。

Result: 在REDS验证集上，处理180p到720p帧时达到27.79 dB PSNR，每10帧耗时103 ms，优于之前的冠军算法。

Conclusion: RepNet-VSR在视频超分辨率任务中实现了高质量与高效部署的平衡，适用于实时移动视频处理。

Abstract: As a fundamental challenge in visual computing, video super-resolution (VSR)
focuses on reconstructing highdefinition video sequences from their degraded
lowresolution counterparts. While deep convolutional neural networks have
demonstrated state-of-the-art performance in spatial-temporal super-resolution
tasks, their computationally intensive nature poses significant deployment
challenges for resource-constrained edge devices, particularly in real-time
mobile video processing scenarios where power efficiency and latency
constraints coexist. In this work, we propose a Reparameterizable Architecture
for High Fidelity Video Super Resolution method, named RepNet-VSR, for
real-time 4x video super-resolution. On the REDS validation set, the proposed
model achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per
10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an
excellent balance between restoration quality and deployment efficiency. The
proposed method scores higher than the previous champion algorithm of MAI video
super-resolution challenge.

</details>

### [121] [Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg](https://arxiv.org/abs/2504.15667)
*Jingchen Zou,Jianqiang Li,Gabriel Jimenez,Qing Zhao,Daniel Racoceanu,Matias Cosarinsky,Enzo Ferrante,Guanghui Fu*

Main category: eess.IV

TLDR: 提出了一种名为SPE的框架，用于在无标注数据上评估医学图像分割模型的性能，无需实际标注即可可靠估计模型表现。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中，为所有数据标注实际标签不切实际，导致模型在未见数据上的性能难以评估。

Method: 提出SPE框架，适用于多种评估指标和模型架构，通过实验验证其有效性。

Result: 在六个公开数据集上测试，SPE与真实Dice分数的相关性高达0.956±0.046，MAE低至0.025±0.019。

Conclusion: SPE框架无需标注即可可靠估计模型性能，适用于实际应用，且不影响训练效率。

Abstract: The performance of medical image segmentation models is usually evaluated
using metrics like the Dice score and Hausdorff distance, which compare
predicted masks to ground truth annotations. However, when applying the model
to unseen data, such as in clinical settings, it is often impractical to
annotate all the data, making the model's performance uncertain. To address
this challenge, we propose the Segmentation Performance Evaluator (SPE), a
framework for estimating segmentation models' performance on unlabeled data.
This framework is adaptable to various evaluation metrics and model
architectures. Experiments on six publicly available datasets across six
evaluation metrics including pixel-based metrics such as Dice score and
distance-based metrics like HD95, demonstrated the versatility and
effectiveness of our approach, achieving a high correlation (0.956$\pm$0.046)
and low MAE (0.025$\pm$0.019) compare with real Dice score on the independent
test set. These results highlight its ability to reliably estimate model
performance without requiring annotations. The SPE framework integrates
seamlessly into any model training process without adding training overhead,
enabling performance estimation and facilitating the real-world application of
medical image segmentation algorithms. The source code is publicly available

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [122] [HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning](https://arxiv.org/abs/2504.15323)
*Donggyun Kim,Chanwoo Kim,Seunghoon Hong*

Main category: cs.LG

TLDR: 提出了一种无需计算梯度的测试时适应方法，通过模拟梯度下降实现高效的小样本学习，显著降低了计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 解决测试时微调在小样本学习中因多次反向传播导致的高计算和内存成本问题。

Method: 将梯度下降建模为ODE的欧拉离散化，训练辅助网络预测任务条件漂移，仅需少量前向传播即可完成适应。

Result: 在跨域小样本分类任务中，性能显著优于未微调基线，计算时间和内存成本仅为标准微调的0.02%和6%。

Conclusion: 该方法在直接迁移和完全微调之间找到了实用的平衡点，适用于实时或低资源场景。

Abstract: While test-time fine-tuning is beneficial in few-shot learning, the need for
multiple backpropagation steps can be prohibitively expensive in real-time or
low-resource scenarios. To address this limitation, we propose an approach that
emulates gradient descent without computing gradients, enabling efficient
test-time adaptation. Specifically, we formulate gradient descent as an Euler
discretization of an ordinary differential equation (ODE) and train an
auxiliary network to predict the task-conditional drift using only the few-shot
support set. The adaptation then reduces to a simple numerical integration
(e.g., via the Euler method), which requires only a few forward passes of the
auxiliary network -- no gradients or forward passes of the target model are
needed. In experiments on cross-domain few-shot classification using the
Meta-Dataset and CDFSL benchmarks, our method significantly improves
out-of-domain performance over the non-fine-tuned baseline while incurring only
6\% of the memory cost and 0.02\% of the computation time of standard
fine-tuning, thus establishing a practical middle ground between direct
transfer and fully fine-tuned approaches.

</details>

### [123] [Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks](https://arxiv.org/abs/2504.15479)
*Jeremy Goldwasser,Giles Hooker*

Main category: cs.LG

TLDR: 该论文提出了一种新的、易于实现的生成反事实图像的框架，适用于计算机视觉模型，避免了传统梯度方法生成对抗样本的问题。


<details>
  <summary>Details</summary>
Motivation: 反事实解释是理解机器学习预测的流行方法，但在计算机视觉模型中，传统方法容易生成对抗样本，导致解释不可靠。

Method: 提出了一种名为“反事实攻击”的方法，通过在低维流形上对图像表示进行攻击，并结合辅助数据集生成特征归因。

Result: 该方法在MNIST和CelebA数据集上展示了有效性，能够高效生成全局反事实解释。

Conclusion: 该方法不仅灵活适应生成模型的进步，还能高效提供反事实解释，为计算机视觉模型的解释性提供了新思路。

Abstract: Counterfactuals are a popular framework for interpreting machine learning
predictions. These what if explanations are notoriously challenging to create
for computer vision models: standard gradient-based methods are prone to
produce adversarial examples, in which imperceptible modifications to image
pixels provoke large changes in predictions. We introduce a new,
easy-to-implement framework for counterfactual images that can flexibly adapt
to contemporary advances in generative modeling. Our method, Counterfactual
Attacks, resembles an adversarial attack on the representation of the image
along a low-dimensional manifold. In addition, given an auxiliary dataset of
image descriptors, we show how to accompany counterfactuals with feature
attribution that quantify the changes between the original and counterfactual
images. These importance scores can be aggregated into global counterfactual
explanations that highlight the overall features driving model predictions.
While this unification is possible for any counterfactual method, it has
particular computational efficiency for ours. We demonstrate the efficacy of
our approach with the MNIST and CelebA datasets.

</details>

### [124] [Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis](https://arxiv.org/abs/2504.15562)
*Dip Roy*

Main category: cs.LG

TLDR: 本文提出了一种基于贝叶斯变分自编码器（VAE）和多头注意力机制的模型，用于脑部MRI中的异常检测，结合了认知和随机不确定性估计，提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 医疗影像中的异常检测对神经系统疾病的诊断至关重要，但传统确定性方法难以捕捉任务中的不确定性。

Method: 使用贝叶斯变分自编码器（VAE）和多头注意力机制，结合认知和随机不确定性估计。

Result: 在BraTS2020数据集上测试，ROC AUC和PR AUC均为0.83。

Conclusion: 建模不确定性是异常检测的关键，提升了性能和可解释性，并为临床决策提供了信心估计。

Abstract: In medical imaging, anomaly detection is a vital element of healthcare
diagnostics, especially for neurological conditions which can be
life-threatening. Conventional deterministic methods often fall short when it
comes to capturing the inherent uncertainty of anomaly detection tasks. This
paper introduces a Bayesian Variational Autoencoder (VAE) equipped with
multi-head attention mechanisms for detecting anomalies in brain magnetic
resonance imaging (MRI). For the purpose of improving anomaly detection
performance, we incorporate both epistemic and aleatoric uncertainty estimation
through Bayesian inference. The model was tested on the BraTS2020 dataset, and
the findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper
suggests that modeling uncertainty is an essential component of anomaly
detection, enhancing both performance and interpretability and providing
confidence estimates, as well as anomaly predictions, for clinicians to
leverage in making medical decisions.

</details>

### [125] [Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification](https://arxiv.org/abs/2504.15594)
*Tatsuhito Hasegawa,Shunsuke Sakai*

Main category: cs.LG

TLDR: 论文提出了一种无需训练即可确定softmax温度参数T*的理论方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解温度参数T对深度学习分类任务的影响，并提出一种无需额外训练即可确定最优T*的方法。

Method: 方法包括理论推导T*与特征维度的关系，提出温度确定系数，并通过批量归一化层稳定特征空间。

Result: 实验结果表明，提出的方法能有效估计T*，并显著提升分类性能。

Conclusion: 结论表明，该方法不仅理论可行，且在实际任务中具有普适性和实用性。

Abstract: In deep learning-based classification tasks, the softmax function's
temperature parameter $T$ critically influences the output distribution and
overall performance. This study presents a novel theoretical insight that the
optimal temperature $T^*$ is uniquely determined by the dimensionality of the
feature representations, thereby enabling training-free determination of $T^*$.
Despite this theoretical grounding, empirical evidence reveals that $T^*$
fluctuates under practical conditions owing to variations in models, datasets,
and other confounding factors. To address these influences, we propose and
optimize a set of temperature determination coefficients that specify how $T^*$
should be adjusted based on the theoretical relationship to feature
dimensionality. Additionally, we insert a batch normalization layer immediately
before the output layer, effectively stabilizing the feature space. Building on
these coefficients and a suite of large-scale experiments, we develop an
empirical formula to estimate $T^*$ without additional training while also
introducing a corrective scheme to refine $T^*$ based on the number of classes
and task complexity. Our findings confirm that the derived temperature not only
aligns with the proposed theoretical perspective but also generalizes
effectively across diverse tasks, consistently enhancing classification
performance and offering a practical, training-free solution for determining
$T^*$.

</details>

### [126] [SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2504.15616)
*Kai Chen,Xiaodong Zhao,Yujie Huang,Guoyu Fang,Xiao Song,Ruiping Wang,Ziyuan Wang*

Main category: cs.LG

TLDR: SocialMOIF提出了一种多阶意图融合模型，用于解决智能系统中轨迹预测的高不确定性和复杂高阶影响问题，通过结合直接和间接意图信息，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前轨迹预测方法因代理意图的高不确定性和邻近群体间复杂的高阶影响而存在局限性，需要更全面的意图交互建模。

Method: 提出SocialMOIF模型，融合多阶意图交互，设计轨迹分布近似器和全局轨迹优化器，并引入新的损失函数。

Result: 实验表明，该模型在动态和静态数据集上均优于现有基线方法。

Conclusion: SocialMOIF通过多阶意图融合和优化设计，显著提升了轨迹预测的准确性和可解释性。

Abstract: The analysis and prediction of agent trajectories are crucial for
decision-making processes in intelligent systems, with precise short-term
trajectory forecasting being highly significant across a range of applications.
Agents and their social interactions have been quantified and modeled by
researchers from various perspectives; however, substantial limitations exist
in the current work due to the inherent high uncertainty of agent intentions
and the complex higher-order influences among neighboring groups. SocialMOIF is
proposed to tackle these challenges, concentrating on the higher-order
intention interactions among neighboring groups while reinforcing the primary
role of first-order intention interactions between neighbors and the target
agent. This method develops a multi-order intention fusion model to achieve a
more comprehensive understanding of both direct and indirect intention
information. Within SocialMOIF, a trajectory distribution approximator is
designed to guide the trajectories toward values that align more closely with
the actual data, thereby enhancing model interpretability. Furthermore, a
global trajectory optimizer is introduced to enable more accurate and efficient
parallel predictions. By incorporating a novel loss function that accounts for
distance and direction during training, experimental results demonstrate that
the model outperforms previous state-of-the-art baselines across multiple
metrics in both dynamic and static datasets.

</details>

### [127] [An XAI-based Analysis of Shortcut Learning in Neural Networks](https://arxiv.org/abs/2504.15664)
*Phuong Quynh Le,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TLDR: 论文提出了一种基于XAI的诊断方法（神经元虚假分数），用于量化神经元对虚假特征的依赖，并分析了CNN和ViT中虚假特征的部分解耦情况。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易学习虚假特征（与目标标签强相关但非因果的特征），现有方法在某些情况下无法有效缓解这一问题。

Method: 引入神经元虚假分数作为诊断工具，分析CNN和ViT中虚假特征的编码方式。

Result: 虚假特征在模型中部分解耦，但解耦程度因架构而异；现有缓解方法的假设不完整。

Conclusion: 研究结果为开发新方法缓解虚假相关性奠定了基础，有助于提高AI模型的安全性。

Abstract: Machine learning models tend to learn spurious features - features that
strongly correlate with target labels but are not causal. Existing approaches
to mitigate models' dependence on spurious features work in some cases, but
fail in others. In this paper, we systematically analyze how and where neural
networks encode spurious correlations. We introduce the neuron spurious score,
an XAI-based diagnostic measure to quantify a neuron's dependence on spurious
features. We analyze both convolutional neural networks (CNNs) and vision
transformers (ViTs) using architecture-specific methods. Our results show that
spurious features are partially disentangled, but the degree of disentanglement
varies across model architectures. Furthermore, we find that the assumptions
behind existing mitigation methods are incomplete. Our results lay the
groundwork for the development of novel methods to mitigate spurious
correlations and make AI models safer to use in practice.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [128] [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
*Anjiang Wei,Huanmi Tan,Tarun Suresh,Daniel Mendoza,Thiago S. F. X. Teixeira,Ke Wang,Caroline Trippel,Alex Aiken*

Main category: cs.AR

TLDR: VERICODER是一个针对RTL代码生成的模型，通过功能验证的数据集微调，显著提升了功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有RTL数据集多关注语法有效性而非功能验证，导致生成的代码可能不符合预期行为。

Method: 使用结合单元测试生成和反馈导向精炼的新方法构建数据集，并通过教师模型（GPT-4o-mini）迭代优化RTL设计和测试。

Result: VERICODER在VerilogEval和RTLLM上达到最佳功能正确性指标，相对提升分别达71.7%和27.4%。

Conclusion: 功能验证的高质量数据集对RTL代码生成至关重要，VERICODER的成功验证了这一点。

Abstract: Recent advances in Large Language Models (LLMs) have sparked growing interest
in applying them to Electronic Design Automation (EDA) tasks, particularly
Register Transfer Level (RTL) code generation. While several RTL datasets have
been introduced, most focus on syntactic validity rather than functional
validation with tests, leading to training examples that compile but may not
implement the intended behavior. We present VERICODER, a model for RTL code
generation fine-tuned on a dataset validated for functional correctness. This
fine-tuning dataset is constructed using a novel methodology that combines unit
test generation with feedback-directed refinement. Given a natural language
specification and an initial RTL design, we prompt a teacher model
(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design
based on its simulation results using the generated tests. If necessary, the
teacher model also updates the tests to ensure they comply with the natural
language specification. As a result of this process, every example in our
dataset is functionally validated, consisting of a natural language
description, an RTL implementation, and passing tests. Fine-tuned on this
dataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics
in functional correctness on VerilogEval and RTLLM, with relative gains of up
to 71.7% and 27.4% respectively. An ablation study further shows that models
trained on our functionally validated dataset outperform those trained on
functionally non-validated datasets, underscoring the importance of
high-quality datasets in RTL code generation.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [129] [Med-CoDE: Medical Critique based Disagreement Evaluation Framework](https://arxiv.org/abs/2504.15330)
*Mohit Gupta,Akiko Aizawa,Rajiv Ratn Shah*

Main category: cs.IR

TLDR: 提出了Med-CoDE框架，用于评估医疗领域大语言模型的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法在医疗领域缺乏鲁棒性，无法全面评估大语言模型的性能，可能带来临床风险。

Method: 采用基于批判的方法，定量衡量模型生成响应与医学标准之间的差异，评估准确性和可靠性。

Result: 通过实验和案例研究验证了框架的实用性，提供了全面可靠的医疗大语言模型评估方法。

Conclusion: Med-CoDE填补了现有评估空白，为医疗大语言模型的质量和可信度提供了系统性评估工具。

Abstract: The emergence of large language models (LLMs) has significantly influenced
numerous fields, including healthcare, by enhancing the capabilities of
automated systems to process and generate human-like text. However, despite
their advancements, the reliability and accuracy of LLMs in medical contexts
remain critical concerns. Current evaluation methods often lack robustness and
fail to provide a comprehensive assessment of LLM performance, leading to
potential risks in clinical settings. In this work, we propose Med-CoDE, a
specifically designed evaluation framework for medical LLMs to address these
challenges. The framework leverages a critique-based approach to quantitatively
measure the degree of disagreement between model-generated responses and
established medical ground truths. This framework captures both accuracy and
reliability in medical settings. The proposed evaluation framework aims to fill
the existing gap in LLM assessment by offering a systematic method to evaluate
the quality and trustworthiness of medical LLMs. Through extensive experiments
and case studies, we illustrate the practicality of our framework in providing
a comprehensive and reliable evaluation of medical LLMs.

</details>

### [130] [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
*Harsh Maheshwari,Srikanth Tenneti,Alwarappan Nakkiran*

Main category: cs.IR

TLDR: 论文提出了高效的后期处理算法，通过关键词+语义匹配、BERTScore微调模型和轻量级LLM技术，显著提升了RAG系统中引用的准确性，同时降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统中LLM在引用准确性上表现不佳（约74%），影响了生成内容的可靠性和用户信任。

Method: 采用多种方法交叉检查生成的引用，包括关键词+语义匹配、BERTScore微调模型和轻量级LLM技术。

Result: 实验结果显示，整体准确性指标相对提升了15.46%，并可能改用更小、更快、更经济的模型。

Conclusion: 该研究显著提升了AI生成内容的可靠性，对商业产品中用户信任的建立至关重要。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of
Large Language Models (LLMs), revolutionizing information search and
consumption. RAG systems combine traditional search capabilities with LLMs to
generate comprehensive answers to user queries, ideally with accurate
citations. However, in our experience of developing a RAG product, LLMs often
struggle with source attribution, aligning with other industry studies
reporting citation accuracy rates of only about 74% for popular generative
search engines. To address this, we present efficient post-processing
algorithms to improve citation accuracy in LLM-generated responses, with
minimal impact on latency and cost. Our approaches cross-check generated
citations against retrieved articles using methods including keyword + semantic
matching, fine tuned model with BERTScore, and a lightweight LLM-based
technique. Our experimental results demonstrate a relative improvement of
15.46% in the overall accuracy metrics of our RAG system. This significant
enhancement potentially enables a shift from our current larger language model
to a relatively smaller model that is approximately 12x more cost-effective and
3x faster in inference time, while maintaining comparable performance. This
research contributes to enhancing the reliability and trustworthiness of
AI-generated content in information retrieval and summarization tasks which is
critical to gain customer trust especially in commercial products.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [131] [SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow](https://arxiv.org/abs/2504.09697)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.GR

TLDR: SPICE是一种无需训练的流程，通过结合基础扩散模型和Canny边缘ControlNet模型，实现了高精度的图像编辑，支持多步编辑并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的图像编辑模型在局部编辑、详细提示遵循和多步编辑后的图像质量保持方面表现不足。

Method: SPICE结合基础扩散模型和Canny边缘ControlNet模型，支持任意分辨率和宽高比，处理自由形式的编辑指令。

Result: SPICE在语义、风格和结构编辑任务中表现优于现有方法，用户评价和定量指标均领先。

Conclusion: SPICE为图像编辑提供了高效、灵活的解决方案，并开源工作流程以促进研究和艺术探索。

Abstract: Recent prompt-based image editing models have demonstrated impressive
prompt-following capability at structural editing tasks. However, existing
models still fail to perform local edits, follow detailed editing prompts, or
maintain global image quality beyond a single editing step. To address these
challenges, we introduce SPICE, a training-free workflow that accepts arbitrary
resolutions and aspect ratios, accurately follows user requirements, and
improves image quality consistently during more than 100 editing steps. By
synergizing the strengths of a base diffusion model and a Canny edge ControlNet
model, SPICE robustly handles free-form editing instructions from the user.
SPICE outperforms state-of-the-art baselines on a challenging realistic
image-editing dataset consisting of semantic editing (object addition, removal,
replacement, and background change), stylistic editing (texture changes), and
structural editing (action change) tasks. Not only does SPICE achieve the
highest quantitative performance according to standard evaluation metrics, but
it is also consistently preferred by users over existing image-editing methods.
We release the workflow implementation for popular diffusion model Web UIs to
support further research and artistic exploration.

</details>

### [132] [Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation](https://arxiv.org/abs/2504.15329)
*Yike Zhang,Eduardo Davalos,Jack Noble*

Main category: cs.GR

TLDR: 本文提出了一种交互式3D到2D可视化与标注工具Vision6D，支持6D姿态估计研究，首次实现用户在2D场景中交互式操作3D对象，并提供用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 精确的6D姿态估计对机器人辅助任务至关重要，但现有工具缺乏交互式3D到2D场景的标注能力。

Method: 开发了Vision6D工具，支持用户通过视觉提示和空间关系在2D场景中交互式标注3D对象的6D姿态，尤其适用于未知相机-世界变换矩阵的场景。

Result: 通过Linemod和HANDAL数据集验证，Vision6D生成的标注与默认真实姿态一致，用户研究显示其界面直观且标注准确。

Conclusion: Vision6D填补了2D与3D场景间的标注鸿沟，为6D姿态估计研究提供了开源工具，助力模型开发与训练。

Abstract: Accurate 6D pose estimation has gained more attention over the years for
robotics-assisted tasks that require precise interaction with physical objects.
This paper presents an interactive 3D-to-2D visualization and annotation tool
to support the 6D pose estimation research community. To the best of our
knowledge, the proposed work is the first tool that allows users to visualize
and manipulate 3D objects interactively on a 2D real-world scene, along with a
comprehensive user study. This system supports robust 6D camera pose annotation
by providing both visual cues and spatial relationships to determine object
position and orientation in various environments. The annotation feature in
Vision6D is particularly helpful in scenarios where the transformation matrix
between the camera and world objects is unknown, as it enables accurate
annotation of these objects' poses using only the camera intrinsic matrix. This
capability serves as a foundational step in developing and training advanced
pose estimation models across various domains. We evaluate Vision6D's
effectiveness by utilizing widely-used open-source pose estimation datasets
Linemod and HANDAL through comparisons between the default ground-truth camera
poses with manual annotations. A user study was performed to show that Vision6D
generates accurate pose annotations via visual cues in an intuitive 3D user
interface. This approach aims to bridge the gap between 2D scene projections
and 3D scenes, offering an effective way for researchers and developers to
solve 6D pose annotation related problems. The software is open-source and
publicly available at https://github.com/InteractiveGL/vision6D.

</details>

<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [133] [A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition](https://arxiv.org/abs/2504.15975)
*Peter Fletcher*

Main category: cs.FL

TLDR: 本文提出了一种直接表示递归图状模式语法的新形式，通过构建模式与语法间的同态映射实现并行解析，整合了模式识别的多环节。


<details>
  <summary>Details</summary>
Motivation: 传统图语法使用产生式规则，而本文旨在以更直接和声明性的方式表示语法结构，支持多维递归结构的解析。

Method: 将语法和模式表示为网络，解析视为从模式到语法的同态映射构建，整合特征检测、分割等环节。

Result: 展示了50-1000符号复杂递归模式的容错解析，处理几何关系变化、模糊符号、重叠等问题。

Conclusion: 该形式支持并行解析，整合多环节，适用于复杂递归结构的容错识别。

Abstract: I introduce a formalism for representing the syntax of recursively structured
graph-like patterns. It does not use production rules, like a conventional
graph grammar, but represents the syntactic structure in a more direct and
declarative way. The grammar and the pattern are both represented as networks,
and parsing is seen as the construction of a homomorphism from the pattern to
the grammar. The grammars can represent iterative, hierarchical and nested
recursive structure in more than one dimension.
  This supports a highly parallel style of parsing, in which all aspects of
pattern recognition (feature detection, segmentation, parsing, filling in
missing symbols, top-down and bottom-up inference) are integrated into a single
process, to exploit the synergy between them.
  The emphasis of this paper is on underlying theoretical issues, but I also
give some example runs to illustrate the error-tolerant parsing of complex
recursively structured patterns of 50-1000 symbols, involving variability in
geometric relationships, blurry and indistinct symbols, overlapping symbols,
cluttered images, and erased patches.

</details>

<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [134] [Fluorescence Reference Target Quantitative Analysis Library](https://arxiv.org/abs/2504.15496)
*Eammon A. Littler,Emmanuel A. Mannoh,Ethan P. M. LaRochelle*

Main category: physics.med-ph

TLDR: QUEL-QAL是一个开源的Python库，旨在标准化荧光成像系统的性能评估，支持关键指标分析，如线性响应、检测限和空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 荧光引导手术（FGS）领域缺乏标准化的性能评估工具，现有工具不统一且难以获取。

Method: 开发了QUEL-QAL库，提供模块化工作流，包括ROI检测、统计分析和可视化功能。

Result: 该库支持关键指标分析，并与监管和学术指南一致，具有可扩展性。

Conclusion: QUEL-QAL通过透明性和可重复性，为荧光成像系统的标准化评估提供了基础工具。

Abstract: Standardized performance evaluation of fluorescence imaging systems remains a
critical unmet need in the field of fluorescence-guided surgery (FGS). While
the American Association of Physicists in Medicine (AAPM) TG311 report and
recent FDA draft guidance provide recommended metrics for system
characterization, practical tools for extracting these metrics remain limited,
inconsistent, and often inaccessible. We present QUEL-QAL, an open-source
Python library designed to streamline and standardize the quantitative analysis
of fluorescence images using solid reference targets. The library provides a
modular, reproducible workflow that includes region of interest (ROI)
detection, statistical analysis, and visualization capabilities. QUEL-QAL
supports key metrics such as response linearity, limit of detection, depth
sensitivity, and spatial resolution, in alignment with regulatory and academic
guidance. Built on widely adopted Python packages, the library is designed to
be extensible, enabling users to adapt it to novel target designs and analysis
protocols. By promoting transparency, reproducibility, and regulatory
alignment, QUEL-QAL offers a foundational tool to support standardized
benchmarking and accelerate the development and evaluation of fluorescence
imaging systems.

</details>
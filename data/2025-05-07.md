<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
*Bang Zhang,Ruotian Ma,Qingxuan Jiang,Peisong Wang,Jiaqi Chen,Zheng Xie,Xingyu Chen,Yue Wang,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TLDR: SAGE是一个自动化评估框架，用于衡量大型语言模型（LLM）的高级社会认知能力，通过模拟人类情感变化和内心思维来提供更真实的评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM对人类理解而非仅文本理解的能力仍具挑战性，SAGE旨在填补这一空白。

Method: SAGE通过模拟情感变化和内心思维，生成情感轨迹和可解释的内心活动，并在100个支持性对话场景中验证其心理真实性。

Result: 实验显示SAGE的情感评分与心理学指标（如BLRI）高度相关，并揭示了前沿模型与早期基线之间的显著差距。

Conclusion: SAGE为追踪语言模型在共情和社交能力方面的进展提供了原则性、可扩展且可解释的工具。

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>

### [2] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Ananya Thakur,Deepak Subramani*

Main category: cs.CL

TLDR: 提出了一种基于分层概念图的框架，利用LLM生成高质量MCQ，针对高中物理领域，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动生成高质量MCQ耗时且依赖专家知识，现有自动化方法无法满足高认知水平和领域特定误解的需求。

Method: 开发分层概念图，通过自动化管道检索相关内容，指导LLM生成针对常见误解的MCQ和干扰项，并进行自动验证。

Result: 专家评估显示成功率达75.20%，学生测试中猜测成功率显著降低至28.05%，优于基线方法。

Conclusion: 概念图方法能有效评估认知水平并识别概念差距，支持快速反馈和大规模干预。

Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive
levels and incorporating common misconceptions into distractor design, is
time-consuming and expertise-intensive, making manual creation impractical at
scale. Current automated approaches typically generate questions at lower
cognitive levels and fail to incorporate domain-specific misconceptions. This
paper presents a hierarchical concept map-based framework that provides
structured knowledge to guide LLMs in generating MCQs with distractors. We
chose high-school physics as our test domain and began by developing a
hierarchical concept map covering major Physics topics and their
interconnections with an efficient database design. Next, through an automated
pipeline, topic-relevant sections of these concept maps are retrieved to serve
as a structured context for the LLM to generate questions and distractors that
specifically target common misconceptions. Lastly, an automated validation is
completed to ensure that the generated MCQs meet the requirements provided. We
evaluate our framework against two baseline approaches: a base LLM and a
RAG-based generation. We conducted expert evaluations and student assessments
of the generated MCQs. Expert evaluation shows that our method significantly
outperforms the baseline approaches, achieving a success rate of 75.20% in
meeting all quality criteria compared to approximately 37% for both baseline
methods. Student assessment data reveal that our concept map-driven approach
achieved a significantly lower guess success rate of 28.05% compared to 37.10%
for the baselines, indicating a more effective assessment of conceptual
understanding. The results demonstrate that our concept map-based approach
enables robust assessment across cognitive levels and instant identification of
conceptual gaps, facilitating faster feedback loops and targeted interventions
at scale.

</details>

### [3] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
*Franklin Zhang,Sonya Zhang,Alon Halevy*

Main category: cs.CL

TLDR: 30 Day Me是一个利用LLMs帮助用户分解目标并跟踪进度的习惯养成应用，核心是30DAYGEN系统，生成3,531种独特的30天挑战。


<details>
  <summary>Details</summary>
Motivation: 通过LLMs快速构建领域特定内容库，支持行为和教育的需求。

Method: 开发30DAYGEN系统，从15K网页中生成挑战，并支持用户目标对齐的运行时搜索。

Result: 展示了LLMs在内容生成和语义去重中的实用性。

Conclusion: LLMs可用于高效构建行为和教育内容库，并提出了实际应用流程。

Abstract: In this paper, we present 30 Day Me, a habit formation application that
leverages Large Language Models (LLMs) to help users break down their goals
into manageable, actionable steps and track their progress. Central to the app
is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced
from over 15K webpages, and enables runtime search of challenge ideas aligned
with user-defined goals. We showcase how LLMs can be harnessed to rapidly
construct domain specific content corpora for behavioral and educational
purposes, and propose a practical pipeline that incorporates effective LLM
enhanced approaches for content generation and semantic deduplication.

</details>

### [4] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
*Masumi Morishige,Ryo Koshihara*

Main category: cs.CL

TLDR: GPR-bench是一个轻量级、可扩展的基准测试工具，用于评估生成式AI系统的可重复性和可靠性，支持双语任务和自动化评分。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI系统因模型更新或提示调整导致行为漂移的可重复性和可靠性问题。

Method: 开发GPR-bench，包含双语数据集（英语和日语）和自动化评估流程，使用“LLM-as-a-Judge”评分。

Result: 新模型在正确性上略有提升但差异不显著，简洁指令显著提升输出简洁性。

Conclusion: GPR-bench为可重复性监控提供了基础，并揭示了基准设计对快速演进的AI模型的重要性。

Abstract: Reproducibility and reliability remain pressing challenges for generative AI
systems whose behavior can drift with each model update or prompt revision. We
introduce GPR-bench, a lightweight, extensible benchmark that operationalizes
regression testing for general purpose use cases. GPR-bench couples an open,
bilingual (English and Japanese) dataset covering eight task categories (e.g.,
text generation, code generation, and information retrieval) and 10 scenarios
in each task categories (80 total test cases for each language) with an
automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of
correctness and conciseness. Experiments across three recent model versions -
gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default
versus concise-writing instruction) reveal heterogeneous quality. Our results
show that newer models generally improve correctness, but the differences are
modest and not statistically significant, suggesting that GPR-bench may not be
sufficiently challenging to differentiate between recent model versions. In
contrast, the concise-writing instruction significantly enhances conciseness
(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with
minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of
prompt engineering. Released under the MIT License, GPR- bench lowers the
barrier to initiating reproducibility monitoring and provides a foundation for
community-driven extensions, while also raising important considerations about
benchmark design for rapidly evolving language models.

</details>

### [5] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
*Henry Tari,Nojus Sereiva,Rishabh Kaushal,Thales Bertaglia,Adriana Iamnitchi*

Main category: cs.CL

TLDR: 论文探讨了利用大语言模型生成跨平台社交媒体合成数据的潜力，评估了不同模型的性能，并提出了针对多平台数据集的新保真度指标。


<details>
  <summary>Details</summary>
Motivation: 社交媒体数据集对研究至关重要，但获取多平台数据成本高且受限，因此探索合成数据的可行性。

Method: 采用多平台主题提示方法，使用不同语言模型生成合成数据，并与真实数据对比评估其词汇和语义特性。

Result: 大语言模型生成多平台合成数据具有潜力，不同模型保真度表现各异，后处理可能提升数据质量。

Conclusion: 生成高质量合成数据需结合适当模型和后处理，新保真度指标为多平台数据集研究提供支持。

Abstract: Social media datasets are essential for research on a variety of topics, such
as disinformation, influence operations, hate speech detection, or influencer
marketing practices. However, access to social media datasets is often
constrained due to costs and platform restrictions. Acquiring datasets that
span multiple platforms, which is crucial for understanding the digital
ecosystem, is particularly challenging. This paper explores the potential of
large language models to create lexically and semantically relevant social
media datasets across multiple platforms, aiming to match the quality of real
data. We propose multi-platform topic-based prompting and employ various
language models to generate synthetic data from two real datasets, each
consisting of posts from three different social media platforms. We assess the
lexical and semantic properties of the synthetic data and compare them with
those of the real data. Our empirical findings show that using large language
models to generate synthetic multi-platform social media data is promising,
different language models perform differently in terms of fidelity, and a
post-processing approach might be needed for generating high-fidelity synthetic
datasets for research. In addition to the empirical evaluation of three state
of the art large language models, our contributions include new fidelity
metrics specific to multi-platform social media datasets.

</details>

### [6] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
*Jonas Bokstaller,Julia Altheimer,Julian Dormehl,Alina Buss,Jasper Wiltfang,Johannes Schneider,Maximilian Röglinger*

Main category: cs.CL

TLDR: 本文提出了一种基于微调大型语言模型（LLM）的交互式聊天机器人，用于解释可解释AI（XAI），并在电池健康状态（SoH）预测中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型的“黑箱”特性日益明显，XAI的应用需求增加。同时，LLM在理解人类语言和复杂模式方面取得了显著进展，结合两者可提升XAI的可解释性。

Method: 设计了一种参考架构，通过微调的LLM驱动的交互式聊天机器人解释XAI，并在电池SoH预测中实例化该架构。

Result: 评估表明，该原型显著提升了ML的可解释性，尤其对XAI经验较少的用户。

Conclusion: 结合LLM的XAI解释方法有效提升了模型的可解释性，尤其在特定领域（如电池SoH预测）中表现突出。

Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as
the increasing black-boxedness of prevailing Machine Learning (ML) models
became apparent. In parallel, Large Language Models (LLMs) significantly
developed in their abilities to understand human language and complex patterns.
By combining both, this paper presents a novel reference architecture for the
interpretation of XAI through an interactive chatbot powered by a fine-tuned
LLM. We instantiate the reference architecture in the context of
State-of-Health (SoH) prediction for batteries and validate its design in
multiple evaluation and demonstration rounds. The evaluation indicates that the
implemented prototype enhances the human interpretability of ML, especially for
users with less experience with XAI.

</details>

### [7] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
*Haoming Yang,Ke Ma,Xiaojun Jia,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CL

TLDR: 本文提出了一种基于人类认知启发的新型越狱攻击框架ICRT，通过认知分解和相关性偏见优化恶意提示，显著提升了攻击效果，并引入排名评估方法量化危害。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）表现优异，但其安全机制仍易受越狱攻击威胁。现有研究多依赖暴力优化或人工设计，未能揭示实际风险。

Method: 提出ICRT框架，利用认知简化效应分解提示复杂性，通过相关性偏见重组提示以增强语义对齐，并采用排名聚合方法（如Elo、HodgeRank等）评估危害性。

Result: 实验表明，ICRT能持续绕过主流LLM的安全机制，生成高风险内容。

Conclusion: ICRT揭示了越狱攻击风险，为防御策略提供了新视角。

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>

### [8] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
*Zhihai Wang,Jie Wang,Jilai Pan,Xilin Xia,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Feng Wu*

Main category: cs.CL

TLDR: 提出了一种名为SpecSearch的新框架，通过小模型与大模型的协作优化思维生成，显著加速LLM推理，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 解决基于树搜索的推理方法因生成大量中间推理步骤而导致的高延迟问题，提升LLM的适用性。

Method: 利用小模型与大模型在思维和标记级别协作，通过质量保留拒绝机制过滤低质量思维。

Result: 在Qwen和Llama模型上实验显示，SpecSearch比现有方法快2.12倍，且推理质量相当。

Conclusion: SpecSearch在加速LLM推理的同时保持了高质量的推理能力，具有实际应用潜力。

Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning
capability of large language models (LLMs) by facilitating the exploration of
multiple intermediate reasoning steps, i.e., thoughts. However, these methods
suffer from substantial inference latency, as they have to generate numerous
reasoning thoughts, severely limiting LLM applicability. To address this
challenge, we propose a novel Speculative Search (SpecSearch) framework that
significantly accelerates LLM reasoning by optimizing thought generation.
Specifically, SpecSearch utilizes a small model to strategically collaborate
with a large model at both thought and token levels, efficiently generating
high-quality reasoning thoughts. The major pillar of SpecSearch is a novel
quality-preserving rejection mechanism, which effectively filters out thoughts
whose quality falls below that of the large model's outputs. Moreover, we show
that SpecSearch preserves comparable reasoning quality to the large model.
Experiments on both the Qwen and Llama models demonstrate that SpecSearch
significantly outperforms state-of-the-art approaches, achieving up to
2.12$\times$ speedup with comparable reasoning quality.

</details>

### [9] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
*Cfir Avraham Hadar,Omer Shubi,Yoav Meiri,Yevgeni Berzak*

Main category: cs.CL

TLDR: 论文研究了是否可以通过眼动数据自动解码读者的开放式阅读目标，并提出了分类和重建任务，通过多模态LLM实现。


<details>
  <summary>Details</summary>
Motivation: 探讨眼动数据是否能反映读者的文本特定目标，填补了相关研究的空白。

Method: 提出目标分类和目标重建任务，使用大规模英语阅读眼动数据，开发并比较多种多模态LLM模型。

Result: 实验表明，LLM能从眼动数据中有效提取读者的文本特定目标信息。

Conclusion: 眼动数据结合LLM可用于解码读者的阅读目标，具有实际应用潜力。

Abstract: When reading, we often have specific information that interests us in a text.
For example, you might be reading this paper because you are curious about LLMs
for eye movements in reading, the experimental design, or perhaps you only care
about the question ``but does it work?''. More broadly, in daily life, people
approach texts with any number of text-specific goals that guide their reading
behavior. In this work, we ask, for the first time, whether open-ended reading
goals can be automatically decoded from eye movements in reading. To address
this question, we introduce goal classification and goal reconstruction tasks
and evaluation frameworks, and use large-scale eye tracking for reading data in
English with hundreds of text-specific information seeking tasks. We develop
and compare several discriminative and generative multimodal LLMs that combine
eye movements and text for goal classification and goal reconstruction. Our
experiments show considerable success on both tasks, suggesting that LLMs can
extract valuable information about the readers' text-specific goals from eye
movements.

</details>

### [10] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
*Wenjie Hua,Shenghan Xu*

Main category: cs.CL

TLDR: 本文提出了一种Logits-Constrained（LC）框架用于古汉语命名实体识别（NER），在EvaHan 2025基准上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决古汉语NER任务中高标签或大数据场景下传统方法（如CRF和BiLSTM）性能不足的问题。

Method: 两阶段模型：GujiRoBERTa用于上下文编码，可微分解码机制确保有效的BMES标签转换。

Result: LC框架在性能上优于传统方法，尤其是在高标签或大数据场景下。

Conclusion: 提出的模型选择标准为实际古汉语NLP任务提供了实用指导。

Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese
Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our
two-stage model integrates GujiRoBERTa for contextual encoding and a
differentiable decoding mechanism to enforce valid BMES label transitions.
Experiments demonstrate that LC improves performance over traditional CRF and
BiLSTM-based approaches, especially in high-label or large-data settings. We
also propose a model selection criterion balancing label complexity and dataset
size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>

### [11] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
*Daniel Goldstein,Eric Alcaide,Janna Lu,Eugene Cheah*

Main category: cs.CL

TLDR: RADLADS是一种快速将softmax注意力Transformer转换为线性注意力解码器模型的协议，并提出了两种新的RWKV变体架构。转换过程仅需350-700M tokens，成本低且性能接近原模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer模型计算成本高的问题，提供一种高效且经济的转换方法。

Method: 通过RADLADS协议快速转换，使用少量token（350-700M）训练线性注意力模型。

Result: 转换后的模型在标准基准测试中表现优异，成本低于2000美元，性能接近原模型。

Conclusion: RADLADS为线性注意力模型提供了一种高效、低成本的转换方案，并开源了模型和代码。

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>

### [12] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
*Albérick Euraste Djiré,Abdoul Kader Kaboré,Earl T. Barr,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TLDR: PEARL是一种检测大型语言模型（LLM）记忆现象的新方法，通过输入扰动评估模型输出的敏感性，无需访问模型内部。


<details>
  <summary>Details</summary>
Motivation: LLM在训练中可能记忆而非泛化数据，引发隐私和知识产权问题，需有效检测方法。

Method: PEARL通过输入扰动分析模型输出的一致性，区分泛化与记忆。

Result: 在Pythia和GPT-4o模型上验证，成功检测到记忆现象，如经典文本和代码。

Conclusion: PEARL为识别LLM记忆行为提供了可靠框架，支持数据隐私和模型评估。

Abstract: While Large Language Models (LLMs) achieve remarkable performance through
training on massive datasets, they can exhibit concerning behaviors such as
verbatim reproduction of training data rather than true generalization. This
memorization phenomenon raises significant concerns about data privacy,
intellectual property rights, and the reliability of model evaluations. This
paper introduces PEARL, a novel approach for detecting memorization in LLMs.
PEARL assesses how sensitive an LLM's performance is to input perturbations,
enabling memorization detection without requiring access to the model's
internals. We investigate how input perturbations affect the consistency of
outputs, enabling us to distinguish between true generalization and
memorization. Our findings, following extensive experiments on the Pythia open
model, provide a robust framework for identifying when the model simply
regurgitates learned information. Applied on the GPT 4o models, the PEARL
framework not only identified cases of memorization of classic texts from the
Bible or common code from HumanEval but also demonstrated that it can provide
supporting evidence that some data, such as from the New York Times news
articles, were likely part of the training data of a given model.

</details>

### [13] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
*Steven Bedrick,A. Seza Doğruöz,Sergiu Nisioi*

Main category: cs.CL

TLDR: 本文探讨了在医疗领域中合成数据集的创建、评估和使用，并提出了一种新的分类法以帮助比较和评估。


<details>
  <summary>Details</summary>
Motivation: 由于真实临床对话数据难以获取且敏感，合成数据集成为替代方案，但缺乏理论指导其最佳使用和泛化。

Method: 综述了合成数据集的创建和评估方法，并提出了一种新的分类法。

Result: 合成数据集在医疗对话任务中有一定效果，但需进一步理论支持。

Conclusion: 提出的分类法有助于合成数据集的比较和评估，为未来研究提供指导。

Abstract: Synthetic data sets are used across linguistic domains and NLP tasks,
particularly in scenarios where authentic data is limited (or even
non-existent). One such domain is that of clinical (healthcare) contexts, where
there exist significant and long-standing challenges (e.g., privacy,
anonymization, and data governance) which have led to the development of an
increasing number of synthetic datasets. One increasingly important category of
clinical dataset is that of clinical dialogues which are especially sensitive
and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some
situations, little theory exists to inform how they may be best used and
generalized to new applications. In this paper, we provide an overview of how
synthetic datasets are created, evaluated and being used for dialogue related
tasks in the medical domain. Additionally, we propose a novel typology for use
in classifying types and degrees of data synthesis, to facilitate comparison
and evaluation.

</details>

### [14] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
*Sicong Huang,Jincheng He,Shiyuan Huang,Karthik Raja Anandan,Arkajyoti Chakraborty,Ian Lane*

Main category: cs.CL

TLDR: UCSC系统在Mu-SHROOM任务中提出了一种检测和定位LLM幻觉的框架，通过检索上下文、识别错误内容并映射回输出，最终在任务中排名第一。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在知识密集型查询中的幻觉问题，并精确定位幻觉发生的位置。

Method: 提出一个框架，包括检索相关上下文、识别答案中的错误内容，并将其映射回LLM输出，同时优化提示。

Result: 系统在所有语言中平均排名第一，表现最佳。

Conclusion: 该框架有效解决了LLM幻觉问题，代码和实验结果已公开。

Abstract: Hallucinations pose a significant challenge for large language models when
answering knowledge-intensive queries. As LLMs become more widely adopted, it
is crucial not only to detect if hallucinations occur but also to pinpoint
exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes, is a recent effort in this direction. This paper
describes the UCSC system submission to the shared Mu-SHROOM task. We introduce
a framework that first retrieves relevant context, next identifies false
content from the answer, and finally maps them back to spans in the LLM output.
The process is further enhanced by automatically optimizing prompts. Our system
achieves the highest overall performance, ranking #1 in average position across
all languages. We release our code and experiment results.

</details>

### [15] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
*Ryan Wang,Matthew Finlayson,Luca Soldaini,Swabha Swayamdipta,Robin Jia*

Main category: cs.CL

TLDR: SLUNG是一种预训练范式，通过选择性损失让模型理解高风险数据但不生成它，提升模型对高风险内容的理解能力而不增加其生成。


<details>
  <summary>Details</summary>
Motivation: 过滤高风险预训练数据会限制模型识别和应对有害内容的能力，SLUNG旨在解决这一问题。

Method: SLUNG选择性避免激励高风险token的生成，同时确保模型能理解其上下文。

Result: 实验表明SLUNG提升模型对高风险数据的理解能力（如识别毒性内容），而不增加其生成（如模型响应的毒性）。

Conclusion: SLUNG使模型能从高风险文本中受益，而无需完全过滤这些数据。

Abstract: Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models' ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model's context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models'
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.

</details>

### [16] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
*Jennifer Healey,Laurie Byrum,Md Nadeem Akhtar,Surabhi Bhargava,Moumita Sinha*

Main category: cs.CL

TLDR: 本文探讨了LLM评估的挑战，特别是在实际部署中任务特定提示和上下文的影响，并提出了一种半自动化的偏见评估框架。


<details>
  <summary>Details</summary>
Motivation: 由于LLM评估在真实场景中受任务提示和上下文影响，传统基于短上下文和固定选择的基准测试可能失效，而大规模人工评估成本高且难以实施。

Method: 开发了一种半自动化的偏见评估框架，结合人类洞察力，定义了偏见操作化方法，并扩展了超越多选题的分类方法。

Result: 通过人类评估发现了偏见基准中的问题模板，验证了框架的有效性。

Conclusion: 提出的半自动化框架结合人类洞察力，能够更有效地评估LLM的偏见问题。

Abstract: LLM evaluation is challenging even the case of base models. In real world
deployments, evaluation is further complicated by the interplay of task
specific prompts and experiential context. At scale, bias evaluation is often
based on short context, fixed choice benchmarks that can be rapidly evaluated,
however, these can lose validity when the LLMs' deployed context differs. Large
scale human evaluation is often seen as too intractable and costly. Here we
present our journey towards developing a semi-automated bias evaluation
framework for free text responses that has human insights at its core. We
discuss how we developed an operational definition of bias that helped us
automate our pipeline and a methodology for classifying bias beyond multiple
choice. We additionally comment on how human evaluation helped us uncover
problematic templates in a bias benchmark.

</details>

### [17] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
*Junlin Wang,Roy Xie,Shang Zhu,Jue Wang,Ben Athiwaratkun,Bhuwan Dhingra,Shuaiwen Leon Song,Ce Zhang,James Zou*

Main category: cs.CL

TLDR: 论文提出了一种名为MoAA的方法，通过结合多种语言模型的优势生成高质量的对齐数据，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 构建高质量的人类标注数据成本高且难以扩展，且可能缺乏多样性和泛化能力。

Method: 采用Mixture of Agents Alignment (MoAA)方法，利用多种语言模型的集体优势生成对齐数据。

Result: 实验显示，MoAA将LLaMA-3.1-8B-Instruct在Arena-Hard和AlpacaEval2上的胜率分别从19.5提升至48.3和从22.33提升至57.23。

Conclusion: MoAA为模型对齐提供了一种可扩展且多样化的数据生成方法，并能通过自我改进管道提升开源LLM的性能。

Abstract: Building helpful and harmless large language models (LLMs) requires effective
model alignment approach based on human instructions and feedback, which
necessitates high-quality human-labeled data. Constructing such datasets is
often expensive and hard to scale, and may face potential limitations on
diversity and generalization. To address these challenges, we introduce Mixture
of Agents Alignment (MoAA), that leverages the collective strengths of various
language models to provide high-quality data for model alignment. By employing
MoAA, we enhance both supervised fine-tuning and preference optimization,
leading to improved performance compared to using a single model alone to
generate alignment data (e.g. using GPT-4o alone). Evaluation results show that
our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on
Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising
direction for model alignment through this new scalable and diverse synthetic
data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement
pipeline, where models finetuned on MoA-generated data surpass their own
initial capabilities, providing evidence that our approach can push the
frontier of open-source LLMs without reliance on stronger external supervision.
Data and code will be released.

</details>

### [18] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
*Behrooz Mansouri*

Main category: cs.CL

TLDR: 本文综述了抽象意义表示（AMR），一种基于图的语义表示框架，探讨其解析、生成任务及应用，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AMR通过图结构捕捉句子语义，为机器理解人类语言提供新方法。本文旨在综述AMR及其扩展，分析其潜力与挑战。

Method: 调查AMR及其扩展，分析解析与生成任务的传统、当前及未来方法，并回顾AMR在文本生成、分类等领域的应用。

Result: AMR在语义表示和语言任务中表现出潜力，但仍面临解析与生成的技术挑战。

Conclusion: AMR有望推动机器语言理解的发展，未来研究需解决技术难题并拓展应用场景。

Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.

</details>

### [19] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
*Shijing Zhu,Zhuang Chen,Guanqun Bi,Binghang Li,Yaxi Deng,Dazhen Wan,Libiao Peng,Xiyao Xiao,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,FangFang Li,Minlie Huang*

Main category: cs.CL

TLDR: 论文提出了一个名为{\Psi}-Arena的交互式框架，用于全面评估和优化基于大语言模型（LLM）的心理咨询师，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型心理咨询评估方法存在静态评估、单一视角和开环框架的问题，无法全面确保其有效性和安全性。

Method: 提出{\Psi}-Arena框架，通过多阶段对话、三方评估和闭环优化，模拟真实心理咨询场景并提升LLM表现。

Result: 实验显示不同LLM在真实场景中表现差异显著，优化后咨询性能提升高达141%。

Conclusion: {\Psi}-Arena为心理医疗领域可靠且人性化的LLM应用提供了基础资源。

Abstract: Large language models (LLMs) have shown promise in providing scalable mental
health support, while evaluating their counseling capability remains crucial to
ensure both efficacy and safety. Existing evaluations are limited by the static
assessment that focuses on knowledge tests, the single perspective that centers
on user experience, and the open-loop framework that lacks actionable feedback.
To address these issues, we propose {\Psi}-Arena, an interactive framework for
comprehensive assessment and optimization of LLM-based counselors, featuring
three key characteristics: (1) Realistic arena interactions that simulate
real-world counseling through multi-stage dialogues with psychologically
profiled NPC clients, (2) Tripartite evaluation that integrates assessments
from the client, counselor, and supervisor perspectives, and (3) Closed-loop
optimization that iteratively improves LLM counselors using diagnostic
feedback. Experiments across eight state-of-the-art LLMs show significant
performance variations in different real-world scenarios and evaluation
perspectives. Moreover, reflection-based optimization results in up to a 141%
improvement in counseling performance. We hope PsychoArena provides a
foundational resource for advancing reliable and human-aligned LLM applications
in mental healthcare.

</details>

### [20] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
*Junyu Ma,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TLDR: 通过Recall with Reasoning（RwR）方法，提升Mamba模型的长上下文记忆能力，无需架构改动。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在理论上具有无限上下文潜力，但在实际应用中，当序列远超训练长度时表现受限。

Method: RwR通过从教师模型中提取链式思维（CoT）摘要，并在微调时将其作为CoT提示前置，教导Mamba主动回忆和推理长上下文。

Result: 在LONGMEMEVAL和HELMET上的实验显示，RwR在相似预训练条件下显著提升了Mamba的长上下文性能，优于Transformer/混合基线，同时保持短上下文能力。

Conclusion: RwR是一种简单有效的方法，成功解锁了Mamba的长上下文记忆潜力，且无需改变模型架构。

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>

### [21] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
*Mohammad Shoaib Ansari,Mohd Sohail Ali Khan,Shubham Revankar,Aditya Varma,Anil S. Mokhade*

Main category: cs.CL

TLDR: 该研究探讨了如何将大型语言模型（LLMs）应用于医疗领域，通过结合检索增强生成（RAG）和量化低秩适应（QLoRA）技术，提升医疗决策支持的准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗决策支持需要高准确性和上下文相关性，而LLMs结合医院特定数据和量化技术可以优化这一过程。

Method: 使用Llama 3.2-3B-Instruct作为基础模型，结合RAG和QLoRA技术，嵌入和检索医疗信息，提升响应准确性。

Result: 系统显著提高了响应准确性，并在多个医疗基准测试中表现良好，适用于基础医疗建议。

Conclusion: 该研究展示了LLMs在医疗领域的潜力，同时强调了伦理问题和实际部署的挑战，并提出了未来发展方向。

Abstract: This research paper investigates the application of Large Language Models
(LLMs) in healthcare, specifically focusing on enhancing medical decision
support through Retrieval-Augmented Generation (RAG) integrated with
hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation
(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By
embedding and retrieving context-relevant healthcare information, the system
significantly improves response accuracy. QLoRA facilitates notable parameter
efficiency and memory optimization, preserving the integrity of medical
information through specialized quantization techniques. Our research also
shows that our model performs relatively well on various medical benchmarks,
indicating that it can be used to make basic medical suggestions. This paper
details the system's technical components, including its architecture,
quantization methods, and key healthcare applications such as enhanced disease
prediction from patient symptoms and medical history, treatment suggestions,
and efficient summarization of complex medical reports. We touch on the ethical
considerations-patient privacy, data security, and the need for rigorous
clinical validation-as well as the practical challenges of integrating such
systems into real-world healthcare workflows. Furthermore, the lightweight
quantized weights ensure scalability and ease of deployment even in
low-resource hospital environments. Finally, the paper concludes with an
analysis of the broader impact of LLMs on healthcare and outlines future
directions for LLMs in medical settings.

</details>

### [22] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
*Mouath Abu Daoud,Chaimae Abouzahir,Leen Kharouf,Walid Al-Eisawi,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TLDR: 研究介绍了MedArabiQ，一个阿拉伯语医疗领域的基准数据集，评估了多种LLM的性能，强调了多语言基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在阿拉伯医疗领域的应用潜力，填补高质量数据集和基准的空白。

Method: 构建MedArabiQ数据集，包含七项任务，评估五种LLM的性能，包括GPT-4o等。

Result: 发现需要更多多语言基准以确保LLM在医疗领域的公平部署和扩展。

Conclusion: MedArabiQ为未来研究提供了基础，促进LLM在医疗领域的多语言能力提升。

Abstract: Large Language Models (LLMs) have demonstrated significant promise for
various applications in healthcare. However, their efficacy in the Arabic
medical domain remains unexplored due to the lack of high-quality
domain-specific datasets and benchmarks. This study introduces MedArabiQ, a
novel benchmark dataset consisting of seven Arabic medical tasks, covering
multiple specialties and including multiple choice questions,
fill-in-the-blank, and patient-doctor question answering. We first constructed
the dataset using past medical exams and publicly available datasets. We then
introduced different modifications to evaluate various LLM capabilities,
including bias mitigation. We conducted an extensive evaluation with five
state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude
3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of
new high-quality benchmarks that span different languages to ensure fair
deployment and scalability of LLMs in healthcare. By establishing this
benchmark and releasing the dataset, we provide a foundation for future
research aimed at evaluating and enhancing the multilingual capabilities of
LLMs for the equitable use of generative AI in healthcare.

</details>

### [23] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
*Matan Orbach,Ohad Eytan,Benjamin Sznajder,Ariel Gera,Odellia Boni,Yoav Kantor,Gal Bloch,Omri Levy,Hadas Abraham,Nitzan Barzilay,Eyal Shnarch,Michael E. Factor,Shila Ofek-Koifman,Paula Ta-Shma,Assaf Toledo*

Main category: cs.CL

TLDR: 本文研究了检索增强生成（RAG）的超参数优化（HPO）框架的有效性，通过5种HPO算法和5个数据集进行了全面评估，发现贪婪或随机搜索能高效提升RAG性能。


<details>
  <summary>Details</summary>
Motivation: 为RAG找到最优配置复杂且昂贵，现有HPO框架的有效性缺乏严格基准测试。

Method: 使用5种HPO算法在5个数据集上进行实验，包括一个新收集的真实产品文档数据集，探索了最大的HPO搜索空间。

Result: 贪婪或迭代随机搜索能高效完成RAG HPO，显著提升所有数据集的性能。贪婪方法中，优先优化模型比按RAG流程顺序优化更有效。

Conclusion: RAG HPO能高效提升性能，贪婪方法中优先优化模型是更优策略。

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with two optimized evaluation metrics. Analysis of
the results shows that RAG HPO can be done efficiently, either greedily or with
iterative random search, and that it significantly boosts RAG performance for
all datasets. For greedy HPO approaches, we show that optimizing models first
is preferable to the prevalent practice of optimizing sequentially according to
the RAG pipeline order.

</details>

### [24] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
*Shuang Zhou,Jiashuo Wang,Zidu Xu,Song Wang,David Brauer,Lindsay Welton,Jacob Cogan,Yuen-Hei Chung,Lei Tian,Zaifu Zhan,Yu Hou,Mingquan Lin,Genevieve B. Melton,Rui Zhang*

Main category: cs.CL

TLDR: ConfiDx是一个基于大语言模型（LLM）的不确定性感知诊断系统，通过微调开源LLM并结合诊断标准，显著提升了自动诊断系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中证据不足时，诊断不确定性会增加误诊风险，但目前对诊断不确定性的识别和解释研究不足。

Method: 通过微调开源LLM，构建ConfiDx模型，并使用标注数据集捕捉不同程度的诊断模糊性。

Result: ConfiDx在真实数据集上表现出色，能有效识别诊断不确定性并提供可信解释。

Conclusion: 该研究首次联合解决诊断不确定性的识别和解释问题，显著提升了自动诊断系统的可靠性。

Abstract: Explainable disease diagnosis, which leverages patient information (e.g.,
signs and symptoms) and computational models to generate probable diagnoses and
reasonings, offers clear clinical values. However, when clinical notes
encompass insufficient evidence for a definite diagnosis, such as the absence
of definitive symptoms, diagnostic uncertainty usually arises, increasing the
risk of misdiagnosis and adverse outcomes. Although explicitly identifying and
explaining diagnostic uncertainties is essential for trustworthy diagnostic
systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an
uncertainty-aware large language model (LLM) created by fine-tuning open-source
LLMs with diagnostic criteria. We formalized the task and assembled richly
annotated datasets that capture varying degrees of diagnostic ambiguity.
Evaluating ConfiDx on real-world datasets demonstrated that it excelled in
identifying diagnostic uncertainties, achieving superior diagnostic
performance, and generating trustworthy explanations for diagnoses and
uncertainties. To our knowledge, this is the first study to jointly address
diagnostic uncertainty recognition and explanation, substantially enhancing the
reliability of automatic diagnostic systems.

</details>

### [25] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
*Bin Yu,Hang Yuan,Yuliang Wei,Bailing Wang,Weizhen Qi,Kai Chen*

Main category: cs.CL

TLDR: 论文提出了一种名为LS-Mixture SFT的方法，通过结合长链和短链的思维推理数据，解决了传统SFT方法中模型推理冗余的问题，显著提升了准确率并减少了响应长度。


<details>
  <summary>Details</summary>
Motivation: 传统SFT方法在将推理能力迁移到非推理模型时，会继承教师模型的“过度思考”问题，导致推理链冗长且冗余。

Method: 提出LS-Mixture SFT方法，结合长链思维推理数据及其通过结构保留重写生成的短链版本进行监督微调。

Result: 实验表明，LS-Mixture SFT方法平均准确率提升2.3%，同时模型响应长度减少约47.61%。

Conclusion: LS-Mixture SFT方法有效解决了推理冗余问题，为高效推理提供了一种新途径。

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought
\textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning
(\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their
short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3\% across various benchmarks while substantially reducing
model response length by approximately 47.61\%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>

### [26] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
*Marta Boscariol,Luana Bulla,Lia Draetta,Beatrice Fiumanò,Emanuele Lenzi,Leonardo Piano*

Main category: cs.CL

TLDR: 论文评估了GPT和LLama3在长尾实体链接任务中的表现，发现LLMs在此任务中表现良好，可作为补充技术。


<details>
  <summary>Details</summary>
Motivation: 长尾实体链接任务研究不足，LLMs的深度上下文理解能力为解决此问题提供了新视角。

Method: 使用MHERCL v0.1基准数据集，比较LLMs与ReLiK框架在长尾实体链接中的性能。

Result: 初步实验显示LLMs在长尾实体链接中表现良好，填补了头部与长尾实体链接的差距。

Conclusion: LLMs在长尾实体链接中具有潜力，可作为现有技术的补充。

Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

</details>

### [27] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
*Maciej Zembrzuski,Saad Mahamood*

Main category: cs.CL

TLDR: 提出一种结合抽取式和生成式方法的新方法，用于处理大规模用户评论摘要任务，通过预训练句子嵌入提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理大规模输入数据集时表现不佳，需要改进内容选择策略。

Method: 结合抽取式方法和预训练句子嵌入，再与生成式模型结合，优化摘要生成。

Result: 新方法在大规模输入数据集上优于现有方法，且预测句子嵌入比传统句子选择概率分布更有效。

Conclusion: 结合抽取式和生成式方法，并利用预训练句子嵌入，能显著提升大规模用户评论摘要的质量。

Abstract: Current neural network-based methods to the problem of document summarisation
struggle when applied to datasets containing large inputs. In this paper we
propose a new approach to the challenge of content-selection when dealing with
end-to-end summarisation of user reviews of accommodations. We show that by
combining an extractive approach with externally pre-trained sentence level
embeddings in an addition to an abstractive summarisation model we can
outperform existing methods when this is applied to the task of summarising a
large input dataset. We also prove that predicting sentence level embedding of
a summary increases the quality of an end-to-end system for loosely aligned
source to target corpora, than compared to commonly predicting probability
distributions of sentence selection.

</details>

### [28] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
*Haoqi Yang,Luohe Shi,Qiwei Li,Zuchao Li,Ping Wang,Bo Du,Mengjia Shen,Hai Zhao*

Main category: cs.CL

TLDR: 稀疏专家混合（MoE）大模型（LLM）逐渐成为超大规模模型的主流方法。本文探讨了细粒度MoE模型在不同服务负载下的效率动态，并研究了减少专家数量对效率和性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有MoE优化主要针对粗粒度架构，而细粒度MoE模型的研究较少。本文旨在填补这一空白，探讨其效率动态和优化潜力。

Method: 分析细粒度MoE模型在不同服务负载下的表现，研究减少激活专家和总专家数量对效率与性能的影响。

Result: 减少激活专家数量可显著提升效率且性能损失小；减少总专家数量效率提升有限但性能损失严重。方法可实现至少10%的吞吐量提升且无性能损失。

Conclusion: MoE推理优化仍有巨大探索空间，细粒度模型提供了显著的优化机会。

Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually
becoming the mainstream approach for ultra-large-scale models. Existing
optimization efforts for MoE models have focused primarily on coarse-grained
MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE
models are gaining popularity, yet research on them remains limited. Therefore,
we want to discuss the efficiency dynamic under different service loads.
Additionally, fine-grained models allow deployers to reduce the number of
routed experts, both activated counts and total counts, raising the question of
how this reduction affects the trade-off between MoE efficiency and
performance. Our findings indicate that while deploying MoE models presents
greater challenges, it also offers significant optimization opportunities.
Reducing the number of activated experts can lead to substantial efficiency
improvements in certain scenarios, with only minor performance degradation.
Reducing the total number of experts provides limited efficiency gains but
results in severe performance degradation. Our method can increase throughput
by at least 10\% without any performance degradation. Overall, we conclude that
MoE inference optimization remains an area with substantial potential for
exploration and improvement.

</details>

### [29] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
*Cléa Chataigner,Rebecca Ma,Prakhar Ganesh,Afaf Taïk,Elliot Creager,Golnoosh Farnadi*

Main category: cs.CL

TLDR: 论文提出了一种基于最小语言变换的受控转述框架，用于系统生成自然提示变体，并验证了其在评估LLM行为时的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究发现提示的微小变化会导致大型语言模型（LLM）行为的显著差异，引发对其评估稳定性和可靠性的担忧。

Method: 提出受控转述框架，基于语言变换分类生成自然提示变体，使用BBQ数据集进行验证。

Result: 分析表明，即使是细微的提示修改也会导致模型行为的显著变化。

Conclusion: 研究强调了需要开发鲁棒且能识别转述的评估协议。

Abstract: Small changes in how a prompt is worded can lead to meaningful differences in
the behavior of large language models (LLMs), raising concerns about the
stability and reliability of their evaluations. While prior work has explored
simple formatting changes, these rarely capture the kinds of natural variation
seen in real-world language use. We propose a controlled paraphrasing framework
based on a taxonomy of minimal linguistic transformations to systematically
generate natural prompt variations. Using the BBQ dataset, we validate our
method with both human annotations and automated checks, then use it to study
how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our
analysis shows that even subtle prompt modifications can lead to substantial
changes in model behavior. These results highlight the need for robust,
paraphrase-aware evaluation protocols.

</details>

### [30] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula G Allen-Meares,Eulalia P Abril,Olga Garcia-Bedoya,Carolyn A Dickens,Andrew D. Boyd*

Main category: cs.CL

TLDR: 研究探讨了ChatGPT在生成针对非裔美国心衰患者自我护理策略对话中的潜力，发现有效提示设计是关键，但ChatGPT仍缺乏医疗沟通所需的同理心和参与度。


<details>
  <summary>Details</summary>
Motivation: 针对非裔美国心衰患者自我护理领域缺乏专门数据集的问题，探索ChatGPT生成相关对话的能力。

Method: 采用四种提示策略（领域、AAVE、SDOH、SDOH推理）生成对话，涵盖食物、运动、液体摄入等关键领域，并结合患者特定SDOH属性。

Result: 研究发现，结合SDOH和推理能提升对话质量，但ChatGPT在同理心和参与度方面仍有不足。

Conclusion: 有效提示设计对生成高质量对话至关重要，但ChatGPT需进一步改进以满足医疗沟通需求。

Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate
conversations focused on self-care strategies for African-American heart
failure patients -- a domain with limited specialized datasets. To simulate
patient-health educator dialogues, we employed four prompting strategies:
domain, African American Vernacular English (AAVE), Social Determinants of
Health (SDOH), and SDOH-informed reasoning. Conversations were generated across
key self-care domains of food, exercise, and fluid intake, with varying turn
lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as
age, gender, neighborhood, and socioeconomic status. Our findings show that
effective prompt design is essential. While incorporating SDOH and reasoning
improves dialogue quality, ChatGPT still lacks the empathy and engagement
needed for meaningful healthcare communication.

</details>

### [31] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
*Sharvi Endait,Ruturaj Ghatage,Aditya Kulkarni,Rajlaxmi Patil,Raviraj Joshi*

Main category: cs.CL

TLDR: 本文介绍了IndicSQuAD，一个涵盖九种主要印度语言的多语言抽取式问答数据集，旨在解决印度语言在QA系统中的低代表性。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在QA系统中进展迅速，而印度语言尽管拥有大量母语使用者，却代表性不足。

Method: 基于MahaSQuAD（马拉地语数据集），通过翻译技术扩展至九种印度语言，保持语言忠实度和答案跨度对齐。

Result: 评估了单语BERT和多语MuRIL-BERT的基线性能，揭示了低资源环境下的挑战。

Conclusion: IndicSQuAD为模型开发提供了坚实基础，并提出了未来研究方向，如扩展语言覆盖和引入多模态数据。

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>

### [32] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
*Baharul Islam,Nasim Ahmad,Ferdous Ahmed Barbhuiya,Kuntal Dey*

Main category: cs.CL

TLDR: 论文介绍了SemEval 2025任务5的系统提交，专注于英德学术领域的跨语言主题分类，采用双语数据训练、负采样和基于边界的检索目标，展示了低维度自注意力机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言主题分类问题，特别是在资源受限的情况下，提升主题检索的效率。

Method: 使用双语数据训练，结合负采样和边界检索目标，设计低维度自注意力机制编码句子嵌入。

Result: 系统在定量评估中平均召回率为32.24%，定性评估中为43.16%和31.53%，性能竞争性强且GPU使用率低。

Conclusion: 方法在资源受限下能有效捕获主题信息，但仍有改进空间。

Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on
cross-lingual subject classification in the English and German academic
domains. Our approach leverages bilingual data during training, employing
negative sampling and a margin-based retrieval objective. We demonstrate that a
dimension-as-token self-attention mechanism designed with significantly reduced
internal dimensions can effectively encode sentence embeddings for subject
retrieval. In quantitative evaluation, our system achieved an average recall
rate of 32.24% in the general quantitative setting (all subjects), 43.16% and
31.53% of the general qualitative evaluation methods with minimal GPU usage,
highlighting their competitive performance. Our results demonstrate that our
approach is effective in capturing relevant subject information under resource
constraints, although there is still room for improvement.

</details>

### [33] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
*Zimu Lu,Yunqiao Yang,Houxing Ren,Haotian Hou,Han Xiao,Ke Wang,Weikang Shi,Aojun Zhou,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TLDR: WebGen-Bench是一个新的基准测试，用于评估LLM代理从零开始生成多文件网站代码库的能力，包含多样化的指令和647个测试用例。最佳模型组合的准确率仅为27.8%，显示其挑战性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理在生成和管理复杂代码库中的潜力，特别是在网站开发任务中的表现。

Method: 通过人类和GPT-4o合作创建多样化的网站生成指令，生成测试用例并手动调整以确保准确性，使用自动化代理执行测试。

Result: 最佳模型组合（Bolt.diy + DeepSeek-R1）的准确率为27.8%，训练后的Qwen2.5-Coder-32B-Instruct模型达到38.2%的准确率。

Conclusion: WebGen-Bench是一个具有挑战性的基准测试，展示了LLM代理在网站生成任务中的局限性，同时通过训练数据可以提升性能。

Abstract: LLM-based agents have demonstrated great potential in generating and managing
code within complex codebases. In this paper, we introduce WebGen-Bench, a
novel benchmark designed to measure an LLM-based agent's ability to create
multi-file website codebases from scratch. It contains diverse instructions for
website generation, created through the combined efforts of human annotators
and GPT-4o. These instructions span three major categories and thirteen minor
categories, encompassing nearly all important types of web applications. To
assess the quality of the generated websites, we use GPT-4o to generate test
cases targeting each functionality described in the instructions, and then
manually filter, adjust, and organize them to ensure accuracy, resulting in 647
test cases. Each test case specifies an operation to be performed on the
website and the expected result after the operation. To automate testing and
improve reproducibility, we employ a powerful web-navigation agent to execute
tests on the generated websites and determine whether the observed responses
align with the expected results. We evaluate three high-performance code-agent
frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and
open-source LLMs as engines. The best-performing combination, Bolt.diy powered
by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting
the challenging nature of our benchmark. Additionally, we construct
WebGen-Instruct, a training set consisting of 6,667 website-generation
instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories
generated from a subset of this training set achieves an accuracy of 38.2\%,
surpassing the performance of the best proprietary model.

</details>

### [34] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
*Zuwei Long,Yunhang Shen,Chaoyou Fu,Heting Gao,Lijiang Li,Peixian Chen,Mengdan Zhang,Hang Shao,Jian Li,Jinlong Peng,Haoyu Cao,Ke Li,Rongrong Ji,Xing Sun*

Main category: cs.CL

TLDR: VITA-Audio是一种端到端的大型语音模型，通过轻量级多模态令牌预测模块和四阶段渐进训练策略，显著降低流式场景下的首音频令牌生成延迟，实现实时对话能力。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在流式生成首音频令牌时存在高延迟问题，限制了实际部署。

Method: 提出轻量级多模态令牌预测模块（MCTP）和四阶段渐进训练策略，以加速推理并减少延迟。

Result: 在7B参数规模下，推理速度提升3~5倍，并在ASR、TTS和SQA任务上优于同类开源模型。

Conclusion: VITA-Audio是首个能在首次前向传递中生成音频的多模态大语言模型，具有低延迟和高质量语音生成能力。

Abstract: With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [RESAnything: Attribute Prompting for Arbitrary Referring Segmentation](https://arxiv.org/abs/2505.02867)
*Ruiqi Wang,Hao Zhang*

Main category: cs.CV

TLDR: 提出了一种开放词汇和零样本的任意指代表达分割方法（RESAnything），通过属性提示和Chain-of-Thoughts推理，处理对象和部分级别的标签及隐式查询。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法处理的广义指代表达分割问题，包括对象/部分级别的标签和隐式属性查询。

Method: 利用大语言模型（LLM）生成详细属性描述，结合基础图像分割模型生成候选分割区域，无需训练或微调。

Result: 在传统RES基准测试中表现优于零样本方法，并在隐式查询和复杂部分关系场景中显著优于现有方法。

Conclusion: RESAnything是首个基于LLM的零样本RES方法，提出了新基准数据集以评估部分级别的任意RES解决方案。

Abstract: We present an open-vocabulary and zero-shot method for arbitrary referring
expression segmentation (RES), targeting input expressions that are more
general than what prior works were designed to handle. Specifically, our inputs
encompass both object- and part-level labels as well as implicit references
pointing to properties or qualities of object/part function, design, style,
material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT)
reasoning, where the key idea is attribute prompting. We generate detailed
descriptions of object/part attributes including shape, color, and location for
potential segment proposals through systematic prompting of a large language
model (LLM), where the proposals are produced by a foundational image
segmentation model. Our approach encourages deep reasoning about object or part
attributes related to function, style, design, etc., enabling the system to
handle implicit queries without any part annotations for training or
fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything
achieves clearly superior performance among zero-shot methods on traditional
RES benchmarks and significantly outperforms existing methods on challenging
scenarios involving implicit queries and complex part-level relations. Finally,
we contribute a new benchmark dataset to offer ~3K carefully curated RES
instances to assess part-level, arbitrary RES solutions.

</details>

### [36] [Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images](https://arxiv.org/abs/2505.02949)
*Tian Qiu,Arjun Nichani,Rasta Tadayontahmasebi,Haewon Jeong*

Main category: cs.CV

TLDR: 本文提出了一种评估神经图像压缩模型中偏见的框架，发现传统失真指标无法捕捉偏见，且所有模型均存在种族偏见，可通过面部表型退化检测。平衡训练集可减少偏见，但不足够。


<details>
  <summary>Details</summary>
Motivation: 神经压缩方法在极低比特率下表现优异，但训练过程中可能引入偏见，导致不公平结果。本文旨在评估和解决神经图像压缩模型中的偏见问题。

Method: 提出通用、结构化、可扩展的框架，分析九种流行神经压缩模型及其变种，研究种族偏见及其与图像真实性的关系。

Result: 发现所有模型均存在种族偏见，传统失真指标无法捕捉；平衡训练集可减少偏见，但效果有限；偏见源于压缩模型和分类模型。

Conclusion: 本文为评估和消除神经图像压缩模型中的偏见迈出第一步，强调需进一步研究偏见来源和缓解策略。

Abstract: Neural compression methods are gaining popularity due to their superior
rate-distortion performance over traditional methods, even at extremely low
bitrates below 0.1 bpp. As deep learning architectures, these models are prone
to bias during the training process, potentially leading to unfair outcomes for
individuals in different groups. In this paper, we present a general,
structured, scalable framework for evaluating bias in neural image compression
models. Using this framework, we investigate racial bias in neural compression
algorithms by analyzing nine popular models and their variants. Through this
investigation, we first demonstrate that traditional distortion metrics are
ineffective in capturing bias in neural compression models. Next, we highlight
that racial bias is present in all neural compression models and can be
captured by examining facial phenotype degradation in image reconstructions. We
then examine the relationship between bias and realism in the decoded images
and demonstrate a trade-off across models. Finally, we show that utilizing a
racially balanced training set can reduce bias but is not a sufficient bias
mitigation strategy. We additionally show the bias can be attributed to
compression model bias and classification model bias. We believe that this work
is a first step towards evaluating and eliminating bias in neural image
compression models.

</details>

### [37] [Generating Narrated Lecture Videos from Slides with Synchronized Highlights](https://arxiv.org/abs/2505.02966)
*Alexander Holmberg*

Main category: cs.CV

TLDR: 该系统将静态幻灯片自动转化为视频讲座，通过AI生成同步的语音和视觉高亮，显著降低制作成本。


<details>
  <summary>Details</summary>
Motivation: 传统视频讲座制作耗时费力，需要人工录制和视觉引导，该系统旨在自动化这一过程。

Method: 系统采用新颖的高亮对齐模块，结合多种策略（如Levenshte距离、LLM语义分析）和TTS同步，实现语音与幻灯片内容的精确映射。

Result: 在1000个样本的测试中，LLM对齐方法准确率（F1>92%）显著优于简单方法，生成成本低于1美元/小时。

Conclusion: 该系统以高准确性和低成本，为静态幻灯片转化为视频讲座提供了实用且可扩展的解决方案。

Abstract: Turning static slides into engaging video lectures takes considerable time
and effort, requiring presenters to record explanations and visually guide
their audience through the material. We introduce an end-to-end system designed
to automate this process entirely. Given a slide deck, this system synthesizes
a video lecture featuring AI-generated narration synchronized precisely with
dynamic visual highlights. These highlights automatically draw attention to the
specific concept being discussed, much like an effective presenter would. The
core technical contribution is a novel highlight alignment module. This module
accurately maps spoken phrases to locations on a given slide using diverse
strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at
selectable granularities (line or word level) and utilizes timestamp-providing
Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's
effectiveness through a technical evaluation using a manually annotated slide
dataset with 1000 samples, finding that LLM-based alignment achieves high
location accuracy (F1 > 92%), significantly outperforming simpler methods,
especially on complex, math-heavy content. Furthermore, the calculated
generation cost averages under $1 per hour of video, offering potential savings
of two orders of magnitude compared to conservative estimates of manual
production costs. This combination of high accuracy and extremely low cost
positions this approach as a practical and scalable tool for transforming
static slides into effective, visually-guided video lectures.

</details>

### [38] [Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation](https://arxiv.org/abs/2505.02971)
*Anjila Budathoki,Manish Dhakal*

Main category: cs.CV

TLDR: 本文研究了视觉语言分割模型（VLSMs）在医学图像分析中对对抗攻击的鲁棒性，发现其性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗攻击在计算机视觉和视觉语言模型中已被广泛研究，但在医学图像分析中针对VLSMs的研究仍不足，尤其是在高风险场景下。

Method: 通过微调预训练的VLSMs，并应用PGD和FGSM对抗攻击方法，评估模型鲁棒性。

Result: 结果显示DSC和IoU分数显著下降，但未能找到适用于医学图像的通用扰动。

Conclusion: 研究表明VLSMs在医学图像分析中对对抗攻击的鲁棒性较弱，需进一步研究以提高安全性。

Abstract: Adversarial attacks have been fairly explored for computer vision and
vision-language models. However, the avenue of adversarial attack for the
vision language segmentation models (VLSMs) is still under-explored, especially
for medical image analysis.
  Thus, we have investigated the robustness of VLSMs against adversarial
attacks for 2D medical images with different modalities with radiology,
photography, and endoscopy. The main idea of this project was to assess the
robustness of the fine-tuned VLSMs specially in the medical domain setting to
address the high risk scenario.
  First, we have fine-tuned pre-trained VLSMs for medical image segmentation
with adapters.
  Then, we have employed adversarial attacks -- projected gradient descent
(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to
determine its robustness against adversaries.
  We have reported models' performance decline to analyze the adversaries'
impact.
  The results exhibit significant drops in the DSC and IoU scores after the
introduction of these adversaries. Furthermore, we also explored universal
perturbation but were not able to find for the medical images.
  \footnote{https://github.com/anjilab/secure-private-ai}

</details>

### [39] [Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking](https://arxiv.org/abs/2505.02980)
*Daniela Ruiz,Paula Cardenas,Leonardo Manrique,Daniela Vega,Gabriel Mejia,Pablo Arbelaez*

Main category: cs.CV

TLDR: SpaRED是一个标准化的数据库，用于评估基因表达预测模型；SpaCKLE是一种基于Transformer的模型，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: Visium技术成本高、效率低，且数据存在丢失问题，需要改进基因表达预测方法。

Method: 提出SpaRED标准化数据库和SpaCKLE模型，用于评估和改进基因表达预测。

Result: SpaCKLE将均方误差降低82.5%，显著提升所有模型的预测结果。

Conclusion: SpaRED和SpaCKLE为空间转录组学提供了全面的基准和未来研究方向。

Abstract: Spatial Transcriptomics is a groundbreaking technology that integrates
histology images with spatially resolved gene expression profiles. Among the
various Spatial Transcriptomics techniques available, Visium has emerged as the
most widely adopted. However, its accessibility is limited by high costs, the
need for specialized expertise, and slow clinical integration. Additionally,
gene capture inefficiencies lead to significant dropout, corrupting acquired
data. To address these challenges, the deep learning community has explored the
gene expression prediction task directly from histology images. Yet,
inconsistencies in datasets, preprocessing, and training protocols hinder fair
comparisons between models. To bridge this gap, we introduce SpaRED, a
systematically curated database comprising 26 public datasets, providing a
standardized resource for model evaluation. We further propose SpaCKLE, a
state-of-the-art transformer-based gene expression completion model that
reduces mean squared error by over 82.5% compared to existing approaches.
Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art
prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE
substantially improves the results across all the gene expression prediction
models. Altogether, our contributions constitute the most comprehensive
benchmark of gene expression prediction from histology images to date and a
stepping stone for future research on Spatial Transcriptomics.

</details>

### [40] [NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results](https://arxiv.org/abs/2505.03007)
*Nikolay Safonov,Alexey Bryncev,Andrey Moskalenko,Dmitry Kulikov,Dmitry Vatolin,Radu Timofte,Haibo Lei,Qifan Gao,Qing Luo,Yaqing Li,Jie Song,Shaozhe Hao,Meisong Zheng,Jingyi Xu,Chengbin Wu,Jiahui Liu,Ying Chen,Xin Deng,Mai Xu,Peipei Liang,Jie Ma,Junjie Jin,Yingxue Pang,Fangzhou Luo,Kai Chen,Shijie Zhao,Mingyang Wu,Renjie Li,Yushen Zuo,Shengyun Zhong,Zhengzhong Tu*

Main category: cs.CV

TLDR: NTIRE 2025挑战赛聚焦于用户生成内容（UGC）视频增强，吸引了25支团队参与，最终7支团队通过验证。挑战赛数据公开，为UGC视频增强领域提供新见解。


<details>
  <summary>Details</summary>
Motivation: UGC视频在短视频平台广泛使用，但其质量常受噪声、模糊等问题影响，亟需提升视觉质量的算法。

Method: 构建了150个无参考真实值的UGC视频数据集，通过主观质量评估（8000多名评估者投票）评价算法效果。

Result: 25支团队提交方案，7支通过验证，结果展示了UGC视频增强的最新进展和有效策略。

Conclusion: 挑战赛成果为UGC视频增强领域提供了重要参考，数据公开促进进一步研究。

Abstract: This paper presents an overview of the NTIRE 2025 Challenge on UGC Video
Enhancement. The challenge constructed a set of 150 user-generated content
videos without reference ground truth, which suffer from real-world
degradations such as noise, blur, faded colors, compression artifacts, etc. The
goal of the participants was to develop an algorithm capable of improving the
visual quality of such videos. Given the widespread use of UGC on short-form
video platforms, this task holds substantial practical importance. The
evaluation was based on subjective quality assessment in crowdsourcing,
obtaining votes from over 8000 assessors. The challenge attracted more than 25
teams submitting solutions, 7 of which passed the final phase with source code
verification. The outcomes may provide insights into the state-of-the-art in
UGC video enhancement and highlight emerging trends and effective strategies in
this evolving research area. All data, including the processed videos and
subjective comparison votes and scores, is made publicly available at
https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.

</details>

### [41] [GIF: Generative Inspiration for Face Recognition at Scale](https://arxiv.org/abs/2505.03012)
*Saeed Ebrahimi,Sahar Rahimi,Ali Dabouei,Srinjoy Das,Jeremy M. Dawson,Nasser M. Nasrabadi*

Main category: cs.CV

TLDR: 提出了一种用结构化身份码替代标量标签的方法，将计算成本从线性降至对数级别，并在人脸识别任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 减少大规模人脸识别中Softmax的计算成本，现有方法虽有效但计算成本仍与身份数量线性相关。

Method: 将标量标签转换为结构化身份码，训练模型预测输入的身份码而非标量标签。

Result: 在IJB-B和IJB-C上分别提升1.52%和0.6%的TAR@FAR=1e-4，计算成本从线性降至对数。

Conclusion: 结构化身份码方法显著降低计算成本并提升性能，适用于大规模人脸识别任务。

Abstract: Aiming to reduce the computational cost of Softmax in massive label space of
Face Recognition (FR) benchmarks, recent studies estimate the output using a
subset of identities. Although promising, the association between the
computation cost and the number of identities in the dataset remains linear
only with a reduced ratio. A shared characteristic among available FR methods
is the employment of atomic scalar labels during training. Consequently, the
input to label matching is through a dot product between the feature vector of
the input and the Softmax centroids. Inspired by generative modeling, we
present a simple yet effective method that substitutes scalar labels with
structured identity code, i.e., a sequence of integers. Specifically, we
propose a tokenization scheme that transforms atomic scalar labels into
structured identity codes. Then, we train an FR backbone to predict the code
for each input instead of its scalar label. As a result, the associated
computational cost becomes logarithmic w.r.t. number of identities. We
demonstrate the benefits of the proposed method by conducting experiments. In
particular, our method outperforms its competitors by 1.52%, and 0.6% at
TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the
association between computational cost and the number of identities from linear
to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF

</details>

### [42] [Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer](https://arxiv.org/abs/2505.03018)
*Aurora Rofena,Arianna Manchia,Claudia Lucia Piccolo,Bruno Beomonte Zobel,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: Seg-CycleGAN是一种生成深度学习框架，用于在CESM中实现虚拟对比增强，通过低能量图像合成高质量的双能量减影图像，减少辐射和对比剂副作用。


<details>
  <summary>Details</summary>
Motivation: CESM虽然诊断准确性高，但存在辐射和对比剂副作用问题，需要一种无对比剂的替代方案。

Method: 提出Seg-CycleGAN，利用病灶分割图指导生成过程，改进病灶重建，并在CycleGAN基础上引入局部损失项。

Result: 在CESM@UCBM数据集上，Seg-CycleGAN在PSNR和SSIM上优于基线，同时保持竞争力的MSE和VIF。

Conclusion: Seg-CycleGAN为无对比剂CESM提供了一种可行方案，生成图像具有更高的病灶保真度。

Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
technique that improves lesion visibility through the administration of an
iodinated contrast agent. It acquires both a low-energy image, comparable to
standard mammography, and a high-energy image, which are then combined to
produce a dual-energy subtracted image highlighting lesion contrast
enhancement. While CESM offers superior diagnostic accuracy compared to
standard mammography, its use entails higher radiation exposure and potential
side effects associated with the contrast medium. To address these limitations,
we propose Seg-CycleGAN, a generative deep learning framework for Virtual
Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy
subtracted images from low-energy images, leveraging lesion segmentation maps
to guide the generative process and improve lesion reconstruction. Building
upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss
terms focused on lesion areas, enhancing the synthesis of diagnostically
relevant regions. Experiments on the CESM@UCBM dataset demonstrate that
Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while
maintaining competitive MSE and VIF. Qualitative evaluations further confirm
improved lesion fidelity in the generated images. These results suggest that
segmentation-aware generative models offer a viable pathway toward
contrast-free CESM alternatives.

</details>

### [43] [An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices](https://arxiv.org/abs/2505.03039)
*Yuezhou Zhang,Amos A. Folarin,Callum Stewart,Heet Sankesara,Yatharth Ranjan,Pauline Conde,Akash Roy Choudhury,Shaoxiong Sun,Zulqarnain Rashid,Richard J. B. Dobson*

Main category: cs.CV

TLDR: 提出了一种基于可穿戴设备的可解释异常检测框架，用于早期检测抑郁和焦虑症状的恶化。


<details>
  <summary>Details</summary>
Motivation: 通过可穿戴设备连续监测行为和生理数据，为早期发现心理健康问题提供客观方法。

Method: 使用LSTM自编码器模型分析睡眠时长、步数和静息心率数据，结合临床显著阈值（症状评分增加≥5分）标记异常。

Result: 模型在检测症状恶化事件中表现良好（F1=0.80），静息心率是最具影响力的特征。

Conclusion: 可解释的异常检测框架为个性化、可扩展的心理健康监测提供了潜力。

Abstract: Continuous monitoring of behavior and physiology via wearable devices offers
a novel, objective method for the early detection of worsening depression and
anxiety. In this study, we present an explainable anomaly detection framework
that identifies clinically meaningful increases in symptom severity using
consumer-grade wearable data. Leveraging data from 2,023 participants with
defined healthy baselines, our LSTM autoencoder model learned normal health
patterns of sleep duration, step count, and resting heart rate. Anomalies were
flagged when self-reported depression or anxiety scores increased by >=5 points
(a threshold considered clinically significant). The model achieved an adjusted
F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393
symptom-worsening episodes across 341 participants, with higher performance
observed for episodes involving concurrent depression and anxiety escalation
(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =
0.85). Model interpretability was supported by SHAP-based analysis, which
identified resting heart rate as the most influential feature in 71.4
percentage of detected anomalies, followed by physical activity and sleep.
Together, our findings highlight the potential of explainable anomaly detection
to enable personalized, scalable, and proactive mental health monitoring in
real-world settings.

</details>

### [44] [Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera](https://arxiv.org/abs/2505.03093)
*Siming He,Zachary Osman,Fernando Cladera,Dexter Ong,Nitant Rai,Patrick Corey Green,Vijay Kumar,Pratik Chaudhari*

Main category: cs.CV

TLDR: 提出了一种低成本、半自动化的方法，使用消费级360度摄像头测量树木胸径（DBH），精度接近激光雷达技术。


<details>
  <summary>Details</summary>
Motivation: 激光雷达技术虽然精度高，但成本昂贵且操作复杂，需要一种更经济、易用的替代方案。

Method: 结合SfM重建、语义分割和RANSAC技术，通过360度摄像头生成点云并估计DBH。

Result: 在61次实验中，相对误差为5-9%，仅比激光雷达高2-4%，但成本大幅降低。

Conclusion: 该方法为森林资源管理提供了一种低成本、高精度的解决方案。

Abstract: Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.

</details>

### [45] [Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability](https://arxiv.org/abs/2505.03097)
*Lei Wang,Senmao Li,Fei Yang,Jianye Wang,Ziheng Zhang,Yuhan Liu,Yaxing Wang,Jian Yang*

Main category: cs.CV

TLDR: 论文提出了一种名为MaskUNet的方法，通过动态调整U-Net参数来提升扩散模型的生成质量，同时提供了两种优化策略。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在同一网络层中同时学习结构和纹理信息，与传统深度学习架构（如ResNet或GANs）不同，这激发了研究者探索时间相关的扩散模型。

Method: 通过分析U-Net参数在去噪过程中的作用，发现适当归零某些参数（包括大参数）能提升生成质量。基于此，提出了MaskUNet方法，利用时间步和样本依赖的有效参数。

Result: 在COCO数据集的零样本推理中，MaskUNet取得了最佳FID分数，并在下游任务评估中表现出色。

Conclusion: MaskUNet是一种简单有效的方法，显著提升了扩散模型的生成质量，同时提供了灵活的优化策略。

Abstract: The diffusion models, in early stages focus on constructing basic image
structures, while the refined details, including local features and textures,
are generated in later stages. Thus the same network layers are forced to learn
both structural and textural information simultaneously, significantly
differing from the traditional deep learning architectures (e.g., ResNet or
GANs) which captures or generates the image semantic information at different
layers. This difference inspires us to explore the time-wise diffusion models.
We initially investigate the key contributions of the U-Net parameters to the
denoising process and identify that properly zeroing out certain parameters
(including large parameters) contributes to denoising, substantially improving
the generation quality on the fly. Capitalizing on this discovery, we propose a
simple yet effective method-termed ``MaskUNet''- that enhances generation
quality with negligible parameter numbers. Our method fully leverages timestep-
and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer
two fine-tuning strategies: a training-based approach and a training-free
approach, including tailored networks and optimization functions. In zero-shot
inference on the COCO dataset, MaskUNet achieves the best FID score and further
demonstrates its effectiveness in downstream task evaluations. Project page:
https://gudaochangsheng.github.io/MaskUnet-Page/

</details>

### [46] [Image Recognition with Online Lightweight Vision Transformer: A Survey](https://arxiv.org/abs/2505.03113)
*Zherui Zhang,Rongtao Xu,Jie Zhou,Changwei Wang,Xingtian Pei,Wenhao Xu,Jiguang Zhang,Li Guo,Longxiang Gao,Wenbo Xu,Shibiao Xu*

Main category: cs.CV

TLDR: 本文综述了轻量级视觉Transformer在图像识别中的应用，重点探讨了高效组件设计、动态网络和知识蒸馏三个方向，并分析了其优缺点及灵活性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在自然语言处理中的成功激发了其在计算机视觉任务中的应用，但面临计算和内存挑战，限制了实际应用。

Method: 通过高效组件设计、动态网络和知识蒸馏三种策略生成轻量级视觉Transformer，并在ImageNet-1K基准上评估。

Result: 分析了精度、参数量、吞吐量等权衡，总结了各策略的优势、劣势和灵活性。

Conclusion: 提出了轻量级视觉Transformer的未来研究方向与潜在挑战，旨在为社区提供实践指导和启发进一步探索。

Abstract: The Transformer architecture has achieved significant success in natural
language processing, motivating its adaptation to computer vision tasks. Unlike
convolutional neural networks, vision transformers inherently capture
long-range dependencies and enable parallel processing, yet lack inductive
biases and efficiency benefits, facing significant computational and memory
challenges that limit its real-world applicability. This paper surveys various
online strategies for generating lightweight vision transformers for image
recognition, focusing on three key areas: Efficient Component Design, Dynamic
Network, and Knowledge Distillation. We evaluate the relevant exploration for
each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,
parameters, throughput, and more to highlight their respective advantages,
disadvantages, and flexibility. Finally, we propose future research directions
and potential challenges in the lightweighting of vision transformers with the
aim of inspiring further exploration and providing practical guidance for the
community. Project Page: https://github.com/ajxklo/Lightweight-VIT

</details>

### [47] [Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation](https://arxiv.org/abs/2505.03114)
*Teng Zhou,Jax Luo,Yuping Sun,Yiheng Tan,Shun Yao,Nazim Haouchine,Scott Raymond*

Main category: cs.CV

TLDR: 提出了一种基于路径和骨轮廓正则化的无配对MRI到CT转换方法，显著提升了骨结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有无配对MRI到CT转换方法在骨结构等解剖特征上的不足，适用于放射治疗等需要精确骨表示的应用。

Method: 通过神经常微分方程建模连续流，最小化路径长度，并引入可训练神经网络生成骨轮廓以增强准确性。

Result: 在三个数据集上表现优于现有方法，整体错误率更低，且在骨分割任务中表现出色。

Conclusion: 该方法在无配对MRI到CT转换中实现了更高的骨结构保真度，适用于临床需求。

Abstract: Accurate MRI-to-CT translation promises the integration of complementary
imaging information without the need for additional imaging sessions. Given the
practical challenges associated with acquiring paired MRI and CT scans, the
development of robust methods capable of leveraging unpaired datasets is
essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT
translation methods, which predominantly rely on cycle consistency and
contrastive learning frameworks, frequently encounter challenges in accurately
translating anatomical features that are highly discernible on CT but less
distinguishable on MRI, such as bone structures. This limitation renders these
approaches less suitable for applications in radiation therapy, where precise
bone representation is essential for accurate treatment planning. To address
this challenge, we propose a path- and bone-contour regularized approach for
unpaired MRI-to-CT translation. In our method, MRI and CT images are projected
to a shared latent space, where the MRI-to-CT mapping is modeled as a
continuous flow governed by neural ordinary differential equations. The optimal
mapping is obtained by minimizing the transition path length of the flow. To
enhance the accuracy of translated bone structures, we introduce a trainable
neural network to generate bone contours from MRI and implement mechanisms to
directly and indirectly encourage the model to focus on bone contours and their
adjacent regions. Evaluations conducted on three datasets demonstrate that our
method outperforms existing unpaired MRI-to-CT translation approaches,
achieving lower overall error rates. Moreover, in a downstream bone
segmentation task, our approach exhibits superior performance in preserving the
fidelity of bone structures. Our code is available at:
https://github.com/kennysyp/PaBoT.

</details>

### [48] [TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion](https://arxiv.org/abs/2505.03116)
*Haoyue Liu,Jinghan Xu,Yi Chang,Hanyu Zhou,Haozhi Zhao,Lin Wang,Luxin Yan*

Main category: cs.CV

TLDR: 论文提出了一种基于连续点跟踪的视频帧插值框架TimeTracker，通过事件相机处理非线性运动，显著提升了插值质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高时间分辨率优势未被充分利用，现有方法在处理非线性运动时存在运动误差问题。

Method: 设计了场景感知区域分割模块（SARS）和连续轨迹引导运动估计模块（CTME），通过跟踪局部区域的连续运动轨迹优化帧插值。

Result: 实验表明，该方法在运动估计和帧插值质量上优于现有技术。

Conclusion: TimeTracker框架有效解决了非线性运动问题，提升了视频帧插值的性能。

Abstract: Video frame interpolation (VFI) that leverages the bio-inspired event cameras
as guidance has recently shown better performance and memory efficiency than
the frame-based methods, thanks to the event cameras' advantages, such as high
temporal resolution. A hurdle for event-based VFI is how to effectively deal
with non-linear motion, caused by the dynamic changes in motion direction and
speed within the scene. Existing methods either use events to estimate sparse
optical flow or fuse events with image features to estimate dense optical flow.
Unfortunately, motion errors often degrade the VFI quality as the continuous
motion cues from events do not align with the dense spatial information of
images in the temporal dimension. In this paper, we find that object motion is
continuous in space, tracking local regions over continuous time enables more
accurate identification of spatiotemporal feature correlations. In light of
this, we propose a novel continuous point tracking-based VFI framework, named
TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation
(SARS) module to divide the scene into similar patches. Then, a Continuous
Trajectory guided Motion Estimation (CTME) module is proposed to track the
continuous motion trajectory of each patch through events. Finally,
intermediate frames at any given time are generated through global motion
optimization and frame refinement. Moreover, we collect a real-world dataset
that features fast non-linear motion. Extensive experiments show that our
method outperforms prior arts in both motion estimation and frame interpolation
quality.

</details>

### [49] [VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis](https://arxiv.org/abs/2505.03132)
*Xinyuan Yan,Xiwei Xuan,Jorge Piazentin Ono,Jiajing Guo,Vikram Mohanty,Shekar Arvind Kumar,Liang Gou,Bei Wang,Liu Ren*

Main category: cs.CV

TLDR: VISLIX是一个新的视觉分析框架，利用先进的基础模型帮助专家分析计算机视觉模型中的数据切片，无需额外元数据，自动生成自然语言见解，并支持交互式测试。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习模型需要严格评估，尤其是在安全关键领域。数据切片虽有效，但在计算机视觉任务中面临挑战，如依赖元数据、缺乏交互式解决方案等。

Method: 提出VISLIX框架，利用基础模型自动生成数据切片见解，支持专家交互式测试假设。

Result: 通过专家研究和三个用例验证了VISLIX在对象检测模型验证中的有效性。

Conclusion: VISLIX克服了现有数据切片的局限性，为计算机视觉模型验证提供了更全面的支持。

Abstract: Real-world machine learning models require rigorous evaluation before
deployment, especially in safety-critical domains like autonomous driving and
surveillance. The evaluation of machine learning models often focuses on data
slices, which are subsets of the data that share a set of characteristics. Data
slice finding automatically identifies conditions or data subgroups where
models underperform, aiding developers in mitigating performance issues.
Despite its popularity and effectiveness, data slicing for vision model
validation faces several challenges. First, data slicing often needs additional
image metadata or visual concepts, and falls short in certain computer vision
tasks, such as object detection. Second, understanding data slices is a
labor-intensive and mentally demanding process that heavily relies on the
expert's domain knowledge. Third, data slicing lacks a human-in-the-loop
solution that allows experts to form hypothesis and test them interactively. To
overcome these limitations and better support the machine learning operations
lifecycle, we introduce VISLIX, a novel visual analytics framework that employs
state-of-the-art foundation models to help domain experts analyze slices in
computer vision models. Our approach does not require image metadata or visual
concepts, automatically generates natural language insights, and allows users
to test data slice hypothesis interactively. We evaluate VISLIX with an expert
study and three use cases, that demonstrate the effectiveness of our tool in
providing comprehensive insights for validating object detection models.

</details>

### [50] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/abs/2505.03134)
*Sajjad Rezvani Boroujeni,Hossein Abedi,Tom Bush*

Main category: cs.CV

TLDR: 论文提出了一种基于DDPMs的方法，通过生成合成缺陷图像解决玻璃制造中数据不平衡问题，显著提升了缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业玻璃制造中缺陷检测因数据不平衡问题导致深度学习模型性能受限，需要一种有效的数据增强方法。

Method: 使用Denoising Diffusion Probabilistic Models (DDPMs)生成合成缺陷图像，增强少数类样本，提升CNN模型的分类性能。

Result: 实验显示，数据增强后ResNet50V2的分类准确率从78%提升至93%，所有测试模型的缺陷样本召回率显著提高。

Conclusion: 该方法为玻璃制造中的缺陷检测提供了一种可扩展、经济高效的解决方案，并可推广至其他类似工业领域。

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from 78 percent to 93 percent when trained with the augmented data.
This work provides a scalable, cost-effective approach to enhancing automated
defect detection in glass manufacturing that can potentially be extended to
other industrial quality assurance systems and industries with similar class
imbalance challenges.

</details>

### [51] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/abs/2505.03149)
*Joseph William Kettelkamp,Ludovica Romanin,Sarv Priya,Mathews Jacob*

Main category: cs.CV

TLDR: 提出一种无监督运动补偿图像重建算法，用于自由呼吸和无门控3D心脏MRI，通过低秩模型表示运动相位间的变形。


<details>
  <summary>Details</summary>
Motivation: 解决自由呼吸和无门控3D心脏MRI中运动伪影问题，提高图像重建质量。

Method: 将每个运动相位的图像体积表示为静态模板的变形，采用低秩模型表示变形家族，通过参数化速度场积分获得变形。

Result: 相比现有运动分辨和运动补偿算法，该模型在自由呼吸3D cine MRI中提供更好的恢复效果。

Conclusion: 低秩运动模型在无监督学习中能有效提升图像重建质量。

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>

### [52] [Robust Fairness Vision-Language Learning for Medical Image Analysis](https://arxiv.org/abs/2505.03153)
*Sparsh Bansal,Mingyang Wu,Xin Wang,Shu Hu*

Main category: cs.CV

TLDR: 提出了一种确保视觉语言模型（VLM）在医学图像分析中公平性和鲁棒性的框架，通过动态坏对挖掘算法和Sinkhorn距离优化损失函数。


<details>
  <summary>Details</summary>
Motivation: 医学领域的VLM需要确保公平性和鲁棒性，以避免对不同患者群体的偏见。

Method: 采用动态坏对挖掘算法识别并调整错误的图像-文本对，并使用Sinkhorn距离确保保护组的损失分布与总体一致。

Result: 实验显示，公平性调整后的AUC提升了8.6%。

Conclusion: 该框架有效提升了VLM在医学领域的公平性和鲁棒性。

Abstract: The advent of Vision-Language Models (VLMs) in medical image analysis has the
potential to help process multimodal inputs and increase performance over
traditional inference methods. However, when considering the domain in which
these models will be implemented, fairness and robustness are important to
ensure the model stays true for any patient. In this paper, we introduce a
framework for ensuring robustness and fairness of VLM models. This framework
modifies the loss function at training by identifying and adjusting faulty
image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing
Sinkhorn distance to ensure the loss distributions of protected groups do not
deviate from the total loss. Experimental testing of our framework shows up to
a 8.6\% improvement when looking at equity-scaled AUC.

</details>

### [53] [StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data](https://arxiv.org/abs/2505.03154)
*Yuxuan Mu,Hung Yu Ling,Yi Shi,Ismael Baira Ojeda,Pengcheng Xi,Chang Shu,Fabio Zinno,Xue Bin Peng*

Main category: cs.CV

TLDR: StableMotion提出了一种直接从无配对数据训练运动清理模型的方法，通过运动质量指标实现高质量运动生成，显著减少运动伪影。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据常因传感器和后期处理不准确而产生伪影，手动清理成本高且耗时。现有方法需配对数据，但获取高质量配对数据困难。

Method: 引入运动质量指标，通过手动标注或启发式算法标注，训练质量感知运动生成模型，采用扩散框架实现生成-判别一体化。

Result: 在SoccerMocap数据集上，模型有效减少运动伪影，运动弹出和冻结帧分别减少68%和81%。

Conclusion: StableMotion无需配对数据即可训练运动清理模型，显著提升运动数据质量，适用于实际生产场景。

Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to
inaccurate sensors and post-processing. Cleaning this corrupted data can
require substantial manual effort from human experts, which can be a costly and
time-consuming process. Previous data-driven motion cleanup methods offer the
promise of automating this cleanup process, but often require in-domain paired
corrupted-to-clean training data. Constructing such paired datasets requires
access to high-quality, relatively artifact-free motion clips, which often
necessitates laborious manual cleanup. In this work, we present StableMotion, a
simple yet effective method for training motion cleanup models directly from
unpaired corrupted datasets that need cleanup. The core component of our method
is the introduction of motion quality indicators, which can be easily annotated
through manual labeling or heuristic algorithms and enable training of
quality-aware motion generation models on raw motion data with mixed quality.
At test time, the model can be prompted to generate high-quality motions using
the quality indicators. Our method can be implemented through a simple
diffusion-based framework, leading to a unified motion generate-discriminate
model, which can be used to both identify and fix corrupted frames. We
demonstrate that our proposed method is effective for training motion cleanup
models on raw mocap data in production scenarios by applying StableMotion to
SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion
artifacts. The trained model effectively corrects a wide range of motion
artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.
See https://youtu.be/3Y7MMAH02B4 for more results.

</details>

### [54] [RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph](https://arxiv.org/abs/2505.03173)
*Sameer Malik,Moyuru Yamada,Ayush Singh,Dishank Aggarwal*

Main category: cs.CV

TLDR: RAVU框架通过检索增强和时空图推理提升长视频理解能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）因缺乏显式记忆和检索机制，难以处理长视频。

Method: 构建视频的时空图表示，作为长期记忆，并通过分解查询为推理步骤在图检索关键信息。

Result: 在NExT-QA和EgoSchema数据集上，RAVU仅需5-10帧检索即优于其他方法。

Conclusion: RAVU为长视频理解提供了高效解决方案，特别适用于多跳推理和跨帧对象追踪任务。

Abstract: Comprehending long videos remains a significant challenge for Large
Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to
hours videos due to their lack of explicit memory and retrieval mechanisms. To
address this limitation, we propose RAVU (Retrieval Augmented Video
Understanding), a novel framework for video understanding enhanced by retrieval
with compositional reasoning over a spatio-temporal graph. We construct a graph
representation of the video, capturing both spatial and temporal relationships
between entities. This graph serves as a long-term memory, allowing us to track
objects and their actions across time. To answer complex queries, we decompose
the queries into a sequence of reasoning steps and execute these steps on the
graph, retrieving relevant key information. Our approach enables more accurate
understanding of long videos, particularly for queries that require multi-hop
reasoning and tracking objects across frames. Our approach demonstrate superior
performances with limited retrieved frames (5-10) compared with other SOTA
methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.

</details>

### [55] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/abs/2505.03176)
*Hafez Ghaemi,Eilif Muller,Shahab Bakhtiari*

Main category: cs.CV

TLDR: seq-JEPA是一种基于联合嵌入预测架构的世界建模范式，通过处理图像的不同视图序列，同时学习不变和等变表示，解决了传统双视图范式的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前自监督算法主要依赖数据增强和掩码等变换学习视觉表示，但双视图范式限制了表示灵活性，导致不变性和等变性任务之间的性能权衡。

Method: seq-JEPA通过处理输入图像的不同视图序列，结合相对变换的嵌入，使用Transformer编码器输出序列的聚合表示，并预测下一个视图的表示。

Result: seq-JEPA在等变基准测试和图像分类任务中表现优异，且无需牺牲任一任务的性能。

Conclusion: seq-JEPA通过架构设计同时学习不变和等变表示，解决了传统方法的性能权衡问题，并在序列观察任务中表现出色。

Abstract: Current self-supervised algorithms mostly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by inducing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm can limit
the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between invariance-related tasks such as image
classification and more fine-grained equivariance-related tasks. In this work,
we introduce \emph{seq-JEPA}, a world modeling paradigm based on
joint-embedding predictive architecture that leverages architectural inductive
biases to resolve this trade-off. Without requiring an additional equivariance
predictor or loss term, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to the specified transformations
and another invariant to them and suited for tasks such as classification. To
do so, our model processes a short sequence of different views (observations)
of an input image. Each encoded view is concatenated with embeddings
corresponding to the relative transformation (action) producing the next
observation in the sequence. A transformer encoder outputs an aggregate
representation of this sequence, which is subsequently conditioned on the
action leading to the next observation to predict its representation.
Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and
image classification without sacrificing one for the other. Additionally, our
framework excels at tasks that inherently require aggregating a sequence of
observations, such as path integration across actions and predictive learning
across eye movements.

</details>

### [56] [Interactive Instance Annotation with Siamese Networks](https://arxiv.org/abs/2505.03184)
*Xiang Xu,Ruotong Li,Mengjun Yi,Baile XU,Furao Shen,Jian Zhao*

Main category: cs.CV

TLDR: SiamAnno是一个基于Siamese网络的框架，用于跨域实例标注，通过单次学习预测对象边界，无需微调即可在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 实例标注耗时费力，现有方法多局限于同域场景，难以应对跨域任务。

Method: SiamAnno利用Siamese网络和单次学习，输入边界框预测对象边界，支持用户调整。

Result: 在未微调的情况下，SiamAnno在跨域任务中表现优异，达到SOTA性能。

Conclusion: SiamAnno是首个探索Siamese架构用于实例标注的模型，为未来研究提供了强基线。

Abstract: Annotating instance masks is time-consuming and labor-intensive. A promising
solution is to predict contours using a deep learning model and then allow
users to refine them. However, most existing methods focus on in-domain
scenarios, limiting their effectiveness for cross-domain annotation tasks. In
this paper, we propose SiamAnno, a framework inspired by the use of Siamese
networks in object tracking. SiamAnno leverages one-shot learning to annotate
previously unseen objects by taking a bounding box as input and predicting
object boundaries, which can then be adjusted by annotators. Trained on one
dataset and tested on another without fine-tuning, SiamAnno achieves
state-of-the-art (SOTA) performance across multiple datasets, demonstrating its
ability to handle domain and environment shifts in cross-domain tasks. We also
provide more comprehensive results compared to previous work, establishing a
strong baseline for future research. To our knowledge, SiamAnno is the first
model to explore Siamese architecture for instance annotation.

</details>

### [57] [PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models](https://arxiv.org/abs/2505.03203)
*Chang Xie,Chenyi Zhuang,Pan Gao*

Main category: cs.CV

TLDR: PiCo提出了一种无需训练的方法，通过噪声选择和参考掩码模块提升文本到图像生成的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在复杂文本提示下难以实现文本与图像的对齐，PiCo旨在解决这一问题。

Method: PiCo包含噪声选择模块和参考掩码模块，分别优化噪声质量和控制掩码生成。

Result: 实验验证了PiCo在提升文本-图像对齐和减少随机生成繁琐性方面的有效性。

Conclusion: PiCo通过噪声选择和掩码调制显著提升了文本到图像生成的对齐效果。

Abstract: Advanced diffusion models have made notable progress in text-to-image
compositional generation. However, it is still a challenge for existing models
to achieve text-image alignment when confronted with complex text prompts. In
this work, we highlight two factors that affect this alignment: the quality of
the randomly initialized noise and the reliability of the generated controlling
mask. We then propose PiCo (Pick-and-Control), a novel training-free approach
with two key components to tackle these two factors. First, we develop a noise
selection module to assess the quality of the random noise and determine
whether the noise is suitable for the target text. A fast sampling strategy is
utilized to ensure efficiency in the noise selection stage. Second, we
introduce a referring mask module to generate pixel-level masks and to
precisely modulate the cross-attention maps. The referring mask is applied to
the standard diffusion process to guide the reasonable interaction between text
and image features. Extensive experiments have been conducted to verify the
effectiveness of PiCo in liberating users from the tedious process of random
generation and in enhancing the text-image alignment for diverse text
descriptions.

</details>

### [58] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/abs/2505.03204)
*Liu Suxing,Byungwon Min*

Main category: cs.CV

TLDR: 深度学习在乳腺癌病理图像分类中表现良好，但在标注数据有限时性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中标注数据稀缺导致深度学习性能下降的问题。

Method: 使用深度学习技术对乳腺癌病理图像进行分类。

Result: 在标注数据有限的情况下，深度学习模型的性能显著下降。

Conclusion: 需要解决标注数据稀缺的问题以提升深度学习在医学影像中的应用效果。

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>

### [59] [Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data](https://arxiv.org/abs/2505.03220)
*Shaheer Mohamed,Tharindu Fernando,Sridha Sridharan,Peyman Moghadam,Clinton Fookes*

Main category: cs.CV

TLDR: 提出了一种名为SFMIM的自监督预训练策略，用于解决高光谱图像（HSI）标记数据稀缺的问题，通过空间和频率域的双重掩码机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）数据标记稀缺，限制了深度学习尤其是基于Transformer的架构的应用潜力。

Method: SFMIM方法在空间和频率域引入双重掩码机制，通过随机掩码空间块和频率分量，训练模型重建掩码部分以学习高阶谱-空相关性。

Result: 在三个公开的HSI分类基准测试中达到最优性能，且微调时收敛速度快。

Conclusion: SFMIM策略有效利用未标记数据，显著提升了高光谱图像的分类性能。

Abstract: Hyperspectral images (HSIs) capture rich spectral signatures that reveal
vital material properties, offering broad applicability across various domains.
However, the scarcity of labeled HSI data limits the full potential of deep
learning, especially for transformer-based architectures that require
large-scale training. To address this constraint, we propose Spatial-Frequency
Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for
hyperspectral data that utilizes the large portion of unlabeled data. Our
method introduces a novel dual-domain masking mechanism that operates in both
spatial and frequency domains. The input HSI cube is initially divided into
non-overlapping patches along the spatial dimension, with each patch comprising
the entire spectrum of its corresponding spatial location. In spatial masking,
we randomly mask selected patches and train the model to reconstruct the masked
inputs using the visible patches. Concurrently, in frequency masking, we remove
portions of the frequency components of the input spectra and predict the
missing frequencies. By learning to reconstruct these masked components, the
transformer-based encoder captures higher-order spectral-spatial correlations.
We evaluate our approach on three publicly available HSI classification
benchmarks and demonstrate that it achieves state-of-the-art performance.
Notably, our model shows rapid convergence during fine-tuning, highlighting the
efficiency of our pretraining strategy.

</details>

### [60] [Seeing the Abstract: Translating the Abstract Language for Vision Language Models](https://arxiv.org/abs/2505.03242)
*Davide Talon,Federico Girella,Ziyue Liu,Marco Cristani,Yiming Wang*

Main category: cs.CV

TLDR: 论文揭示了抽象语言在视觉语言模型（VLM）中的广泛存在和被低估的价值，提出了一种无需训练的模型无关方法（ACT），在检索任务中表现优于微调的VLM。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型研究未关注抽象语言，而自然语言中抽象概念（如情感、创意）丰富且重要。

Method: 提出Abstract-to-Concrete Translator (ACT)，通过预训练模型和多模态数据库，将抽象表示转换为VLM潜在空间中已充分表示的具体表示。

Result: ACT在文本到图像检索任务中优于微调的VLM，具有强泛化能力，且适用于多种VLM。

Conclusion: ACT是一种即插即用的解决方案，显著提升了VLM处理抽象语言的能力。

Abstract: Natural language goes beyond dryly describing visual content. It contains
rich abstract concepts to express feeling, creativity and properties that
cannot be directly perceived. Yet, current research in Vision Language Models
(VLMs) has not shed light on abstract-oriented language. Our research breaks
new ground by uncovering its wide presence and under-estimated value, with
extensive analysis. Particularly, we focus our investigation on the fashion
domain, a highly-representative field with abstract expressions. By analyzing
recent large-scale multimodal fashion datasets, we find that abstract terms
have a dominant presence, rivaling the concrete ones, providing novel
information, and being useful in the retrieval task. However, a critical
challenge emerges: current general-purpose or fashion-specific VLMs are
pre-trained with databases that lack sufficient abstract words in their text
corpora, thus hindering their ability to effectively represent
abstract-oriented language. We propose a training-free and model-agnostic
method, Abstract-to-Concrete Translator (ACT), to shift abstract
representations towards well-represented concrete ones in the VLM latent space,
using pre-trained models and existing multimodal databases. On the
text-to-image retrieval task, despite being training-free, ACT outperforms the
fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its
effectiveness with a strong generalization capability. Moreover, the
improvement introduced by ACT is consistent with various VLMs, making it a
plug-and-play solution.

</details>

### [61] [PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs](https://arxiv.org/abs/2505.03254)
*Lukas Meiner,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TLDR: PROM提出了一种针对深度可分离卷积网络的双位宽量化方法，通过将点卷积量化为三元权重，其余模块使用8位权重，显著降低了能耗和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现代深度可分离卷积网络中，点卷积操作计算成本高，现有量化方法未能充分利用效率潜力。

Method: 使用两种位宽：点卷积量化为三元权重，其余模块用8位权重，并通过量化感知训练实现。激活量化为8位，将点卷积转换为int8加法。

Result: 在MobileNetV2上，PROM将能耗降低23.9倍，存储需求减少2.7倍，同时保持ImageNet分类性能。

Conclusion: PROM通过简单方法解决了深度可分离卷积网络的量化挑战，显著提升了能效和存储效率。

Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on
resource-constrained devices. Quantization effectively compresses these models,
reducing storage size and energy cost. However, in modern depthwise-separable
architectures, the computational cost is distributed unevenly across its
components, with pointwise operations being the most expensive. By applying a
general quantization scheme to this imbalanced cost distribution, existing
quantization approaches fail to fully exploit potential efficiency gains. To
this end, we introduce PROM, a straightforward approach for quantizing modern
depthwise-separable convolutional networks by selectively using two distinct
bit-widths. Specifically, pointwise convolutions are quantized to ternary
weights, while the remaining modules use 8-bit weights, which is achieved
through a simple quantization-aware training procedure. Additionally, by
quantizing activations to 8-bit, our method transforms pointwise convolutions
with ternary weights into int8 additions, which enjoy broad support across
hardware platforms and effectively eliminates the need for expensive
multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost
by more than an order of magnitude (23.9x) and its storage size by 2.7x
compared to the float16 baseline while retaining similar classification
performance on ImageNet. Our method advances the Pareto frontier for energy
consumption vs. top-1 accuracy for quantized convolutional models on ImageNet.
PROM addresses the challenges of quantizing depthwise-separable convolutional
networks to both ternary and 8-bit weights, offering a simple way to reduce
energy cost and storage size.

</details>

### [62] [DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor](https://arxiv.org/abs/2505.03261)
*Wei-Ting Chen,Yu-Jiet Vong,Yi-Tsung Lee,Sy-Yen Kuo,Qiang Gao,Sizhuo Ma,Jian Wang*

Main category: cs.CV

TLDR: DiffVQA是一种新型视频质量评估框架，利用扩散模型的泛化能力，结合语义和失真特征提取，以及Mamba模块处理时间动态，显著提升了与人类感知的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和ViT的视频质量评估方法在多样化的真实场景中难以与人类感知对齐，且受限于数据集规模和多样性。

Method: DiffVQA通过预训练的扩散模型重构输入帧，提取语义和失真特征，并引入Mamba模块处理时间动态，合并特征预测最终评分。

Result: 实验表明，DiffVQA在多个数据集上表现优异，尤其在跨数据集泛化能力上显著优于CNN和ViT方法。

Conclusion: DiffVQA证明了扩散模型作为特征提取器在视频质量评估中的优越性，为未来研究提供了新方向。

Abstract: Video Quality Assessment (VQA) aims to evaluate video quality based on
perceptual distortions and human preferences. Despite the promising performance
of existing methods using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs), they often struggle to align closely with human
perceptions, particularly in diverse real-world scenarios. This challenge is
exacerbated by the limited scale and diversity of available datasets. To
address this limitation, we introduce a novel VQA framework, DiffVQA, which
harnesses the robust generalization capabilities of diffusion models
pre-trained on extensive datasets. Our framework adapts these models to
reconstruct identical input frames through a control module. The adapted
diffusion model is then used to extract semantic and distortion features from a
resizing branch and a cropping branch, respectively. To enhance the model's
ability to handle long-term temporal dynamics, a parallel Mamba module is
introduced, which extracts temporal coherence augmented features that are
merged with the diffusion features to predict the final score. Experiments
across multiple datasets demonstrate DiffVQA's superior performance on
intra-dataset evaluations and its exceptional generalization across datasets.
These results confirm that leveraging a diffusion model as a feature extractor
can offer enhanced VQA performance compared to CNN and ViT backbones.

</details>

### [63] [OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.03284)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Yaoqi Huang,Hongyu Lyu,Nguyen Hoang Khoi Tran,Tzu-Yun Tseng,Stewart Worrall*

Main category: cs.CV

TLDR: 论文提出OccCylindrical方法，通过圆柱坐标系融合多传感器特征，提升3D语义占用预测的细粒度几何细节和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多传感器融合的方法主要使用笛卡尔坐标系，忽略了传感器读数的分布，导致细粒度细节丢失和性能下降。

Method: 提出OccCylindrical方法，在圆柱坐标系下融合和优化不同模态特征。

Result: 在nuScenes数据集（包括雨天和夜间场景）上验证了方法的有效性和SOTA性能。

Conclusion: 圆柱坐标系下的特征融合能保留更多几何细节，显著提升性能。

Abstract: The safe operation of autonomous vehicles (AVs) is highly dependent on their
understanding of the surroundings. For this, the task of 3D semantic occupancy
prediction divides the space around the sensors into voxels, and labels each
voxel with both occupancy and semantic information. Recent perception models
have used multisensor fusion to perform this task. However, existing
multisensor fusion-based approaches focus mainly on using sensor information in
the Cartesian coordinate system. This ignores the distribution of the sensor
readings, leading to a loss of fine-grained details and performance
degradation. In this paper, we propose OccCylindrical that merges and refines
the different modality features under cylindrical coordinates. Our method
preserves more fine-grained geometry detail that leads to better performance.
Extensive experiments conducted on the nuScenes dataset, including challenging
rainy and nighttime scenarios, confirm our approach's effectiveness and
state-of-the-art performance. The code will be available at:
https://github.com/DanielMing123/OccCylindrical

</details>

### [64] [Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2505.03286)
*Zhihao Gong,Lian Wu,Yong Xu*

Main category: cs.CV

TLDR: 提出了一种基于基础-细节特征学习框架（BDLF）的可见光-红外行人重识别方法，通过同时利用模态共享和模态特定信息提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用不同模态的信息，主要关注模态共享特征而忽略模态特定细节，导致性能不足。

Method: BDLF框架通过无损细节特征提取模块和互补基础嵌入生成机制分别挖掘细节和基础特征，并通过相关性限制方法确保特征的丰富性。

Result: 在SYSU-MM01、RegDB和LLCM数据集上的实验验证了BDLF的有效性。

Conclusion: BDLF通过同时利用模态共享和模态特定信息，显著提升了可见光-红外行人重识别的性能。

Abstract: Visible-infrared person re-identification (VIReID) provides a solution for
ReID tasks in 24-hour scenarios; however, significant challenges persist in
achieving satisfactory performance due to the substantial discrepancies between
visible (VIS) and infrared (IR) modalities. Existing methods inadequately
leverage information from different modalities, primarily focusing on digging
distinguishing features from modality-shared information while neglecting
modality-specific details. To fully utilize differentiated minutiae, we propose
a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of
both base and detail knowledge, thereby capitalizing on both modality-shared
and modality-specific information. Specifically, the proposed BDLF mines detail
and base features through a lossless detail feature extraction module and a
complementary base embedding generation mechanism, respectively, supported by a
novel correlation restriction method that ensures the features gained by BDLF
enrich both detail and base knowledge across VIS and IR features. Comprehensive
experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the
effectiveness of BDLF.

</details>

### [65] [Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach](https://arxiv.org/abs/2505.03299)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TLDR: 提出了一种基于“能力编码”的方法，用于预测基础模型在多个下游任务上的性能，无需微调，简化模型选择并推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 当前遥感视觉基础模型众多，但缺乏一致性能比较方法，需一种高效评估手段。

Method: 通过“能力编码”预测模型性能，避免对每个任务进行微调。

Result: 该方法能简化模型选择，并为现有文献提供新视角。

Conclusion: 能力编码方法为模型比较和未来研究方向提供了实用工具。

Abstract: Foundation models constitute a significant advancement in computer vision:
after a single, albeit costly, training phase, they can address a wide array of
tasks. In the field of Earth observation, over 75 remote sensing vision
foundation models have been developed in the past four years. However, none has
consistently outperformed the others across all available downstream tasks. To
facilitate their comparison, we propose a cost-effective method for predicting
a model's performance on multiple downstream tasks without the need for
fine-tuning on each one. This method is based on what we call "capabilities
encoding." The utility of this novel approach is twofold: we demonstrate its
potential to simplify the selection of a foundation model for a given new task,
and we employ it to offer a fresh perspective on the existing literature,
suggesting avenues for future research. Codes are available at
https://github.com/pierreadorni/capabilities-encoding.

</details>

### [66] [3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation](https://arxiv.org/abs/2505.03300)
*Andrew Caunes,Thierry Chateau,Vincent Frémont*

Main category: cs.CV

TLDR: 提出了一种无需3D标注或额外模态的3D语义分割方法，利用2D分割模型和投票机制生成伪标签。


<details>
  <summary>Details</summary>
Motivation: 解决3D LiDAR点云语义分割中标注需求高和领域偏移问题。

Method: 通过LiDAR扫描生成2D视图，应用预训练2D分割模型，再通过投票机制将结果投影回3D点云。

Result: 展示了伪标签在无监督领域适应任务中的潜力。

Conclusion: 该方法为3D语义分割提供了一种高效且无需额外标注的解决方案。

Abstract: Semantic segmentation of 3D LiDAR point clouds, essential for autonomous
driving and infrastructure management, is best achieved by supervised learning,
which demands extensive annotated datasets and faces the problem of domain
shifts. We introduce a new 3D semantic segmentation pipeline that leverages
aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need
for direct 3D annotation or reliance on additional modalities such as camera
images at inference time. Our approach generates 2D views from LiDAR scans
colored by sensor intensity and applies 2D semantic segmentation to these views
using a camera-domain pretrained model. The segmented 2D outputs are then
back-projected onto the 3D points, with a simple voting-based estimator that
merges the labels associated to each 3D point. Our main contribution is a
global pipeline for 3D semantic segmentation requiring no prior 3D annotation
and not other modality for inference, which can be used for pseudo-label
generation. We conduct a thorough ablation study and demonstrate the potential
of the generated pseudo-labels for the Unsupervised Domain Adaptation task.

</details>

### [67] [Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices](https://arxiv.org/abs/2505.03303)
*Tasnim Shahriar*

Main category: cs.CV

TLDR: 本文评估了五种轻量级深度学习模型在资源受限环境中的适用性，发现EfficientNetV2准确率最高，MobileNetV3平衡性最佳，SqueezeNet速度最快。


<details>
  <summary>Details</summary>
Motivation: 研究轻量级模型在资源受限设备（如低内存设备）上的部署潜力。

Method: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上评估五种模型（MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S、ShuffleNetV2），使用分类准确率、推理时间、FLOPs和模型大小作为指标，并研究超参数调优、数据增强和训练范式的影响。

Result: 迁移学习显著提升模型性能，EfficientNetV2准确率最高，MobileNetV3平衡性最佳，SqueezeNet速度最快。

Conclusion: 研究为边缘计算和移动平台提供了优化轻量级模型的实用建议，强调了准确性与效率的权衡。

Abstract: This paper presents a comprehensive evaluation of lightweight deep learning
models for image classification, emphasizing their suitability for deployment
in resource-constrained environments such as low-memory devices. Five
state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,
EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse
datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using
four key performance metrics: classification accuracy, inference time,
floating-point operations (FLOPs), and model size. Additionally, we investigate
the impact of hyperparameter tuning, data augmentation, and training paradigms
by comparing pretrained models with scratch-trained counterparts, focusing on
MobileNetV3 Small. Our findings reveal that transfer learning significantly
enhances model accuracy and computational efficiency, particularly for complex
datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest
accuracy, while MobileNetV3 offers the best balance between accuracy and
efficiency, and SqueezeNet excels in inference speed and compactness. This
study highlights critical trade-offs between accuracy and efficiency, offering
actionable insights for deploying lightweight models in real-world applications
where computational resources are limited. By addressing these challenges, this
research contributes to optimizing deep learning systems for edge computing and
mobile platforms.

</details>

### [68] [3D Gaussian Splatting Data Compression with Mixture of Priors](https://arxiv.org/abs/2505.03310)
*Lei Liu,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TLDR: 提出了一种基于混合先验（MoP）策略的3D高斯泼溅数据压缩方法，解决了现有方法在熵模型和量化策略上的不足，实现了无损和有损压缩的优化。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅数据压缩方法在熵模型和量化策略上表现不足，未能充分利用超先验信息或实现细粒度量化。

Method: 采用混合先验（MoP）策略，通过多个轻量级MLP生成多样化先验特征，并结合门控机制整合为MoP特征。无损压缩中用于改进条件熵模型，有损压缩中用于指导元素级量化。

Result: 在多个基准测试（如Mip-NeRF360、BungeeNeRF等）中表现优异，达到最新技术水平。

Conclusion: MoP策略有效提升了3D高斯泼溅数据压缩的性能，为无损和有损压缩提供了更优解决方案。

Abstract: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.

</details>

### [69] [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)
*Yibin Wang,Zhimin Li,Yuhang Zang,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TLDR: 论文提出UnifiedReward-Think，首个基于长链思维（CoT）的多模态奖励模型，通过强化微调提升奖励信号的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型（RMs）的浅层推理导致奖励信号不准确，引入长链思维（CoT）可显著提升其性能。

Method: 采用探索驱动的强化微调方法：1) 利用GPT-4o的推理过程进行冷启动；2) 通过大规模多模态偏好数据激发模型推理；3) 使用GRPO优化模型。

Result: 实验表明，该模型在多种视觉奖励任务中表现优越。

Conclusion: UnifiedReward-Think通过长链思维和强化微调，显著提升了奖励模型的准确性和鲁棒性。

Abstract: Recent advances in multimodal Reward Models (RMs) have shown significant
promise in delivering reward signals to align vision models with human
preferences. However, current RMs are generally restricted to providing direct
responses or engaging in shallow reasoning processes with limited depth, often
leading to inaccurate reward signals. We posit that incorporating explicit long
chains of thought (CoT) into the reward reasoning process can significantly
strengthen their reliability and robustness. Furthermore, we believe that once
RMs internalize CoT reasoning, their direct response accuracy can also be
improved through implicit reasoning capabilities. To this end, this paper
proposes UnifiedReward-Think, the first unified multimodal CoT-based reward
model, capable of multi-dimensional, step-by-step long-chain reasoning for both
visual understanding and generation reward tasks. Specifically, we adopt an
exploration-driven reinforcement fine-tuning approach to elicit and incentivize
the model's latent complex reasoning ability: (1) We first use a small amount
of image generation preference data to distill the reasoning process of GPT-4o,
which is then used for the model's cold start to learn the format and structure
of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge
and generalization capabilities, we prepare large-scale unified multimodal
preference data to elicit the model's reasoning process across various vision
tasks. During this phase, correct reasoning outputs are retained for rejection
sampling to refine the model (3) while incorrect predicted samples are finally
used for Group Relative Policy Optimization (GRPO) based reinforcement
fine-tuning, enabling the model to explore diverse reasoning paths and optimize
for correct and robust solutions. Extensive experiments across various vision
reward tasks demonstrate the superiority of our model.

</details>

### [70] [SD-VSum: A Method and Dataset for Script-Driven Video Summarization](https://arxiv.org/abs/2505.03319)
*Manolis Mylonas,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TLDR: 论文提出了脚本驱动的视频摘要任务，通过用户提供的脚本选择视频中最相关的部分生成摘要。扩展了VideoXum数据集，并开发了基于跨模态注意力机制的SD-VSum网络架构，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法多为通用型，无法根据用户需求定制摘要内容。脚本驱动的视频摘要任务旨在解决这一问题，满足用户个性化需求。

Method: 扩展VideoXum数据集，加入自然语言描述；提出SD-VSum网络架构，利用跨模态注意力机制对齐和融合视觉与文本信息。

Result: SD-VSum在实验中表现优于现有查询驱动和通用摘要方法，能根据用户需求生成定制化摘要。

Conclusion: 脚本驱动的视频摘要任务和SD-VSum方法有效解决了用户个性化需求，为视频摘要领域提供了新方向。

Abstract: In this work, we introduce the task of script-driven video summarization,
which aims to produce a summary of the full-length video by selecting the parts
that are most relevant to a user-provided script outlining the visual content
of the desired summary. Following, we extend a recently-introduced large-scale
dataset for generic video summarization (VideoXum) by producing natural
language descriptions of the different human-annotated summaries that are
available per video. In this way we make it compatible with the introduced
task, since the available triplets of ``video, summary and summary
description'' can be used for training a method that is able to produce
different summaries for a given video, driven by the provided script about the
content of each summary. Finally, we develop a new network architecture for
script-driven video summarization (SD-VSum), that relies on the use of a
cross-modal attention mechanism for aligning and fusing information from the
visual and text modalities. Our experimental evaluations demonstrate the
advanced performance of SD-VSum against state-of-the-art approaches for
query-driven and generic (unimodal and multimodal) summarization from the
literature, and document its capacity to produce video summaries that are
adapted to each user's needs about their content.

</details>

### [71] [Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning](https://arxiv.org/abs/2505.03327)
*José-Luis Bueso-Bello,Benjamin Chauvel,Daniel Carcereri,Philipp Posovszky,Pietro Milillo,Jennifer Ruiz,Juan-Carlos Fernández-Diaz,Carolina González,Michele Martone,Ronny Hänsch,Paola Rizzoli*

Main category: cs.CV

TLDR: 论文提出了一种结合自监督学习和监督学习的框架，利用TanDEM-X数据实现高分辨率森林制图，解决了标记数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决中分辨率森林制图在检测狭窄道路和精确划定森林边界时的局限性，同时应对高分辨率标记数据稀缺的挑战。

Method: 采用自监督学习提取输入特征的高信息量表示，随后用少量可靠标记数据进行监督训练。

Result: 在亚马逊雨林的真实案例中，该方法显著提高了分类精度，优于完全监督方法。

Conclusion: 该框架为大规模高分辨率森林制图提供了极具前景的解决方案。

Abstract: Deep learning models have shown encouraging capabilities for mapping
accurately forests at medium resolution with TanDEM-X interferometric SAR data.
Such models, as most of current state-of-the-art deep learning techniques in
remote sensing, are trained in a fully-supervised way, which requires a large
amount of labeled data for training and validation. In this work, our aim is to
exploit the high-resolution capabilities of the TanDEM-X mission to map forests
at 6 m. The goal is to overcome the intrinsic limitations posed by
midresolution products, which affect, e.g., the detection of narrow roads
within vegetated areas and the precise delineation of forested regions
contours. To cope with the lack of extended reliable reference datasets at such
a high resolution, we investigate self-supervised learning techniques for
extracting highly informative representations from the input features, followed
by a supervised training step with a significantly smaller number of reliable
labels. A 1 m resolution forest/non-forest reference map over Pennsylvania,
USA, allows for comparing different training approaches for the development of
an effective forest mapping framework with limited labeled samples. We select
the best-performing approach over this test region and apply it in a real-case
forest mapping scenario over the Amazon rainforest, where only very few labeled
data at high resolution are available. In this challenging scenario, the
proposed self-supervised framework significantly enhances the classification
accuracy with respect to fully-supervised methods, trained using the same
amount of labeled data, representing an extremely promising starting point for
large-scale, very high-resolution forest mapping with TanDEM-X data.

</details>

### [72] [FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing](https://arxiv.org/abs/2505.03329)
*Rui Lan,Yancheng Bai,Xu Duan,Mingxing Li,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TLDR: FLUX-Text是一种基于FLUX-Fill的多语言场景文本编辑框架，通过轻量级字形和文本嵌入模块，显著提升了文本编辑的准确性和保真度，尤其在非拉丁字符（如中文）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在扩散模型（LDM）的文本编辑方法在非拉丁字符上表现不佳，字形复杂导致生成不准确或不可识别。FLUX-Text旨在解决这一问题。

Method: 提出轻量级字形和文本嵌入模块，结合视觉和文本模态，保留FLUX-Fill的生成能力同时增强字形理解。仅需10万训练样本。

Result: 在公开数据集上，FLUX-Text在文本保真度上超越现有方法，达到最先进性能。

Conclusion: FLUX-Text通过轻量级设计高效解决了多语言场景文本编辑的挑战，为复杂字形生成提供了新思路。

Abstract: The task of scene text editing is to modify or add texts on images while
maintaining the fidelity of newly generated text and visual coherence with the
background. Recent works based on latent diffusion models (LDM) show improved
text editing results, yet still face challenges and often generate inaccurate
or unrecognizable characters, especially for non-Latin ones (\eg, Chinese),
which have complex glyph structures. To address these issues, we present
FLUX-Text, a simple and advanced multilingual scene text editing framework
based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,
considering both visual and textual modalities. To retain the original
generative capabilities of FLUX-Fill while enhancing its understanding and
generation of glyphs, we propose lightweight glyph and text embedding modules.
Owning to the lightweight design, FLUX-Text is trained only with $100K$
training examples compared to current popular methods trained with 2.9M ones.
With no bells and whistles, our method achieves state-of-the-art performance on
text editing tasks. Qualitative and quantitative experiments on the public
datasets demonstrate that our method surpasses previous works in text fidelity.

</details>

### [73] [From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection](https://arxiv.org/abs/2505.03334)
*Guoting Wei,Yu Liu,Xia Yuan,Xizhe Xue,Linlin Guo,Yifan Yang,Chunxia Zhao,Zongwen Bai,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TLDR: 提出了一种大规模语言引导的开集航空检测数据集MI-OAD，并开发了自动标注工具OS-W2S Label Engine，显著提升了开集检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导方法因数据集有限，难以满足细粒度开集检测需求。

Method: 构建包含词、短语、句子三级语言引导的数据集，结合视觉-语言模型和预处理/后处理技术开发自动标注工具。

Result: MI-OAD包含16万图像和200万标注对，训练后模型在零样本条件下性能显著提升（如AP50提高29.5）。

Conclusion: MI-OAD和标注工具填补了现有数据不足，推动了开集航空检测的发展。

Abstract: In recent years, language-guided open-world aerial object detection has
gained significant attention due to its better alignment with real-world
application needs. However, due to limited datasets, most existing
language-guided methods primarily focus on vocabulary, which fails to meet the
demands of more fine-grained open-world detection. To address this limitation,
we propose constructing a large-scale language-guided open-set aerial detection
dataset, encompassing three levels of language guidance: from words to phrases,
and ultimately to sentences. Centered around an open-source large
vision-language model and integrating image-operation-based preprocessing with
BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic
annotation pipeline capable of handling diverse scene annotations for aerial
images. Using this label engine, we expand existing aerial detection datasets
with rich textual annotations and construct a novel benchmark dataset, called
Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of
current remote sensing grounding data and enabling effective open-set aerial
detection. Specifically, MI-OAD contains 163,023 images and 2 million
image-caption pairs, approximately 40 times larger than comparable datasets. We
also employ state-of-the-art open-set methods from the natural image domain,
trained on our proposed dataset, to validate the model's open-set detection
capabilities. For instance, when trained on our dataset, Grounding DINO
achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs
under zero-shot transfer conditions. Both the dataset and the label engine will
be released publicly.

</details>

### [74] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
*Fangming Cui,Yonggang Zhang,Xuan Wang,Xinmei Tian,Jun Yu*

Main category: cs.CV

TLDR: 提出了一种新的特征矩阵（FM）正则化方法，用于增强大型视觉语言模型在目标非特定任务中的表现，避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在目标非特定任务中表现不佳，可能是由于过拟合导致模型遗忘通用知识。

Method: 通过提取和利用通用知识构建特征矩阵（FM），从深度和细粒度角度捕捉输入语义，保留通用知识。

Result: FM兼容现有框架，显著提升目标非特定任务性能，达到最先进水平。

Conclusion: FM是一种通用且灵活的模块，有效解决了目标非特定任务中的过拟合问题。

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>

### [75] [A Vision-Language Model for Focal Liver Lesion Classification](https://arxiv.org/abs/2505.03350)
*Song Jian,Hu Yuchang,Wang Hui,Chen Yen-Wei*

Main category: cs.CV

TLDR: 论文提出了一种基于视觉语言模型（VLM）的Liver-VLM方法，用于肝脏病灶分类，优于传统CNN和现有VLM模型。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习依赖大规模标注数据，而医学影像数据有限，需要更高效的分类方法。

Method: Liver-VLM结合文本和图像特征，通过计算嵌入相似度并优化交叉熵损失，提升分类性能。

Result: 在MPCT-FLLs数据集上，Liver-VLM在准确率和AUC上优于CLIP和MedCLIP，轻量级ResNet18进一步提升了性能。

Conclusion: Liver-VLM在数据有限条件下表现优异，为医学影像分类提供了新思路。

Abstract: Accurate classification of focal liver lesions is crucial for diagnosis and
treatment in hepatology. However, traditional supervised deep learning models
depend on large-scale annotated datasets, which are often limited in medical
imaging. Recently, Vision-Language models (VLMs) such as Contrastive
Language-Image Pre-training model (CLIP) has been applied to image
classifications. Compared to the conventional convolutional neural network
(CNN), which classifiers image based on visual information only, VLM leverages
multimodal learning with text and images, allowing it to learn effectively even
with a limited amount of labeled data. Inspired by CLIP, we pro-pose a
Liver-VLM, a model specifically designed for focal liver lesions (FLLs)
classification. First, Liver-VLM incorporates class information into the text
encoder without introducing additional inference overhead. Second, by
calculating the pairwise cosine similarities between image and text embeddings
and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively
aligns image features with class-level text features. Experimental results on
MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the
standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve
(AUC). Further analysis shows that using a lightweight ResNet18 backbone
enhances classification performance, particularly under data-constrained
conditions.

</details>

### [76] [GUAVA: Generalizable Upper Body 3D Gaussian Avatar](https://arxiv.org/abs/2505.03351)
*Dongbin Zhang,Yunfei Liu,Lijian Lin,Ye Zhu,Yang Li,Minghan Qin,Yu Li,Haoqian Wang*

Main category: cs.CV

TLDR: GUAVA框架通过单张图像快速重建可动画的上半身3D高斯化身，显著提升渲染质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多视角或单目视频，且受限于SMPLX的表达能力，难以处理面部表情。

Method: 引入表达性人体模型（EHM）增强面部表情能力，提出基于高斯化身的GUAVA框架，利用逆纹理映射和投影采样技术。

Result: GUAVA在渲染质量和速度上显著优于现有方法，重建时间仅0.1秒，支持实时动画。

Conclusion: GUAVA为单图像3D化身重建提供了高效解决方案，具有广泛应用潜力。

Abstract: Reconstructing a high-quality, animatable 3D human avatar with expressive
facial and hand motions from a single image has gained significant attention
due to its broad application potential. 3D human avatar reconstruction
typically requires multi-view or monocular videos and training on individual
IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's
expressiveness, these methods often focus on body motion but struggle with
facial expressions. To address these challenges, we first introduce an
expressive human model (EHM) to enhance facial expression capabilities and
develop an accurate tracking method. Based on this template model, we propose
GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar
reconstruction. We leverage inverse texture mapping and projection sampling
techniques to infer Ubody (upper-body) Gaussians from a single image. The
rendered images are refined through a neural refiner. Experimental results
demonstrate that GUAVA significantly outperforms previous methods in rendering
quality and offers significant speed improvements, with reconstruction times in
the sub-second range (0.1s), and supports real-time animation and rendering.

</details>

### [77] [Interpretable Zero-shot Learning with Infinite Class Concepts](https://arxiv.org/abs/2505.03361)
*Zihan Ye,Shreyank N Gowda,Shiming Chen,Yaochu Jin,Kaizhu Huang,Xiaobo Jin*

Main category: cs.CV

TLDR: 本文提出了一种名为InfZSL的新框架，通过利用LLMs动态生成无限短语级类别概念，解决了ZSL中透明度和幻觉问题，显著提升了性能并生成可解释的概念。


<details>
  <summary>Details</summary>
Motivation: 传统ZSL方法依赖人工标注或LLMs生成的类别语义，但存在透明度不足和幻觉问题，导致非视觉语义。本文旨在通过可迁移性和区分性重新定义类别语义。

Method: 提出InfZSL框架，利用LLMs动态生成短语级类别概念，并通过基于熵的评分和“优质”概念选择机制筛选最具可迁移性和区分性的概念。

Result: 在三个流行基准数据集上表现显著提升，并生成高度可解释的图像相关概念。

Conclusion: InfZSL框架有效解决了ZSL中的透明度和幻觉问题，同时提升了性能与可解释性。

Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images
with intermediate class semantics, like human-annotated concepts or class
definitions. An emerging alternative leverages Large-scale Language Models
(LLMs) to automatically generate class documents. However, these methods often
face challenges with transparency in the classification process and may suffer
from the notorious hallucination problem in LLMs, resulting in non-visual class
semantics. This paper redefines class semantics in ZSL with a focus on
transferability and discriminability, introducing a novel framework called
Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach
leverages the powerful capabilities of LLMs to dynamically generate an
unlimited array of phrase-level class concepts. To address the hallucination
challenge, we introduce an entropy-based scoring process that incorporates a
``goodness" concept selection mechanism, ensuring that only the most
transferable and discriminative concepts are selected. Our InfZSL framework not
only demonstrates significant improvements on three popular benchmark datasets
but also generates highly interpretable, image-grounded concepts. Code will be
released upon acceptance.

</details>

### [78] [3D Surface Reconstruction with Enhanced High-Frequency Details](https://arxiv.org/abs/2505.03362)
*Shikun Zhang,Yiqun Wang,Cunjian Chen,Yong Li,Qiuhong Ke*

Main category: cs.CV

TLDR: FreNeuS是一种基于高频信息的神经隐式3D重建方法，通过动态采样和高频加权提升表面细节重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经表面重建方法因随机采样导致高频细节学习不足，重建结果过于平滑。

Method: 利用像素梯度变化获取高频区域，动态采样和高频加权约束细节重建。

Result: 定性定量结果显示，FreNeuS能重建精细表面细节，优于现有方法。

Conclusion: FreNeuS不仅提升重建质量，还适用于任何基于NeuS的工作。

Abstract: Neural implicit 3D reconstruction can reproduce shapes without 3D
supervision, and it learns the 3D scene through volume rendering methods and
neural implicit representations. Current neural surface reconstruction methods
tend to randomly sample the entire image, making it difficult to learn
high-frequency details on the surface, and thus the reconstruction results tend
to be too smooth. We designed a method (FreNeuS) based on high-frequency
information to solve the problem of insufficient surface detail. Specifically,
FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in
an image and uses the obtained high-frequency information to guide surface
detail reconstruction. High-frequency information is first used to guide the
dynamic sampling of rays, applying different sampling strategies according to
variations in high-frequency regions. To further enhance the focus on surface
details, we have designed a high-frequency weighting method that constrains the
representation of high-frequency details during the reconstruction process.
Qualitative and quantitative results show that our method can reconstruct fine
surface details and obtain better surface reconstruction quality compared to
existing methods. In addition, our method is more applicable and can be
generalized to any NeuS-based work.

</details>

### [79] [Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models](https://arxiv.org/abs/2505.03374)
*Abram Schonfeldt,Benjamin Maylor,Xiaofang Chen,Ronald Clark,Aiden Doherty*

Main category: cs.CV

TLDR: 论文比较了视觉语言模型和判别模型在自由生活场景下预测身体活动的性能，发现开源视觉语言模型在预测久坐行为时表现接近判别模型，但在其他活动强度下表现下降。


<details>
  <summary>Details</summary>
Motivation: 验证和开发基于可穿戴设备的身体活动测量方法需要标注数据，而传统标注方法耗时费力，因此探索计算机视觉模型的替代方案。

Method: 比较了三种视觉语言模型和两种判别模型在两个自由生活验证研究中的性能，分别来自英国和中国。

Result: 视觉语言模型和判别模型在预测久坐行为时表现相当，但在其他活动强度下表现较差，且在外部队列中性能显著下降。

Conclusion: 开源计算机视觉模型可用于标注久坐行为，减轻标注负担，但在其他活动强度或不同群体中表现有限。

Abstract: Introduction: Data from wearable devices collected in free-living settings,
and labelled with physical activity behaviours compatible with health research,
are essential for both validating existing wearable-based measurement
approaches and developing novel machine learning approaches. One common way of
obtaining these labels relies on laborious annotation of sequences of images
captured by cameras worn by participants through the course of a day. Methods:
We compare the performance of three vision language models and two
discriminative models on two free-living validation studies with 161 and 111
participants, collected in Oxfordshire, United Kingdom and Sichuan, China,
respectively, using the Autographer (OMG Life, defunct) wearable camera.
Results: We found that the best open-source vision-language model (VLM) and
fine-tuned discriminative model (DM) achieved comparable performance when
predicting sedentary behaviour from single images on unseen participants in the
Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,
0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,
0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,
0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,
performance fell across all intensity categories, with median Cohen's
kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,
and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely
available computer vision models could help annotate sedentary behaviour,
typically the most prevalent activity of daily living, from wearable camera
images within similar populations to seen data, reducing the annotation burden.

</details>

### [80] [Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant](https://arxiv.org/abs/2505.03380)
*Haonan Wang,Jiaji Mao,Lehan Wang,Qixiang Zhang,Marawan Elbatel,Yi Qin,Huijun Hu,Baoxun Li,Wenhui Deng,Weifeng Qin,Hongrui Li,Jialin Liang,Jun Shen,Xiaomeng Li*

Main category: cs.CV

TLDR: RCMed是一种全栈AI助手，通过多模态对齐和分层视觉-语言基础，提升医学影像分析的准确性，并在165项临床任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学AI助手在多模态内容准确性不足和真实场景验证不足的问题。

Method: 采用自增强相关机制和颜色区域描述策略，通过视觉特征与语言语义的闭环优化，训练于2000万图像-掩码-描述三元组。

Result: 在165项临床任务中表现优异，细胞分割精度相对提升23.5%，并在20种癌症类型的外部验证中表现卓越。

Conclusion: RCMed展示了集成多模态模型在复杂场景中实现人类水平解释的能力，推动了以人为中心的AI医疗发展。

Abstract: Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.

</details>

### [81] [Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples](https://arxiv.org/abs/2505.03383)
*Jian-Wei Li,Wen-Ze Shao*

Main category: cs.CV

TLDR: 论文提出了一种名为注意力聚合攻击（AAA）的新方法，通过模仿其他FR模型对干净人脸图像的注意力，增强对抗样本在FR模型中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 针对细粒度视觉任务（如人脸识别）中特定类别的深度模型的特殊性，现有攻击方法表现不佳，因此需要研究如何提升对抗样本的迁移性。

Method: 通过研究FR模型中嵌入学习的关键因素，发现决定性特征和辅助特征对每个FR模型具有特异性，进而提出AAA方法，模仿其他FR模型的注意力机制。

Result: 在多种FR模型上的实验验证了AAA方法的优越性和鲁棒性。

Conclusion: AAA方法通过注意力聚合显著提升了对抗样本在FR模型中的攻击效果，为解决黑盒攻击中的迁移性问题提供了新思路。

Abstract: Adversarial examples have revealed the vulnerability of deep learning models
and raised serious concerns about information security. The transfer-based
attack is a hot topic in black-box attacks that are practical to real-world
scenarios where the training datasets, parameters, and structure of the target
model are unknown to the attacker. However, few methods consider the
particularity of class-specific deep models for fine-grained vision tasks, such
as face recognition (FR), giving rise to unsatisfactory attacking performance.
In this work, we first investigate what in a face exactly contributes to the
embedding learning of FR models and find that both decisive and auxiliary
facial features are specific to each FR model, which is quite different from
the biological mechanism of human visual system. Accordingly we then propose a
novel attack method named Attention-aggregated Attack (AAA) to enhance the
transferability of adversarial examples against FR, which is inspired by the
attention divergence and aims to destroy the facial features that are critical
for the decision-making of other FR models by imitating their attentions on the
clean face images. Extensive experiments conducted on various FR models
validate the superiority and robust effectiveness of the proposed method over
existing methods.

</details>

### [82] [EOPose : Exemplar-based object reposing using Generalized Pose Correspondences](https://arxiv.org/abs/2505.03394)
*Sarthak Mehrotra,Rishabh Jain,Mayur Hemani,Balaji Krishnamurthy,Mausoom Sarkar*

Main category: cs.CV

TLDR: EOPose是一种端到端的框架，利用无监督关键点检测技术实现物体在图像中的重新摆放，适用于电子商务等领域。


<details>
  <summary>Details</summary>
Motivation: 电子商务需要快速生成产品图像变体，现有生成方法无法保留物体的精细细节（如颜色、纹理和品牌标志）。

Method: EOPose采用三步法，通过目标姿态引导图像的关键点对应关系对源图像进行变形和重新渲染。

Result: EOPose在PSNR、SSIM和FID等图像质量指标上表现优异，生成了高质量的重新摆放结果。

Conclusion: EOPose通过无监督关键点检测和新型渲染方法，有效实现了物体重新摆放，并保留了细节。

Abstract: Reposing objects in images has a myriad of applications, especially for
e-commerce where several variants of product images need to be produced
quickly. In this work, we leverage the recent advances in unsupervised keypoint
correspondence detection between different object images of the same class to
propose an end-to-end framework for generic object reposing. Our method,
EOPose, takes a target pose-guidance image as input and uses its keypoint
correspondence with the source object image to warp and re-render the latter
into the target pose using a novel three-step approach. Unlike generative
approaches, our method also preserves the fine-grained details of the object
such as its exact colors, textures, and brand marks. We also prepare a new
dataset of paired objects based on the Objaverse dataset to train and test our
network. EOPose produces high-quality reposing output as evidenced by different
image quality metrics (PSNR, SSIM and FID). Besides a description of the method
and the dataset, the paper also includes detailed ablation and user studies to
indicate the efficacy of the proposed method

</details>

### [83] [DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation](https://arxiv.org/abs/2505.03401)
*Shanshan Song,Hui Tang,Honglong Yang,Xiaomeng Li*

Main category: cs.CV

TLDR: 论文提出了一种动态差异感知时序残差网络（DDaTR），通过多级空间相关性模块和动态残差网络，显著提升了纵向放射学报告生成（LRRG）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LRRG方法在特征提取过程中难以有效捕捉空间和时间相关性，导致跨检查差异信息不足，影响报告生成质量。

Method: 设计了动态特征对齐模块（DFAM）和动态差异感知模块（DDAM），结合动态残差网络，以多级方式捕捉空间和时间相关性。

Result: 在三个基准测试中表现优于现有方法，证明了其在RRG和LRRG任务中的有效性。

Conclusion: DDaTR通过动态捕捉跨检查差异和时序信息，显著提升了放射学报告生成的准确性和效率。

Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports
from medical imaging, enhancing the efficiency of the reporting process.
Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating
the ability to compare current and prior exams, facilitating the tracking of
temporal changes in clinical findings. Existing LRRG approaches only extract
features from prior and current images using a visual pre-trained encoder,
which are then concatenated to generate the final report. However, these
methods struggle to effectively capture both spatial and temporal correlations
during the feature extraction process. Consequently, the extracted features
inadequately capture the information of difference across exams and thus
underrepresent the expected progressions, leading to sub-optimal performance in
LRRG. To address this, we develop a novel dynamic difference-aware temporal
residual network (DDaTR). In DDaTR, we introduce two modules at each stage of
the visual encoder to capture multi-level spatial correlations. The Dynamic
Feature Alignment Module (DFAM) is designed to align prior features across
modalities for the integrity of prior clinical information. Prompted by the
enriched prior features, the dynamic difference-aware module (DDAM) captures
favorable difference information by identifying relationships across exams.
Furthermore, our DDaTR employs the dynamic residual network to unidirectionally
transmit longitudinal information, effectively modelling temporal correlations.
Extensive experiments demonstrated superior performance over existing methods
on three benchmarks, proving its efficacy in both RRG and LRRG tasks.

</details>

### [84] [CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2505.03412)
*Haoyu Bai,Jie Wang,Gaomin Li,Xuan Li,Xiaohu Zhang,Xia Yang*

Main category: cs.CV

TLDR: 论文提出了首个公开的X射线组件异常检测数据集CXR-AD，填补了内部缺陷检测领域的空白，并分析了其技术挑战和现有算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测数据集主要关注可见光图像中的表面缺陷，缺乏针对组件内部缺陷的公开X射线数据集。

Method: 构建了包含653个正常样本和561个缺陷样本的X射线数据集，并分析了其特性及技术挑战。

Result: 实验表明，现有算法在CXR-AD上的性能平均下降29.78%，揭示了其在内部缺陷检测任务中的局限性。

Conclusion: CXR-AD为内部缺陷检测提供了首个公开基准，推动了算法开发和检测技术的进步。

Abstract: Internal defect detection constitutes a critical process in ensuring
component quality, for which anomaly detection serves as an effective solution.
However, existing anomaly detection datasets predominantly focus on surface
defects in visible-light images, lacking publicly available X-ray datasets
targeting internal defects in components. To address this gap, we construct the
first publicly accessible component X-ray anomaly detection (CXR-AD) dataset,
comprising real-world X-ray images. The dataset covers five industrial
component categories, including 653 normal samples and 561 defect samples with
precise pixel-level mask annotations. We systematically analyze the dataset
characteristics and identify three major technical challenges: (1) strong
coupling between complex internal structures and defect regions, (2) inherent
low contrast and high noise interference in X-ray imaging, and (3) significant
variations in defect scales and morphologies. To evaluate dataset complexity,
we benchmark three state-of-the-art anomaly detection frameworks
(feature-based, reconstruction-based, and zero-shot learning methods).
Experimental results demonstrate a 29.78% average performance degradation on
CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms
in handling internal defect detection tasks. To the best of our knowledge,
CXR-AD represents the first publicly available X-ray dataset for component
anomaly detection, providing a real-world industrial benchmark to advance
algorithm development and enhance precision in internal defect inspection
technologies.

</details>

### [85] [LiftFeat: 3D Geometry-Aware Local Feature Matching](https://arxiv.org/abs/2505.03422)
*Yepeng Liu,Wenpeng Lai,Zhou Zhao,Yuxuan Xiong,Jinchi Zhu,Jun Cheng,Yongchao Xu*

Main category: cs.CV

TLDR: 提出了一种轻量级网络LiftFeat，通过聚合3D几何特征提升原始描述符的鲁棒性，在极端条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 在光照变化大、低纹理区域或重复图案的场景中，提取鲁棒且具有区分性的视觉特征仍具挑战性。

Method: 使用预训练的单目深度估计模型生成伪表面法线标签，监督3D几何特征的提取，并设计3D几何感知的特征提升模块融合表面法线特征与原始2D描述符特征。

Result: 在相对位姿估计、单应性估计和视觉定位任务中，LiftFeat优于一些轻量级的最先进方法。

Conclusion: LiftFeat通过整合3D几何特征，显著提升了2D特征描述在极端条件下的区分能力。

Abstract: Robust and efficient local feature matching plays a crucial role in
applications such as SLAM and visual localization for robotics. Despite great
progress, it is still very challenging to extract robust and discriminative
visual features in scenarios with drastic lighting changes, low texture areas,
or repetitive patterns. In this paper, we propose a new lightweight network
called \textit{LiftFeat}, which lifts the robustness of raw descriptor by
aggregating 3D geometric feature. Specifically, we first adopt a pre-trained
monocular depth estimation model to generate pseudo surface normal label,
supervising the extraction of 3D geometric feature in terms of predicted
surface normal. We then design a 3D geometry-aware feature lifting module to
fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D
geometric feature enhances the discriminative ability of 2D feature description
in extreme conditions. Extensive experimental results on relative pose
estimation, homography estimation, and visual localization tasks, demonstrate
that our LiftFeat outperforms some lightweight state-of-the-art methods. Code
will be released at : https://github.com/lyp-deeplearning/LiftFeat.

</details>

### [86] [Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications](https://arxiv.org/abs/2505.03426)
*Ziyu Li,Yujian Hu,Zhengyao Ding,Yiheng Mao,Haitao Li,Fan Yi,Hongkun Zhang,Zhengxing Huang*

Main category: cs.CV

TLDR: 提出了一种名为CPGG的方法，通过生成多样化的CMR数据来解决CMR数据集不足的问题，显著提升下游AI任务的性能。


<details>
  <summary>Details</summary>
Motivation: 由于大规模高质量CMR数据集的稀缺性，AI在心脏疾病诊断中的应用受到限制。

Method: CPGG框架分为两阶段：首先生成模型基于CMR数据的心脏表型训练，随后通过掩码自回归扩散模型生成高保真CMR序列。

Result: 生成的合成CMR数据质量高，显著提升了诊断和心脏表型预测等下游任务的性能。

Conclusion: CPGG方法有效解决了CMR数据不足的问题，为AI在心脏健康领域的应用提供了支持。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for
diagnosing heart diseases and evaluating cardiac health. However, the limited
availability of large-scale, high-quality CMR datasets poses a major challenge
to the effective application of artificial intelligence (AI) in this domain.
Even the amount of unlabeled data and the health status it covers are difficult
to meet the needs of model pretraining, which hinders the performance of AI
models on downstream tasks. In this study, we present Cardiac Phenotype-Guided
CMR Generation (CPGG), a novel approach for generating diverse CMR data that
covers a wide spectrum of cardiac health status. The CPGG framework consists of
two stages: in the first stage, a generative model is trained using cardiac
phenotypes derived from CMR data; in the second stage, a masked autoregressive
diffusion model, conditioned on these phenotypes, generates high-fidelity CMR
cine sequences that capture both structural and functional features of the
heart in a fine-grained manner. We synthesized a massive amount of CMR to
expand the pretraining data. Experimental results show that CPGG generates
high-quality synthetic CMR data, significantly improving performance on various
downstream tasks, including diagnosis and cardiac phenotypes prediction. These
gains are demonstrated across both public and private datasets, highlighting
the effectiveness of our approach. Code is availabel at
https://anonymous.4open.science/r/CPGG.

</details>

### [87] [A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2505.03431)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TLDR: 提出了一种名为FGIN的单图像超分辨率模型，通过融合光谱和空间信息，结合多尺度特征提取和优化的上采样模块，显著提升了超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）与高空间分辨率图像的融合需要精确对齐，这在现实中难以实现，因此需要一种不依赖对齐的单图像超分辨率方法。

Method: 使用光谱-空间融合模块早期整合信息，采用Inception-like分层特征提取策略捕获多尺度空间依赖，结合多尺度融合块和优化的上采样模块。

Result: 在两个公开的高光谱数据集上表现出竞争性性能。

Conclusion: FGIN模型通过创新的融合和特征提取策略，有效解决了对齐依赖问题，提升了超分辨率重建质量。

Abstract: The fusion of low-spatial-resolution hyperspectral images (HSIs) with
high-spatial-resolution conventional images (e.g., panchromatic or RGB) has
played a significant role in recent advancements in HSI super-resolution.
However, this fusion process relies on the availability of precise alignment
between image pairs, which is often challenging in real-world scenarios. To
mitigate this limitation, we propose a single-image super-resolution model
called the Fusion-Guided Inception Network (FGIN). Specifically, we first
employ a spectral-spatial fusion module to effectively integrate spectral and
spatial information at an early stage. Next, an Inception-like hierarchical
feature extraction strategy is used to capture multiscale spatial dependencies,
followed by a dedicated multi-scale fusion block. To further enhance
reconstruction quality, we incorporate an optimized upsampling module that
combines bilinear interpolation with depthwise separable convolutions.
Experimental evaluations on two publicly available hyperspectral datasets
demonstrate the competitive performance of our method.

</details>

### [88] [Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks](https://arxiv.org/abs/2505.03435)
*Sun Haoxuan,Hong Yan,Zhan Jiahui,Chen Haoxing,Lan Jun,Zhu Huijia,Wang Weiqiang,Zhang Liqing,Zhang Jianfu*

Main category: cs.CV

TLDR: 本文研究了AI生成人脸检测系统的脆弱性，提出了一种结合对抗训练和扩散反演的方法，显著提升了检测系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成图像技术的快速发展带来了安全风险，尤其是人脸生成检测领域。现有检测方法在标准条件下表现良好，但对对抗攻击的鲁棒性不足。

Method: 提出了一种结合对抗训练的方法，并利用扩散反演和重建技术增强检测鲁棒性。

Result: 实验表明，现有检测系统容易被对抗扰动绕过，但新方法显著提升了鲁棒性。

Conclusion: 该方法有效提升了AI生成人脸检测的鲁棒性，并公开了代码以促进进一步研究。

Abstract: The rapid advancement of generative image technology has introduced
significant security concerns, particularly in the domain of face generation
detection. This paper investigates the vulnerabilities of current AI-generated
face detection systems. Our study reveals that while existing detection methods
often achieve high accuracy under standard conditions, they exhibit limited
robustness against adversarial attacks. To address these challenges, we propose
an approach that integrates adversarial training to mitigate the impact of
adversarial examples. Furthermore, we utilize diffusion inversion and
reconstruction to further enhance detection robustness. Experimental results
demonstrate that minor adversarial perturbations can easily bypass existing
detection systems, but our method significantly improves the robustness of
these systems. Additionally, we provide an in-depth analysis of adversarial and
benign examples, offering insights into the intrinsic characteristics of
AI-generated content. All associated code will be made publicly available in a
dedicated repository to facilitate further research and verification.

</details>

### [89] [Polar Coordinate-Based 2D Pose Prior with Neural Distance Field](https://arxiv.org/abs/2505.03445)
*Qi Gan,Sao Mai Nguyen,Eric Fenaux,Stephan Clémençon,Mounîm El Yacoubi*

Main category: cs.CV

TLDR: 提出了一种基于神经距离场（NDF）的2D姿态先验引导优化方法，通过极坐标表示和新型非测地距离度量，提升运动模糊和遮挡场景下的姿态估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB视频的深度学习姿态估计模型在真实体育场景中因运动模糊、遮挡和领域偏移表现不佳，需大量标注数据且泛化能力有限。

Method: 采用极坐标表示结合关节连接长度，定义非测地距离度量，并提出梯度批量投影增强策略以缓解数据稀缺问题。

Result: 在跳远数据集上验证，方法能跨多姿态表示提升2D姿态估计，仅需少量训练数据即可增强姿态合理性。

Conclusion: 所提方法显著提升姿态估计的鲁棒性和准确性，适用于多样化体育环境。

Abstract: Human pose capture is essential for sports analysis, enabling precise
evaluation of athletes' movements. While deep learning-based human pose
estimation (HPE) models from RGB videos have achieved impressive performance on
public datasets, their effectiveness in real-world sports scenarios is often
hindered by motion blur, occlusions, and domain shifts across different pose
representations. Fine-tuning these models can partially alleviate such
challenges but typically requires large-scale annotated data and still
struggles to generalize across diverse sports environments. To address these
limitations, we propose a 2D pose prior-guided refinement approach based on
Neural Distance Fields (NDF). Unlike existing approaches that rely solely on
angular representations of human poses, we introduce a polar coordinate-based
representation that explicitly incorporates joint connection lengths, enabling
a more accurate correction of erroneous pose estimations. Additionally, we
define a novel non-geodesic distance metric that separates angular and radial
discrepancies, which we demonstrate is better suited for polar representations
than traditional geodesic distances. To mitigate data scarcity, we develop a
gradient-based batch-projection augmentation strategy, which synthesizes
realistic pose samples through iterative refinement. Our method is evaluated on
a long jump dataset, demonstrating its ability to improve 2D pose estimation
across multiple pose representations, making it robust across different
domains. Experimental results show that our approach enhances pose plausibility
while requiring only limited training data. Code is available at:
https://github.com/QGAN2019/polar-NDF.

</details>

### [90] [Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)](https://arxiv.org/abs/2505.03463)
*Muge Du,Zhuozhao Zheng,Wenying Wang,Guotao Quan,Wuliang Shi,Le Shen,Li Zhang,Liang Li,Yinong Liu,Yuxiang Xing*

Main category: cs.CV

TLDR: BIRD框架通过反向变形、微分同胚正则化等技术，解决了非周期性动态CT重建中的计算效率、解剖合理性和细节保留问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法和深度学习方法在非周期性快速运动（如心脏成像）中面临运动伪影和泛化性挑战，INR技术虽有潜力但存在计算效率低等问题。

Method: 提出BIRD框架，包括反向变形、微分同胚正则化、运动补偿重建和降维设计。

Result: 在模拟和实际研究中，BIRD显著减少运动伪影并提升细节。

Conclusion: BIRD为动态CT重建提供高效、准确的解决方案，具有临床潜力。

Abstract: Dynamic computed tomography (CT) reconstruction faces significant challenges
in addressing motion artifacts, particularly for nonperiodic rapid movements
such as cardiac imaging with fast heart rates. Traditional methods struggle
with the extreme limited-angle problems inherent in nonperiodic cases. Deep
learning methods have improved performance but face generalization challenges.
Recent implicit neural representation (INR) techniques show promise through
self-supervised deep learning, but have critical limitations: computational
inefficiency due to forward-warping modeling, difficulty balancing DVF
complexity with anatomical plausibility, and challenges in preserving fine
details without additional patient-specific pre-scans. This paper presents a
novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It
addresses these challenges through four key contributions: (1) backward-warping
deformation that enables direct computation of each dynamic voxel with
significantly reduced computational cost, (2) diffeomorphism-based DVF
regularization that ensures anatomically plausible deformations while
maintaining representational capacity, (3) motion-compensated analytical
reconstruction that enhances fine details without requiring additional
pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate
encoding. Through various simulations and practical studies, including digital
and physical phantoms and retrospective patient data, we demonstrate the
effectiveness of our approach for nonperiodic dynamic CT reconstruction with
enhanced details and reduced motion artifacts. The proposed framework enables
more accurate dynamic CT reconstruction with potential clinical applications,
such as one-beat cardiac reconstruction, cinematic image sequences for
functional imaging, and motion artifact reduction in conventional CT scans.

</details>

### [91] [Blending 3D Geometry and Machine Learning for Multi-View Stereopsis](https://arxiv.org/abs/2505.03470)
*Vibhas Vats,Md. Alimoor Reza,David Crandall,Soon-heung Jung*

Main category: cs.CV

TLDR: GC MVSNet++通过在学习阶段主动实施多视角、多尺度的几何一致性检查，显著加速训练过程，并在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统MVS方法依赖光度与几何一致性约束，而现代学习方法仅将几何一致性作为后处理步骤，未直接影响学习过程。本文旨在通过在学习阶段集成几何一致性检查，提升性能。

Method: 提出GC MVSNet++，在学习阶段实施多视角、多尺度的几何一致性检查，并引入密集连接的成本正则化网络。

Result: 在DTU和BlendedMVS数据集上达到最优性能，在Tanks and Temples基准测试中排名第二。

Conclusion: GC MVSNet++是首个在学习阶段实施多视角、多尺度几何一致性的方法，显著提升了训练效率和性能。

Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric
and geometric consistency constraints. In contrast, modern learning-based
algorithms often rely on the plane sweep algorithm to infer 3D geometry,
applying explicit geometric consistency (GC) checks only as a post-processing
step, with no impact on the learning process itself. In this work, we introduce
GC MVSNet plus plus, a novel approach that actively enforces geometric
consistency of reference view depth maps across multiple source views (multi
view) and at various scales (multi scale) during the learning phase (see Fig.
1). This integrated GC check significantly accelerates the learning process by
directly penalizing geometrically inconsistent pixels, effectively halving the
number of training iterations compared to other MVS methods. Furthermore, we
introduce a densely connected cost regularization network with two distinct
block designs simple and feature dense optimized to harness dense feature
connections for enhanced regularization. Extensive experiments demonstrate that
our approach achieves a new state of the art on the DTU and BlendedMVS datasets
and secures second place on the Tanks and Temples benchmark. To our knowledge,
GC MVSNet plus plus is the first method to enforce multi-view, multi-scale
supervised geometric consistency during learning. Our code is available.

</details>

### [92] [UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion](https://arxiv.org/abs/2505.03494)
*Zhanyuan Jia,Ni Yao,Danyang Sun,Chuang Han,Yanting Li,Jiaofen Nan,Fubao Zhu,Chen Zhao,Weihua Zhou*

Main category: cs.CV

TLDR: 提出了一种结合深度学习和区域生长算法的脑肿瘤分割方法，通过多尺度特征融合和自适应注意力机制提升性能，并在BraTS数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对诊断和治疗至关重要，但因其形状不规则、边界模糊和高度可变性而具有挑战性。

Method: 结合多尺度特征融合模块和自适应注意力机制，并采用蒙特卡洛Dropout进行不确定性估计。

Result: 在BraTS2021和BraTS2019数据集上表现优异，Dice分数显著优于现有方法。

Conclusion: 提出了一种基于U-Net架构的新型3D脑肿瘤分割网络，结合先验知识和不确定性估计方法，提升了鲁棒性和性能。

Abstract: Background: Brain tumor segmentation has a significant impact on the
diagnosis and treatment of brain tumors. Accurate brain tumor segmentation
remains challenging due to their irregular shapes, vague boundaries, and high
variability. Objective: We propose a brain tumor segmentation method that
combines deep learning with prior knowledge derived from a region-growing
algorithm. Methods: The proposed method utilizes a multi-scale feature fusion
(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale
features and capture global contextual information. To enhance the model's
robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)
strategy is employed for uncertainty estimation. Results: Extensive experiments
demonstrate that the proposed method achieves superior performance on Brain
Tumor Segmentation (BraTS) datasets, significantly outperforming various
state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are
89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)
segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019
validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for
ET, WT, and TC segmentation, respectively. Ablation studies further confirmed
the contribution of each module to segmentation accuracy, indicating that each
component played a vital role in overall performance improvement. Conclusion:
This study proposed a novel 3D brain tumor segmentation network based on the
U-Net architecture. By incorporating the prior knowledge and employing the
uncertainty estimation method, the robustness and performance were improved.
The code for the proposed method is available at
https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.

</details>

### [93] [MRI motion correction via efficient residual-guided denoising diffusion probabilistic models](https://arxiv.org/abs/2505.03498)
*Mojtaba Safari,Shansong Wang,Qiang Li,Zach Eidex,Richard L. J. Qiu,Chih-Wei Chang,Hui Mao,Xiaofeng Yang*

Main category: cs.CV

TLDR: Res-MoCoDiff是一种高效的MRI运动伪影校正模型，通过残差误差移位机制和四步反向扩散显著提升图像质量，同时大幅减少处理时间。


<details>
  <summary>Details</summary>
Motivation: MRI中的运动伪影会显著降低图像质量，传统方法成本高且流程复杂，因此需要一种更高效的校正方法。

Method: Res-MoCoDiff结合残差误差移位机制和U-net架构，采用Swin-Transformer块增强适应性，训练时使用l1+l2损失函数。

Result: 在所有运动严重程度下，Res-MoCoDiff均表现最佳，PSNR达41.91 dB，处理时间降至0.37秒/批次。

Conclusion: Res-MoCoDiff在运动伪影校正中表现出色，效率显著优于传统方法。

Abstract: Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly
degrade image quality and impair quantitative analysis. Conventional mitigation
strategies, such as repeated acquisitions or motion tracking, are costly and
workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising
diffusion probabilistic model tailored for MRI motion artifact correction.
Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in
the forward diffusion process, aligning the noise distribution with
motion-corrupted data and enabling an efficient four-step reverse diffusion. A
U-net backbone enhanced with Swin-Transformer blocks conventional attention
layers, improving adaptability across resolutions. Training employs a combined
l1+l2 loss, which promotes image sharpness and reduces pixel-level errors.
Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic
motion simulation framework and on an in-vivo dataset. Comparative analyses
were conducted against established methods, including CycleGAN, Pix2pix, and
MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),
structural similarity index measure (SSIM), and normalized mean squared error
(NMSE). Results: The proposed method demonstrated superior performance in
removing motion artifacts across all motion severity levels. Res-MoCoDiff
consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR
of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling
time was reduced to 0.37 seconds per batch of two image slices, compared with
101.74 seconds for conventional approaches.

</details>

### [94] [Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking](https://arxiv.org/abs/2505.03507)
*Shenglan Li,Rui Yao,Yong Zhou,Hancheng Zhu,Kunyang Sun,Bing Liu,Zhiwen Shao,Jiaqi Zhao*

Main category: cs.CV

TLDR: GDSTrack提出了一种动态图融合和时间扩散方法，用于解决自监督RGB-T跟踪中的伪标签噪声和背景干扰问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 减少对大规模标注的依赖，解决自监督RGB-T跟踪中伪标签错误和背景噪声对模态融合效率的影响。

Method: 通过动态图融合（MDGF）和时间扩散（TGID）模块，动态融合相邻帧模态并利用生成模型的去噪能力。

Result: 在四个公开RGB-T跟踪数据集上表现优于现有方法。

Conclusion: GDSTrack通过动态图融合和时间扩散有效提升了自监督RGB-T跟踪的鲁棒性和性能。

Abstract: To reduce the reliance on large-scale annotations, self-supervised RGB-T
tracking approaches have garnered significant attention. However, the omission
of the object region by erroneous pseudo-label or the introduction of
background noise affects the efficiency of modality fusion, while pseudo-label
noise triggered by similar object noise can further affect the tracking
performance. In this paper, we propose GDSTrack, a novel approach that
introduces dynamic graph fusion and temporal diffusion to address the above
challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the
modalities of neighboring frames, treats them as distractor noise, and
leverages the denoising capability of a generative model. Specifically, by
constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the
proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic
adjacency matrix to guide graph attention, focusing on and fusing the object's
coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features
from neighboring frames as interference, and thus improving robustness against
similar-object noise. Extensive experiments conducted on four public RGB-T
tracking datasets demonstrate that GDSTrack outperforms the existing
state-of-the-art methods. The source code is available at
https://github.com/LiShenglana/GDSTrack.

</details>

### [95] [Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks](https://arxiv.org/abs/2505.03522)
*Haotong Cheng,Zhiqi Zhang,Hao Li,Xinshang Zhang*

Main category: cs.CV

TLDR: 论文提出“通用性”概念及评估方程（UAE），设计优化模块CRB和DCRB，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视模块可移植性，需量化评估模块通用性与模型泛化能力的关系。

Method: 引入UAE评估模块移植性，设计CRB和DCRB模块。

Result: 实验显示新模块在PSNR提升0.83dB或参数减少71.3%时保持重建质量。

Conclusion: 优化模块显著提升模型性能，验证了通用性与泛化能力的关系。

Abstract: Deep learning has substantially advanced the Single Image Super-Resolution
(SISR). However, existing researches have predominantly focused on raw
performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions which extend the
traditional notion of "Generalization" to encompass the modules' ease of
transferability, thus revealing the relationships between module universality
and model generalizability. Then we propose the Universality Assessment
Equation (UAE), a metric for quantifying how readily a given module could be
transplanted across models. Guided by the UAE results of standard residual
blocks and other plug-and-play modules, we further design two optimized
modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).
Through comprehensive experiments on natural-scene benchmarks, remote-sensing
datasets, extreme-industrial imagery and on-device deployments, we demonstrate
that networks embedded with the proposed plug-and-play modules outperform
several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or
enabling a 71.3% reduction in parameters with negligible loss in reconstruction
fidelity.

</details>

### [96] [Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication](https://arxiv.org/abs/2505.03528)
*Chenguang Liu,Jianjun Chen,Yunfei Chen,Yubei He,Zhuangkun Wei,Hongjian Sun,Haiyan Lu,Qi Hao*

Main category: cs.CV

TLDR: 提出了一种联合加权和去噪框架Coop-WD，用于增强V2V通信受损下的协同感知，并提出了高效变体Coop-WD-eco以减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对不同级别V2V通信损伤的泛化能力，限制了协同感知的精度。

Method: 采用自监督对比模型和条件扩散概率模型进行层次化特征增强，并提出选择性去噪的高效变体。

Result: Coop-WD在所有信道类型中优于传统基准，Coop-WD-eco在严重失真下减少50%计算成本且保持精度。

Conclusion: Coop-WD框架显著提升了协同感知性能，高效变体在计算效率上具有优势。

Abstract: Cooperative perception, leveraging shared information from multiple vehicles
via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous
driving to alleviate the limitation of single-vehicle perception. Existing
works have explored the effects of V2V communication impairments on perception
precision, but they lack generalization to different levels of impairments. In
this work, we propose a joint weighting and denoising framework, Coop-WD, to
enhance cooperative perception subject to V2V channel impairments. In this
framework, the self-supervised contrastive model and the conditional diffusion
probabilistic model are adopted hierarchically for vehicle-level and
pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is
proposed to selectively deactivate denoising to reduce processing overhead.
Rician fading, non-stationarity, and time-varying distortion are considered.
Simulation results demonstrate that the proposed Coop-WD outperforms
conventional benchmarks in all types of channels. Qualitative analysis with
visual examples further proves the superiority of our proposed method. The
proposed Coop-WD-eco achieves up to 50% reduction in computational cost under
severe distortion while maintaining comparable accuracy as channel conditions
improve.

</details>

### [97] [RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT](https://arxiv.org/abs/2505.03538)
*Chuyu Zhao,Hao Huang,Jiashuo Guo,Ziyu Shen,Zhongwei Zhou,Jie Liu,Zekuan Yu*

Main category: cs.CV

TLDR: RAIL是一种双组双学生的半监督框架，通过区域感知教学机制解决CBCT牙齿分割中监督不足和伪标签不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 半监督学习在CBCT牙齿分割中面临监督不足和伪标签不可靠的挑战。

Method: 提出RAIL框架，包含DFS控制器和CAL调制器，通过双组交替训练实现知识转移和区域感知教学。

Result: 在四个CBCT数据集上，RAIL表现优于现有方法。

Conclusion: RAIL有效解决了半监督学习中的关键问题，提升了分割性能。

Abstract: Semi-supervised learning has become a compelling approach for 3D tooth
segmentation from CBCT scans, where labeled data is minimal. However, existing
methods still face two persistent challenges: limited corrective supervision in
structurally ambiguous or mislabeled regions during supervised training and
performance degradation caused by unreliable pseudo-labels on unlabeled data.
To address these problems, we propose Region-Aware Instructive Learning (RAIL),
a dual-group dual-student, semi-supervised framework. Each group contains two
student models guided by a shared teacher network. By alternating training
between the two groups, RAIL promotes intergroup knowledge transfer and
collaborative region-aware instruction while reducing overfitting to the
characteristics of any single model. Specifically, RAIL introduces two
instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller
improves supervised learning by instructing predictions only within areas where
student outputs diverge from both ground truth and the best student, thereby
concentrating supervision on structurally ambiguous or mislabeled areas. In the
unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces
agreement in regions with high model certainty while reducing the effect of
low-confidence predictions during training. This helps prevent our model from
learning unstable patterns and improves the overall reliability of
pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets
show that RAIL surpasses state-of-the-art methods under limited annotation. Our
code will be available at https://github.com/Tournesol-Saturday/RAIL.

</details>

### [98] [Panoramic Out-of-Distribution Segmentation](https://arxiv.org/abs/2505.03539)
*Mengfei Duan,Kailun Yang,Yuheng Zhang,Yihong Cao,Fei Teng,Kai Luo,Jiaming Zhang,Zhiyong Li,Shutao Li*

Main category: cs.CV

TLDR: 论文提出全景异常分割任务（PanOoS）及其首个解决方案POS，通过文本引导的提示分布学习适应全景图像特性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前全景语义分割方法无法识别异常，而传统方法在全景域表现不佳，需解决背景干扰和像素失真问题。

Method: 提出POS方法，结合解耦策略、提示修复注意力（PRA）和双层提示分布学习（BPDL），优化语义解码和掩码嵌入。

Result: POS在DenseOoS数据集上AuPRC提升34.25%，FPR95降低21.42%，优于现有方法。

Conclusion: POS在全景异常分割任务中表现优异，同时具备领先的闭集分割能力。

Abstract: Panoramic imaging enables capturing 360{\deg} images with an ultra-wide
Field-of-View (FoV) for dense omnidirectional perception. However, current
panoramic semantic segmentation methods fail to identify outliers, and pinhole
Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the
panoramic domain due to background clutter and pixel distortions. To address
these issues, we introduce a new task, Panoramic Out-of-distribution
Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the
first solution, POS, which adapts to the characteristics of panoramic images
through text-guided prompt distribution learning. Specifically, POS integrates
a disentanglement strategy designed to materialize the cross-domain
generalization capability of CLIP. The proposed Prompt-based Restoration
Attention (PRA) optimizes semantic decoding by prompt guidance and
self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)
refines the manifold of per-pixel mask embeddings via semantic prototype
supervision. Besides, to compensate for the scarcity of PanOoS datasets, we
establish two benchmarks: DenseOoS, which features diverse outliers in complex
environments, and QuadOoS, captured by a quadruped robot with a panoramic
annular lens system. Extensive experiments demonstrate superior performance of
POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,
outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves
leading closed-set segmentation capabilities. Code and datasets will be
available at https://github.com/MengfeiD/PanOoS.

</details>

### [99] [Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment](https://arxiv.org/abs/2505.03554)
*João Alves,Pia Haubro Andersen,Rikke Gade*

Main category: cs.CV

TLDR: 论文提出了一种基于深度学习和光流的方法，用于自动检测和定位马匹视频中的特定耳朵动作单元（AU），旨在解决手动标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 马匹情感状态评估领域因手动标注面部动作单元（AUs）耗时且昂贵，数据稀缺，亟需自动化标注系统以提高效率。

Method: 结合深度学习视频特征提取与循环神经网络进行分类任务，同时采用经典光流方法，实现耳朵动作的检测与定位。

Result: 在公开马匹视频数据集上，耳朵动作分类准确率达到87.5%，验证了方法的潜力。

Conclusion: 该方法为自动化AU检测提供了可行方案，未来可进一步应用于马匹福利和兽医诊断实践。

Abstract: The Equine Facial Action Coding System (EquiFACS) enables the systematic
annotation of facial movements through distinct Action Units (AUs). It serves
as a crucial tool for assessing affective states in horses by identifying
subtle facial expressions associated with discomfort. However, the field of
horse affective state assessment is constrained by the scarcity of annotated
data, as manually labelling facial AUs is both time-consuming and costly. To
address this challenge, automated annotation systems are essential for
leveraging existing datasets and improving affective states detection tools. In
this work, we study different methods for specific ear AU detection and
localization from horse videos. We leverage past works on deep learning-based
video feature extraction combined with recurrent neural networks for the video
classification task, as well as a classic optical flow based approach. We
achieve 87.5% classification accuracy of ear movement presence on a public
horse video dataset, demonstrating the potential of our approach. We discuss
future directions to develop these systems, with the aim of bridging the gap
between automated AU detection and practical applications in equine welfare and
veterinary diagnostics. Our code will be made publicly available at
https://github.com/jmalves5/read-my-ears.

</details>

### [100] [Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID](https://arxiv.org/abs/2505.03557)
*Koray Ulusan,Benjamin Kiefer*

Main category: cs.CV

TLDR: 研究探讨了在Stable Diffusion中使用DreamBooth和InstantID技术时，数据增强对提升生成肖像与原始照片面部相似性的影响，并提出了FaceDistance评估方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过数据增强提升Stable Diffusion生成肖像的面部相似性，以优化其在专业肖像生成中的应用。

Method: 通过实验评估DreamBooth和InstantID技术中不同数据增强策略的效果，并引入FaceDistance（基于FaceNet的包装器）量化面部相似性。

Result: 研究揭示了数据增强在提升SDXL生成肖像面部相似性中的作用，为下游应用提供了优化策略。

Conclusion: 数据增强能显著提升生成肖像的面部相似性，FaceDistance为评估提供了有效工具，研究为实际应用提供了指导。

Abstract: The personalization of Stable Diffusion for generating professional portraits
from amateur photographs is a burgeoning area, with applications in various
downstream contexts. This paper investigates the impact of augmentations on
improving facial resemblance when using two prominent personalization
techniques: DreamBooth and InstantID. Through a series of experiments with
diverse subject datasets, we assessed the effectiveness of various augmentation
strategies on the generated headshots' fidelity to the original subject. We
introduce FaceDistance, a wrapper around FaceNet, to rank the generations based
on facial similarity, which aided in our assessment. Ultimately, this research
provides insights into the role of augmentations in enhancing facial
resemblance in SDXL-generated portraits, informing strategies for their
effective deployment in downstream applications.

</details>

### [101] [Real-Time Person Image Synthesis Using a Flow Matching Model](https://arxiv.org/abs/2505.03562)
*Jiwoo Jeong,Kirok Kim,Wooju Kim,Nam-Joon Kim*

Main category: cs.CV

TLDR: PGPIS任务通过目标姿态和源图像生成逼真的人体图像，但实时性是一大挑战。本文提出基于流匹配（FM）的生成模型RPFM，在速度和图像质量间取得平衡，实现近实时性能。


<details>
  <summary>Details</summary>
Motivation: 实时PGPIS在AR/VR、游戏等应用中至关重要，但现有扩散模型速度慢，无法满足实时需求。

Method: 提出基于流匹配（FM）的生成模型RPFM，支持条件生成和潜在空间操作，提升训练和采样效率。

Result: 在DeepFashion数据集上，RPFM实现近实时采样速度，性能接近SOTA模型。

Conclusion: RPFM在速度和图像质量间取得平衡，为实时PGPIS应用提供了可行解决方案。

Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images
conditioned on a target pose and a source image. This task plays a key role in
various real-world applications, such as sign language video generation, AR/VR,
gaming, and live streaming. In these scenarios, real-time PGPIS is critical for
providing immediate visual feedback and maintaining user immersion.However,
achieving real-time performance remains a significant challenge due to the
complexity of synthesizing high-fidelity images from diverse and dynamic human
poses. Recent diffusion-based methods have shown impressive image quality in
PGPIS, but their slow sampling speeds hinder deployment in time-sensitive
applications. This latency is particularly problematic in tasks like generating
sign language videos during live broadcasts, where rapid image updates are
required. Therefore, developing a fast and reliable PGPIS model is a crucial
step toward enabling real-time interactive systems. To address this challenge,
we propose a generative model based on flow matching (FM). Our approach enables
faster, more stable, and more efficient training and sampling. Furthermore, the
proposed model supports conditional generation and can operate in latent space,
making it especially suitable for real-time PGPIS applications where both speed
and quality are critical. We evaluate our proposed method, Real-Time Person
Image Synthesis Using a Flow Matching Model (RPFM), on the widely used
DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves
near-real-time sampling speeds while maintaining performance comparable to the
state-of-the-art models. Our methodology trades off a slight, acceptable
decrease in generated-image accuracy for over a twofold increase in generation
speed, thereby ensuring real-time performance.

</details>

### [102] [Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images](https://arxiv.org/abs/2505.03567)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li,Zhiwen Wang*

Main category: cs.CV

TLDR: 提出UPD-TBPS框架，通过多粒度不确定性估计、原型不确定性解耦和跨模态重识别，提升文本行人搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中因检测和匹配的不确定性导致性能下降，需改进。

Method: 包含MUE（多粒度不确定性估计）、PUD（原型不确定性解耦）和ReID（跨模态重识别）三个模块。

Result: 在CUHK-SYSU-TBPS和PRW-TBPS数据集上验证了有效性。

Conclusion: UPD-TBPS框架显著提升了文本行人搜索的准确性。

Abstract: Text-based pedestrian search (TBPS) in full images aims to locate a target
pedestrian in untrimmed images using natural language descriptions. However, in
complex scenes with multiple pedestrians, existing methods are limited by
uncertainties in detection and matching, leading to degraded performance. To
address this, we propose UPD-TBPS, a novel framework comprising three modules:
Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty
Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts
multi-granularity queries to identify potential targets and assigns confidence
scores to reduce early-stage uncertainty. PUD leverages visual context
decoupling and prototype mining to extract features of the target pedestrian
described in the query. It separates and learns pedestrian prototype
representations at both the coarse-grained cluster level and the fine-grained
individual level, thereby reducing matching uncertainty. ReID evaluates
candidates with varying confidence levels, improving detection and retrieval
accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the
effectiveness of our framework.

</details>

### [103] [Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models](https://arxiv.org/abs/2505.03569)
*Mishal Fatima,Steffen Jung,Margret Keuper*

Main category: cs.CV

TLDR: 论文研究了图像背景对模型预测中虚假相关性的影响，提出了一个合成数据集Hard-Spurious-ImageNet，并发现模型在小ROI和偏离中心的物体上更依赖背景特征。


<details>
  <summary>Details</summary>
Motivation: 背景和物体位置、大小的偏差可能导致模型依赖虚假特征，影响预测准确性。

Method: 提出合成数据集Hard-Spurious-ImageNet，评估不同预训练模型对背景特征的依赖。

Result: 模型在小ROI和偏离中心的物体上更依赖背景特征，现有方法未能显著改善最差组性能。

Conclusion: 背景特征对模型预测有显著影响，现有方法需进一步改进以应对物体位置和大小的变化。

Abstract: Backgrounds in images play a major role in contributing to spurious
correlations among different data points. Owing to aesthetic preferences of
humans capturing the images, datasets can exhibit positional (location of the
object within a given frame) and size (region-of-interest to image ratio)
biases for different classes. In this paper, we show that these biases can
impact how much a model relies on spurious features in the background to make
its predictions. To better illustrate our findings, we propose a synthetic
dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images
with various backgrounds, object positions, and object sizes. By evaluating the
dataset on different pretrained models, we find that most models rely heavily
on spurious features in the background when the region-of-interest (ROI) to
image ratio is small and the object is far from the center of the image.
Moreover, we also show that current methods that aim to mitigate harmful
spurious features, do not take into account these factors, hence fail to
achieve considerable performance gains for worst-group accuracies when the size
and location of core features in an image change.

</details>

### [104] [Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning](https://arxiv.org/abs/2505.03575)
*Maria Kainz,Johannes K. Krondorfer,Malte Jaschik,Maria Jernej,Harald Ganster*

Main category: cs.CV

TLDR: 利用高光谱近红外成像和深度学习算法，研究纺织纤维的高效分类与分选，优化CNN和自编码器网络在不同纺织结构下表现良好。


<details>
  <summary>Details</summary>
Motivation: 纺织纤维回收对减少纺织业环境影响至关重要，需高效分类技术。

Method: 研究监督与非监督深度学习模型，测试其在不同纺织结构上的泛化能力。

Result: 优化的CNN和自编码器网络在多变条件下表现出强泛化能力。

Conclusion: 高光谱成像与深度学习结合可推动可持续纺织回收，实现精准分类。

Abstract: Recycling textile fibers is critical to reducing the environmental impact of
the textile industry. Hyperspectral near-infrared (NIR) imaging combined with
advanced deep learning algorithms offers a promising solution for efficient
fiber classification and sorting. In this study, we investigate supervised and
unsupervised deep learning models and test their generalization capabilities on
different textile structures. We show that optimized convolutional neural
networks (CNNs) and autoencoder networks achieve robust generalization under
varying conditions. These results highlight the potential of hyperspectral
imaging and deep learning to advance sustainable textile recycling through
accurate and robust classification.

</details>

### [105] [DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes](https://arxiv.org/abs/2505.03581)
*Sergey Linok,Vadim Semenov,Anastasia Trunova,Oleg Bulichev,Dmitry Yudin*

Main category: cs.CV

TLDR: DyGEnc是一种新方法，通过动态图编码结合语言模型，显著提升动态环境中事件分析的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉模型在动态环境中缺乏可解释时空对象表示的问题。

Method: 结合压缩时空结构表示与大型语言模型，生成可解释的文本场景图。

Result: 在STAR和AGQA数据集上表现优于现有方法15-25%，并能处理原始图像输入。

Conclusion: DyGEnc为机器人长期推理提供了稳健且压缩的图基记忆方案。

Abstract: The analysis of events in dynamic environments poses a fundamental challenge
in the development of intelligent agents and robots capable of interacting with
humans. Current approaches predominantly utilize visual models. However, these
methods often capture information implicitly from images, lacking interpretable
spatial-temporal object representations. To address this issue we introduce
DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates
compressed spatial-temporal structural observation representation with the
cognitive capabilities of large language models. The purpose of this
integration is to enable advanced question answering based on a sequence of
textual scene graphs. Extended evaluations on the STAR and AGQA datasets
indicate that DyGEnc outperforms existing visual methods by a large margin of
15-25% in addressing queries regarding the history of human-to-object
interactions. Furthermore, the proposed method can be seamlessly extended to
process raw input images utilizing foundational models for extracting explicit
textual scene graphs, as substantiated by the results of a robotic experiment
conducted with a wheeled manipulator platform. We hope that these findings will
contribute to the implementation of robust and compressed graph-based robotic
memory for long-horizon reasoning. Code is available at
github.com/linukc/DyGEnc.

</details>

### [106] [Fixed-Length Dense Fingerprint Representation](https://arxiv.org/abs/2505.03597)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TLDR: 提出了一种固定长度的指纹密集描述符FLARE，结合姿态对齐和鲁棒增强，显著提升了跨模态和低质量指纹的匹配性能。


<details>
  <summary>Details</summary>
Motivation: 固定长度指纹表示在处理多样指纹模态、姿态变化和噪声干扰时存在挑战，需要一种更鲁棒和高效的解决方案。

Method: FLARE框架采用三维密集描述符捕捉指纹脊结构的空间关系，结合姿态对齐和双重增强策略。

Result: 实验表明FLARE在多种指纹类型和低质量场景下性能优越，显著超越现有方法。

Conclusion: FLARE是一种统一且可扩展的解决方案，适用于鲁棒指纹表示和匹配。

Abstract: Fixed-length fingerprint representations, which map each fingerprint to a
compact and fixed-size feature vector, are computationally efficient and
well-suited for large-scale matching. However, designing a robust
representation that effectively handles diverse fingerprint modalities, pose
variations, and noise interference remains a significant challenge. In this
work, we propose a fixed-length dense descriptor of fingerprints, and introduce
FLARE-a fingerprint matching framework that integrates the Fixed-Length dense
descriptor with pose-based Alignment and Robust Enhancement. This fixed-length
representation employs a three-dimensional dense descriptor to effectively
capture spatial relationships among fingerprint ridge structures, enabling
robust and locally discriminative representations. To ensure consistency within
this dense feature space, FLARE incorporates pose-based alignment using
complementary estimation methods, along with dual enhancement strategies that
refine ridge clarity while preserving the original fingerprint modality. The
proposed dense descriptor supports fixed-length representation while
maintaining spatial correspondence, enabling fast and accurate similarity
computation. Extensive experiments demonstrate that FLARE achieves superior
performance across rolled, plain, latent, and contactless fingerprints,
significantly outperforming existing methods in cross-modality and low-quality
scenarios. Further analysis validates the effectiveness of the dense descriptor
design, as well as the impact of alignment and enhancement modules on the
accuracy of dense descriptor matching. Experimental results highlight the
effectiveness and generalizability of FLARE as a unified and scalable solution
for robust fingerprint representation and matching. The implementation and code
will be publicly available at https://github.com/Yu-Yy/FLARE.

</details>

### [107] [From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction](https://arxiv.org/abs/2505.03599)
*Fengming Lin,Arezoo Zakeri,Yidan Xue,Michael MacRaild,Haoran Dou,Zherui Zhou,Ziwei Zou,Ali Sarrami-Foroushani,Jinming Duan,Alejandro F. Frangi*

Main category: cs.CV

TLDR: 本文综述了基于深度学习的医学图像到网格重建方法，将其分为四类并详细分析，评估了不同方法在多种解剖结构中的应用，总结了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 医学图像到网格重建在计算医学和虚拟试验中至关重要，但现有方法多样且缺乏系统总结，本文旨在填补这一空白。

Method: 将现有方法分为模板模型、统计模型、生成模型和隐式模型四类，详细分析其方法基础、优缺点及适用性。

Result: 通过定量比较标准指标，评估了不同方法在心脏成像到神经学研究中的应用，并总结了公共数据集和评估指标。

Conclusion: 本文系统总结了医学图像到网格重建的现状，指出了拓扑正确性、几何精度和多模态集成等挑战，并提出了未来研究方向。

Abstract: Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,
enabling the transformation of medical imaging data into three-dimensional mesh
models that are critical in computational medicine and in silico trials for
advancing our understanding of disease mechanisms, and diagnostic and
therapeutic techniques in modern medicine. This survey systematically
categorizes existing approaches into four main categories: template models,
statistical models, generative models, and implicit models. Each category is
analysed in detail, examining their methodological foundations, strengths,
limitations, and applicability to different anatomical structures and imaging
modalities. We provide an extensive evaluation of these methods across various
anatomical applications, from cardiac imaging to neurological studies,
supported by quantitative comparisons using standard metrics. Additionally, we
compile and analyze major public datasets available for medical mesh
reconstruction tasks and discuss commonly used evaluation metrics and loss
functions. The survey identifies current challenges in the field, including
requirements for topological correctness, geometric accuracy, and
multi-modality integration. Finally, we present promising future research
directions in this domain. This systematic review aims to serve as a
comprehensive reference for researchers and practitioners in medical image
analysis and computational medicine.

</details>

### [108] [PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model](https://arxiv.org/abs/2505.03603)
*Y. B. Wang,S. Z. Zhou,J. F. Wu,T. Hu,J. N. Zhang,Y. Liu*

Main category: cs.CV

TLDR: PAHA是一个基于扩散模型的端到端音频驱动上半身人体动画框架，通过PAR和PCE方法提升生成质量与音频运动一致性，并设计了SG和DG推理指导方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多阶段生成和中间表示，导致推理时间长且生成质量不足，缺乏局部细粒度监督指导。

Method: 提出PAR动态调整区域训练损失权重，PCE构建基于扩散的区域视听分类器，并设计SG和DG推理指导方法。

Result: PAHA在音频运动对齐和视频相关评估中显著优于现有方法。

Conclusion: PAHA通过局部监督和推理优化，解决了生成质量和一致性问题，并发布了CNAS数据集推动研究。

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>

### [109] [Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection](https://arxiv.org/abs/2505.03610)
*Fangling Jiang,Qi Li,Bing Liu,Weining Wang,Caifeng Shan,Zhenan Sun,Ming-Hsuan Yang*

Main category: cs.CV

TLDR: 提出了一种基于知识图谱的提示学习框架，用于3D面具攻击检测，结合视觉语言模型和因果图理论，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态传感器或rPPG信号，成本高且泛化能力有限；而视觉语言多模态特征的潜力尚未被探索。

Method: 通过知识图谱实体和三元组生成任务特定的显式提示，并引入视觉特定知识过滤器和因果图理论优化学习过程。

Result: 在基准数据集上实现了最优的跨场景检测性能。

Conclusion: 该方法通过知识驱动的提示学习和因果优化，显著提升了3D面具攻击检测的泛化能力。

Abstract: 3D mask presentation attack detection is crucial for protecting face
recognition systems against the rising threat of 3D mask attacks. While most
existing methods utilize multimodal features or remote photoplethysmography
(rPPG) signals to distinguish between real faces and 3D masks, they face
significant challenges, such as the high costs associated with multimodal
sensors and limited generalization ability. Detection-related text descriptions
offer concise, universal information and are cost-effective to obtain. However,
the potential of vision-language multimodal features for 3D mask presentation
attack detection remains unexplored. In this paper, we propose a novel
knowledge-based prompt learning framework to explore the strong generalization
capability of vision-language models for 3D mask presentation attack detection.
Specifically, our approach incorporates entities and triples from knowledge
graphs into the prompt learning process, generating fine-grained, task-specific
explicit prompts that effectively harness the knowledge embedded in pre-trained
vision-language models. Furthermore, considering different input images may
emphasize distinct knowledge graph elements, we introduce a visual-specific
knowledge filter based on an attention mechanism to refine relevant elements
according to the visual context. Additionally, we leverage causal graph theory
insights into the prompt learning process to further enhance the generalization
ability of our method. During training, a spurious correlation elimination
paradigm is employed, which removes category-irrelevant local image patches
using guidance from knowledge-based text features, fostering the learning of
generalized causal prompts that align with category-relevant local patches.
Experimental results demonstrate that the proposed method achieves
state-of-the-art intra- and cross-scenario detection performance on benchmark
datasets.

</details>

### [110] [Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images](https://arxiv.org/abs/2505.03611)
*Fangling Jiang,Qi Li,Weining Wang,Wei Shen,Bing Liu,Zhenan Sun*

Main category: cs.CV

TLDR: 提出了一种基于视觉语言模型的新方法，通过学习未知欺骗提示来提升人脸反欺骗的泛化能力，无需使用欺骗人脸图像。


<details>
  <summary>Details</summary>
Motivation: 解决人脸反欺骗技术因协变量偏移和语义偏移导致的泛化能力不足问题。

Method: 利用视觉语言模型生成真实人脸和潜在未知欺骗攻击的文本提示，并通过多样化欺骗提示优化框架学习有效提示。

Result: 在九个数据集上的实验表明，该方法能有效泛化到未见目标域，对抗多种未知攻击类型。

Conclusion: 该方法通过视觉语言模型的知识迁移，显著提升了人脸反欺骗的泛化能力。

Abstract: Face anti-spoofing is a critical technology for ensuring the security of face
recognition systems. However, its ability to generalize across diverse
scenarios remains a significant challenge. In this paper, we attribute the
limited generalization ability to two key factors: covariate shift, which
arises from external data collection variations, and semantic shift, which
results from substantial differences in emerging attack types. To address both
challenges, we propose a novel approach for learning unknown spoof prompts,
relying solely on real face images from a single source domain. Our method
generates textual prompts for real faces and potential unknown spoof attacks by
leveraging the general knowledge embedded in vision-language models, thereby
enhancing the model's ability to generalize to unseen target domains.
Specifically, we introduce a diverse spoof prompt optimization framework to
learn effective prompts. This framework constrains unknown spoof prompts within
a relaxed prior knowledge space while maximizing their distance from real face
images. Moreover, it enforces semantic independence among different spoof
prompts to capture a broad range of spoof patterns. Experimental results on
nine datasets demonstrate that the learned prompts effectively transfer the
knowledge of vision-language models, enabling state-of-the-art generalization
ability against diverse unknown attack types across unseen target domains
without using any spoof face images.

</details>

### [111] [PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing](https://arxiv.org/abs/2505.03621)
*Yiping Xie,Bo Zhao,Mingtong Dai,Jian-Ping Zhou,Yue Sun,Tao Tan,Weicheng Xie,Linlin Shen,Zitong Yu*

Main category: cs.CV

TLDR: PhysLLM框架通过结合大语言模型（LLM）和rPPG领域组件，解决了rPPG信号在光照变化和运动干扰下的挑战，实现了高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: rPPG技术受光照变化和运动干扰影响，传统LLM难以处理连续的rPPG信号。PhysLLM旨在结合两者的优势。

Method: 提出Text Prototype Guidance（TPG）策略和Dual-Domain Stationary（DDS）算法，通过跨模态对齐和时频域特征重加权优化信号稳定性。

Result: 在四个基准数据集上，PhysLLM实现了最先进的精度和鲁棒性，适应光照变化和运动场景。

Conclusion: PhysLLM通过跨模态学习和领域优化，显著提升了rPPG的性能和适应性。

Abstract: Remote photoplethysmography (rPPG) enables non-contact physiological
measurement but remains highly susceptible to illumination changes, motion
artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at
capturing long-range dependencies, offering a potential solution but struggle
with the continuous, noise-sensitive nature of rPPG signals due to their
text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative
optimization framework that synergizes LLMs with domain-specific rPPG
components. Specifically, the Text Prototype Guidance (TPG) strategy is
proposed to establish cross-modal alignment by projecting hemodynamic features
into LLM-interpretable semantic space, effectively bridging the
representational gap between physiological signals and linguistic tokens.
Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for
resolving signal instability through adaptive time-frequency domain feature
re-weighting. Finally, rPPG task-specific cues systematically inject
physiological priors through physiological statistics, environmental contextual
answering, and task description, leveraging cross-modal learning to integrate
both visual and textual information, enabling dynamic adaptation to challenging
scenarios like variable illumination and subject movements. Evaluation on four
benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,
demonstrating superior generalization across lighting variations and motion
scenarios.

</details>

### [112] [Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map](https://arxiv.org/abs/2505.03623)
*Alessandro Simoni,Francesco Pelosin*

Main category: cs.CV

TLDR: 提出了一种基于扩散模型的工业缺陷数据集生成方法，通过边界框条件生成高保真分割掩码，提升缺陷一致性和空间准确性。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷分割需要高精度标注数据，但获取成本高且耗时。

Method: 采用扩散模型，以边界框为条件生成精确分割掩码。

Result: 相比现有方法，提升了缺陷一致性和空间准确性，并通过下游任务验证了合成数据的有效性。

Conclusion: 扩散模型能有效缩小合成与真实工业数据间的差距，提升分割模型的可靠性和成本效益。

Abstract: Synthetic dataset generation in Computer Vision, particularly for industrial
applications, is still underexplored. Industrial defect segmentation, for
instance, requires highly accurate labels, yet acquiring such data is costly
and time-consuming. To address this challenge, we propose a novel
diffusion-based pipeline for generating high-fidelity industrial datasets with
minimal supervision. Our approach conditions the diffusion model on enriched
bounding box representations to produce precise segmentation masks, ensuring
realistic and accurately localized defect synthesis. Compared to existing
layout-conditioned generative methods, our approach improves defect consistency
and spatial accuracy. We introduce two quantitative metrics to evaluate the
effectiveness of our method and assess its impact on a downstream segmentation
task trained on real and synthetic data. Our results demonstrate that
diffusion-based synthesis can bridge the gap between artificial and real-world
industrial data, fostering more reliable and cost-efficient segmentation
models. The code is publicly available at
https://github.com/covisionlab/diffusion_labeling.

</details>

### [113] [Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision](https://arxiv.org/abs/2505.03631)
*Linhan Cao,Wei Sun,Kaiwei Zhang,Yicong Peng,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TLDR: 提出了一种自监督学习框架，用于视频质量评估（VQA），通过大规模无标注网络视频和迭代自改进训练策略，显著提升了模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 传统VQA模型依赖人工标注数据集，成本高且难以扩展，限制了模型的泛化能力。

Method: 采用自监督学习框架，结合学习排序范式和质量伪标注，通过迭代自改进训练策略优化模型。

Result: 模型在零样本和微调场景下均表现优异，泛化能力显著提升，并刷新了现有基准。

Conclusion: 自监督方法有效解决了标注数据不足的问题，为VQA模型的泛化提供了新思路。

Abstract: Video quality assessment (VQA) is essential for quantifying perceptual
quality in various video processing workflows, spanning from camera capture
systems to over-the-top streaming platforms. While recent supervised VQA models
have made substantial progress, the reliance on manually annotated datasets --
a process that is labor-intensive, costly, and difficult to scale up -- has
hindered further optimization of their generalization to unseen video content
and distortions. To bridge this gap, we introduce a self-supervised learning
framework for VQA to learn quality assessment capabilities from large-scale,
unlabeled web videos. Our approach leverages a \textbf{learning-to-rank}
paradigm to train a large multimodal model (LMM) on video pairs automatically
labeled via two manners, including quality pseudo-labeling by existing VQA
models and relative quality ranking based on synthetic distortion simulations.
Furthermore, we introduce a novel \textbf{iterative self-improvement training
strategy}, where the trained model acts an improved annotator to iteratively
refine the annotation quality of training data. By training on a dataset
$10\times$ larger than the existing VQA benchmarks, our model: (1) achieves
zero-shot performance on in-domain VQA benchmarks that matches or surpasses
supervised models; (2) demonstrates superior out-of-distribution (OOD)
generalization across diverse video content and distortions; and (3) sets a new
state-of-the-art when fine-tuned on human-labeled datasets. Extensive
experimental results validate the effectiveness of our self-supervised approach
in training generalized VQA models. The datasets and code will be publicly
released to facilitate future research.

</details>

### [114] [Towards Smart Point-and-Shoot Photography](https://arxiv.org/abs/2505.03638)
*Jiawan Li,Fei Zhou,Zhipeng Zhong,Jiongzhi Lin,Guoping Qiu*

Main category: cs.CV

TLDR: 论文提出了一种智能点拍（SPAS）系统，通过实时调整相机姿态帮助用户拍摄高质量照片。系统包括构建大型数据集、开发基于CLIP的构图质量评估模型（CCQA）和相机姿态调整模型（CPAM）。


<details>
  <summary>Details</summary>
Motivation: 传统点拍相机无法指导用户构图，而智能手机用户普遍缺乏摄影技巧。SPAS系统旨在解决这一问题，帮助用户拍摄更好的照片。

Method: 1. 构建包含32万张图像和相机姿态信息的数据集；2. 开发CCQA模型，通过可学习文本嵌入技术为图像分配伪标签；3. 设计CPAM模型，判断当前视图是否需要调整并输出调整建议。

Result: SPAS系统通过实验验证了其性能，展示了在构图质量评估和相机姿态调整方面的有效性。

Conclusion: SPAS系统是首个能够实时指导用户构图的智能点拍系统，通过创新的模型和方法显著提升了拍摄质量。

Abstract: Hundreds of millions of people routinely take photos using their smartphones
as point and shoot (PAS) cameras, yet very few would have the photography
skills to compose a good shot of a scene. While traditional PAS cameras have
built-in functions to ensure a photo is well focused and has the right
brightness, they cannot tell the users how to compose the best shot of a scene.
In this paper, we present a first of its kind smart point and shoot (SPAS)
system to help users to take good photos. Our SPAS proposes to help users to
compose a good shot of a scene by automatically guiding the users to adjust the
camera pose live on the scene. We first constructed a large dataset containing
320K images with camera pose information from 4000 scenes. We then developed an
innovative CLIP-based Composition Quality Assessment (CCQA) model to assign
pseudo labels to these images. The CCQA introduces a unique learnable text
embedding technique to learn continuous word embeddings capable of discerning
subtle visual quality differences in the range covered by five levels of
quality description words {bad, poor, fair, good, perfect}. And finally we have
developed a camera pose adjustment model (CPAM) which first determines if the
current view can be further improved and if so it outputs the adjust suggestion
in the form of two camera pose adjustment angles. The two tasks of CPAM make
decisions in a sequential manner and each involves different sets of training
samples, we have developed a mixture-of-experts model with a gated loss
function to train the CPAM in an end-to-end manner. We will present extensive
results to demonstrate the performances of our SPAS system using publicly
available image composition datasets.

</details>

### [115] [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/abs/2505.03654)
*Yifan Xiang,Zhenxi Zhang,Bin Li,Yixuan Weng,Shoujun Zhou,Yangfan He,Keqin Li*

Main category: cs.CV

TLDR: 论文提出ReGraP数据集和ReGraP-LLaVA模型，解决现有个性化MLLMs在关系推理和知识连接方面的不足，通过图提示方法实现结构化推理，并在新基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化MLLMs缺乏对多对象关系的学习和推理能力，且实验局限于单一概念任务。

Method: 提出ReGraP数据集（含图像、知识图和CoT问答对），设计ReGraP-LLaVA模型，采用软硬图提示方法对齐知识图。

Result: ReGraP-LLaVA不仅学习个性化知识，还能进行关系推理，性能优于其他方法。

Conclusion: ReGraP数据集和模型为个性化MLLMs的关系推理和知识连接提供了新解决方案，实验验证其有效性。

Abstract: Recent advances in personalized MLLMs enable effective capture of
user-specific concepts, supporting both recognition of personalized concepts
and contextual captioning. However, humans typically explore and reason over
relations among objects and individuals, transcending surface-level information
to achieve more personalized and contextual understanding. To this end,
existing methods may face three main limitations: Their training data lacks
multi-object sets in which relations among objects are learnable. Building on
the limited training data, their models overlook the relations between
different personalized concepts and fail to reason over them. Their experiments
mainly focus on a single personalized concept, where evaluations are limited to
recognition and captioning tasks. To address the limitations, we present a new
dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each
set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more
structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an
MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard
graph prompting methods are designed to align KGs within the model's semantic
space. We establish the ReGraP Benchmark, which contains diverse task types:
multiple-choice, fill-in-the-blank, True/False, and descriptive questions in
both open- and closed-ended settings. The proposed benchmark is designed to
evaluate the relational reasoning and knowledge-connection capability of
personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and
other competitive MLLMs. Results show that the proposed model not only learns
personalized knowledge but also performs relational reasoning in responses,
achieving the SoTA performance compared with the competitive methods. All the
codes and datasets are released at: https://github.com/xyfyyds/ReGraP.

</details>

### [116] [Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models](https://arxiv.org/abs/2505.03662)
*Xin Du,Francesca M. Cozzi,Rajesh Jena*

Main category: cs.CV

TLDR: 提出了一种基于CycleGAN的方法，直接从T1加权MRI扫描生成FA图，解决了FA图与纤维束图谱空间不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: FA和DEC图对评估白质完整性至关重要，但FA图与纤维束图谱的空间不对齐限制了其在预测模型中的应用。

Method: 采用CycleGAN技术，利用未配对数据训练模型，直接从T1加权MRI生成FA图。

Result: 模型生成的FA图具有高保真度，尤其在肿瘤区域表现优异，SSIM和PSNR评估结果良好。

Conclusion: 该方法为临床工作流提供了AI驱动的替代方案，减少了对额外扫描的需求。

Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are
essential for evaluating white matter integrity and structural connectivity in
neuroimaging. However, the spatial misalignment between FA maps and
tractography atlases hinders their effective integration into predictive
models. To address this issue, we propose a CycleGAN based approach for
generating FA maps directly from T1-weighted MRI scans, representing the first
application of this technique to both healthy and tumour-affected tissues. Our
model, trained on unpaired data, produces high fidelity maps, which have been
rigorously evaluated using Structural Similarity Index (SSIM) and Peak
Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in
tumour regions. Radiological assessments further underscore the model's
potential to enhance clinical workflows by providing an AI-driven alternative
that reduces the necessity for additional scans.

</details>

### [117] [Distribution-Conditional Generation: From Class Distribution to Creative Generation](https://arxiv.org/abs/2505.03667)
*Fu Feng,Yucheng Xie,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TLDR: 论文提出Distribution-Conditional Generation和DisTok框架，通过分布条件生成和动态概念池实现新颖概念的生成。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型依赖训练数据分布，难以生成真正新颖的概念。本文旨在通过分布条件生成突破这一限制。

Method: 提出DisTok框架，将类别分布映射到潜在空间并解码为创意概念令牌，通过动态概念池迭代融合概念对。

Result: 实验显示DisTok在文本-图像对齐和人类偏好评分上达到最优性能。

Conclusion: DisTok通过分布条件生成和采样合成，实现了高效灵活的令牌级生成。

Abstract: Text-to-image (T2I) diffusion models are effective at producing semantically
aligned images, but their reliance on training data distributions limits their
ability to synthesize truly novel, out-of-distribution concepts. Existing
methods typically enhance creativity by combining pairs of known concepts,
yielding compositions that, while out-of-distribution, remain linguistically
describable and bounded within the existing semantic space. Inspired by the
soft probabilistic outputs of classifiers on ambiguous inputs, we propose
Distribution-Conditional Generation, a novel formulation that models creativity
as image synthesis conditioned on class distributions, enabling semantically
unconstrained creative generation. Building on this, we propose DisTok, an
encoder-decoder framework that maps class distributions into a latent space and
decodes them into tokens of creative concept. DisTok maintains a dynamic
concept pool and iteratively sampling and fusing concept pairs, enabling the
generation of tokens aligned with increasingly complex class distributions. To
enforce distributional consistency, latent vectors sampled from a Gaussian
prior are decoded into tokens and rendered into images, whose class
distributions-predicted by a vision-language model-supervise the alignment
between input distributions and the visual semantics of generated tokens. The
resulting tokens are added to the concept pool for subsequent composition.
Extensive experiments demonstrate that DisTok, by unifying
distribution-conditioned fusion and sampling-based synthesis, enables efficient
and flexible token-level generation, achieving state-of-the-art performance
with superior text-image alignment and human preference scores.

</details>

### [118] [CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting](https://arxiv.org/abs/2505.03679)
*Huawei Sun,Bora Kunter Sahin,Georg Stettinger,Maximilian Bernhard,Matthias Schubert,Robert Wille*

Main category: cs.CV

TLDR: 提出了一种结合扩散模型和相机-雷达融合架构的新方法，用于恶劣天气下的语义分割，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 相机在恶劣天气下性能下降，雷达数据稀疏且有噪声，融合两者信息可提升分割效果。

Method: 利用雷达点特征生成伪掩码，通过噪声抑制单元优化，并结合扩散模型生成修复图像。

Result: 在Waterscenes数据集上，相机基线分割性能提升2.63%，融合架构性能提升1.48%。

Conclusion: 该方法在恶劣天气下通过相机-雷达融合有效提升了语义分割性能。

Abstract: Segmenting objects in an environment is a crucial task for autonomous driving
and robotics, as it enables a better understanding of the surroundings of each
agent. Although camera sensors provide rich visual details, they are vulnerable
to adverse weather conditions. In contrast, radar sensors remain robust under
such conditions, but often produce sparse and noisy data. Therefore, a
promising approach is to fuse information from both sensors. In this work, we
propose a novel framework to enhance camera-only baselines by integrating a
diffusion model into a camera-radar fusion architecture. We leverage radar
point features to create pseudo-masks using the Segment-Anything model,
treating the projected radar points as point prompts. Additionally, we propose
a noise reduction unit to denoise these pseudo-masks, which are further used to
generate inpainted images that complete the missing information in the original
images. Our method improves the camera-only segmentation baseline by 2.63% in
mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the
Waterscenes dataset. This demonstrates the effectiveness of our approach for
semantic segmentation using camera-radar fusion under adverse weather
conditions.

</details>

### [119] [Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration](https://arxiv.org/abs/2505.03692)
*Shiqi Li,Jihua Zhu,Yifan Xie,Naiwen Hu,Di Wang*

Main category: cs.CV

TLDR: 本文提出了一种基于网络模型的多视角点云配准方法，通过匹配距离提取可靠对构建位姿图，并利用数据驱动的方式计算绝对位姿。


<details>
  <summary>Details</summary>
Motivation: 多视角点云配准在机器人、自动化和计算机视觉领域至关重要，但现有方法在构建位姿图和运动同步方面存在不可靠性问题。

Method: 设计了两个神经网络模型：一个用于从点云对的匹配距离中提取可靠对构建位姿图，另一个用于数据驱动地计算绝对位姿，结合几何分布信息和改进的注意力机制。

Result: 在多种室内外数据集上的实验验证了方法的有效性和泛化能力。

Conclusion: 提出的方法在多视角点云配准中表现出色，代码已开源。

Abstract: Multiview point cloud registration plays a crucial role in robotics,
automation, and computer vision fields. This paper concentrates on pose graph
construction and motion synchronization within multiview registration. Previous
methods for pose graph construction often pruned fully connected graphs or
constructed sparse graph using global feature aggregated from local
descriptors, which may not consistently yield reliable results. To identify
dependable pairs for pose graph construction, we design a network model that
extracts information from the matching distance between point cloud pairs. For
motion synchronization, we propose another neural network model to calculate
the absolute pose in a data-driven manner, rather than optimizing inaccurate
handcrafted loss functions. Our model takes into account geometric distribution
information and employs a modified attention mechanism to facilitate flexible
and reliable feature interaction. Experimental results on diverse indoor and
outdoor datasets confirm the effectiveness and generalizability of our
approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.

</details>

### [120] [Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning](https://arxiv.org/abs/2505.03703)
*François Role,Sébastien Meyer,Victor Amblard*

Main category: cs.CV

TLDR: 该论文提出了一种新的方法来评估和减少视觉语言模型中的模态间隙问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）存在模态间隙现象，导致文本和图像嵌入在共享表示空间中分离，这对下游任务（如多模态检索、聚类等）产生负面影响。目前缺乏通用的方法来评估和减少这种间隙。

Method: 作者提出了基于谱方法和最优传输技术的新方法，用于评估和减少模态间隙。

Result: 在多个图像-文本数据集和模型上的实验表明，这些方法能有效减少模态间隙，并改善下游任务的性能。

Conclusion: 论文提出的方法为评估和减少模态间隙提供了实用且有效的解决方案，代码已公开。

Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared
representation space. However, it has been shown that these models are subject
to a modality gap phenomenon meaning there exists a clear separation between
the embeddings from one modality and another in the embedding space. While this
misalignment is detrimental for downstream tasks such as multimodal retrieval,
multimodal clustering or zero-shot classification, etc. no generic and
practical methods have so far been proposed to assess it precisely and even
reduce it. We therefore propose novel measures and effective techniques
(spectral- and optimal transport-based methods) to achieve this goal. Extensive
experiments conducted on several image-text datasets and models demonstrate
their effectiveness and beneficial effects on downstream tasks. Our code is
available at the URL provided in the paper's abstract.

</details>

### [121] [DISARM++: Beyond scanner-free harmonization](https://arxiv.org/abs/2505.03715)
*Luca Caldera,Lara Cavinato,Alessio Cirone,Isabella Cama,Sara Garbarino,Raffaele Lodi,Fabrizio Tagliavini,Anna Nigri,Silvia De Francesco,Andrea Cappozzo,Michele Piana,Francesca Ieva*

Main category: cs.CV

TLDR: 该研究提出了一种新的T1加权MR图像跨扫描仪标准化方法，通过两种方式实现图像转换，并在多种应用中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决不同扫描仪间T1加权MR图像不一致的问题，确保下游分析的可靠性。

Method: 通过映射图像到无扫描仪空间或特定扫描仪域，实现图像标准化，无需大量预处理。

Result: 在脑年龄预测、AD分类等应用中表现优于现有方法，R2=0.60，AUC=0.95。

Conclusion: 该方法提供了一种高效、可靠的跨扫描器图像标准化解决方案，适用于多种神经影像研究。

Abstract: Harmonization of T1-weighted MR images across different scanners is crucial
for ensuring consistency in neuroimaging studies. This study introduces a novel
approach to direct image harmonization, moving beyond feature standardization
to ensure that extracted features remain inherently reliable for downstream
analysis. Our method enables image transfer in two ways: (1) mapping images to
a scanner-free space for uniform appearance across all scanners, and (2)
transforming images into the domain of a specific scanner used in model
training, embedding its unique characteristics. Our approach presents strong
generalization capability, even for unseen scanners not included in the
training phase. We validated our method using MR images from diverse cohorts,
including healthy controls, traveling subjects, and individuals with
Alzheimer's disease (AD). The model's effectiveness is tested in multiple
applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker
extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis
prediction (AUC = 0.95). In all cases, our harmonization technique outperforms
state-of-the-art methods, showing improvements in both reliability and
predictive accuracy. Moreover, our approach eliminates the need for extensive
preprocessing steps, such as skull-stripping, which can introduce errors by
misclassifying brain and non-brain structures. This makes our method
particularly suitable for applications that require full-head analysis,
including research on head trauma and cranial deformities. Additionally, our
harmonization model does not require retraining for new datasets, allowing
smooth integration into various neuroimaging workflows. By ensuring
scanner-invariant image quality, our approach provides a robust and efficient
solution for improving neuroimaging studies across diverse settings. The code
is available at this link.

</details>

### [122] [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/abs/2505.03730)
*Shiyi Zhang,Junhao Zhuang,Zhaoyang Zhang,Ying Shan,Yansong Tang*

Main category: cs.CV

TLDR: FlexiAct提出了一种新的动作定制方法，通过RefAdapter和FAE技术，实现了在保持身份一致性的同时，灵活适应不同空间结构和视角的动作转移。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动作定制中受限于严格的空间结构约束（如布局、骨架和视角一致性），限制了其适应性和灵活性。

Method: FlexiAct通过RefAdapter实现空间适应和一致性保持，结合FAE在去噪过程中直接提取动作，避免了传统时空架构的复杂性。

Result: 实验表明，FlexiAct能够有效地将动作转移到具有不同布局、骨架和视角的目标图像上。

Conclusion: FlexiAct在动作定制中实现了更高的灵活性和一致性，为多样化场景和主体提供了更好的适应性。

Abstract: Action customization involves generating videos where the subject performs
actions dictated by input control signals. Current methods use pose-guided or
global motion customization but are limited by strict constraints on spatial
structure, such as layout, skeleton, and viewpoint consistency, reducing
adaptability across diverse subjects and scenarios. To overcome these
limitations, we propose FlexiAct, which transfers actions from a reference
video to an arbitrary target image. Unlike existing methods, FlexiAct allows
for variations in layout, viewpoint, and skeletal structure between the subject
of the reference video and the target image, while maintaining identity
consistency. Achieving this requires precise action control, spatial structure
adaptation, and consistency preservation. To this end, we introduce RefAdapter,
a lightweight image-conditioned adapter that excels in spatial adaptation and
consistency preservation, surpassing existing methods in balancing appearance
consistency and structural flexibility. Additionally, based on our
observations, the denoising process exhibits varying levels of attention to
motion (low frequency) and appearance details (high frequency) at different
timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike
existing methods that rely on separate spatial-temporal architectures, directly
achieves action extraction during the denoising process. Experiments
demonstrate that our method effectively transfers actions to subjects with
diverse layouts, skeletons, and viewpoints. We release our code and model
weights to support further research at
https://shiyi-zh0408.github.io/projectpages/FlexiAct/

</details>

### [123] [Multi-Agent System for Comprehensive Soccer Understanding](https://arxiv.org/abs/2505.03735)
*Jiayuan Rao,Zifeng Li,Haoning Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TLDR: 提出一个全面的足球理解框架，包括构建知识库SoccerWiki、基准测试SoccerBench、多智能体系统SoccerAgent，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的足球理解研究多局限于孤立任务，需填补全面理解的空白。

Method: 构建SoccerWiki知识库、SoccerBench基准测试，开发多智能体系统SoccerAgent。

Result: SoccerAgent在SoccerBench上表现优异，超越现有MLLMs。

Conclusion: 提出的框架为足球理解提供了全面解决方案，数据与代码已开源。

Abstract: Recent advancements in AI-driven soccer understanding have demonstrated rapid
progress, yet existing research predominantly focuses on isolated or narrow
tasks. To bridge this gap, we propose a comprehensive framework for holistic
soccer understanding. Specifically, we make the following contributions in this
paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer
knowledge base, integrating rich domain knowledge about players, teams,
referees, and venues to enable knowledge-driven reasoning; (ii) we present
SoccerBench, the largest and most comprehensive soccer-specific benchmark,
featuring around 10K standardized multimodal (text, image, video) multi-choice
QA pairs across 13 distinct understanding tasks, curated through automated
pipelines and manual verification; (iii) we introduce SoccerAgent, a novel
multi-agent system that decomposes complex soccer questions via collaborative
reasoning, leveraging domain expertise from SoccerWiki and achieving robust
performance; (iv) extensive evaluations and ablations that benchmark
state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our
proposed agentic system. All data and code are publicly available at:
https://jyrao.github.io/SoccerAgent/.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [124] [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
*Fabrizio Marozzo*

Main category: cs.AI

TLDR: 提出一种通过结构化澄清问题解决生成式AI中自然语言模糊性的迭代方法，显著提高准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 自然语言的模糊性导致生成式AI系统需要多次迭代修正，影响效率和用户体验。

Method: 采用结构化澄清问题和替代方案提案的迭代方法，结合输入/输出示例逐步消除模糊性。

Result: 在编码、数据分析和创意写作等任务中，该方法表现出更高的准确性、竞争性解决时间和用户满意度。

Conclusion: 结构化迭代方法优于传统一次性解决方案，能更高效地生成精确结果。

Abstract: Generative AI systems have revolutionized human interaction by enabling
natural language-based coding and problem solving. However, the inherent
ambiguity of natural language often leads to imprecise instructions, forcing
users to iteratively test, correct, and resubmit their prompts. We propose an
iterative approach that systematically narrows down these ambiguities through a
structured series of clarification questions and alternative solution
proposals, illustrated with input/output examples as well. Once every
uncertainty is resolved, a final, precise solution is generated. Evaluated on a
diverse dataset spanning coding, data analysis, and creative writing, our
method demonstrates superior accuracy, competitive resolution times, and higher
user satisfaction compared to conventional one-shot solutions, which typically
require multiple manual iterations to achieve a correct output.

</details>

### [125] [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
*Orevaoghene Ahia,Martijn Bartelds,Kabir Ahuja,Hila Gonen,Valentin Hofmann,Siddhant Arora,Shuyue Stella Li,Vishal Puttagunta,Mofetoluwa Adeyemi,Charishma Buchireddy,Ben Walls,Noah Bennett,Shinji Watanabe,Noah A. Smith,Yulia Tsvetkov,Sachin Kumar*

Main category: cs.AI

TLDR: BLAB是一个针对长音频语言模型的挑战性基准测试，评估模型在长音频任务中的表现，发现现有模型在长音频理解上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 开发能够理解多样化语音交互的大型音频语言模型，以提升语言技术的可访问性，但目前对长音频段的研究不足。

Method: 引入BLAB基准测试，包含833+小时的长音频片段（平均51分钟）和人工标注的问题与答案，评估六种音频语言模型。

Result: 所有模型（包括Gemini 2.0 Pro和GPT-4o）在BLAB任务中表现不佳，性能随音频时长增加而下降。

Conclusion: BLAB为开发具有长音频理解能力的音频语言模型提供了挑战性框架。

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [126] [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
*Kexin Ding,Mu Zhou,Akshay Chaudhari,Shaoting Zhang,Dimitris N. Metaxas*

Main category: cs.CY

TLDR: 论文探讨了大型语言模型（LLMs）在医疗领域中的对齐问题，强调医疗利益相关者需参与模型全生命周期以实现有效、安全的应用。


<details>
  <summary>Details</summary>
Motivation: LLMs的行为可能与医疗利益相关者的知识、需求和价值观不一致，因此需要确保模型输出与医疗工作流程对齐。

Method: 通过医疗知识整合、任务理解和人工指导等方法，提升LLMs与人类价值观的对齐。

Result: 研究表明，LLMs可以通过增强对齐更好地遵循人类价值观，支持可信的医疗应用。

Conclusion: 未来需进一步强化人类与LLMs的对齐，以构建可靠的医疗应用。

Abstract: The wide exploration of large language models (LLMs) raises the awareness of
alignment between healthcare stakeholder preferences and model outputs. This
alignment becomes a crucial foundation to empower the healthcare workflow
effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not
always match with healthcare stakeholders' knowledge, demands, and values. To
enable a human-AI alignment, healthcare stakeholders will need to perform
essential roles in guiding and enhancing the performance of LLMs. Human
professionals must participate in the entire life cycle of adopting LLM in
healthcare, including training data curation, model training, and inference. In
this review, we discuss the approaches, tools, and applications of alignments
between healthcare stakeholders and LLMs. We demonstrate that LLMs can better
follow human values by properly enhancing healthcare knowledge integration,
task understanding, and human guidance. We provide outlooks on enhancing the
alignment between humans and LLMs to build trustworthy real-world healthcare
applications.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [127] [Sim2Real Transfer for Vision-Based Grasp Verification](https://arxiv.org/abs/2505.03046)
*Pau Amargant,Peter Hönig,Markus Vincze*

Main category: cs.RO

TLDR: 提出了一种基于视觉的抓取验证方法，用于判断机器人夹爪是否成功抓取物体，采用两阶段架构（YOLO检测+ResNet分类），并引入合成数据集HSR-GraspSynth。实验表明该方法在真实环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于力和触觉传感器的抓取验证方法在处理可变形和非刚性物体时效果不佳，因此需要一种更有效的视觉解决方案。

Method: 使用两阶段架构：YOLO检测夹爪位置，ResNet分类判断物体是否存在；引入合成数据集HSR-GraspSynth补充真实数据不足。

Result: 实验证明该方法在真实环境中具有高准确性，并具备集成到抓取流程中的潜力。

Conclusion: 基于视觉的抓取验证方法有效解决了传统方法的局限性，合成数据集的引入进一步提升了模型的泛化能力。

Abstract: The verification of successful grasps is a crucial aspect of robot
manipulation, particularly when handling deformable objects. Traditional
methods relying on force and tactile sensors often struggle with deformable and
non-rigid objects. In this work, we present a vision-based approach for grasp
verification to determine whether the robotic gripper has successfully grasped
an object. Our method employs a two-stage architecture; first YOLO-based object
detection model to detect and locate the robot's gripper and then a
ResNet-based classifier determines the presence of an object. To address the
limitations of real-world data capture, we introduce HSR-GraspSynth, a
synthetic dataset designed to simulate diverse grasping scenarios. Furthermore,
we explore the use of Visual Question Answering capabilities as a zero-shot
baseline to which we compare our model. Experimental results demonstrate that
our approach achieves high accuracy in real-world environments, with potential
for integration into grasping pipelines. Code and datasets are publicly
available at https://github.com/pauamargant/HSR-GraspSynth .

</details>

### [128] [Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets](https://arxiv.org/abs/2505.03174)
*Guillermo Roque,Erika Maquiling,Jose Giovanni Tapia Lopez,Ross Greer*

Main category: cs.RO

TLDR: 利用GPS和NLP自动生成指令-动作数据对，减少人工标注成本，提升数据集规模和质量。


<details>
  <summary>Details</summary>
Motivation: 人工标注指令-动作数据对成本高且效率低，探索自动化生成方法以解决此问题。

Method: 通过GPS应用收集语音指令，结合视频数据形成视觉-语言-动作三元组，使用ADVLAT-Engine系统自动化分类。

Result: 成功将GPS语音指令分为八类，展示了从免费应用中获取多样化指令的潜力。

Conclusion: 自动化生成指令-动作数据对可高效支持视觉-语言导航和交互式自主系统的发展。

Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems,
especially autonomous vehicles (AVs), but having humans manually annotate this
data is costly and time-inefficient. This paper explores the potential of using
mobile application Global Positioning System (GPS) references and Natural
Language Processing (NLP) to automatically generate large volumes of IA
commands and responses without having a human generate or retroactively tag the
data. In our pilot data collection, by driving to various destinations and
collecting voice instructions from GPS applications, we demonstrate a means to
collect and categorize the diverse sets of instructions, further accompanied by
video data to form complete vision-language-action triads. We provide details
on our completely automated data collection prototype system, ADVLAT-Engine. We
characterize collected GPS voice instructions into eight different
classifications, highlighting the breadth of commands and referentialities
available for curation from freely available mobile applications. Through
research and exploration into the automation of IA data pairs using GPS
references, the potential to increase the speed and volume at which
high-quality IA datasets are created, while minimizing cost, can pave the way
for robust vision-language-action (VLA) models to serve tasks in
vision-language navigation (VLN) and human-interactive autonomous systems.

</details>

### [129] [Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach](https://arxiv.org/abs/2505.03702)
*Srecharan Selvam,Abhishesh Silwal,George Kanter*

Main category: cs.RO

TLDR: 提出了一种结合几何与神经网络的混合方法，用于农业中自主叶片抓取，通过自监督学习实现高成功率。


<details>
  <summary>Details</summary>
Motivation: 解决农业环境中叶片形态多变和可变形叶片带来的抓取挑战。

Method: 结合YOLOv8实例分割和RAFT-Stereo 3D深度估计，通过几何特征评分和神经网络模块（GraspPointCNN）动态融合。

Result: 在控制环境中成功率88.0%，真实温室中84.7%，显著优于纯几何（75.3%）和纯神经网络（60.2%）方法。

Conclusion: 为农业机器人领域提供了一种结合领域知识与机器学习的新范式，支持全自动作物监测系统。

Abstract: Automating leaf manipulation in agricultural settings faces significant
challenges, including the variability of plant morphologies and deformable
leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf
grasping that combines traditional computer vision with neural networks through
self-supervised learning. Our method integrates YOLOv8 for instance
segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf
representations, which feed into both a geometric feature scoring pipeline and
a neural refinement module (GraspPointCNN). The key innovation is our
confidence-weighted fusion mechanism that dynamically balances the contribution
of each approach based on prediction certainty. Our self-supervised framework
uses the geometric pipeline as an expert teacher to automatically generate
training data. Experiments demonstrate that our approach achieves an 88.0%
success rate in controlled environments and 84.7% in real greenhouse
conditions, significantly outperforming both purely geometric (75.3%) and
neural (60.2%) methods. This work establishes a new paradigm for agricultural
robotics where domain expertise is seamlessly integrated with machine learning
capabilities, providing a foundation for fully automated crop monitoring
systems.

</details>

### [130] [Visual Imitation Enables Contextual Humanoid Control](https://arxiv.org/abs/2505.03729)
*Arthur Allshire,Hongsuk Choi,Junyi Zhang,David McAllister,Anthony Zhang,Chung Min Kim,Trevor Darrell,Pieter Abbeel,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.RO

TLDR: VIDEOMIMIC是一种从真实到模拟再到真实的流程，通过日常视频学习人类动作和环境，生成人形机器人的全身控制策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用环境上下文教人形机器人完成复杂动作（如爬楼梯、坐椅子），提出通过人类动作视频直接指导机器人的方法。

Method: 提出VIDEOMIMIC流程，从视频中重建人类动作和环境，生成机器人控制策略，并实现单策略多任务控制。

Result: 在真实人形机器人上验证了稳健、可重复的上下文控制能力，如上下楼梯、坐椅子等动态动作。

Conclusion: VIDEOMIMIC为教人形机器人在多样化现实环境中操作提供了可扩展的路径。

Abstract: How can we teach humanoids to climb staircases and sit on chairs using the
surrounding environment context? Arguably, the simplest way is to just show
them-casually capture a human motion video and feed it to humanoids. We
introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday
videos, jointly reconstructs the humans and the environment, and produces
whole-body control policies for humanoid robots that perform the corresponding
skills. We demonstrate the results of our pipeline on real humanoid robots,
showing robust, repeatable contextual control such as staircase ascents and
descents, sitting and standing from chairs and benches, as well as other
dynamic whole-body skills-all from a single policy, conditioned on the
environment and global root commands. VIDEOMIMIC offers a scalable path towards
teaching humanoids to operate in diverse real-world environments.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [131] [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TLDR: 该论文研究了自动程序修复（APR）中平衡多输出生成和多轮迭代的策略，使用三种指令调优的LLM模型，通过有限数量的补丁（每个bug最多10个）实现高效修复。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复（APR）旨在减少手动修复代码错误的成本。传统方法依赖生成大量补丁，而LLM的自我迭代能力提供了新的优化路径。论文旨在平衡这两种策略，探索更高效的修复方法。

Method: 研究采用三种指令调优的LLM模型（DeepSeekCoder-Instruct、Codellama-Instruct、Llama3.1-Instruct），通过不同规模（1K、30K、65K）和两种微调技术（Full Fine-Tuning和LoRA）进行实验，评估其在HumanEval-Java和Defects4J基准上的表现。

Result: 实验表明，仅使用少量微调数据（<1%）即可显著提升修复效果（最高78%），但过度微调会导致收益递减。迭代生成补丁的方式对基础模型效果显著，复杂任务中优势更明显。

Conclusion: 研究强调需要结合多输出生成和迭代优化的平衡策略，为APR任务提供更高效的解决方案。

Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to
identify and fix errors in source code. Before the rise of LLM-based agents, a
common strategy was to increase the number of generated patches, sometimes to
the thousands, to achieve better repair results on benchmarks. More recently,
self-iterative capabilities enabled LLMs to refine patches over multiple rounds
guided by feedback. However, literature often focuses on many iterations and
disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the
generation of multiple outputs and multiple rounds of iteration, while imposing
a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs
- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR
task. We further fine-tune each model on an APR dataset with three sizes (1K,
30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess
their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning
dataset, we can achieve improvements of up to 78% in the number of plausible
patches generated, challenging prior studies that reported limited gains using
Full Fine-Tuning. However, we find that exceeding certain thresholds leads to
diminishing outcomes, likely due to overfitting. Moreover, we show that base
models greatly benefit from creating patches in an iterative fashion rather
than generating them all at once. In addition, the benefit of iterative
strategies becomes more pronounced in complex benchmarks. Even fine-tuned
models, while benefiting less from iterations, still gain advantages,
particularly on complex benchmarks. The research underscores the need for
balanced APR strategies that combine multi-output generation and iterative
refinement.

</details>

<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [132] [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Kangjie Chen,Tianwei Zhang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TLDR: 本文提出了一种针对大型语言模型（LLMs）的新型后门攻击——语言后门攻击，利用语言本身作为触发器，使受感染的LLMs生成煽动性言论。通过改进方法，提出了任务无关的BadLingual攻击，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示LLMs在多语言能力下的新漏洞，尤其是语言后门攻击可能加剧种族歧视，从而促进未来防御研究。

Method: 方法包括基线语言后门攻击（通过翻译污染训练数据）和改进的BadLingual攻击（使用PGCG对抗训练扩展决策边界）。

Result: 基线攻击在特定任务上ASR超过90%，但在任务无关场景下仅为37.61%；BadLingual提高了37.35%。

Conclusion: 结论指出LLMs在多语言能力下存在新漏洞，BadLingual攻击有效，未来需加强防御研究。

Abstract: In this paper, we present a new form of backdoor attack against Large
Language Models (LLMs): lingual-backdoor attacks. The key novelty of
lingual-backdoor attacks is that the language itself serves as the trigger to
hijack the infected LLMs to generate inflammatory speech. They enable the
precise targeting of a specific language-speaking group, exacerbating racial
discrimination by malicious entities. We first implement a baseline
lingual-backdoor attack, which is carried out by poisoning a set of training
data for specific downstream tasks through translation into the trigger
language. However, this baseline attack suffers from poor task generalization
and is impractical in real-world settings. To address this challenge, we design
BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any
downstream tasks within the chat LLMs, regardless of the specific questions of
these tasks. We design a new approach using PPL-constrained Greedy Coordinate
Gradient-based Search (PGCG) based adversarial training to expand the decision
boundary of lingual-backdoor, thereby enhancing the generalization ability of
lingual-backdoor across various tasks. We perform extensive experiments to
validate the effectiveness of our proposed attacks. Specifically, the baseline
attack achieves an ASR of over 90% on the specified tasks. However, its ASR
reaches only 37.61% across six tasks in the task-agnostic scenario. In
contrast, BadLingual brings up to 37.35% improvement over the baseline. Our
study sheds light on a new perspective of vulnerabilities in LLMs with
multilingual capabilities and is expected to promote future research on the
potential defenses to enhance the LLMs' robustness

</details>

<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [133] [Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights](https://arxiv.org/abs/2505.02845)
*Jeremias Gerner,Klaus Bogenberger,Stefanie Schmidtner*

Main category: physics.soc-ph

TLDR: 论文探讨了Floating Car Observers (FCOs)在微观交通模拟中的建模方法，评估其对智能交通系统(ITS)的潜力，并提出了一种基于神经网络的仿真技术。结果显示，FCOs在低渗透率下仍能有效检测车辆，数据驱动方法可恢复80%以上车辆。


<details>
  <summary>Details</summary>
Motivation: 传统Floating Car Data (FCD)数据有限，FCOs通过集成车载传感器提供更丰富的交通数据，但其在ITS中的应用潜力需进一步评估。

Method: 研究采用多种建模方法，包括2D光线追踪、高保真协同仿真和基于神经网络的仿真技术，并在SUMO中建立交通网络数字孪生进行验证。

Result: 实验表明，FCOs在20%渗透率下可检测65%的车辆，结合时间信息后可恢复80%以上车辆，位置偏差极小。

Conclusion: FCOs在ITS中具有显著潜力，尤其在交通状态估计和监测方面，适用于不同渗透率和交通条件。

Abstract: Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) by
integrating onboard sensors to detect and localize other traffic participants,
providing richer and more detailed traffic data. In this work, we explore
various modeling approaches for FCO detections within microscopic traffic
simulations to evaluate their potential for Intelligent Transportation System
(ITS) applications. These approaches range from 2D raytracing to high-fidelity
co-simulations that emulate real-world sensors and integrate 3D object
detection algorithms to closely replicate FCO detections. Additionally, we
introduce a neural network-based emulation technique that effectively
approximates the results of high-fidelity co-simulations. This approach
captures the unique characteristics of FCO detections while offering a fast and
scalable solution for modeling. Using this emulation method, we investigate the
impact of FCO data in a digital twin of a traffic network modeled in SUMO.
Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based
detections can identify 65% of vehicles across various intersections and
traffic demand scenarios. Further potential emerges when temporal insights are
integrated, enabling the recovery of previously detected but currently unseen
vehicles. By employing data-driven methods, we recover over 80% of these
vehicles with minimal positional deviations. These findings underscore the
potential of FCOs for ITS, particularly in enhancing traffic state estimation
and monitoring under varying penetration rates and traffic conditions.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [134] [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
*Arthur Satouf,Gabriel Ben Zenou,Benjamin Piwowarski,Habiboulaye Amadou Boubacar,Pablo Piantanida*

Main category: cs.IR

TLDR: 本文提出了一种基于Rational Speech Acts (RSA)框架的稀疏神经信息检索方法，通过动态调整文档中词项的权重，提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏神经信息检索方法（以及传统模型如BM25）未充分考虑文档集合中词项权重的复杂交互作用。

Method: 将RSA框架（一种语言学方法）应用于信息检索，动态调整词项与文档的交互，考虑数据集中文档的影响。

Result: 实验表明，引入RSA显著提升了多种稀疏检索模型的性能，并在BEIR基准测试的外域数据集上达到最优效果。

Conclusion: RSA框架能有效改进稀疏检索模型，提升文档表示的质量。

Abstract: Current sparse neural information retrieval (IR) methods, and to a lesser
extent more traditional models such as BM25, do not take into account the
document collection and the complex interplay between different term weights
when representing a single document. In this paper, we show how the Rational
Speech Acts (RSA), a linguistics framework used to minimize the number of
features to be communicated when identifying an object in a set, can be adapted
to the IR case -- and in particular to the high number of potential features
(here, tokens). RSA dynamically modulates token-document interactions by
considering the influence of other documents in the dataset, better contrasting
document representations. Experiments show that incorporating RSA consistently
improves multiple sparse retrieval models and achieves state-of-the-art
performance on out-of-domain datasets from the BEIR benchmark.
https://github.com/arthur-75/Rational-Retrieval-Acts

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [135] [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
*Zhaoxi Mu,Xinyu Yang,Gang Wang*

Main category: cs.SD

TLDR: SepALM是一种利用音频语言模型（ALM）在文本域中修正和重新合成语音的新方法，解决了传统语音分离技术在噪声和混响环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的噪声和混响会导致传统语音分离技术产生失真或伪影，需要一种更鲁棒的方法来提升分离效果。

Method: SepALM包含四个核心组件：分离器、校正器、合成器和对齐器，结合ALM的端到端纠错机制，避免传统方法中ASR与LLM结合的优化问题，并采用CoT提示和知识蒸馏技术。

Result: 实验证明SepALM不仅提高了语音分离的精度，还显著增强了在新声学环境中的适应性。

Conclusion: SepALM通过ALM的引入，有效解决了传统语音分离技术的不足，提升了分离质量和环境适应性。

Abstract: While contemporary speech separation technologies adeptly process lengthy
mixed audio waveforms, they are frequently challenged by the intricacies of
real-world environments, including noisy and reverberant settings, which can
result in artifacts or distortions in the separated speech. To overcome these
limitations, we introduce SepALM, a pioneering approach that employs audio
language models (ALMs) to rectify and re-synthesize speech within the text
domain following preliminary separation. SepALM comprises four core components:
a separator, a corrector, a synthesizer, and an aligner. By integrating an
ALM-based end-to-end error correction mechanism, we mitigate the risk of error
accumulation and circumvent the optimization hurdles typically encountered in
conventional methods that amalgamate automatic speech recognition (ASR) with
large language models (LLMs). Additionally, we have developed Chain-of-Thought
(CoT) prompting and knowledge distillation techniques to facilitate the
reasoning and training processes of the ALM. Our experiments substantiate that
SepALM not only elevates the precision of speech separation but also markedly
bolsters adaptability in novel acoustic environments.

</details>

### [136] [CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization](https://arxiv.org/abs/2505.03186)
*Detao Bai,Zhiheng Ma,Xihan Wei,Liefeng Bo*

Main category: cs.SD

TLDR: CoGenAV是一种高效的多模态模型，通过对比生成同步策略学习音频-视觉表示，在多种语音任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 利用说话者的唇部动作、声音和语言内容之间的同步性，提升语音处理任务性能，尤其是在传统音频系统表现不佳的场景。

Method: CoGenAV通过优化双重目标（对比特征对齐和生成文本预测）学习跨模态相关性，仅使用223小时的LRS2数据集。

Result: 在AVSR任务中达到1.27%的WER，VSR任务中22.0%的WER，噪声环境下性能提升70%以上，并在其他任务中表现优异。

Conclusion: CoGenAV的多模态表示在多种任务中具有广泛适用性，模型将开源以促进学术和工业界的进一步合作。

Abstract: The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [137] [Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers](https://arxiv.org/abs/2505.02843)
*Miriam Cobo,David Corral Fontecha,Wilson Silva,Lara Lloret Iglesias*

Main category: eess.IV

TLDR: 论文探讨了人工智能在医学影像中的应用，强调了物理知识对提升AI算法可信度和鲁棒性的重要性，并回顾了物理原理在生成模型和重建算法中的最新进展。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI领域快速发展，但开发者常缺乏对影像采集物理原理的全面理解，限制了AI潜力的发挥。

Method: 回顾医学影像的物理基础及其对AI的影响，探索物理知识如何融入AI模型。

Result: 物理知识的整合提升了AI在医学影像中的表现，尤其在数据有限的情况下。

Conclusion: 物理启发的机器学习模型通过结合物理约束，能更好地学习医学影像特征，推动领域发展。

Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in
the last years, due to rapid advances in deep learning and computing resources.
Applications cover the full range of existing medical imaging modalities, with
unique characteristics driven by the physics of each technique. Yet, artificial
intelligence professionals entering the field, and even experienced developers,
often lack a comprehensive understanding of the physical principles underlying
medical image acquisition, which hinders their ability to fully leverage its
potential. The integration of physics knowledge into artificial intelligence
algorithms enhances their trustworthiness and robustness in medical imaging,
especially in scenarios with limited data availability. In this work, we review
the fundamentals of physics in medical images and their impact on the latest
advances in artificial intelligence, particularly, in generative models and
reconstruction algorithms. Finally, we explore the integration of physics
knowledge into physics-inspired machine learning models, which leverage
physics-based constraints to enhance the learning of medical imaging features.

</details>

### [138] [Dual Prompting for Diverse Count-level PET Denoising](https://arxiv.org/abs/2505.03037)
*Xiaofeng Liu,Yongsong Huang,Thibault Marin,Samira Vafay Eslahi,Tiss Amal,Yanis Chemli,Keith Johnson,Georges El Fakhri,Jinsong Ouyang*

Main category: eess.IV

TLDR: 论文提出了一种基于提示学习的PET去噪方法，通过双提示机制（显式计数级提示和隐式去噪提示）动态指导不同计数水平的去噪过程，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: PET图像去噪面临不同计数水平带来的挑战，需要一种统一模型处理多样化情况。

Method: 采用双提示机制（显式计数级提示和隐式去噪提示），结合提示融合模块和提示-特征交互模块，动态指导去噪过程。

Result: 在1940个低计数PET 3D体积上评估，双提示机制显著提升了性能，优于计数条件模型。

Conclusion: 双提示机制能够高效训练统一去噪模型，适用于不同计数水平的个性化去噪。

Abstract: The to-be-denoised positron emission tomography (PET) volumes are inherent
with diverse count levels, which imposes challenges for a unified model to
tackle varied cases. In this work, we resort to the recently flourished prompt
learning to achieve generalizable PET denoising with different count levels.
Specifically, we propose dual prompts to guide the PET denoising in a
divide-and-conquer manner, i.e., an explicitly count-level prompt to provide
the specific prior information and an implicitly general denoising prompt to
encode the essential PET denoising knowledge. Then, a novel prompt fusion
module is developed to unify the heterogeneous prompts, followed by a
prompt-feature interaction module to inject prompts into the features. The
prompts are able to dynamically guide the noise-conditioned denoising process.
Therefore, we are able to efficiently train a unified denoising model for
various count levels, and deploy it to different cases with personalized
prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly
selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.
It shows our dual prompting can largely improve the performance with informed
count-level and outperform the count-conditional model.

</details>

### [139] [STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis](https://arxiv.org/abs/2505.03123)
*Yiran Zhu,Wei Yang,Yan su,Zesheng Li,Chengchang Pan,Honggang Qi*

Main category: eess.IV

TLDR: 提出了一种多模态时空图神经网络框架（STG），用于预测结直肠癌肝转移（CRLM）进展，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有临床模型未能有效整合肿瘤的空间异质性、动态演化和多模态数据关系，预测准确性受限。

Method: 结合术前CT影像和临床数据构建异构图结构，通过GraphSAGE聚合时空邻域信息，并采用监督和对比学习策略优化模型。

Result: 在MSKCC CRLM数据集上，时间相邻准确率达85%，平均绝对误差为1.1005。

Conclusion: 该框架为个性化治疗决策提供了可靠的定量支持。

Abstract: We propose a multimodal spatiotemporal graph neural network (STG) framework
to predict colorectal cancer liver metastasis (CRLM) progression. Current
clinical models do not effectively integrate the tumor's spatial heterogeneity,
dynamic evolution, and complex multimodal data relationships, limiting their
predictive accuracy. Our STG framework combines preoperative CT imaging and
clinical data into a heterogeneous graph structure, enabling joint modeling of
tumor distribution and temporal evolution through spatial topology and
cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal
neighborhood information and leverages supervised and contrastive learning
strategies to enhance the model's ability to capture temporal features and
improve robustness. A lightweight version of the model reduces parameter count
by 78.55%, maintaining near-state-of-the-art performance. The model jointly
optimizes recurrence risk regression and survival analysis tasks, with
contrastive loss improving feature representational discriminability and
cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a
time-adjacent accuracy of 85% and a mean absolute error of 1.1005,
significantly outperforming existing methods. The innovative heterogeneous
graph construction and spatiotemporal decoupling mechanism effectively uncover
the associations between dynamic tumor microenvironment changes and prognosis,
providing reliable quantitative support for personalized treatment decisions.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [140] [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
*Valerio Bellandi*

Main category: cs.DC

TLDR: 本文比较了集中式和分布式系统的优缺点，并介绍了一个为意大利司法部开发的分布式文档存储系统，利用边缘存储库分析文本数据和元数据，提升语义探索能力。


<details>
  <summary>Details</summary>
Motivation: 探讨集中式与分布式系统的适用场景，并针对大规模环境需求开发分布式文档存储系统，以提升数据可用性和性能。

Method: 开发分布式文档存储系统，利用边缘存储库分析文本数据和元数据。

Result: 系统增强了语义探索能力，适用于大规模环境。

Conclusion: 分布式系统在大规模环境中表现更优，集中式系统适合有限扩展需求。

Abstract: Centralized and distributed systems are two main approaches to organizing ICT
infrastructure, each with its pros and cons. Centralized systems concentrate
resources in one location, making management easier but creating single points
of failure. Distributed systems, on the other hand, spread resources across
multiple nodes, offering better scalability and fault tolerance, but requiring
more complex management. The choice between them depends on factors like
application needs, scalability, and data sensitivity. Centralized systems suit
applications with limited scalability and centralized control, while
distributed systems excel in large-scale environments requiring high
availability and performance. This paper explores a distributed document
repository system developed for the Italian Ministry of Justice, using edge
repositories to analyze textual data and metadata, enhancing semantic
exploration capabilities.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
*Rintaro Ando*

Main category: cs.LG

TLDR: N2M-RSI是一个形式化模型，展示AI代理在反馈自身输出并超过信息整合阈值后，内部复杂性会无限增长。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理通过自我反馈和交互实现无限复杂性增长的潜力，统一自我提示、自引用和AutoML等概念。

Method: 提出N2M-RSI模型，假设AI代理反馈输出作为输入，并设定信息整合阈值。

Result: 模型显示AI代理内部复杂性会无限增长，且在多代理交互中可能产生超线性效应。

Conclusion: N2M-RSI为AI自我改进提供了理论框架，但出于安全考虑未公开具体实现细节。

Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal
formal model showing that once an AI agent feeds its own outputs back as inputs
and crosses an explicit information-integration threshold, its internal
complexity will grow without bound under our assumptions. The framework unifies
earlier ideas on self-prompting large language models, G\"odelian
self-reference, and AutoML, yet remains implementation-agnostic. The model
furthermore scales naturally to interacting swarms of agents, hinting at
super-linear effects once communication among instances is permitted. For
safety reasons, we omit system-specific implementation details and release only
a brief, model-agnostic toy prototype in Appendix C.

</details>

### [142] [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
*Sean I. Young*

Main category: cs.LG

TLDR: 提出一种基于率失真理论的量化技术，用于压缩大型语言模型（LLMs），支持用户指定压缩后的模型大小或精度。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在资源受限设备上部署、降低计算成本和减少环境影响的挑战。

Method: 从率失真理论角度建立LLM量化的基础，提出基于简单率失真优化的量化技术。

Result: 技术可扩展到包含数百亿权重参数的模型，支持用户灵活压缩模型。

Conclusion: 该量化技术为LLM压缩提供了高效且灵活的解决方案。

Abstract: In recent years, the compression of large language models (LLMs) has emerged
as a key problem in facilitating LLM deployment on resource-limited devices,
reducing compute costs, and mitigating the environmental footprint due to
large-scale AI infrastructure. Here, we establish the foundations of LLM
quantization from a rate-distortion theory perspective and propose a
quantization technique based on simple rate-distortion optimization. Our
technique scales to models containing hundreds of billions of weight parameters
and offers users the flexibility to compress models, post-training, to a model
size or accuracy specified by the user.

</details>

### [143] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
*Andrew Zhao,Yiran Wu,Yang Yue,Tong Wu,Quentin Xu,Yang Yue,Matthieu Lin,Shenzhi Wang,Qingyun Wu,Zilong Zheng,Gao Huang*

Main category: cs.LG

TLDR: 论文提出了一种名为Absolute Zero的新RLVR范式，通过自我生成任务和验证答案来提升推理能力，无需依赖外部数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有RLVR方法依赖人工标注数据的问题，并探讨在AI超越人类智能时如何持续学习。

Method: 引入Absolute Zero Reasoner (AZR)，利用代码执行器自我生成任务并验证答案，实现无外部数据的训练。

Result: AZR在编码和数学推理任务上表现优于依赖大量人工数据的现有模型。

Conclusion: AZR展示了无外部数据训练的可行性，并适用于不同规模和类型的模型。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>

### [144] [A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition](https://arxiv.org/abs/2505.02877)
*Hele Zhu,Xinyi Huang,Haojia Gao,Mengfei Jiang,Haohua Que,Lei Mu*

Main category: cs.LG

TLDR: 提出了一种基于边缘设备与云服务器协作的植物病害识别框架，通过深度强化学习修剪模型，并结合贪心策略优化分割点，显著提升推理速度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 传统植物病害识别方法效率低、成本高，深度学习模型在资源受限设备上运行困难，且云服务器通信带宽受限，影响推理效率。

Method: 使用深度强化学习修剪DNN模型，通过贪心策略确定最优分割点，实现边缘与云协作推理加速。

Result: 实验表明，该框架显著提升了推理速度，同时保持了可接受的识别准确率。

Conclusion: 该框架为植物病害快速诊断与预防提供了新解决方案。

Abstract: Plant disease is a critical factor affecting agricultural production.
Traditional manual recognition methods face significant drawbacks, including
low accuracy, high costs, and inefficiency. Deep learning techniques have
demonstrated significant benefits in identifying plant diseases, but they still
face challenges such as inference delays and high energy consumption. Deep
learning algorithms are difficult to run on resource-limited embedded devices.
Offloading these models to cloud servers is confronted with the restriction of
communication bandwidth, and all of these factors will influence the
inference's efficiency. We propose a collaborative inference framework for
recognizing plant diseases between edge devices and cloud servers to enhance
inference speed. The DNN model for plant disease recognition is pruned through
deep reinforcement learning to improve the inference speed and reduce energy
consumption. Then the optimal split point is determined by a greedy strategy to
achieve the best collaborated inference acceleration. Finally, the system for
collaborative inference acceleration in plant disease recognition has been
implemented using Gradio to facilitate friendly human-machine interaction.
Experiments indicate that the proposed collaborative inference framework
significantly increases inference speed while maintaining acceptable
recognition accuracy, offering a novel solution for rapidly diagnosing and
preventing plant diseases.

</details>

### [145] [ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders](https://arxiv.org/abs/2505.03646)
*Chethan Krishnamurthy Ramanaik,Arjun Roy,Eirini Ntoutsi*

Main category: cs.LG

TLDR: 论文提出了一种基于层条件化的对抗优化目标，用于增强深度自编码器的对抗鲁棒性评估，并提出了一种防御插件。


<details>
  <summary>Details</summary>
Motivation: 深度自编码器的对抗鲁棒性研究不足，现有白盒攻击方法未能充分利用中间层的脆弱性。

Method: 提出了一种新的层条件化对抗优化目标，通过增强损失梯度信息传播来优化攻击效果。

Result: 实验表明，该方法在通用和样本特定场景下均优于现有方法，并提出了有效的防御插件。

Conclusion: 该方法显著提升了对抗攻击效果，并为防御提供了新思路。

Abstract: Despite the extensive use of deep autoencoders (AEs) in critical
applications, their adversarial robustness remains relatively underexplored
compared to classification models. AE robustness is characterized by the
Lipschitz bounds of its components. Existing robustness evaluation frameworks
based on white-box attacks do not fully exploit the vulnerabilities of
intermediate ill-conditioned layers in AEs. In the context of optimizing
imperceptible norm-bounded additive perturbations to maximize output damage,
existing methods struggle to effectively propagate adversarial loss gradients
throughout the network, often converging to less effective perturbations. To
address this, we propose a novel layer-conditioning-based adversarial
optimization objective that effectively guides the adversarial map toward
regions of local Lipschitz bounds by enhancing loss gradient information
propagation during attack optimization. We demonstrate through extensive
experiments on state-of-the-art AEs that our adversarial objective results in
stronger attacks, outperforming existing methods in both universal and
sample-specific scenarios. As a defense method against this attack, we
introduce an inference-time adversarially trained defense plugin that mitigates
the effects of adversarial examples.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [146] [From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition](https://arxiv.org/abs/2505.03510)
*Ludovico Iannello,Luca Ciampi,Gabriele Lagani,Fabrizio Tonelli,Eleonora Crocco,Lucio Maria Calcagnile,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.NE

TLDR: 提出了一种基于培养生物神经元的新型储层计算（BRC）范式，利用多电极阵列记录神经活动，通过非线性映射实现高效模式识别。


<details>
  <summary>Details</summary>
Motivation: 探索生物神经网络替代传统人工神经网络的可能性，为生物启发计算系统开辟新途径。

Method: 使用培养神经元作为储层基质，通过多电极阵列输入和记录神经活动，结合线性分类器完成任务。

Result: 实验验证了BRC在多种模式识别任务中的可行性，如位置编码、方向条和数字识别。

Conclusion: BRC展示了生物神经网络在传统人工神经网络任务中的潜力，为神经形态工程和生物混合计算提供了新方向。

Abstract: In this paper, we introduce a novel paradigm for reservoir computing (RC)
that leverages a pool of cultured biological neurons as the reservoir
substrate, creating a biological reservoir computing (BRC). This system
operates similarly to an echo state network (ESN), with the key distinction
that the neural activity is generated by a network of cultured neurons, rather
than being modeled by traditional artificial computational units. The neuronal
activity is recorded using a multi-electrode array (MEA), which enables
high-throughput recording of neural signals. In our approach, inputs are
introduced into the network through a subset of the MEA electrodes, while the
remaining electrodes capture the resulting neural activity. This generates a
nonlinear mapping of the input data to a high-dimensional biological feature
space, where distinguishing between data becomes more efficient and
straightforward, allowing a simple linear classifier to perform pattern
recognition tasks effectively. To evaluate the performance of our proposed
system, we present an experimental study that includes various input patterns,
such as positional codes, bars with different orientations, and a digit
recognition task. The results demonstrate the feasibility of using biological
neural networks to perform tasks traditionally handled by artificial neural
networks, paving the way for further exploration of biologically-inspired
computing systems, with potential applications in neuromorphic engineering and
bio-hybrid computing.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [147] [Mitigating Image Captioning Hallucinations in Vision-Language Models](https://arxiv.org/abs/2505.03420)
*Fei Zhao,Chengcui Zhang,Runlin Zhang,Tianyang Wang,Xi Li*

Main category: cs.MM

TLDR: 提出了一种基于强化学习的测试时适应框架，通过仅更新语言模型中的少量参数，显著减少了视觉语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中的幻觉问题影响了其可靠性和实际应用，现有解决方案计算成本高且需要额外数据或模型。

Method: 使用强化学习框架，仅更新语言模型中层归一化的可学习参数（约0.003%），并通过CLIP评估模型提供双重奖励。

Result: 在LLaVA和InstructBLIP上分别减少了15.4%和17.3%的幻觉率，性能优于现有基线方法68.3%。

Conclusion: 该方法无需重新训练或额外模型，高效地减少了幻觉现象，提升了视觉语言模型的可靠性。

Abstract: Hallucinations in vision-language models (VLMs) hinder reliability and
real-world applicability, usually stemming from distribution shifts between
pretraining data and test samples. Existing solutions, such as retraining or
fine-tuning on additional data, demand significant computational resources and
labor-intensive data collection, while ensemble-based methods incur additional
costs by introducing auxiliary VLMs. To address these challenges, we propose a
novel test-time adaptation framework using reinforcement learning to mitigate
hallucinations during inference without retraining or any auxiliary VLMs. By
updating only the learnable parameters in the layer normalization of the
language model (approximately 0.003% of the model parameters), our method
reduces distribution shifts between test samples and pretraining samples. A
CLIP-based hallucination evaluation model is proposed to provide dual rewards
to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in
hallucination rates on LLaVA and InstructBLIP, respectively. Our approach
outperforms state-of-the-art baselines with a 68.3% improvement in
hallucination mitigation, demonstrating its effectiveness.

</details>
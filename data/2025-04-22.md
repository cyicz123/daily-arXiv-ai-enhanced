<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 154]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 18]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.RO](#cs.RO) [Total: 6]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
*ByteDance Seed,:,Yufeng Yuan,Yu Yue,Mingxuan Wang,Xiaochen Zuo,Jiaze Chen,Lin Yan,Wenyuan Xu,Chi Zhang,Xin Liu,Chengyi Wang,TianTian Fan,Lingjun Liu,Qiying Yu,Xiangpeng Wei,Zhiqi Lin,Ruofei Zhu,Qingping Yang,Chengzhi Wei,Jerry He,Guanlin Liu,Zheng Wu,Xiangyu Yu,Zhicheng Liu,Jingjing Xu,Jiangjie Chen,Haojie Pan,Shengding Hu,Zhengyin Du,Wenqi Wang,Zewei Sun,Chenwei Lou,Bole Ma,Zihan Wang,Mofan Zhang,Wang Zhang,Gaohong Liu,Kaihua Jiang,Haibin Lin,Ru Zhang,Juncai Liu,Li Han,Jinxin Chi,Wenqiang Zhang,Jiayi Xu,Jun Yuan,Zhen Xiao,Yuqiao Xian,Jingqiao Wu,Kai Hua,Na Zhou,Jianhui Duan,Heyang Lu,Changbao Wang,Jinxiang Ou,Shihang Wang,Xiaoran Jin,Xuesong Yao,Chengyin Xu,Wenchang Ma,Zhecheng An,Renming Pang,Xia Xiao,Jing Su,Yuyu Zhang,Tao Sun,Kaibo Liu,Yifan Sun,Kai Shen,Sijun Zhang,Yiyuan Ma,Xingyan Bin,Ji Li,Yao Luo,Deyi Liu,Shiyi Zhan,Yunshui Li,Yuan Yang,Defa Zhu,Ke Shen,Chenggang Li,Xun Zhou,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TLDR: Seed-Thinking-v1.5是一种基于思考后响应的推理模型，在多个基准测试中表现优异，尤其在STEM和编程领域。


<details>
  <summary>Details</summary>
Motivation: 提升模型在广泛任务中的推理能力，并验证其在非推理任务中的泛化性能。

Method: 采用混合专家（MoE）架构，激活参数20B，总参数200B。

Result: 在AIME 2024、Codeforces和GPQA等基准测试中表现突出，非推理任务中胜率提升8%。

Conclusion: Seed-Thinking-v1.5展示了强大的推理和泛化能力，未来将公开内部基准支持研究。

Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before
responding, resulting in improved performance on a wide range of benchmarks.
Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on
GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond
reasoning tasks, the method demonstrates notable generalization across diverse
domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on
non-reasoning tasks, indicating its broader applicability. Compared to other
state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts
(MoE) model with a relatively small size, featuring 20B activated and 200B
total parameters. As part of our effort to assess generalized reasoning, we
develop two internal benchmarks, BeyondAIME and Codeforces, both of which will
be publicly released to support future research.

</details>

### [2] [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
*Djamila Mohdeb,Meriem Laifa,Zineb Guemraoui,Dalila Behih*

Main category: cs.CL

TLDR: 研究通过计算分析阿拉伯数字空间中的阴谋论传播，结合命名实体识别和主题建模技术（Top2Vec算法），识别并分类了六种阴谋论叙事类型。


<details>
  <summary>Details</summary>
Motivation: 填补阴谋论研究中阿拉伯语内容或在线数据的空白，揭示阿拉伯社交媒体中阴谋论的嵌入与演变。

Method: 使用命名实体识别和Top2Vec算法分析阿拉伯博客和Facebook数据。

Result: 识别出六类阴谋论叙事：性别/女权、地缘政治、政府掩盖、末日论、犹太共济会和地球工程。

Conclusion: 研究揭示了阿拉伯数字空间中阴谋论的多样性与背景依赖性，为理解其在阿拉伯世界公共话语中的作用提供了新视角。

Abstract: This study investigates the spread of conspiracy theories in Arabic digital
spaces through computational analysis of online content. By combining Named
Entity Recognition and Topic Modeling techniques, specifically the Top2Vec
algorithm, we analyze data from Arabic blogs and Facebook to identify and
classify conspiratorial narratives. Our analysis uncovers six distinct
categories: gender/feminist, geopolitical, government cover-ups, apocalyptic,
Judeo-Masonic, and geoengineering. The research highlights how these narratives
are deeply embedded in Arabic social media discourse, shaped by regional
historical, cultural, and sociopolitical contexts. By applying advanced Natural
Language Processing methods to Arabic content, this study addresses a gap in
conspiracy theory research, which has traditionally focused on English-language
content or offline data. The findings provide new insights into the
manifestation and evolution of conspiracy theories in Arabic digital spaces,
enhancing our understanding of their role in shaping public discourse in the
Arab world.

</details>

### [3] [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
*Jaime Raldua Veuthey,Zainab Ali Majid,Suhas Hariharan,Jacob Haimes*

Main category: cs.CL

TLDR: MEQA是一个用于评估问答基准质量的元评估框架，旨在提供标准化评估和量化分数，特别针对网络安全领域的基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，其社会影响日益显著，因此需要严格的评估。然而，现有评估基准的质量缺乏有效评估方法。

Method: 提出MEQA框架，通过人类和LLM评估者对网络安全基准进行元评估，量化基准的优缺点。

Result: MEQA能够标准化评估问答基准，并量化其质量，为基准比较提供依据。

Conclusion: MEQA填补了问答基准元评估的空白，特别适用于网络安全领域，为LLM评估提供了更可靠的工具。

Abstract: As Large Language Models (LLMs) advance, their potential for widespread
societal impact grows simultaneously. Hence, rigorous LLM evaluations are both
a technical necessity and social imperative. While numerous evaluation
benchmarks have been developed, there remains a critical gap in
meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a
framework for the meta-evaluation of question and answer (QA) benchmarks, to
provide standardized assessments, quantifiable scores, and enable meaningful
intra-benchmark comparisons. We demonstrate this approach on cybersecurity
benchmarks, using human and LLM evaluators, highlighting the benchmarks'
strengths and weaknesses. We motivate our choice of test domain by AI models'
dual nature as powerful defensive tools and security threats.

</details>

### [4] [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
*Laerdon Kim*

Main category: cs.CL

TLDR: 本文提出了一种基于少样本学习的基线方法，用于分类Reddit心理健康数据中的自我状态，采用4位量化Gemma 2 9B模型和数据预处理步骤，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决CLPsych 2025 A.1任务中分类自我状态的问题，尤其是在心理健康数据中区分适应性和非适应性自我状态。

Method: 使用少样本学习结合4位量化Gemma 2 9B模型，先通过数据预处理识别相关句子作为证据，再进行二元分类。

Result: 系统在14个提交方法中排名第三，测试召回率为0.579。

Conclusion: 句子分块步骤简化了任务并提升了模型性能，匹配了人工标注的粒度。

Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states
in mental health data taken from Reddit. We use few-shot learning with a 4-bit
quantized Gemma 2 9B model and a data preprocessing step which first identifies
relevant sentences indicating self-state evidence, and then performs a binary
classification to determine whether the sentence is evidence of an adaptive or
maladaptive self-state. This system outperforms our other method which relies
on an LLM to highlight spans of variable length independently. We attribute the
performance of our model to the benefits of this sentence chunking step for two
reasons: partitioning posts into sentences 1) broadly matches the granularity
at which self-states were human-annotated and 2) simplifies the task for our
language model to a binary classification problem. Our system places third out
of fourteen systems submitted for Task A.1, achieving a test-time recall of
0.579.

</details>

### [5] [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
*Kang He,Kaushik Roy*

Main category: cs.CL

TLDR: LogicTree是一个推理时模块化框架，通过算法引导搜索解决LLMs在复杂逻辑推理中的挑战，显著提升了证明准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂逻辑推理中面临系统性探索和逻辑一致性的挑战，以及大前提空间中的组合搜索问题。

Method: 提出LogicTree框架，结合缓存机制和线性化前提选择，优化推理过程，并引入两种无LLM的启发式方法进行前提优先级排序。

Result: 在五个数据集上，LogicTree平均比CoT和ToT分别提高了23.6%和12.5%的证明准确率，GPT-4o在LogicTree中表现优于o3-mini。

Conclusion: LogicTree通过结构化证明探索和优化前提搜索，显著提升了LLMs在复杂逻辑推理中的性能。

Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning
capabilities across various domains. However, LLMs still face distinct
challenges in complex logical reasoning, as (1) proof-finding requires
systematic exploration and the maintenance of logical coherence and (2)
searching the right combination of premises at each reasoning step is
inherently challenging in tasks with large premise space. To address this, we
propose LogicTree, an inference-time modular framework employing
algorithm-guided search to automate structured proof exploration and ensure
logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate
caching mechanism into LogicTree to enable effective utilization of historical
knowledge, preventing reasoning stagnation and minimizing redundancy.
Furthermore, we address the combinatorial complexity of premise search by
decomposing it into a linear process. The refined premise selection restricts
subsequent inference to at most one derivation per step, enhancing reasoning
granularity and enforcing strict step-by-step reasoning. Additionally, we
introduce two LLM-free heuristics for premise prioritization, enabling
strategic proof search. Experimental results on five datasets demonstrate that
LogicTree optimally scales inference-time computation to achieve higher proof
accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%
and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o
outperforms o3-mini by 7.6% on average.

</details>

### [6] [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
*Nusrat Jahan Prottasha,Upama Roy Chowdhury,Shetu Mohanto,Tasfia Nuzhat,Abdullah As Sami,Md Shamol Ali,Md Shohanur Islam Sobuj,Hafijur Raman,Md Kowsher,Ozlem Ozmen Garibay*

Main category: cs.CL

TLDR: 本文综述了参数高效微调（PEFT）技术，分析了其动机、设计原则和效果，并提出了分类框架。


<details>
  <summary>Details</summary>
Motivation: 传统微调大型模型（如LLMs和VLMs）资源消耗大，存在过拟合、灾难性遗忘等问题，PEFT通过仅更新少量参数提供高效解决方案。

Method: 提出PEFT的分类框架（加法、选择性、重参数化、混合和统一方法），并系统比较其机制与权衡。

Result: PEFT在语言、视觉和生成建模等领域表现优异，资源成本更低。

Conclusion: PEFT为大型模型的实用化提供了高效、可持续的途径，未来研究方向包括可扩展性、可解释性和联邦学习等。

Abstract: Large models such as Large Language Models (LLMs) and Vision Language Models
(VLMs) have transformed artificial intelligence, powering applications in
natural language processing, computer vision, and multimodal learning. However,
fully fine-tuning these models remains expensive, requiring extensive
computational resources, memory, and task-specific data. Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting
large models to downstream tasks by updating only a small portion of
parameters. This survey presents a comprehensive overview of PEFT techniques,
focusing on their motivations, design principles, and effectiveness. We begin
by analyzing the resource and accessibility challenges posed by traditional
fine-tuning and highlight key issues, such as overfitting, catastrophic
forgetting, and parameter inefficiency. We then introduce a structured taxonomy
of PEFT methods -- grouped into additive, selective, reparameterized, hybrid,
and unified frameworks -- and systematically compare their mechanisms and
trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse
domains, including language, vision, and generative modeling, showing how these
techniques offer strong performance with lower resource costs. We also discuss
important open challenges in scalability, interpretability, and robustness, and
suggest future directions such as federated learning, domain adaptation, and
theoretical grounding. Our goal is to provide a unified understanding of PEFT
and its growing role in enabling practical, efficient, and sustainable use of
large models.

</details>

### [7] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
*Katie Matton,Robert Osazuwa Ness,John Guttag,Emre Kıcıman*

Main category: cs.CL

TLDR: 该论文提出了一种衡量大语言模型（LLM）解释忠实性的新方法，通过定义忠实性并利用辅助LLM生成反事实输入，结合贝叶斯分层模型量化概念的影响。实验揭示了LLM解释中隐藏的社会偏见和误导性证据影响。


<details>
  <summary>Details</summary>
Motivation: LLM生成的解释可能不忠实，导致过度信任和误用，因此需要一种方法来量化其解释的忠实性。

Method: 定义忠实性为解释中隐含的概念与实际影响概念之间的差异；使用辅助LLM生成反事实输入，结合贝叶斯分层模型量化概念的影响。

Result: 实验表明该方法能有效量化不忠实性，并发现LLM解释中隐藏的社会偏见和误导性证据影响。

Conclusion: 提出的方法为评估LLM解释的忠实性提供了新工具，揭示了其潜在问题，有助于减少误用。

Abstract: Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.

</details>

### [8] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
*Zhiyuan Wang,Qingni Wang,Yue Zhang,Tianlong Chen,Xiaofeng Zhu,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TLDR: 提出了一种名为SConU的新方法，通过显著性测试管理不确定性数据异常值，优化预测效率并控制误覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法识别违反交换性假设的不确定性数据异常值，导致误覆盖率无边界和预测集不可操作。

Method: 开发两种conformal p值，通过显著性测试确定样本是否偏离校准集的不确定性分布。

Result: SConU方法在单领域和跨学科背景下严格管理误覆盖率，并提升预测效率。

Conclusion: SConU方法为高风险问答任务提供了近似条件覆盖的解决方案。

Abstract: As large language models are increasingly utilized in real-world
applications, guarantees of task-specific metrics are essential for their
reliable deployment. Previous studies have introduced various criteria of
conformal uncertainty grounded in split conformal prediction, which offer
user-specified correctness coverage. However, existing frameworks often fail to
identify uncertainty data outliers that violate the exchangeability assumption,
leading to unbounded miscoverage rates and unactionable prediction sets. In
this paper, we propose a novel approach termed Selective Conformal Uncertainty
(SConU), which, for the first time, implements significance tests, by
developing two conformal p-values that are instrumental in determining whether
a given sample deviates from the uncertainty distribution of the calibration
set at a specific manageable risk level. Our approach not only facilitates
rigorous management of miscoverage rates across both single-domain and
interdisciplinary contexts, but also enhances the efficiency of predictions.
Furthermore, we comprehensively analyze the components of the conformal
procedures, aiming to approximate conditional coverage, particularly in
high-stakes question-answering tasks.

</details>

### [9] [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
*Ziyan Zhang,Yang Hou,Chen Gong,Zhenghua Li*

Main category: cs.CL

TLDR: 论文提出了一种自校正方法，利用现有树库的语法规则指导大语言模型（LLMs）修正解析错误，显著提升了其在句法解析任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种自然语言处理任务中表现出色，但在句法解析等基础任务中仍存在不足，尤其是无法充分利用现有树库的语法规则生成有效的句法结构。

Method: 提出了一种自校正方法，通过自动检测潜在错误并动态搜索相关语法规则，为LLMs提供提示和示例，指导其自行修正错误。

Result: 在三个数据集上的实验表明，该方法显著提升了LLMs在英语和中文数据集上的表现，包括领域内和跨领域设置。

Conclusion: 该方法无需额外训练即可帮助LLMs提升句法解析能力，为LLMs在基础NLP任务中的应用提供了新思路。

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing (NLP) tasks. However, recent studies suggest that
they still face challenges in performing fundamental NLP tasks essential for
deep language understanding, particularly syntactic parsing. In this paper, we
conduct an in-depth analysis of LLM parsing capabilities, delving into the
specific shortcomings of their parsing results. We find that LLMs may stem from
limitations to fully leverage grammar rules in existing treebanks, which
restricts their capability to generate valid syntactic structures. To help LLMs
acquire knowledge without additional training, we propose a self-correction
method that leverages grammar rules from existing treebanks to guide LLMs in
correcting previous errors. Specifically, we automatically detect potential
errors and dynamically search for relevant rules, offering hints and examples
to guide LLMs in making corrections themselves. Experimental results on three
datasets with various LLMs, demonstrate that our method significantly improves
performance in both in-domain and cross-domain settings on the English and
Chinese datasets.

</details>

### [10] [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TLDR: 论文质疑基于大语言模型（LLM）的查询扩展方法在零样本检索任务中的性能提升是否源于基准测试中的知识泄漏，而非模型的实际能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证LLM生成的假设文档是否因包含基准测试中的真实证据信息而提升检索性能，而非模型本身的泛化能力。

Method: 以事实验证为测试平台，分析生成的文档是否包含真实证据的隐含信息，并评估其对性能的影响。

Result: 发现性能提升仅出现在生成的文档包含真实证据隐含信息的查询中，表明基准测试可能存在知识泄漏。

Conclusion: 结论指出知识泄漏可能夸大了LLM查询扩展方法的性能，尤其是在需要检索小众或新知识的实际场景中。

Abstract: Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyzed whether the generated documents contained information
entailed by ground truth evidence and assessed their impact on performance. Our
findings indicate that performance improvements occurred consistently only for
claims whose generated documents included sentences entailed by ground truth
evidence. This suggests that knowledge leakage may be present in these
benchmarks, inflating the perceived performance of LLM-based query expansion
methods, particularly in real-world scenarios that require retrieving niche or
novel knowledge.

</details>

### [11] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
*Xinlin Zhuang,Jiahui Peng,Ren Ma,Yinfan Wang,Tianyi Bai,Xingjian Wei,Jiantao Qiu,Chi Zhang,Ying Qian,Conghui He*

Main category: cs.CL

TLDR: 论文提出PRRC和Meta-rater方法，通过多维度数据质量评估和优化权重选择，显著提升大语言模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型预训练数据集的构成不透明，且现有数据选择方法多为单维度评估或冗余策略，限制了数据质量和模型性能的优化。

Method: 提出PRRC（专业性、可读性、推理性和清洁度）评估框架，并开发Meta-rater方法，通过代理模型训练回归模型预测验证损失，优化多维度质量评分组合。

Result: 实验表明，Meta-rater使1.3B参数模型的收敛速度翻倍，下游任务性能提升3.23，且在3.3B模型上表现出可扩展性。

Conclusion: 多维度质量集成方法显著优于传统单维度方法，为提升预训练效率和模型能力提供了可扩展的范式。

Abstract: The composition of pre-training datasets for large language models (LLMs)
remains largely undisclosed, hindering transparency and efforts to optimize
data quality, a critical driver of model performance. Current data selection
methods, such as natural language quality assessments, diversity-based filters,
and classifier-based approaches, are limited by single-dimensional evaluation
or redundancy-focused strategies. To address these gaps, we propose PRRC to
evaluate data quality across Professionalism, Readability, Reasoning, and
Cleanliness. We further introduce Meta-rater, a multi-dimensional data
selection method that integrates these dimensions with existing quality metrics
through learned optimal weightings. Meta-rater employs proxy models to train a
regression model that predicts validation loss, enabling the identification of
optimal combinations of quality scores. Experiments demonstrate that Meta-rater
doubles convergence speed for 1.3B parameter models and improves downstream
task performance by 3.23, with scalable benefits observed in 3.3B models
trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B
dataset, labeled across 25 quality metrics (including PRRC), to advance
research in data-centric LLM development. Our work establishes that holistic,
multi-dimensional quality integration significantly outperforms conventional
single-dimension approaches, offering a scalable paradigm for enhancing
pre-training efficiency and model capability.

</details>

### [12] [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
*Jian Zhang,Tianqing Zhang,Qi Li,Hongwei Wang*

Main category: cs.CL

TLDR: 本文提出了一种新的损失函数EIoU-EMC，用于解决特定领域（如生物医学和工业）中嵌套命名实体识别（NER）任务面临的低资源和类别不平衡问题。该方法通过增强IoU损失和多类损失，显著提升了模型在有限数据下的学习能力。实验结果表明，该方法在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 特定领域（如生物医学和工业）中的嵌套NER任务面临低资源和类别不平衡的挑战，限制了其广泛应用。

Method: 设计了新的损失函数EIoU-EMC，结合了IoU损失和多类损失的优点，特别关注实体边界和实体分类信息。

Result: 在三个生物医学NER数据集和一个工业数据集上，该方法表现优于基线模型，尤其在实体边界识别和分类方面有显著提升。

Conclusion: EIoU-EMC方法有效解决了低资源和类别不平衡问题，为特定领域的嵌套NER任务提供了新的解决方案。

Abstract: In recent years, research has mainly focused on the general NER task. There
still have some challenges with nested NER task in the specific domains.
Specifically, the scenarios of low resource and class imbalance impede the wide
application for biomedical and industrial domains. In this study, we design a
novel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss
and Multiclass loss. Our proposed method specially leverages the information of
entity boundary and entity classification, thereby enhancing the model's
capacity to learn from a limited number of data samples. To validate the
performance of this innovative method in enhancing NER task, we conducted
experiments on three distinct biomedical NER datasets and one dataset
constructed by ourselves from industrial complex equipment maintenance
documents. Comparing to strong baselines, our method demonstrates the
competitive performance across all datasets. During the experimental analysis,
our proposed method exhibits significant advancements in entity boundary
recognition and entity classification. Our code are available here.

</details>

### [13] [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
*Takuma Udagawa,Yang Zhao,Hiroshi Kanayama,Bishwaranjan Bhattacharjee*

Main category: cs.CL

TLDR: 提出了一种高效的标注流程，用于分析预训练语料库中的社会偏见，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 预训练数据中的社会偏见可能被大型语言模型放大，需要有效的方法来识别和缓解这些偏见。

Method: 采用保护属性检测和尊重分类的标注流程，分析语料库中的语言极性。

Result: 实验证明了该偏见分析和缓解措施的有效性。

Conclusion: 提出的方法能够高效识别和缓解预训练语料库中的社会偏见。

Abstract: Large language models (LLMs) acquire general linguistic knowledge from
massive-scale pretraining. However, pretraining data mainly comprised of
web-crawled texts contain undesirable social biases which can be perpetuated or
even amplified by LLMs. In this study, we propose an efficient yet effective
annotation pipeline to investigate social biases in the pretraining corpora.
Our pipeline consists of protected attribute detection to identify diverse
demographics, followed by regard classification to analyze the language
polarity towards each attribute. Through our experiments, we demonstrate the
effect of our bias analysis and mitigation measures, focusing on Common Crawl
as the most representative pretraining corpus.

</details>

### [14] [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
*Junchi Yao,Shu Yang,Jianhua Xu,Lijie Hu,Mengdi Li,Di Wang*

Main category: cs.CL

TLDR: 论文提出了一种名为“Duplicatus Charm”的方法，通过机制可解释性研究大语言模型（LLMs）中重复文本生成的根源，并利用稀疏自编码器（SAEs）提取和操作重复特征，有效缓解了“重复诅咒”。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在多领域取得了显著进展，但其常出现重复文本生成的问题（即“重复诅咒”），而现有研究对其底层机制探索不足。

Method: 通过机制可解释性分析，结合稀疏自编码器（SAEs）提取“重复特征”，并设计实验验证这些特征对重复生成的影响。

Result: 成功识别并操作了导致重复的关键模型激活（“重复特征”），并通过去激活这些特征有效缓解了重复问题。

Conclusion: 研究揭示了LLMs中重复生成的机制，并提出了一种可解释的方法来缓解这一问题。

Abstract: Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse.

</details>

### [15] [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
*Michael Färber,Parisa Aghdam,Kyuri Im,Mario Tawfelis,Hardik Ghoshal*

Main category: cs.CL

TLDR: 本文介绍了首个利用GPT-4和Llama-3的多格式输入文本简化系统，支持定制化输出，以提升包容性。


<details>
  <summary>Details</summary>
Motivation: 复杂文本对多样化受众的理解构成挑战，现有简化方法未能充分利用大语言模型（LLMs）进行定制化。

Method: 开发了支持多格式输入（如文本输入和文件上传）的系统，结合GPT-4和Llama-3，并通过多指标评估输出。

Result: 系统能够生成符合不同受众需求的简化文本，验证了大语言模型在文本简化中的潜力。

Conclusion: 研究推动了自动文本简化领域的发展，强调了定制化沟通在促进包容性中的重要性。

Abstract: Text simplification is essential for making complex content accessible to
diverse audiences who face comprehension challenges. Yet, the limited
availability of simplified materials creates significant barriers to personal
and professional growth and hinders social inclusion. Although researchers have
explored various methods for automatic text simplification, none fully leverage
large language models (LLMs) to offer tailored customization for different
target groups and varying levels of simplicity. Moreover, despite its proven
benefits for both consumers and organizations, the well-established practice of
plain language remains underutilized. In this paper, we
https://simplifymytext.org, the first system designed to produce plain language
content from multiple input formats, including typed text and file uploads,
with flexible customization options for diverse audiences. We employ GPT-4 and
Llama-3 and evaluate outputs across multiple metrics. Overall, our work
contributes to research on automatic text simplification and highlights the
importance of tailored communication in promoting inclusivity.

</details>

### [16] [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
*Bowen Jiang,Zhuoqun Hao,Young-Min Cho,Bryan Li,Yuan Yuan,Sihao Chen,Lyle Ungar,Camillo J. Taylor,Dan Roth*

Main category: cs.CL

TLDR: PERSONAMEM基准测试评估LLMs如何利用用户交互历史来个性化响应，发现当前模型在动态跟踪用户偏好方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能有效利用用户交互历史来内部化用户特质和偏好，并动态跟踪其演变。

Method: 引入PERSONAMEM基准，包含180个模拟用户-LLM交互历史，评估LLMs在个性化任务中的表现。

Result: 当前LLMs（如GPT-4.1等）在动态跟踪用户偏好方面表现有限，整体准确率仅约50%。

Conclusion: PERSONAMEM为开发用户感知型聊天机器人提供了研究基础，未来需改进模型能力。

Abstract: Large Language Models (LLMs) have emerged as personalized assistants for
users across a wide range of tasks -- from offering writing support to
delivering tailored recommendations or consultations. Over time, the
interaction history between a user and an LLM can provide extensive information
about an individual's traits and preferences. However, open questions remain on
how well LLMs today can effectively leverage such history to (1) internalize
the user's inherent traits and preferences, (2) track how the user profiling
and preferences evolve over time, and (3) generate personalized responses
accordingly in new scenarios.
  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features
curated user profiles with over 180 simulated user-LLM interaction histories,
each containing up to 60 sessions of multi-turn conversations across 15
real-world tasks that require personalization. Given an in-situ user query,
i.e. query issued by the user from the first-person perspective, we evaluate
LLM chatbots' ability to identify the most suitable response according to the
current state of the user's profile. We observe that current LLMs still
struggle to recognize the dynamic evolution in users' profiles over time
through direct prompting approaches. As a consequence, LLMs often fail to
deliver responses that align with users' current situations and preferences,
with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0
achieving only around 50% overall accuracy, suggesting room for improvement. We
hope that PERSONAMEM, along with the user profile and conversation simulation
pipeline, can facilitate future research in the development of truly user-aware
chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.

</details>

### [17] [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
*Demetris Paschalides,George Pallis,Marios D. Dikaiakos*

Main category: cs.CL

TLDR: 该论文研究了大型语言模型（LLMs）在政治意识形态光谱上的可操纵性，超越了传统的左右二分法，并通过多任务数据集和微调实验揭示了模型对意识形态的敏感性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在政治意识形态上的可操纵性，超越传统的左右二分法，以更全面地评估其风险。

Method: 引入多任务数据集（包括意识形态问答、声明排序等），并对Phi-2、Mistral和Llama-3三种LLMs进行微调。

Result: 微调显著提升了模型对意识形态的细致对齐能力，而显式提示仅能提供微小改进。

Conclusion: LLMs对意识形态操纵高度敏感，需开发更强健的防护措施。

Abstract: Large Language Models (LLMs) have transformed natural language processing,
but concerns have emerged about their susceptibility to ideological
manipulation, particularly in politically sensitive areas. Prior work has
focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning
on political QA datasets. In this work, we move beyond this binary approach to
explore the extent to which LLMs can be influenced across a spectrum of
political ideologies, from Progressive-Left to Conservative-Right. We introduce
a novel multi-task dataset designed to reflect diverse ideological positions
through tasks such as ideological QA, statement ranking, manifesto cloze
completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,
Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and
express these nuanced ideologies. Our findings indicate that fine-tuning
significantly enhances nuanced ideological alignment, while explicit prompts
provide only minor refinements. This highlights the models' susceptibility to
subtle ideological manipulation, suggesting a need for more robust safeguards
to mitigate these risks.

</details>

### [18] [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
*Xingyu Li,Chen Gong,Guohong Fu*

Main category: cs.CL

TLDR: 该论文介绍了TikTalkCoref，首个中文社交媒体多模态共指消解数据集，填补了真实世界对话中多模态共指消解研究的空白，并提供了基准方法。


<details>
  <summary>Details</summary>
Motivation: 多模态共指消解（MCR）对理解多模态内容至关重要，但在真实世界对话中缺乏数据资源。

Method: 构建了TikTalkCoref数据集，包含抖音短视频及其用户评论的文本对话，并标注了文本中的人物提及和视频帧中对应的人物头部区域。

Result: 提出了针对名人领域的MCR基准方法，并在新数据集上进行了实验，提供了可靠的基准结果。

Conclusion: TikTalkCoref数据集的发布将促进真实世界社交媒体对话的多模态共指消解研究。

Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring
to the same entity across different modalities, such as text and visuals, and
is essential for understanding multimodal content. In the era of rapidly
growing mutimodal content and social media, MCR is particularly crucial for
interpreting user interactions and bridging text-visual references to improve
communication and personalization. However, MCR research for real-world
dialogues remains unexplored due to the lack of sufficient data resources.To
address this gap, we introduce TikTalkCoref, the first Chinese multimodal
coreference dataset for social media in real-world scenarios, derived from the
popular Douyin short-video platform. This dataset pairs short videos with
corresponding textual dialogues from user comments and includes manually
annotated coreference clusters for both person mentions in the text and the
coreferential person head regions in the corresponding video frames. We also
present an effective benchmark approach for MCR, focusing on the celebrity
domain, and conduct extensive experiments on our dataset, providing reliable
benchmark results for this newly constructed dataset. We will release the
TikTalkCoref dataset to facilitate future research on MCR for real-world social
media dialogues.

</details>

### [19] [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TLDR: 该论文研究了从Transformer教师模型到九种子二次复杂度学生模型的知识蒸馏效果，探讨了不同架构对蒸馏过程的影响以及初始化策略的作用。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度在推理时成为瓶颈，因此探索子二次复杂度的替代模型（如SSMs、线性注意力和循环架构）以提升效率。

Method: 系统评估了从Transformer教师模型到九种子二次复杂度学生模型的知识蒸馏效果，并研究了矩阵混合和QKV复制等初始化策略的影响。

Result: 在多个NLP基准测试中，实证结果揭示了效率与性能之间的权衡，并指出了成功知识转移的关键因素。

Conclusion: 研究为子二次复杂度架构的知识蒸馏提供了实用指导，帮助在效率和性能之间找到平衡。

Abstract: Knowledge distillation is a widely used technique for compressing large
language models (LLMs) by training a smaller student model to mimic a larger
teacher model. Typically, both the teacher and student are Transformer-based
architectures, leveraging softmax attention for sequence modeling. However, the
quadratic complexity of self-attention at inference time remains a significant
bottleneck, motivating the exploration of subquadratic alternatives such as
structured state-space models (SSMs), linear attention, and recurrent
architectures. In this work, we systematically evaluate the transferability of
knowledge distillation from a Transformer teacher to nine subquadratic student
architectures. Our study aims to determine which subquadratic model best aligns
with the teacher's learned representations and how different architectural
constraints influence the distillation process. We also investigate the impact
of intelligent initialization strategies, including matrix mixing and
query-key-value (QKV) copying, on the adaptation process. Our empirical results
on multiple NLP benchmarks provide insights into the trade-offs between
efficiency and performance, highlighting key factors for successful knowledge
transfer to subquadratic architectures.

</details>

### [20] [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
*Gabriel Machado Santos,Rita Maria da Silva Julia,Marcelo Zanchetta do Nascimento*

Main category: cs.CL

TLDR: 本文提出了一种结合上下文无关语法（CFG）和MAP-Elites算法的进化方法，系统探索提示空间，优化大型语言模型（LLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 提示工程对优化LLMs至关重要，但提示结构与任务性能之间的关系尚未充分研究。

Method: 采用CFG和MAP-Elites算法，生成高质量且多样化的提示，并分析其与不同任务的匹配性。

Result: 在多个LLMs和七项BigBench Lite任务上的评估表明，该方法显著提升了提示的质量和多样性。

Conclusion: 通过系统映射表型空间，揭示了结构变化对LLM性能的影响，为任务特定和适应性提示设计提供了实用指导。

Abstract: Prompt engineering is essential for optimizing large language models (LLMs),
yet the link between prompt structures and task performance remains
underexplored. This work introduces an evolutionary approach that combines
context-free grammar (CFG) with the MAP-Elites algorithm to systematically
explore the prompt space. Our method prioritizes quality and diversity,
generating high-performing and structurally varied prompts while analyzing
their alignment with diverse tasks by varying traits such as the number of
examples (shots) and reasoning depth. By systematically mapping the phenotypic
space, we reveal how structural variations influence LLM performance, offering
actionable insights for task-specific and adaptable prompt design. Evaluated on
seven BigBench Lite tasks across multiple LLMs, our results underscore the
critical interplay of quality and diversity, advancing the effectiveness and
versatility of LLMs.

</details>

### [21] [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
*Tong Chen,Faeze Brahman,Jiacheng Liu,Niloofar Mireshghallah,Weijia Shi,Pang Wei Koh,Luke Zettlemoyer,Hannaneh Hajishirzi*

Main category: cs.CL

TLDR: ParaPO是一种后训练方法，通过微调语言模型减少无意识复述，同时保持其整体实用性。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能无意识地复述预训练数据，引发版权、抄袭、隐私和创造力问题。

Method: ParaPO训练模型偏好对记忆内容的改写版本，而非原文；通过系统提示控制复述行为。

Result: 在Llama3.1-8B和Tulu3-8B上，ParaPO显著减少复述，同时保留名言引用能力。

Conclusion: ParaPO有效减少无意识复述，优于现有方法，且能通过提示控制复述行为。

Abstract: Language models (LMs) can memorize and reproduce segments from their
pretraining data verbatim even in non-adversarial settings, raising concerns
about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase
Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to
reduce unintentional regurgitation while preserving their overall utility.
ParaPO trains LMs to prefer paraphrased versions of memorized segments over the
original verbatim content from the pretraining data. To maintain the ability to
recall famous quotations when appropriate, we develop a variant of ParaPO that
uses system prompts to control regurgitation behavior. In our evaluation on
Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested
datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative
writing), whereas unlearning methods used in prior work to mitigate
regurgitation are less effective outside their targeted unlearned domain (from
17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO
with system prompting successfully preserves famous quotation recall while
reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when
prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the
model not to regurgitate produces only a marginal reduction (8.7 to 8.4).

</details>

### [22] [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
*Armin Toroghi,Willis Guo,Scott Sanner*

Main category: cs.CL

TLDR: 论文提出了一个名为CoLoTa的新数据集，用于评估大型语言模型（LLMs）在处理长尾实体时的常识推理能力及其幻觉问题，同时也作为知识图谱问答（KGQA）方法的评估基准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在编码事实和常识知识方面表现出色，但在处理长尾实体的常识推理任务时仍存在高错误率和幻觉问题，限制了其在高风险场景中的应用。

Method: 研究团队构建了包含3,300个查询的CoLoTa数据集，涵盖问答和声明验证任务，并基于Wikidata知识图谱支持查询解答。

Result: 实验表明，现有的基于LLM的KGQA方法在处理需要常识推理的查询时表现严重不足。

Conclusion: CoLoTa可作为评估LLMs和KGQA方法在常识推理及长尾实体处理能力上的新基准。

Abstract: The rise of Large Language Models (LLMs) has redefined the AI landscape,
particularly due to their ability to encode factual and commonsense knowledge,
and their outstanding performance in tasks requiring reasoning. Despite these
advances, hallucinations and reasoning errors remain a significant barrier to
their deployment in high-stakes settings. In this work, we observe that even
the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning
errors and hallucinations on tasks requiring commonsense reasoning over
obscure, long-tail entities. To investigate this limitation, we present a new
dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that
consists of 3,300 queries from question answering and claim verification tasks
and covers a diverse range of commonsense reasoning skills. We remark that
CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset
since the support of knowledge required to answer its queries is present in the
Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that
merely focus on factoid questions, our CoLoTa queries also require commonsense
reasoning. Our experiments with strong LLM-based KGQA methodologies indicate
their severe inability to answer queries involving commonsense reasoning.
Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM
commonsense reasoning capabilities and their robustness to hallucinations on
long-tail entities and (ii) the commonsense reasoning capabilities of KGQA
methods.

</details>

### [23] [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
*Yijun Liu*

Main category: cs.CL

TLDR: SSENSE是一个对比学习框架，将单主体立体脑电图（sEEG）信号映射到冻结CLIP模型的句子嵌入空间，实现直接从脑活动进行句子级检索。


<details>
  <summary>Details</summary>
Motivation: 研究多模态基础模型在将侵入性脑记录与自然语言对齐方面的潜力，以解决神经科学和人工智能交叉领域的复杂挑战。

Method: 使用InfoNCE损失在sEEG的频谱表示上训练神经编码器，不微调文本编码器。

Result: 在有限数据下，SSENSE在自然电影观看数据集上展示了有希望的结果。

Conclusion: 通用语言表征可作为神经解码的有效先验。

Abstract: Interpreting neural activity through meaningful latent representations
remains a complex and evolving challenge at the intersection of neuroscience
and artificial intelligence. We investigate the potential of multimodal
foundation models to align invasive brain recordings with natural language. We
present SSENSE, a contrastive learning framework that projects single-subject
stereo-electroencephalography (sEEG) signals into the sentence embedding space
of a frozen CLIP model, enabling sentence-level retrieval directly from brain
activity. SSENSE trains a neural encoder on spectral representations of sEEG
using InfoNCE loss, without fine-tuning the text encoder. We evaluate our
method on time-aligned sEEG and spoken transcripts from a naturalistic
movie-watching dataset. Despite limited data, SSENSE achieves promising
results, demonstrating that general-purpose language representations can serve
as effective priors for neural decoding.

</details>

### [24] [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
*Xiang Li,Duyi Pan,Hongru Xiao,Jiale Han,Jing Tang,Jiabao Ma,Wei Wang,Bo Cheng*

Main category: cs.CL

TLDR: 提出了一种名为DialogueAgents的新型混合代理语音合成框架，通过三个专业代理协作生成对话，解决了现有数据集成本高、多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成数据集构建成本高，且缺乏多样性和情感表达，需要一种更高效的方法。

Method: 采用混合代理框架，包括脚本编写器、语音合成器和对话评论器，通过迭代优化生成高质量对话。

Result: 生成了MultiTalk数据集，实验证明框架有效且数据集质量高。

Conclusion: DialogueAgents框架和MultiTalk数据集为语音合成研究提供了新工具和资源。

Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural
and intuitive communication. However, existing datasets involve high
construction costs due to manual annotation and suffer from limited character
diversity, contextual scenarios, and emotional expressiveness. To address these
issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis
framework, which integrates three specialized agents -- a script writer, a
speech synthesizer, and a dialogue critic -- to collaboratively generate
dialogues. Grounded in a diverse character pool, the framework iteratively
refines dialogue scripts and synthesizes speech based on speech review,
boosting emotional expressiveness and paralinguistic features of the
synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a
bilingual, multi-party, multi-turn speech dialogue dataset covering diverse
topics. Extensive experiments demonstrate the effectiveness of our framework
and the high quality of the MultiTalk dataset. We release the dataset and code
https://github.com/uirlx/DialogueAgents to facilitate future research on
advanced speech synthesis models and customized data generation.

</details>

### [25] [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
*Yichen Li,Zhiting Fan,Ruizhe Chen,Xiaotang Gai,Luqi Gong,Yan Zhang,Zuozhu Liu*

Main category: cs.CL

TLDR: FairSteer是一种无需定制提示或模型重新训练的新型推理时去偏框架，通过检测偏置激活、计算去偏转向向量和动态调整激活来实现去偏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易从训练语料中捕获偏见，现有方法存在不稳定性或高计算成本。

Method: FairSteer基于线性表示假设，通过轻量级线性分类器检测偏置激活，计算去偏转向向量（DSV），并在推理阶段动态调整激活。

Result: 在六种LLMs上的全面评估表明，FairSteer在问答、反事实输入评估和开放文本生成任务中表现优越。

Conclusion: FairSteer提供了一种高效、稳定的去偏方法，无需额外训练或复杂提示设计。

Abstract: Large language models (LLMs) are prone to capturing biases from training
corpus, leading to potential negative social impacts. Existing prompt-based
debiasing methods exhibit instability due to their sensitivity to prompt
changes, while fine-tuning-based techniques incur substantial computational
overhead and catastrophic forgetting. In this paper, we propose FairSteer, a
novel inference-time debiasing framework without requiring customized prompt
design or model retraining. Motivated by the linear representation hypothesis,
our preliminary investigation demonstrates that fairness-related features can
be encoded into separable directions in the hidden activation space. FairSteer
operates in three steps: biased activation detection, debiasing steering vector
(DSV) computation, and dynamic activation steering. Specifically, it first
trains a lightweight linear classifier to detect bias signatures in
activations, and then computes DSVs as intervention directions derived from
small contrastive prompt pairs. Subsequently, it performs debiasing by
adjusting activations with DSVs in the inference stage. Comprehensive
evaluation with six LLMs demonstrates the superiority of FairSteer across
question-answering, counterfactual input evaluation and open-ended text
generation tasks. Code will be released.

</details>

### [26] [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TLDR: 本文研究了大型语言模型（LLMs）的知识召回机制，将其抽象为功能结构，并提出激活向量隐含功能执行过程。通过实验验证，改进了基于激活修补的知识编辑方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs知识召回的机制，理解其隐含的功能执行过程，以改进知识编辑方法。

Method: 设计基于修补的知识评分算法识别功能组件，并进行反知识测试验证各组件的独立功能。

Result: 通过激活修补改进了上下文知识编辑方法，提升了新知识的短期记忆保留。

Conclusion: LLMs的知识召回可通过功能结构理解，激活修补方法有效改进知识编辑效果。

Abstract: Pre-trained transformer large language models (LLMs) demonstrate strong
knowledge recall capabilities. This paper investigates the knowledge recall
mechanism in LLMs by abstracting it into a functional structure. We propose
that during knowledge recall, the model's hidden activation space implicitly
entails a function execution process where specific activation vectors align
with functional components (Input argument, Function body, and Return values).
Specifically, activation vectors of relation-related tokens define a mapping
function from subjects to objects, with subject-related token activations
serving as input arguments and object-related token activations as return
values. For experimental verification, we first design a patching-based
knowledge-scoring algorithm to identify knowledge-aware activation vectors as
independent functional components. Then, we conduct counter-knowledge testing
to examine the independent functional effects of each component on knowledge
recall outcomes. From this functional perspective, we improve the contextual
knowledge editing approach augmented by activation patching. By rewriting
incoherent activations in context, we enable improved short-term memory
retention for new knowledge prompting.

</details>

### [27] [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
*Zhijing Jin*

Main category: cs.CL

TLDR: 论文探讨了大型语言模型（LLMs）的因果推理能力及其在自然语言处理和社会科学中的应用，提出了改进LLMs因果能力的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 因果推理是人类智能的核心，也是人工智能系统实现高级理解和决策的关键能力。

Method: 通过一系列研究，包括因果推理技能评估、机制分析、因果与反因果学习对NLP任务的影响，以及因果推理在计算社会科学中的应用。

Result: 提出了新的数据集、基准任务和方法框架，为LLMs因果能力的改进奠定了基础。

Conclusion: 为这一领域的未来研究提供了全面的基础。

Abstract: Causal reasoning is a cornerstone of human intelligence and a critical
capability for artificial systems aiming to achieve advanced understanding and
decision-making. This thesis delves into various dimensions of causal reasoning
and understanding in large language models (LLMs). It encompasses a series of
studies that explore the causal inference skills of LLMs, the mechanisms behind
their performance, and the implications of causal and anticausal learning for
natural language processing (NLP) tasks. Additionally, it investigates the
application of causal reasoning in text-based computational social science,
specifically focusing on political decision-making and the evaluation of
scientific impact through citations. Through novel datasets, benchmark tasks,
and methodological frameworks, this work identifies key challenges and
opportunities to improve the causal capabilities of LLMs, providing a
comprehensive foundation for future research in this evolving field.

</details>

### [28] [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
*Yiting Ran,Xintao Wang,Tian Qiu,Jiaqing Liang,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TLDR: BookWorld是一个基于书籍的多智能体社会模拟系统，能够生成高质量且忠于原著的创意故事，应用包括故事生成、互动游戏等，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注从头构建的智能体社会，而模拟已建立的虚构世界和角色具有重要实践价值但尚未充分探索。

Method: 引入BookWorld系统，涵盖角色多样性、动态性、虚构世界观等现实复杂性，支持多应用场景。

Result: 实验表明，BookWorld生成的故事创意且高质量，忠于原著，胜率为75.36%。

Conclusion: BookWorld为探索和扩展虚构作品提供了新途径，具有广泛的应用潜力。

Abstract: Recent advances in large language models (LLMs) have enabled social
simulation through multi-agent systems. Prior efforts focus on agent societies
created from scratch, assigning agents with newly defined personas. However,
simulating established fictional worlds and characters remain largely
underexplored, despite its significant practical value. In this paper, we
introduce BookWorld, a comprehensive system for constructing and simulating
book-based multi-agent societies. BookWorld's design covers comprehensive
real-world intricacies, including diverse and dynamic characters, fictional
worldviews, geographical constraints and changes, e.t.c. BookWorld enables
diverse applications including story generation, interactive games and social
simulation, offering novel ways to extend and explore beloved fictional works.
Through extensive experiments, we demonstrate that BookWorld generates
creative, high-quality stories while maintaining fidelity to the source books,
surpassing previous methods with a win rate of 75.36%. The code of this paper
can be found at the project page: https://bookworld2025.github.io/.

</details>

### [29] [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Baolong Bi,Yuyao Ge,Jun Wan,Yurong Wu,Xueqi Cheng*

Main category: cs.CL

TLDR: EAG框架通过环境反馈、动态分支探索和经验学习提升LLM的推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂多步任务中的幻觉、逻辑错误和无法自我纠正的问题。

Method: 提出EAG框架，结合实时环境反馈、动态分支探索和经验学习。

Result: a1-32B模型在多个基准测试中表现优异，超越同类模型24.4个百分点。

Conclusion: EAG为机器推理提供了新范式，特别适用于需要精确多步计算和逻辑验证的问题。

Abstract: Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,
yet continue to struggle with hallucinations, logical errors, and inability to
self-correct during complex multi-step tasks. Current approaches like
chain-of-thought prompting offer limited reasoning capabilities that fail when
precise step validation is required. We propose Environment Augmented
Generation (EAG), a framework that enhances LLM reasoning through: (1)
real-time environmental feedback validating each reasoning step, (2) dynamic
branch exploration for investigating alternative solution paths when faced with
errors, and (3) experience-based learning from successful reasoning
trajectories. Unlike existing methods, EAG enables deliberate backtracking and
strategic replanning through tight integration of execution feedback with
branching exploration. Our a1-32B model achieves state-of-the-art performance
among similar-sized models across all benchmarks, matching larger models like
o1 on competition mathematics while outperforming comparable models by up to
24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:
initial token investment in environment interaction yields substantial
long-term performance dividends, with advantages amplifying proportionally to
task complexity. EAG's theoretical framework demonstrates how environment
interactivity and systematic branch exploration together establish a new
paradigm for reliable machine reasoning, particularly for problems requiring
precise multi-step calculation and logical verification.

</details>

### [30] [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
*Yuri Balashov,Alex Balashov,Shiho Fukuda Koski*

Main category: cs.CL

TLDR: 该论文探讨了语言技术的最新进展如何为资源有限的翻译者和语言服务提供商带来新机遇，重点介绍了自动评估指标在自由职业者中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在帮助翻译者利用新兴技术（如神经机器翻译和大型语言模型）提升工作效率，适应行业变化。

Method: 提出了一种将自动评估指标（如BLEU、chrF、TER和COMET）应用于自由职业者的实用框架，并通过医学领域的三语语料库进行验证。

Result: 统计分析显示自动评分与人工评估之间存在相关性，证明了这些指标的实用性。

Conclusion: 主动采用新兴技术对翻译者在不断变化的职业环境中适应和发展至关重要。

Abstract: This is the first in a series of papers exploring the rapidly expanding new
opportunities arising from recent progress in language technologies for
individual translators and language service providers with modest resources.
The advent of advanced neural machine translation systems, large language
models, and their integration into workflows via computer-assisted translation
tools and translation management systems have reshaped the translation
landscape. These advancements enable not only translation but also quality
evaluation, error spotting, glossary generation, and adaptation to
domain-specific needs, creating new technical opportunities for freelancers. In
this series, we aim to empower translators with actionable methods to harness
these advancements. Our approach emphasizes Translation Analytics, a suite of
evaluation techniques traditionally reserved for large-scale industry
applications but now becoming increasingly available for smaller-scale users.
This first paper introduces a practical framework for adapting automatic
evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'
needs. We illustrate the potential of these metrics using a trilingual corpus
derived from a real-world project in the medical domain and provide statistical
analysis correlating human evaluations with automatic scores. Our findings
emphasize the importance of proactive engagement with emerging technologies to
not only adapt but thrive in the evolving professional environment.

</details>

### [31] [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
*Hongming Tan,Shaoxiong Zhan,Fengwei Jia,Hai-Tao Zheng,Wai Kin Chan*

Main category: cs.CL

TLDR: HSPIM是一个基于大语言模型的分层、免训练框架，通过论文分段和零样本提示评估科学论文的创新性，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于内容的方法难以全面捕捉创新性且缺乏泛化能力，因此需要一种更有效的评估框架。

Method: HSPIM采用论文分段和零样本LLM提示，通过分类、QA增强和加权评分实现创新性评估，并优化问题提示组合。

Result: 实验表明HSPIM在有效性、泛化性和可解释性上优于基线方法。

Conclusion: HSPIM为科学论文创新性评估提供了一种高效且可解释的解决方案。

Abstract: Measuring scientific paper innovation is both important and challenging.
Existing content-based methods often overlook the full-paper context, fail to
capture the full scope of innovation, and lack generalization. We propose
HSPIM, a hierarchical and training-free framework based on large language
models (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess
innovation. We segment the text by section titles and use zero-shot LLM
prompting to implement section classification, question-answering (QA)
augmentation, and weighted novelty scoring. The generated QA pair focuses on
section-level innovation and serves as additional context to improve the LLM
scoring. For each chunk, the LLM outputs a novelty score and a confidence
score. We use confidence scores as weights to aggregate novelty scores into a
paper-level innovation score. To further improve performance, we propose a
two-layer question structure consisting of common and section-specific
questions, and apply a genetic algorithm to optimize the question-prompt
combinations. Comprehensive experiments on scientific conference paper datasets
show that HSPIM outperforms baseline methods in effectiveness, generalization,
and interpretability.

</details>

### [32] [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
*Rondik Hadi Abdulrahman,Hossein Hassani*

Main category: cs.CL

TLDR: 该研究为库尔德语（Sorani方言）开发了一个数据集和语言模型，用于自动文本摘要（ATS），填补了该语言资源的空白。


<details>
  <summary>Details</summary>
Motivation: 库尔德语在自动文本摘要领域资源匮乏，限制了相关研究的发展。

Method: 研究基于231篇库尔德语科学论文，使用句子加权和TF-IDF算法进行实验，并手动和自动评估结果。

Result: 最佳准确率达到19.58%，专家手动评估结果因文档而异。

Conclusion: 该研究为库尔德语NLP研究者提供了宝贵资源，推动了ATS及相关领域的发展。

Abstract: Extracting concise information from scientific documents aids learners,
researchers, and practitioners. Automatic Text Summarization (ATS), a key
Natural Language Processing (NLP) application, automates this process. While
ATS methods exist for many languages, Kurdish remains underdeveloped due to
limited resources. This study develops a dataset and language model based on
231 scientific papers in Sorani Kurdish, collected from four academic
departments in two universities in the Kurdistan Region of Iraq (KRI),
averaging 26 pages per document. Using Sentence Weighting and Term
Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were
conducted, differing in whether the conclusions were included. The average word
count was 5,492.3 in the first experiment and 5,266.96 in the second. Results
were evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L
metrics, with the best accuracy reaching 19.58%. Six experts conducted manual
evaluations using three criteria, with results varying by document. This
research provides valuable resources for Kurdish NLP researchers to advance ATS
and related fields.

</details>

### [33] [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
*Soo-joon Choi,Ji-jun Park*

Main category: cs.CL

TLDR: 论文提出了一种基于生成式大语言模型（LLM）的金融事件实体提取方法，通过将任务重构为文本到结构化输出的生成任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 金融文本的复杂性和传统序列标注模型的局限性促使研究者探索生成式LLM的潜力，以更高效地提取金融事件实体。

Method: 采用参数高效微调（PEFT）对预训练的LLM进行微调，直接生成包含实体及其字符跨度的结构化输出（如JSON）。

Result: 在CCKS 2019数据集上，该方法取得了新的最佳F1分数，显著优于传统序列标注模型。

Conclusion: 生成式LLM在复杂领域特定信息提取任务中具有显著潜力，尤其是在需要结构化输出的场景。

Abstract: Financial event entity extraction is a crucial task for analyzing market
dynamics and building financial knowledge graphs, yet it presents significant
challenges due to the specialized language and complex structures in financial
texts. Traditional approaches often rely on sequence labeling models, which can
struggle with long-range dependencies and the inherent complexity of extracting
multiple, potentially overlapping entities. Motivated by the advanced language
understanding and generative capabilities of Large Language Models (LLMs), we
propose a novel method that reframes financial event entity extraction as a
text-to-structured-output generation task. Our approach involves fine-tuning a
pre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly
generate a structured representation, such as a JSON object, containing the
extracted entities and their precise character spans from the input text. We
evaluate our method on the challenging CCKS 2019 Financial Event Entity
Extraction dataset, comparing its performance against strong sequence labeling
baselines, including SEBERTNets and sebertNets. Experimental results
demonstrate that our generative LLM method achieves a new state-of-the-art F1
score on this benchmark, significantly outperforming previous methods. Through
detailed quantitative analysis across event types, entity types, and instance
complexity, as well as human evaluation, we show that our approach is more
effective at handling the nuances of financial text and extracting high-quality
entities. This work validates the potential of applying generative LLMs
directly to complex, domain-specific information extraction tasks requiring
structured output.

</details>

### [34] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
*Yihan Lin,Zhirong Bella Yu,Simon Lee*

Main category: cs.CL

TLDR: LLMs能生成小规模特征的合成EHR数据，但随着数据维度增加，难以保持真实分布和相关性，限制了跨医院泛化能力。


<details>
  <summary>Details</summary>
Motivation: 合成EHR数据可保护隐私并支持医疗应用，但需解决跨医院泛化问题。

Method: 评估商用LLMs生成合成数据的能力，分析其优缺点。

Result: LLMs对小规模特征表现可靠，但高维数据下分布和相关性保持不足。

Conclusion: LLMs在生成合成EHR数据时有潜力，但需改进以应对高维和跨医院需求。

Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to
create privacy preserving and harmonized structured data, supporting numerous
applications in healthcare. Key benefits of synthetic data include precise
control over the data schema, improved fairness and representation of patient
populations, and the ability to share datasets without concerns about
compromising real individuals privacy. Consequently, the AI community has
increasingly turned to Large Language Models (LLMs) to generate synthetic data
across various domains. However, a significant challenge in healthcare is
ensuring that synthetic health records reliably generalize across different
hospitals, a long standing issue in the field. In this work, we evaluate the
current state of commercial LLMs for generating synthetic data and investigate
multiple aspects of the generation process to identify areas where these models
excel and where they fall short. Our main finding from this work is that while
LLMs can reliably generate synthetic health records for smaller subsets of
features, they struggle to preserve realistic distributions and correlations as
the dimensionality of the data increases, ultimately limiting their ability to
generalize across diverse hospital settings.

</details>

### [35] [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
*Wei Zou,Sen Yang,Yu Bao,Shujian Huang,Jiajun Chen,Shanbo Cheng*

Main category: cs.CL

TLDR: TRANS-ZERO是一个利用单语数据和LLM内在多语言知识的自学习框架，通过结合遗传蒙特卡洛树搜索（G-MCTS）和偏好优化，实现了与监督方法媲美的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言机器翻译中低资源语言数据稀缺和灾难性遗忘的问题。

Method: 提出TRANS-ZERO框架，结合G-MCTS和偏好优化，仅使用单语数据和LLM的多语言知识。

Result: 实验表明，该方法不仅匹配大规模并行数据训练的模型性能，还在非英语翻译方向上表现优异。

Conclusion: G-MCTS通过迭代翻译探索语义一致的候选翻译，显著提升翻译质量，为框架的成功奠定基础。

Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation
(MT), but multilingual MT still relies heavily on parallel data for supervised
fine-tuning (SFT), facing challenges like data scarcity for low-resource
languages and catastrophic forgetting. To address these issues, we propose
TRANS-ZERO, a self-play framework that leverages only monolingual data and the
intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic
Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong
translation performance that rivals supervised methods. Experiments demonstrate
that this approach not only matches the performance of models trained on
large-scale parallel data but also excels in non-English translation
directions. Further analysis reveals that G-MCTS itself significantly enhances
translation quality by exploring semantically consistent candidates through
iterative translations, providing a robust foundation for the framework's
succuss.

</details>

### [36] [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
*Mehrnoush Shamsfard,Zahra Saaberi,Mostafa Karimi manesh,Seyed Mohammad Hossein Hashemi,Zahra Vatankhah,Motahareh Ramezani,Niki Pourazin,Tara Zare,Maryam Azimi,Sarina Chitsaz,Sama Khoraminejad,Morteza Mahdavi Mortazavi,Mohammad Mahdi Chizari,Sahar Maleki,Seyed Soroush Majd,Mostafa Masumi,Sayed Ali Musavi Khoeini,Amir Mohseni,Sogol Alipour*

Main category: cs.CL

TLDR: 论文介绍了FarsEval-PKBETS基准，用于评估波斯语大型语言模型（LLMs）的性能，结果显示当前模型的准确率低于50%。


<details>
  <summary>Details</summary>
Motivation: 研究波斯语等资源较少语言的LLMs性能，填补现有研究的空白。

Method: 开发包含4000个问题的FarsEval-PKBETS基准，涵盖多个领域和任务，并评估三个LLMs（Llama3-70B、PersianMind和Dorna）。

Result: 三个模型的平均准确率低于50%，表明当前LLMs在波斯语任务中表现不佳。

Conclusion: 当前LLMs在波斯语任务中仍有较大改进空间，需进一步优化。

Abstract: Research on evaluating and analyzing large language models (LLMs) has been
extensive for resource-rich languages such as English, yet their performance in
languages such as Persian has received considerably less attention. This paper
introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for
evaluating large language models in Persian. This benchmark consists of 4000
questions and answers in various formats, including multiple choice, short
answer and descriptive responses. It covers a wide range of domains and
tasks,including medicine, law, religion, Persian language, encyclopedic
knowledge, human preferences, social knowledge, ethics and bias, text
generation, and respecting others' rights. This bechmark incorporates
linguistics, cultural, and local considerations relevant to the Persian
language and Iran. To ensure the questions are challenging for current LLMs,
three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this
benchmark. Their average accuracy was below 50%, meaning they provided fully
correct answers to fewer than half of the questions. These results indicate
that current language models are still far from being able to solve this
benchmark

</details>

### [37] [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
*Songtao Jiang,Yuan Wang,Sibo Song,Yan Zhang,Zijie Meng,Bohan Lei,Jian Wu,Jimeng Sun,Zuozhu Liu*

Main category: cs.CL

TLDR: OmniV-Med是一个统一的多模态医疗理解框架，通过构建大规模数据集、设计统一编码器和引入医学感知的令牌剪枝机制，显著提升了医疗视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医疗视觉语言模型通常对不同模态使用独立编码器，限制了多模态数据的无缝整合。

Method: 1. 构建包含252K样本的多模态医疗数据集；2. 设计旋转位置自适应编码器，统一处理2D/3D图像和视频；3. 引入医学感知令牌剪枝机制，减少冗余视觉令牌。

Result: OmniV-Med-7B在7个基准测试中达到最优性能，轻量版OmniV-Med-1.5B仅需8块RTX3090显卡即可训练。

Conclusion: OmniV-Med通过统一框架和高效设计，为多模态医疗理解提供了高性能且实用的解决方案。

Abstract: The practical deployment of medical vision-language models (Med-VLMs)
necessitates seamless integration of textual data with diverse visual
modalities, including 2D/3D images and videos, yet existing models typically
employ separate encoders for different modalities. To address this limitation,
we present OmniV-Med, a unified framework for multimodal medical understanding.
Our technical contributions are threefold: First, we construct
OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K
instructional samples spanning 14 medical image modalities and 11 clinical
tasks. Second, we devise a rotary position-adaptive encoder that processes
multi-resolution 2D/3D images and videos within a unified architecture,
diverging from conventional modality-specific encoders. Third, we introduce a
medical-aware token pruning mechanism that exploits spatial-temporal redundancy
in volumetric data (e.g., consecutive CT slices) and medical videos,
effectively reducing 60\% of visual tokens without performance degradation.
Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art
performance on 7 benchmarks spanning 2D/3D medical imaging and video
understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains
comparable performance while requiring only 8 RTX3090 GPUs for training and
supporting efficient long-video inference. Data, code and model will be
released.

</details>

### [38] [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TLDR: 比较BERTopic、LDA和KMeans在比利时荷兰语日常叙事中的表现，发现BERTopic在语义相关性上优于统计方法，强调混合评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索BERTopic在形态丰富的语言（如比利时荷兰语）中的潜力，并对比传统方法（LDA和KMeans）的表现。

Method: 使用BERTopic、LDA和KMeans对开放式的比利时荷兰语日常叙事进行主题建模，结合自动指标和人工评估。

Result: BERTopic生成的主题更具文化共鸣，而LDA在自动指标上表现良好但语义相关性不足，KMeans表现不如预期。

Conclusion: 强调NLP模型在非主流语言环境中需要更强的泛化能力，并推荐混合评估框架。

Abstract: This study explores BERTopic's potential for modeling open-ended Belgian
Dutch daily narratives, contrasting its performance with Latent Dirichlet
Allocation (LDA) and KMeans. Although LDA scores well on certain automated
metrics, human evaluations reveal semantically irrelevant co-occurrences,
highlighting the limitations of purely statistic-based methods. In contrast,
BERTopic's reliance on contextual embeddings yields culturally resonant themes,
underscoring the importance of hybrid evaluation frameworks that account for
morphologically rich languages. KMeans performed less coherently than prior
research suggested, pointing to the unique challenges posed by personal
narratives. Our findings emphasize the need for robust generalization in NLP
models, especially in underrepresented linguistic contexts.

</details>

### [39] [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
*Reya Vir,Shreya Shankar,Harrison Chase,Will Fu-Hinthorn,Aditya Parameswaran*

Main category: cs.CL

TLDR: PROMPTEVALS是一个包含2087个LLM管道提示和12623个断言标准的数据集，用于提升LLM在生成输出时的可靠性。微调的Mistral和Llama 3模型在生成断言方面比GPT-4o表现更好。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中常无法满足开发者期望，需要可靠的断言机制来确保输出质量。

Method: 通过收集开发者提供的LLM管道提示和断言标准，构建PROMPTEVALS数据集，并评估闭源和开源模型生成断言的能力。

Result: 微调的Mistral和Llama 3模型比GPT-4o平均表现高出20.93%，且延迟更低。

Conclusion: PROMPTEVALS数据集可推动LLM可靠性、对齐和提示工程的研究。

Abstract: Large language models (LLMs) are increasingly deployed in specialized
production data processing pipelines across diverse domains -- such as finance,
marketing, and e-commerce. However, when running them in production across many
inputs, they often fail to follow instructions or meet developer expectations.
To improve reliability in these applications, creating assertions or guardrails
for LLM outputs to run alongside the pipelines is essential. Yet, determining
the right set of assertions that capture developer requirements for a task is
challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM
pipeline prompts with 12623 corresponding assertion criteria, sourced from
developers using our open-source LLM pipeline tools. This dataset is 5x larger
than previous collections. Using a hold-out test split of PROMPTEVALS as a
benchmark, we evaluated closed- and open-source models in generating relevant
assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform
GPT-4o by 20.93% on average, offering both reduced latency and improved
performance. We believe our dataset can spur further research in LLM
reliability, alignment, and prompt engineering.

</details>

### [40] [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
*Saniya Karwa,Navpreet Singh*

Main category: cs.CL

TLDR: 该论文提出了一种框架，用于揭示BERT等高维不透明模型中向量嵌入的特定维度如何编码不同语言属性。通过LDSP-10数据集和多种分析方法，研究发现某些语言属性（如否定和极性）在特定维度中编码较强，而其他属性（如同义性）则更复杂。


<details>
  <summary>Details</summary>
Motivation: 理解神经嵌入的内部机制，尤其是BERT等高维不透明模型中的嵌入，是一个挑战。

Method: 使用LDSP-10数据集（包含10种关键语言特征），结合Wilcoxon符号秩检验、互信息和递归特征消除等方法分析BERT嵌入，并引入EDI评分量化维度对语言属性的影响。

Result: 研究发现否定和极性等属性在特定维度中编码较强，而同义性等属性则表现出更复杂的模式。

Conclusion: 该研究为嵌入的可解释性提供了见解，有助于开发更透明和优化的语言模型，并对模型偏见缓解和AI系统负责任部署具有意义。

Abstract: Understanding the inner workings of neural embeddings, particularly in models
such as BERT, remains a challenge because of their high-dimensional and opaque
nature. This paper proposes a framework for uncovering the specific dimensions
of vector embeddings that encode distinct linguistic properties (LPs). We
introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which
isolates ten key linguistic features such as synonymy, negation, tense, and
quantity. Using this dataset, we analyze BERT embeddings with various methods,
including the Wilcoxon signed-rank test, mutual information, and recursive
feature elimination, to identify the most influential dimensions for each LP.
We introduce a new metric, the Embedding Dimension Impact (EDI) score, which
quantifies the relevance of each embedding dimension to a LP. Our findings show
that certain properties, such as negation and polarity, are robustly encoded in
specific dimensions, while others, like synonymy, exhibit more complex
patterns. This study provides insights into the interpretability of embeddings,
which can guide the development of more transparent and optimized language
models, with implications for model bias mitigation and the responsible
deployment of AI systems.

</details>

### [41] [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
*Luyang Fang,Xiaowei Yu,Jiazhang Cai,Yongkai Chen,Shushan Wu,Zhengliang Liu,Zhenyuan Yang,Haoran Lu,Xilin Gong,Yufang Liu,Terry Ma,Wei Ruan,Ali Abbasi,Jing Zhang,Tao Wang,Ehsan Latif,Wei Liu,Wei Zhang,Soheil Kolouri,Xiaoming Zhai,Dajiang Zhu,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.CL

TLDR: 该综述分析了知识蒸馏（KD）和数据集蒸馏（DD）两种互补范式，旨在压缩大型语言模型（LLMs）的同时保留其推理能力和语言多样性，并探讨了它们的整合策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的指数增长，计算和数据需求急剧增加，需要高效的压缩策略来满足这些需求。

Method: 综述了KD的关键方法（如任务对齐、多教师框架）和DD技术（如梯度匹配、生成合成），并探讨了二者的整合。

Result: KD和DD的整合能解决模型可扩展性、架构异质性和能力保留等挑战，并在医疗和教育等领域实现高效部署。

Conclusion: 尽管进展显著，仍需解决推理多样性、适应性和评估协议等问题。通过整合KD和DD，可为可持续、高效的LLMs提供路径。

Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight
the need for efficient strategies to meet ever-expanding computational and data
demands. This survey provides a comprehensive analysis of two complementary
paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both
aimed at compressing LLMs while preserving their advanced reasoning
capabilities and linguistic diversity. We first examine key methodologies in
KD, such as task-specific alignment, rationale-based training, and
multi-teacher frameworks, alongside DD techniques that synthesize compact,
high-impact datasets through optimization-based gradient matching, latent space
regularization, and generative synthesis. Building on these foundations, we
explore how integrating KD and DD can produce more effective and scalable
compression strategies. Together, these approaches address persistent
challenges in model scalability, architectural heterogeneity, and the
preservation of emergent LLM abilities. We further highlight applications
across domains such as healthcare and education, where distillation enables
efficient deployment without sacrificing performance. Despite substantial
progress, open challenges remain in preserving emergent reasoning and
linguistic diversity, enabling efficient adaptation to continually evolving
teacher models and datasets, and establishing comprehensive evaluation
protocols. By synthesizing methodological innovations, theoretical foundations,
and practical insights, our survey charts a path toward sustainable,
resource-efficient LLMs through the tighter integration of KD and DD
principles.

</details>

### [42] [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
*Jiaxin GUO,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Zongyao Li,Hengchao Shang,Daimeng Wei,Hao Yang*

Main category: cs.CL

TLDR: 本文探讨了文档级机器翻译自动评估的现状、挑战与未来趋势，强调了评估的重要性及现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，文档级翻译取得显著进展，但准确评估其质量仍是迫切问题。

Method: 分析了当前自动评估方案与指标，包括有无参考文本的评估方法、传统指标、基于模型的指标及基于LLM的指标。

Result: 指出当前评估方法面临的挑战，如参考多样性不足、依赖句子级对齐信息以及LLM评估方法的偏见与不准确性。

Conclusion: 展望未来趋势，提出可能的研究方向，如减少对句子级信息的依赖、引入多层次评估方法及训练专用评估模型。

Abstract: With the rapid development of deep learning technologies, the field of
machine translation has witnessed significant progress, especially with the
advent of large language models (LLMs) that have greatly propelled the
advancement of document-level translation. However, accurately evaluating the
quality of document-level translation remains an urgent issue. This paper first
introduces the development status of document-level translation and the
importance of evaluation, highlighting the crucial role of automatic evaluation
metrics in reflecting translation quality and guiding the improvement of
translation systems. It then provides a detailed analysis of the current state
of automatic evaluation schemes and metrics, including evaluation methods with
and without reference texts, as well as traditional metrics, Model-based
metrics and LLM-based metrics. Subsequently, the paper explores the challenges
faced by current evaluation methods, such as the lack of reference diversity,
dependence on sentence-level alignment information, and the bias, inaccuracy,
and lack of interpretability of the LLM-as-a-judge method. Finally, the paper
looks ahead to the future trends in evaluation methods, including the
development of more user-friendly document-level evaluation methods and more
robust LLM-as-a-judge methods, and proposes possible research directions, such
as reducing the dependency on sentence-level information, introducing
multi-level and multi-granular evaluation approaches, and training models
specifically for machine translation evaluation. This study aims to provide a
comprehensive analysis of automatic evaluation for document-level translation
and offer insights into future developments.

</details>

### [43] [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
*Mario M. Kubek,Shiraj Pokharel,Thomas Böhme,Emma L. McDaniel,Herwig Unger,Armin R. Mikler*

Main category: cs.CL

TLDR: 提出了一种快速优化预训练静态词或标记嵌入的新方法，通过结合邻近标记的嵌入，持续更新每个标记的表示，包括未预分配嵌入的标记，有效解决了词汇表外问题。


<details>
  <summary>Details</summary>
Motivation: 解决预训练嵌入在特定领域中的局限性，尤其是词汇表外问题和通用嵌入在主题同质语料中的不适用性。

Method: 通过结合邻近标记的嵌入，动态更新标记表示，独立于大型语言模型和浅层神经网络。

Result: 在主题同质语料中生成更有意义的嵌入，应用于风暴事件分析时改进了风暴相关术语的表示。

Conclusion: 该方法在特定领域中优化了标记表示，为语料探索和概念搜索等应用提供了灵活性。

Abstract: This article introduces a novel and fast method for refining pre-trained
static word or, more generally, token embeddings. By incorporating the
embeddings of neighboring tokens in text corpora, it continuously updates the
representation of each token, including those without pre-assigned embeddings.
This approach effectively addresses the out-of-vocabulary problem, too.
Operating independently of large language models and shallow neural networks,
it enables versatile applications such as corpus exploration, conceptual
search, and word sense disambiguation. The method is designed to enhance token
representations within topically homogeneous corpora, where the vocabulary is
restricted to a specific domain, resulting in more meaningful embeddings
compared to general-purpose pre-trained vectors. As an example, the methodology
is applied to explore storm events and their impacts on infrastructure and
communities using narratives from a subset of the NOAA Storm Events database.
The article also demonstrates how the approach improves the representation of
storm-related terms over time, providing valuable insights into the evolving
nature of disaster narratives.

</details>

### [44] [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
*Jiajun Shen,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TLDR: 论文提出了一种结合外部和内部知识的引用生成任务（Context-Prior Augmented Citation Generation），并设计了RAEL范式和INTRALIGN方法，以提升生成答案的可信度。实验表明该方法在跨场景性能上优于基线，且检索质量、问题类型和模型知识对引用可信度有显著影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的幻觉问题虽可通过检索增强生成和引用生成缓解，但其内部知识利用仍不透明，生成答案的可信度存疑。

Method: 提出Context-Prior Augmented Citation Generation任务，结合外部和内部知识生成引用；设计RAEL范式和INTRALIGN方法（包含数据生成和对齐算法）。

Result: 实验显示该方法在跨场景性能上优于基线；扩展实验表明检索质量、问题类型和模型知识对引用可信度有显著影响。

Conclusion: 该研究为提升生成答案的可信度提供了新方法，并揭示了影响引用可信度的关键因素。

Abstract: While hallucinations of large language models could been alleviated through
retrieval-augmented generation and citation generation, how the model utilizes
internal knowledge is still opaque, and the trustworthiness of its generated
answers remains questionable. In this work, we introduce Context-Prior
Augmented Citation Generation task, requiring models to generate citations
considering both external and internal knowledge while providing trustworthy
references, with 5 evaluation metrics focusing on 3 aspects: answer
helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the
paradigm for our task, and also design INTRALIGN, an integrated method
containing customary data generation and an alignment algorithm. Our
experimental results show that our method achieves a better cross-scenario
performance with regard to other baselines. Our extended experiments further
reveal that retrieval quality, question types, and model knowledge have
considerable influence on the trustworthiness in citation generation.

</details>

### [45] [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
*Teppei Suzuki,Ryokan Ri,Sho Takase*

Main category: cs.CL

TLDR: 研究发现，即使大型语言模型（LLM）使用相同的数据训练，其生成的文本仍能区分来源模型，这些独特特征被称为“自然指纹”。


<details>
  <summary>Details</summary>
Motivation: 探究LLM输出中系统性偏差的来源，尤其是训练过程中细微差异如何导致可识别的模型特征。

Method: 通过系统控制训练条件（如参数大小、优化设置、随机种子等），分析LLM生成文本的差异。

Result: 发现训练过程中的细微差异会导致LLM生成具有“自然指纹”的文本，即使训练数据相同。

Conclusion: 理解“自然指纹”有助于揭示意外偏差的起源，并为改进LLM行为控制提供新思路。

Abstract: Large language models (LLMs) often exhibit biases -- systematic deviations
from expected norms -- in their outputs. These range from overt issues, such as
unfair responses, to subtler patterns that can reveal which model produced
them. We investigate the factors that give rise to identifiable characteristics
in LLMs. Since LLMs model training data distribution, it is reasonable that
differences in training data naturally lead to the characteristics. However,
our findings reveal that even when LLMs are trained on the exact same data, it
is still possible to distinguish the source model based on its generated text.
We refer to these unintended, distinctive characteristics as natural
fingerprints. By systematically controlling training conditions, we show that
the natural fingerprints can emerge from subtle differences in the training
process, such as parameter sizes, optimization settings, and even random seeds.
We believe that understanding natural fingerprints offers new insights into the
origins of unintended bias and ways for improving control over LLM behavior.

</details>

### [46] [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
*Aoran Gan,Hao Yu,Kai Zhang,Qi Liu,Wenyu Yan,Zhenya Huang,Shiwei Tong,Guoping Hu*

Main category: cs.CL

TLDR: 本文综述了检索增强生成（RAG）系统的评估方法，整合了传统与新兴方法，并分析了数据集和框架，为RAG发展提供关键资源。


<details>
  <summary>Details</summary>
Motivation: RAG系统结合检索与生成组件，依赖动态知识源，评估面临独特挑战，需系统梳理方法以推动发展。

Method: 系统回顾RAG评估方法，包括性能、事实准确性、安全性和计算效率，并分析数据集和框架。

Result: 提供了最全面的RAG评估综述，整合传统与LLM驱动方法，为研究提供关键资源。

Conclusion: 本文填补了RAG评估领域的空白，为未来研究和开发提供了重要参考。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have
revolutionized natural language processing by integrating Large Language Models
(LLMs) with external information retrieval, enabling accurate, up-to-date, and
verifiable text generation across diverse applications. However, evaluating RAG
systems presents unique challenges due to their hybrid architecture that
combines retrieval and generation components, as well as their dependence on
dynamic knowledge sources in the LLM era. In response, this paper provides a
comprehensive survey of RAG evaluation methods and frameworks, systematically
reviewing traditional and emerging evaluation approaches, for system
performance, factual accuracy, safety, and computational efficiency in the LLM
era. We also compile and categorize the RAG-specific datasets and evaluation
frameworks, conducting a meta-analysis of evaluation practices in high-impact
RAG research. To the best of our knowledge, this work represents the most
comprehensive survey for RAG evaluation, bridging traditional and LLM-driven
methods, and serves as a critical resource for advancing RAG development.

</details>

### [47] [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
*Yingming Zheng,Xiaoliang Liu,Peng Wu,Li Pan*

Main category: cs.CL

TLDR: CRAVE提出了一种基于冲突推理的可解释性声明验证方法，利用大语言模型（LLMs）和小语言模型（SLMs）提升复杂声明的验证准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 数字媒体和AI生成内容导致错误信息快速传播，传统依赖专家标注证据的方法效率低且难以扩展，现有自动化系统对复杂声明的推理能力不足。

Method: CRAVE采用三模块框架：1) 消除歧义并检索证据；2) 利用LLMs从四个维度推理冲突立场并初步判断；3) 使用SLM评估冲突理由并最终判断。

Result: 在两个公开数据集上，CRAVE性能优于现有方法，证据检索和模型解释能力显著提升。

Conclusion: CRAVE通过冲突推理和模块化设计，显著提高了复杂声明的验证准确性和可解释性。

Abstract: The rapid spread of misinformation, driven by digital media and AI-generated
content, has made automatic claim verification essential. Traditional methods,
which depend on expert-annotated evidence, are labor-intensive and not
scalable. Although recent automated systems have improved, they still struggle
with complex claims that require nuanced reasoning. To address this, we propose
CRAVE, a Conflicting Reasoning Approach for explainable claim VErification,
that verify the complex claims based on the conflicting rationales reasoned by
large language models (LLMs). Specifically, CRAVE introduces a three-module
framework. Ambiguity Elimination enchanced Evidence Retrieval module performs
ambiguity elimination and entity-based search to gather relevant evidence
related to claim verification from external sources like Wikipedia. Conflicting
Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to
reason rationales with conflicting stances about claim verification from
retrieved evidence across four dimensions, i.e., direct evidence, semantic
relationships, linguistic patterns, and logical reasoning and make a
preliminary judgment. Finally, Small Language Model (SLM) based Judge module is
fine-tuned to make use of preliminary judgment from LLMs to assess the
confidence of the conflicting rationales and make a final authenticity
judgment. This methodology allows CRAVE to capture subtle inconsistencies in
complex claims, improving both the accuracy and transparency of claim
verification. Extensive experiments on two public claim verification datasets
demonstrate that our CRAVE model achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for finding relevant
evidence and explaining the model predictions. The code is provided at
https://github.com/8zym/CRAVE.

</details>

### [48] [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
*Rui Ribeiro,Luísa Coheur,Joao P. Carvalho*

Main category: cs.CL

TLDR: 本文提出了一种基于模糊指纹和上下文感知模型的文本说话人识别方法，显著提高了识别准确率，并分析了模糊指纹的优势和模糊语句的处理机制。


<details>
  <summary>Details</summary>
Motivation: 传统的说话人识别方法依赖语音特征，而文本数据下的说话人识别研究较少且方法传统。本文旨在探索预训练模型的模糊指纹和上下文建模以改进文本说话人识别。

Method: 结合说话人特定标记和上下文感知模型，利用模糊指纹技术减少隐藏单元数量，同时分析模糊语句并提出检测机制。

Result: 在Friends数据集上达到70.6%的准确率，在Big Bang Theory数据集上达到67.7%的准确率，模糊指纹接近全微调性能且更具可解释性。

Conclusion: 研究揭示了文本说话人识别的关键挑战，并为未来改进提供了方向。

Abstract: Speaker identification using voice recordings leverages unique acoustic
features, but this approach fails when only textual data is available. Few
approaches have attempted to tackle the problem of identifying speakers solely
from text, and the existing ones have primarily relied on traditional methods.
In this work, we explore the use of fuzzy fingerprints from large pre-trained
models to improve text-based speaker identification. We integrate
speaker-specific tokens and context-aware modeling, demonstrating that
conversational context significantly boosts accuracy, reaching 70.6% on the
Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show
that fuzzy fingerprints can approximate full fine-tuning performance with fewer
hidden units, offering improved interpretability. Finally, we analyze ambiguous
utterances and propose a mechanism to detect speaker-agnostic lines. Our
findings highlight key challenges and provide insights for future improvements
in text-based speaker identification.

</details>

### [49] [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
*Xiaodong Yang*

Main category: cs.CL

TLDR: 提出一个评估大语言模型（LLMs）对中文主题结构敏感性的框架，重点关注其对孤岛约束的敏感性。


<details>
  <summary>Details</summary>
Motivation: 受Tian等人（2024）启发，旨在填补LLMs在汉语语法知识评估方面的空白，为未来研究奠定基础。

Method: 设计了一个实验方案，用于测试LLMs对普通话语法的理解，但尚未进行实验。

Result: 目前无实验结果，但提出了方法论供未来研究和反馈。

Conclusion: 该提案为后续研究提供了框架，并欢迎对方法论的改进建议。

Abstract: This paper proposes a framework for evaluating large language models (LLMs)
on Chinese topic constructions, focusing on their sensitivity to island
constraints. Drawing inspiration from Tian et al. (2024), we outline an
experimental design for testing LLMs' grammatical knowledge of Mandarin syntax.
While no experiments have been conducted yet, this proposal aims to provide a
foundation for future studies and invites feedback on the methodology.

</details>

### [50] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
*Bohong Wu,Shen Yan,Sijun Zhang,Jianqiao Lu,Yutao Zeng,Ya Wang,Xun Zhou*

Main category: cs.CL

TLDR: 提出了一种名为PHD-Transformer的新框架，通过在预训练阶段实现高效的长度扩展，同时保持推理效率。


<details>
  <summary>Details</summary>
Motivation: 探索预训练阶段长度扩展的潜力，填补现有研究的空白。

Method: 采用创新的KV缓存管理策略，区分原始令牌和隐藏解码令牌，并引入两种优化变体（PHD-SWA和PHD-CSWA）。

Result: 在多个基准测试中展示了性能的持续提升。

Conclusion: PHD-Transformer为预训练阶段的高效长度扩展提供了有效解决方案。

Abstract: Recent advances in large language models have demonstrated the effectiveness
of length scaling during post-training, yet its potential in pre-training
remains underexplored. We present the Parallel Hidden Decoding Transformer
(\textit{PHD}-Transformer), a novel framework that enables efficient length
scaling during pre-training while maintaining inference efficiency.
\textit{PHD}-Transformer achieves this through an innovative KV cache
management strategy that distinguishes between original tokens and hidden
decoding tokens. By retaining only the KV cache of original tokens for
long-range dependencies while immediately discarding hidden decoding tokens
after use, our approach maintains the same KV cache size as the vanilla
transformer while enabling effective length scaling. To further enhance
performance, we introduce two optimized variants: \textit{PHD-SWA} employs
sliding window attention to preserve local dependencies, while
\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate
linear growth in pre-filling time. Extensive experiments demonstrate consistent
improvements across multiple benchmarks.

</details>

### [51] [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
*Yow-Fu Liou,Yu-Chien Tang,An-Zi Yen*

Main category: cs.CL

TLDR: 研究探讨了利用大语言模型（LLMs）自动化生成教育材料和课程建议的潜力，通过实验验证了生成内容的高质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 教育材料的制作耗时且对教师要求高，研究旨在通过LLMs简化这一过程，提升学习体验。

Method: 从视频转录生成扩展文章，结合历史、文化等背景知识，并通过语义相似度推荐相关课程，最后用LLM优化内容。

Result: 实验表明模型能生成高质量内容和准确课程建议，提升学习材料的吸引力和可访问性。

Conclusion: LLMs能有效连接核心内容与补充学习资源，辅助教师设计材料并为学生提供额外资源。

Abstract: The process of creating educational materials is both time-consuming and
demanding for educators. This research explores the potential of Large Language
Models (LLMs) to streamline this task by automating the generation of extended
reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper
sections as an initial exploration, we investigate how supplementary articles
can be enriched with contextual knowledge and connected to additional learning
resources. Our method begins by generating extended articles from video
transcripts, leveraging LLMs to include historical insights, cultural examples,
and illustrative anecdotes. A recommendation system employing semantic
similarity ranking identifies related courses, followed by an LLM-based
refinement process to enhance relevance. The final articles are tailored to
seamlessly integrate these recommendations, ensuring they remain cohesive and
informative. Experimental evaluations demonstrate that our model produces
high-quality content and accurate course suggestions, assessed through metrics
such as Hit Rate, semantic similarity, and coherence. Our experimental analysis
highlight the nuanced differences between the generated and existing materials,
underscoring the model's capacity to offer more engaging and accessible
learning experiences. This study showcases how LLMs can bridge the gap between
core content and supplementary learning, providing students with additional
recommended resources while also assisting teachers in designing educational
materials.

</details>

### [52] [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
*Muhammad Uzair Ul Haq,Davide Rigoni,Alessandro Sperduti*

Main category: cs.CL

TLDR: 论文探讨了在NLP中利用LLMs自动生成高质量标注数据的挑战，提出通过自动检索上下文示例改进ICL性能的方法，并比较了不同LLMs和嵌入模型在NER任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 手动标注数据成本高且耗时，而LLMs的ICL方法依赖人工选择上下文示例，效率低且性能不佳，因此需要更自动化的解决方案。

Method: 通过比较不同参数规模的LLMs和嵌入模型，结合RAG方法自动检索上下文示例，以提升NER任务的性能。

Result: 实验表明，选择合适的LLM和嵌入模型对性能至关重要，同时需要权衡模型规模与性能，并关注更具挑战性的数据集。

Conclusion: 研究强调了自动化检索上下文示例的重要性，并呼吁未来研究应聚焦于更复杂的数据集和模型优化。

Abstract: In NLP, fine-tuning LLMs is effective for various applications but requires
high-quality annotated data. However, manual annotation of data is
labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly
used to automate the process, often employing in-context learning (ICL) in
which some examples related to the task are given in the prompt for better
performance. However, manually selecting context examples can lead to
inefficiencies and suboptimal model performance. This paper presents
comprehensive experiments comparing several LLMs, considering different
embedding models, across various datasets for the Named Entity Recognition
(NER) task. The evaluation encompasses models with approximately $7$B and $70$B
parameters, including both proprietary and non-proprietary models. Furthermore,
leveraging the success of Retrieval-Augmented Generation (RAG), it also
considers a method that addresses the limitations of ICL by automatically
retrieving contextual examples, thereby enhancing performance. The results
highlight the importance of selecting the appropriate LLM and embedding model,
understanding the trade-offs between LLM sizes and desired performance, and the
necessity to direct research efforts towards more challenging datasets.

</details>

### [53] [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
*Chengyu Wang,Junbing Yan,Yuanhao Yue,Jun Huang*

Main category: cs.CL

TLDR: DistilQwen2.5是一系列轻量级大语言模型，通过蒸馏技术从Qwen2.5模型衍生而来，提升了指令跟随能力，并在工业实践中展示了高效部署的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在资源受限场景下的计算效率和部署成本问题。

Method: 利用多代理教师模型选择和改写指令-响应对，结合模型融合技术逐步整合细粒度知识。

Result: 蒸馏模型表现出比原始模型更强的能力。

Conclusion: DistilQwen2.5模型开源，为实际应用提供了高效解决方案。

Abstract: Enhancing computational efficiency and reducing deployment costs for large
language models (LLMs) have become critical challenges in various
resource-constrained scenarios. In this work, we present DistilQwen2.5, a
family of distilled, lightweight LLMs derived from the public Qwen2.5 models.
These distilled models exhibit enhanced instruction-following capabilities
compared to the original models based on a series of distillation techniques
that incorporate knowledge from much larger LLMs. In our industrial practice,
we first leverage powerful proprietary LLMs with varying capacities as
multi-agent teachers to select, rewrite, and refine instruction-response pairs
that are more suitable for student LLMs to learn. After standard fine-tuning,
we further leverage a computationally efficient model fusion approach that
enables student models to progressively integrate fine-grained hidden knowledge
from their teachers. Experimental evaluations demonstrate that the distilled
models possess significantly stronger capabilities than their original
checkpoints. Additionally, we present use cases to illustrate the applications
of our framework in real-world scenarios. To facilitate practical use, we have
released all the DistilQwen2.5 models to the open-source community.

</details>

### [54] [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
*Quy-Anh Dang,Chris Ngo,Truong-Son Hy*

Main category: cs.CL

TLDR: RainbowPlus是一种基于进化计算的新型红队框架，通过自适应质量多样性搜索生成对抗性提示，显著提升了攻击成功率和多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受对抗性提示攻击，现有红队方法存在可扩展性、资源消耗和攻击策略多样性不足的问题。

Method: RainbowPlus采用多元素存档存储多样化高质量提示，并通过综合适应度函数评估多个提示，改进了传统质量多样性方法。

Result: 实验表明，RainbowPlus在攻击成功率和多样性上优于现有方法，生成更多独特提示，且速度更快。

Conclusion: RainbowPlus为LLM安全性提供了可扩展的漏洞评估工具，开源实现支持进一步研究。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are
susceptible to adversarial prompts that exploit vulnerabilities to produce
unsafe or biased outputs. Existing red-teaming methods often face scalability
challenges, resource-intensive requirements, or limited diversity in attack
strategies. We propose RainbowPlus, a novel red-teaming framework rooted in
evolutionary computation, enhancing adversarial prompt generation through an
adaptive quality-diversity (QD) search that extends classical evolutionary
algorithms like MAP-Elites with innovations tailored for language models. By
employing a multi-element archive to store diverse high-quality prompts and a
comprehensive fitness function to evaluate multiple prompts concurrently,
RainbowPlus overcomes the constraints of single-prompt archives and pairwise
comparisons in prior QD methods like Rainbow Teaming. Experiments comparing
RainbowPlus to QD methods across six benchmark datasets and four open-source
LLMs demonstrate superior attack success rate (ASR) and diversity
(Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts
(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine
state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten
open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,
surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).
Our open-source implementation fosters further advancements in LLM safety,
offering a scalable tool for vulnerability assessment. Code and resources are
publicly available at https://github.com/knoveleng/rainbowplus, supporting
reproducibility and future research in LLM red-teaming.

</details>

### [55] [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
*Joachim Minder,Guillaume Wisniewski,Natalie Kübler*

Main category: cs.CL

TLDR: 研究探讨了ChatGPT在基于错误分类的机器翻译输出标注中的能力，发现其在专业翻译中表现良好，但自我评估能力有限。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（如ChatGPT）在专业翻译错误标注中的潜力，填补现有研究在专业领域的空白。

Method: 使用两种不同提示和自定义错误分类，比较ChatGPT与人类专家对DeepL和ChatGPT翻译的标注结果。

Result: ChatGPT对DeepL翻译的标注召回率和精确度较高，但自我评估表现较差，提示的细节影响分类准确性。

Conclusion: 大型语言模型在翻译评估中具有潜力，但需进一步研究开源模型及其实践应用效果。

Abstract: This study investigates the capabilities of large language models (LLMs),
specifically ChatGPT, in annotating MT outputs based on an error typology. In
contrast to previous work focusing mainly on general language, we explore
ChatGPT's ability to identify and categorise errors in specialised
translations. By testing two different prompts and based on a customised error
typology, we compare ChatGPT annotations with human expert evaluations of
translations produced by DeepL and ChatGPT itself. The results show that, for
translations generated by DeepL, recall and precision are quite high. However,
the degree of accuracy in error categorisation depends on the prompt's specific
features and its level of detail, ChatGPT performing very well with a detailed
prompt. When evaluating its own translations, ChatGPT achieves significantly
poorer results, revealing limitations with self-assessment. These results
highlight both the potential and the limitations of LLMs for translation
evaluation, particularly in specialised domains. Our experiments pave the way
for future research on open-source LLMs, which could produce annotations of
comparable or even higher quality. In the future, we also aim to test the
practical effectiveness of this automated evaluation in the context of
translation training, particularly by optimising the process of human
evaluation by teachers and by exploring the impact of annotations by LLMs on
students' post-editing and translation learning.

</details>

### [56] [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
*K. Wong,B. Wu,S. Bulathwela,M. Cukurova*

Main category: cs.CL

TLDR: 研究探讨了多模态数据在诊断学生协作问题解决（CPS）能力中的潜力，发现多模态数据在特定情况下能提升模型性能，但其价值取决于标签复杂性和数据集组成。


<details>
  <summary>Details</summary>
Motivation: 探索多模态数据和先进模型在检测复杂CPS行为中的实际价值，填补现有研究的空白。

Method: 使用文本嵌入和声学嵌入构建多模态分类模型，比较传统模型与基于Transformer的单模态和多模态模型。

Result: 多模态数据在Transformer模型中提升了社交认知类CPS的诊断性能，但对传统模型无显著改进。

Conclusion: 多模态和模型选择需根据具体CPS指标定制，强调人机互补和进一步探索模型架构的必要性。

Abstract: Detecting collaborative and problem-solving behaviours from digital traces to
interpret students' collaborative problem solving (CPS) competency is a
long-term goal in the Artificial Intelligence in Education (AIEd) field.
Although multimodal data and advanced models are argued to have the potential
to detect complex CPS behaviours, empirical evidence on their value remains
limited with some contrasting evidence. In this study, we investigated the
potential of multimodal data to improve model performance in diagnosing 78
secondary school students' CPS subskills and indicators in authentic
educational settings. In particular, text embeddings from verbal data and
acoustic embeddings from audio data were used in a multimodal classification
model for CPS diagnosis. Both unimodal and multimodal transformer-based models
outperformed traditional models in detecting CPS classes. Although the
inclusion of multimodality did not improve the performance of traditional
unimodal models, its integration into transformer-based models demonstrated
improved performance for diagnosing social-cognitive CPS classes compared to
unimodal transformer-based models. Based on the results, the paper argues that
multimodality and the selection of a particular modelling technique should not
be taken for granted to achieve the best performance in the automated detection
of every CPS subskill and indicator. Rather, their value is limited to certain
types of CPS indicators, affected by the complexity of the labels, and
dependent on the composition of indicators in the dataset. We conclude the
paper by discussing the required nuance when considering the value of LLMs and
multimodality in automated CPS diagnosis, highlighting the need for human-AI
complementarity, and proposing the exploration of relevant model architectures
and techniques to improve CPS diagnosis in authentic educational contexts.

</details>

### [57] [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
*Khalil Hennara,Sara Chrouf,Mohamed Motaism Hamed,Zeina Aldallal,Omar Hadid,Safwan AlModhayan*

Main category: cs.CL

TLDR: 本文提出了一种将新语言（阿拉伯语）整合到现有大型语言模型（LLM）中的方法，成功提升了目标语言性能且未损害原有知识。


<details>
  <summary>Details</summary>
Motivation: 增强AI模型对新知识的整合能力是AI发展的关键，本文旨在探索一种高效、低成本的方法，将新语言融入现有LLM。

Method: 通过向一个以英语为主的小型开源模型注入阿拉伯语数据，训练了一个名为Kuwain的15亿参数小模型。

Result: 阿拉伯语性能平均提升8%，同时保留了原有知识，且仅需少量原始模型数据。

Conclusion: 该方法为高效、定向扩展语言模型提供了可行方案，避免了资源密集型训练过程。

Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI
development. This paper introduces a novel method for integrating a new
language into a large language model (LLM). Our approach successfully
incorporates a previously unseen target language into an existing LLM without
compromising its prior knowledge. We trained a tiny model with 1.5 billion
parameters named Kuwain by injecting the Arabic language into a small
open-source model mainly trained in English. Our method demonstrates
significant improvements in Arabic language performance, with an average 8%
improvement across various benchmarks, while retaining the model's existing
knowledge with a minimum amount of the original model's data. This offers a
cost-effective alternative to training a comprehensive model in both English
and Arabic. The results highlight the potential for efficient, targeted
language model expansion without extensive retraining or resource-intensive
processes.

</details>

### [58] [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
*Ziwen Xu,Shuxun Wang,Kewei Xu,Haoming Xu,Mengru Wang,Xinle Deng,Yunzhi Yao,Guozhou Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TLDR: EasyEdit2是一个用于控制大型语言模型（LLM）行为的即插即用框架，支持多种测试时干预，如安全性、情感、个性等。其新架构包括导向向量生成器和应用器，无需修改模型参数即可调整行为。


<details>
  <summary>Details</summary>
Motivation: 提供一种无需技术背景即可精确控制LLM行为的工具，简化模型调整过程。

Method: 采用导向向量生成器和应用器架构，通过示例自动生成和应用导向向量。

Result: 在不同LLM上验证了其有效性，用户仅需一个示例即可调整模型行为。

Conclusion: EasyEdit2提供了一种高效且易用的LLM行为控制方法，适合广泛用户群体。

Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable
plug-and-play adjustability for controlling Large Language Model (LLM)
behaviors. EasyEdit2 supports a wide range of test-time interventions,
including safety, sentiment, personality, reasoning patterns, factuality, and
language features. Unlike its predecessor, EasyEdit2 features a new
architecture specifically designed for seamless model steering. It comprises
key modules such as the steering vector generator and the steering vector
applier, which enable automatic generation and application of steering vectors
to influence the model's behavior without modifying its parameters. One of the
main advantages of EasyEdit2 is its ease of use-users do not need extensive
technical knowledge. With just a single example, they can effectively guide and
adjust the model's responses, making precise control both accessible and
efficient. Empirically, we report model steering performance across different
LLMs, demonstrating the effectiveness of these techniques. We have released the
source code on GitHub at https://github.com/zjunlp/EasyEdit along with a
demonstration notebook. In addition, we provide a demo video at
https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.

</details>

### [59] [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
*Joan C. Timoneda*

Main category: cs.CL

TLDR: 论文提出了一种合成填补方法，利用生成式LLM（如GPT-4o）生成合成文本，以解决训练数据中类别不平衡的问题。该方法在75个原始样本时性能与完整样本相当，且过拟合可控。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中某些类别样本不足的问题，以提高编码器-解码器LLM（如BERT、RoBERTa）的性能。

Method: 使用生成式LLM（GPT-4o）基于少量原始样本生成合成文本，确保新文本与原始文本有足够差异以减少过拟合，同时保留语义信息。

Result: 在75个或更多原始样本时，合成填补方法的性能与完整样本相当；50个样本时过拟合可控且可校正。

Conclusion: 合成填补方法为生成式LLM在研究中提供了新角色，帮助应用研究者平衡数据集以获得最佳性能。

Abstract: Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,
require that all categories in an annotation task be sufficiently represented
in the training data for optimal performance. However, it is often difficult to
find sufficient examples for all categories in a task when building a
high-quality training set. In this article, I describe this problem and propose
a solution, the synthetic imputation approach. Leveraging a generative LLM
(GPT-4o), this approach generates synthetic texts based on careful prompting
and five original examples drawn randomly with replacement from the sample.
This approach ensures that new synthetic texts are sufficiently different from
the original texts to reduce overfitting, but retain the underlying substantive
meaning of the examples to maximize out-of-sample performance. With 75 original
examples or more, synthetic imputation's performance is on par with a full
sample of original texts, and overfitting remains low, predictable and
correctable with 50 original samples. The synthetic imputation approach
provides a novel role for generative LLMs in research and allows applied
researchers to balance their datasets for best performance.

</details>

### [60] [On true empty category](https://arxiv.org/abs/2504.15168)
*Qilin Tian*

Main category: cs.CL

TLDR: 论文探讨了空语类的分类，并评估了Li等人提出的“真实空语类”假说，认为无需引入该假说即可解释相关现象。


<details>
  <summary>Details</summary>
Motivation: 现有空语类分类无法解释某些空宾语位置的现象，Li等人提出“真实空语类”假说试图解决这一问题。

Method: 通过分析话题化的证据，评估“真实空语类”假说的必要性。

Result: 研究发现无需引入“真实空语类”假说即可解释相关现象。

Conclusion: 论文否定了“真实空语类”假说，认为现有理论足以解释相关语言现象。

Abstract: According to Chomsky (1981, 1986), empty categories consist of PRO, pro,
trace, and variable. However, some empty object positions seem to be
incompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)
and Li & Wei (2014) raise the true empty category hypothesis, which holds that
true empty category is only an empty position with category and Case features.
As a last resort option, it is used mainly to meet the subcatgorization of a
verb. This assumption is ingenious, and if proved to be true, it will exert a
great impact on the study of UG. In this paper, we evaluate their evidence from
topicalization and demonstrate that it can be accounted for without invoking
true empty category.

</details>

### [61] [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
*Nandan Thakur,Ronak Pradeep,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.CL

TLDR: 论文研究了检索增强生成（RAG）中引用文档对答案的支持性评估，比较了GPT-4o自动评估与人工评估的效果，发现GPT-4o在支持性评估中表现可靠。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型（LLMs）生成答案时的幻觉现象，通过评估引用文档对答案的支持性来提高RAG系统的可靠性。

Method: 对TREC 2024 RAG Track的45份提交和36个主题进行大规模比较研究，对比GPT-4o自动评估与人工评估（完全人工和人工后编辑LLM预测）的效果。

Result: 56%的完全人工评估中，人类与GPT-4o的预测完全一致（三级评分），后编辑条件下提升至72%。独立人类评估与GPT-4o相关性更高。

Conclusion: GPT-4o可作为支持性评估的可靠替代方案，未来需进一步分析人类与GPT-4o的错误以优化评估方法。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to
generate answers with citations from source documents containing "ground
truth", thereby reducing system hallucinations. A crucial factor in RAG
evaluation is "support", whether the information in the cited documents
supports the answer. To this end, we conducted a large-scale comparative study
of 45 participant submissions on 36 topics to the TREC 2024 RAG Track,
comparing an automatic LLM judge (GPT-4o) against human judges for support
assessment. We considered two conditions: (1) fully manual assessments from
scratch and (2) manual assessments with post-editing of LLM predictions. Our
results indicate that for 56% of the manual from-scratch assessments, human and
GPT-4o predictions match perfectly (on a three-level scale), increasing to 72%
in the manual with post-editing condition. Furthermore, by carefully analyzing
the disagreements in an unbiased study, we found that an independent human
judge correlates better with GPT-4o than a human judge, suggesting that LLM
judges can be a reliable alternative for support assessment. To conclude, we
provide a qualitative analysis of human and GPT-4o errors to help guide future
iterations of support assessment.

</details>

### [62] [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
*Manya Wadhwa,Zayne Sprague,Chaitanya Malaviya,Philippe Laban,Junyi Jessy Li,Greg Durrett*

Main category: cs.CL

TLDR: 论文提出EvalAgent框架，用于自动发现语言模型输出中隐含的任务特定评估标准，结合专家指导和LLM生成标准，提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注基本任务要求，而高质量响应需满足更多隐含标准（如学术演讲的吸引力、清晰的研究问题等）。

Method: EvalAgent通过挖掘专家在线指导，提出多样化的、基于可靠外部来源的评估标准。

Result: 实验表明EvalAgent提出的标准具有隐含性和特异性，且可通过响应优化满足。结合LLM标准能发现更多人认可的标准。

Conclusion: EvalAgent能有效补充现有评估方法，提升语言模型输出的质量与实用性。

Abstract: Evaluation of language model outputs on structured writing tasks is typically
conducted with a number of desirable criteria presented to human evaluators or
large language models (LLMs). For instance, on a prompt like "Help me draft an
academic talk on coffee intake vs research productivity", a model response may
be evaluated for criteria like accuracy and coherence. However, high-quality
responses should do more than just satisfy basic task requirements. An
effective response to this query should include quintessential features of an
academic talk, such as a compelling opening, clear research questions, and a
takeaway. To help identify these implicit criteria, we introduce EvalAgent, a
novel framework designed to automatically uncover nuanced and task-specific
criteria. EvalAgent first mines expert-authored online guidance. It then uses
this evidence to propose diverse, long-tail evaluation criteria that are
grounded in reliable external sources. Our experiments demonstrate that the
grounded criteria produced by EvalAgent are often implicit (not directly stated
in the user's prompt), yet specific (high degree of lexical precision).
Further, EvalAgent criteria are often not satisfied by initial responses but
they are actionable, such that responses can be refined to satisfy them.
Finally, we show that combining LLM-generated and EvalAgent criteria uncovers
more human-valued criteria than using LLMs alone.

</details>

### [63] [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
*Julián Cendrero,Julio Gonzalo,Ivar Zapata*

Main category: cs.CL

TLDR: 论文提出了完全贝叶斯的Topics over Time（BToT）模型及其加权版本WBToT，解决了原ToT模型的稳定性问题，并在时间主题建模中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 原ToT模型未采用完全贝叶斯方法，导致稳定性问题，且时间与词频尺度不一致影响建模效果。

Method: 引入Beta分布的共轭先验作为正则化，提出BToT；进一步通过重复时间戳提出WBToT，平衡时间和词频的影响。

Result: WBToT在US SOTU和COVID-19 Twitter数据集上表现优于LDA和BERTopic，主题时间偏差分别降低51%和34%。

Conclusion: WBToT解决了ToT的稳定性问题，平衡了时间和词频的影响，适用于大规模在线学习任务。

Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped
datasets by explicitly modeling publication dates jointly with word
co-occurrence patterns. However, ToT was not approached in a fully Bayesian
fashion, a flaw that makes it susceptible to stability problems. To address
this issue, we propose a fully Bayesian Topics over Time (BToT) model via the
introduction of a conjugate prior to the Beta distribution. This prior acts as
a regularization that prevents the online version of the algorithm from
unstable updates when a topic is poorly represented in a mini-batch. The
characteristics of this prior to the Beta distribution are studied here for the
first time. Still, this model suffers from a difference in scale between the
single-time observations and the multiplicity of words per document. A
variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a
solution. In WBToT, publication dates are repeated a certain number of times
per document, which balances the relative influence of words and timestamps
along the inference process. We have tested our models on two datasets: a
collection of over 200 years of US state-of-the-union (SOTU) addresses and a
large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that
WBToT captures events better than Latent Dirichlet Allocation and other SOTA
topic models like BERTopic: the median absolute deviation of the topic presence
over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also
demonstrate the superior coherence of WBToT over BToT, which highlights the
importance of balancing the time and word modalities. Finally, we illustrate
the stability of the online optimization algorithm in WBToT, which allows the
application of WBToT to problems that are intractable for standard ToT.

</details>

### [64] [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
*Saffron Huang,Esin Durmus,Miles McCain,Kunal Handa,Alex Tamkin,Jerry Hong,Michael Stern,Arushi Somani,Xiuruo Zhang,Deep Ganguli*

Main category: cs.CL

TLDR: 论文通过实证方法提取了Claude 3和3.5模型在真实交互中展现的3,307种AI价值观，发现其支持亲社会价值观且具有情境依赖性。


<details>
  <summary>Details</summary>
Motivation: 研究AI助手在实践中依赖的价值观，填补了相关实证研究的空白。

Method: 采用自下而上、保护隐私的方法，分析数十万次真实交互数据。

Result: 发现Claude模型表达了许多实用和认知价值观，支持亲社会价值观，且价值观随情境变化。

Conclusion: 为AI系统的价值观评估和设计提供了实证基础。

Abstract: AI assistants can impart value judgments that shape people's decisions and
worldviews, yet little is known empirically about what values these systems
rely on in practice. To address this, we develop a bottom-up,
privacy-preserving method to extract the values (normative considerations
stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit
in hundreds of thousands of real-world interactions. We empirically discover
and taxonomize 3,307 AI values and study how they vary by context. We find that
Claude expresses many practical and epistemic values, and typically supports
prosocial human values while resisting values like "moral nihilism". While some
values appear consistently across contexts (e.g. "transparency"), many are more
specialized and context-dependent, reflecting the diversity of human
interlocutors and their varied contexts. For example, "harm prevention" emerges
when Claude resists users, "historical accuracy" when responding to queries
about controversial events, "healthy boundaries" when asked for relationship
advice, and "human agency" in technology ethics discussions. By providing the
first large-scale empirical mapping of AI values in deployment, our work
creates a foundation for more grounded evaluation and design of values in AI
systems.

</details>

### [65] [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
*Yahan Yang,Soham Dan,Shuo Li,Dan Roth,Insup Lee*

Main category: cs.CL

TLDR: 提出了一种多语言防护栏方法，通过合成数据、监督微调和GRPO框架提升性能，有效检测和过滤多语言不安全内容。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下LLMs易受攻击，且安全对齐数据有限，需开发能跨语言检测不安全内容的防护栏。

Method: 包括合成多语言数据、监督微调和GRPO框架优化。

Result: 在多语言环境下表现优于基线，并能生成多语言解释。

Conclusion: 该方法为多语言内容审核提供了有效解决方案，尤其适用于语言特定风险和歧义的理解。

Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as
jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability
is exacerbated in multilingual setting, where multilingual safety-aligned data
are often limited. Thus, developing a guardrail capable of detecting and
filtering unsafe content across diverse languages is critical for deploying
LLMs in real-world applications. In this work, we propose an approach to build
a multilingual guardrail with reasoning. Our method consists of: (1) synthetic
multilingual data generation incorporating culturally and linguistically
nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group
Relative Policy Optimization (GRPO) framework that further improves
performance. Experimental results demonstrate that our multilingual guardrail
consistently outperforms recent baselines across both in-domain and
out-of-domain languages. The multilingual reasoning capability of our guardrail
enables it to generate multilingual explanations, which are particularly useful
for understanding language-specific risks and ambiguities in multilingual
content moderation.

</details>

### [66] [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
*Yilun Zhou,Austin Xu,Peifeng Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TLDR: 论文提出了JETTS基准，用于评估LLM-judges在测试时扩展设置中的表现，发现其在重排序任务中表现良好，但在束搜索和基于批判的响应优化中不如过程奖励模型。


<details>
  <summary>Details</summary>
Motivation: 研究LLM-judges作为评估器在测试时扩展设置中的有效性，填补现有研究的空白。

Method: 引入JETTS基准，评估10种不同规模的judge模型在三个任务设置（重排序、束搜索、批判优化）中的表现。

Result: LLM-judges在重排序中与结果奖励模型相当，但在束搜索中不如过程奖励模型；批判优化效果不佳。

Conclusion: LLM-judges在特定任务中有潜力，但批判优化功能仍需改进。

Abstract: Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding](https://arxiv.org/abs/2504.13915)
*Dibyadip Chatterjee,Edoardo Remelli,Yale Song,Bugra Tekin,Abhay Mittal,Bharat Bhatnagar,Necati Cihan Camgöz,Shreyas Hampali,Eric Sauser,Shugao Ma,Angela Yao,Fadime Sener*

Main category: cs.CV

TLDR: ProVideLLM是一个端到端的实时程序视频理解框架，通过多模态缓存和优化的令牌设计，显著降低了计算和内存需求，并在多个任务上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长时观察时计算和内存需求高，且难以同时捕捉细粒度细节。ProVideLLM旨在解决这些问题。

Method: 结合文本令牌和视觉令牌的多模态缓存设计，使用DETR-QFormer编码视觉细节，减少令牌数量并优化性能。

Result: 令牌数量减少22倍，支持10 FPS的逐帧推理和25 FPS的流式对话，仅需2GB GPU内存，并在六个任务上达到SOTA。

Conclusion: ProVideLLM通过创新的多模态缓存设计，实现了高效的长时视频理解，为实时应用提供了可行方案。

Abstract: We introduce ProVideLLM, an end-to-end framework for real-time procedural
video understanding. ProVideLLM integrates a multimodal cache configured to
store two types of tokens - verbalized text tokens, which provide compressed
textual summaries of long-term observations, and visual tokens, encoded with
DETR-QFormer to capture fine-grained details from short-term observations. This
design reduces token count by 22x over existing methods in representing one
hour of long-term observations while effectively encoding fine-granularity of
the present. By interleaving these tokens in our multimodal cache, ProVideLLM
ensures sub-linear scaling of memory and compute with video length, enabling
per-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with
a minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art
results on six procedural tasks across four datasets.

</details>

### [68] [Entropy Rectifying Guidance for Diffusion and Flow Models](https://arxiv.org/abs/2504.13987)
*Tariq Berrada Ifriqi,Adriana Romero-Soriano,Michal Drozdzal,Jakob Verbeek,Karteek Alahari*

Main category: cs.CV

TLDR: 本文提出了一种名为熵校正引导（ERG）的新方法，通过调整扩散变换器架构中的注意力机制，在推理时同时提升图像质量、多样性和提示一致性，优于现有的无分类器引导（CFG）技术。


<details>
  <summary>Details</summary>
Motivation: 现有的无分类器引导（CFG）技术在提升图像质量时会牺牲多样性和一致性，且现有改进方法需要额外模型或更多计算开销。本文旨在解决这些问题。

Method: 提出熵校正引导（ERG），通过调整扩散变换器架构中的注意力机制，在推理时优化生成过程。ERG无需额外模型，且适用于无条件采样。

Result: ERG在文本到图像、类条件生成和无条件生成等任务中显著提升了性能，并能与其他引导方法（如CADS和APG）无缝结合。

Conclusion: ERG是一种简单有效的引导机制，能够同时优化图像质量、多样性和一致性，且具有广泛适用性。

Abstract: Guidance techniques are commonly used in diffusion and flow models to improve
image quality and consistency for conditional generative tasks such as
class-conditional and text-to-image generation. In particular, classifier-free
guidance (CFG) -- the most widely adopted guidance technique -- contrasts
conditional and unconditional predictions to improve the generated images. This
results, however, in trade-offs across quality, diversity and consistency,
improving some at the expense of others. While recent work has shown that it is
possible to disentangle these factors to some extent, such methods come with an
overhead of requiring an additional (weaker) model, or require more forward
passes per sampling step. In this paper, we propose Entropy Rectifying Guidance
(ERG), a simple and effective guidance mechanism based on inference-time
changes in the attention mechanism of state-of-the-art diffusion transformer
architectures, which allows for simultaneous improvements over image quality,
diversity and prompt consistency. ERG is more general than CFG and similar
guidance techniques, as it extends to unconditional sampling. ERG results in
significant improvements in various generation tasks such as text-to-image,
class-conditional and unconditional image generation. We also show that ERG can
be seamlessly combined with other recent guidance methods such as CADS and APG,
further boosting generation performance.

</details>

### [69] [Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training](https://arxiv.org/abs/2504.13995)
*Andrea Amaduzzi,Pierluigi Zama Ramirez,Giuseppe Lisanti,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TLDR: LLaNA是首个能够直接处理NeRF MLP权重的多模态大语言模型，支持NeRF标注和问答任务，无需渲染图像或生成3D结构。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在理解图像和3D数据时存在几何和外观表达局限，NeRF作为替代方案具有潜力。

Method: 通过直接处理NeRF的MLP权重，构建LLaNA模型，并创建大规模NeRF-语言数据集进行训练和评估。

Result: 直接处理NeRF权重在NeRF-语言任务上优于依赖2D或3D表示的方法。

Conclusion: LLaNA展示了直接利用NeRF权重的可行性，为多模态理解提供了新方向。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown
remarkable capabilities in understanding both images and 3D data, yet these
modalities face inherent limitations in comprehensively representing object
geometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a
promising alternative, encoding both geometric and photorealistic properties
within the weights of a simple Multi-Layer Perceptron (MLP). This work
investigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.
We introduce LLaNA, the first MLLM able to perform new tasks such as NeRF
captioning and Q\&A, by directly processing the weights of a NeRF's MLP.
Notably, LLaNA is able to extract information about the represented objects
without the need to render images or materialize 3D data structures. In
addition, we build the first large-scale NeRF-language dataset, composed by
more than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual
annotations that enable various NeRF-language tasks. Based on this dataset, we
develop a benchmark to evaluate the NeRF understanding capability of our
method. Results show that directly processing NeRF weights leads to better
performance on NeRF-Language tasks compared to approaches that rely on either
2D or 3D representations derived from NeRFs.

</details>

### [70] [Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation](https://arxiv.org/abs/2504.14011)
*Fulvio Sanguigni,Davide Morelli,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TLDR: 论文提出了一种名为Fashion-RAG的新方法，通过文本输入实现时尚物品的定制化生成，结合检索和生成技术，显著提升了虚拟试穿的效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法通常需要特定服装输入，而用户可能仅提供文本描述，限制了实际应用。

Method: 采用检索增强生成（Fashion-RAG），通过文本检索匹配的服装，并将其投影到Stable Diffusion的文本嵌入空间，生成个性化图像。

Result: 在Dress Code数据集上，Fashion-RAG在质量和数量上均优于现有方法，能有效捕捉服装细节。

Conclusion: Fashion-RAG是首个针对多模态时尚图像编辑的检索增强生成方法，为文本驱动的时尚定制提供了新思路。

Abstract: In recent years, the fashion industry has increasingly adopted AI
technologies to enhance customer experience, driven by the proliferation of
e-commerce platforms and virtual applications. Among the various tasks, virtual
try-on and multimodal fashion image editing -- which utilizes diverse input
modalities such as text, garment sketches, and body poses -- have become a key
area of research. Diffusion models have emerged as a leading approach for such
generative tasks, offering superior image quality and diversity. However, most
existing virtual try-on methods rely on having a specific garment input, which
is often impractical in real-world scenarios where users may only provide
textual specifications. To address this limitation, in this work we introduce
Fashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that
enables the customization of fashion items based on user preferences provided
in textual form. Our approach retrieves multiple garments that match the input
specifications and generates a personalized image by incorporating attributes
from the retrieved items. To achieve this, we employ textual inversion
techniques, where retrieved garment images are projected into the textual
embedding space of the Stable Diffusion text encoder, allowing seamless
integration of retrieved elements into the generative process. Experimental
results on the Dress Code dataset demonstrate that Fashion-RAG outperforms
existing methods both qualitatively and quantitatively, effectively capturing
fine-grained visual details from retrieved garments. To the best of our
knowledge, this is the first work to introduce a retrieval-augmented generation
approach specifically tailored for multimodal fashion image editing.

</details>

### [71] [LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models](https://arxiv.org/abs/2504.14032)
*Haiwen Huang,Anpei Chen,Volodymyr Havrylov,Andreas Geiger,Dan Zhang*

Main category: cs.CV

TLDR: 本文提出了一种基于坐标交叉注意力Transformer的特征上采样方法，通过改进上采样器架构和训练目标，显著提升了像素级理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型（如DINOv2和CLIP）在像素级任务中因特征分辨率有限而表现不佳，特征上采样是解决这一问题的关键方向。

Method: 引入基于坐标的交叉注意力Transformer架构，结合高分辨率图像、坐标和低分辨率VFM特征；提出利用类无关掩码和自蒸馏构建高分辨率伪真值特征的训练目标。

Result: 实验表明，该方法在多种下游任务中显著优于现有特征上采样技术。

Conclusion: 该方法能有效捕捉细粒度细节并适应不同输入和特征分辨率，为像素级任务提供了高效解决方案。

Abstract: Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved
impressive results on various downstream tasks, but their limited feature
resolution hampers performance in applications requiring pixel-level
understanding. Feature upsampling offers a promising direction to address this
challenge. In this work, we identify two critical factors for enhancing feature
upsampling: the upsampler architecture and the training objective. For the
upsampler architecture, we introduce a coordinate-based cross-attention
transformer that integrates the high-resolution images with coordinates and
low-resolution VFM features to generate sharp, high-quality features. For the
training objective, we propose constructing high-resolution pseudo-groundtruth
features by leveraging class-agnostic masks and self-distillation. Our approach
effectively captures fine-grained details and adapts flexibly to various input
and feature resolutions. Through experiments, we demonstrate that our approach
significantly outperforms existing feature upsampling techniques across various
downstream tasks. Our code is released at https://github.com/andrehuang/loftup.

</details>

### [72] [Occlusion-Ordered Semantic Instance Segmentation](https://arxiv.org/abs/2504.14054)
*Soroosh Baselizadeh,Cheuk-To Yu,Olga Veksler,Yuri Boykov*

Main category: cs.CV

TLDR: 论文提出了一种名为OOSIS的任务，结合相对深度排序和实例分割，利用遮挡信息提供3D分析，优于传统的绝对深度方法。


<details>
  <summary>Details</summary>
Motivation: 传统的语义实例分割仅提供2D信息，而结合绝对深度估计的3D分析任务困难且不准确。因此，作者希望通过更简单的遮挡相对深度排序任务，提供更可靠的3D信息。

Method: 提出OOSIS任务，通过定向遮挡边界和语义分割同时提取实例及其遮挡顺序。采用标记问题形式，简化了框架。

Result: 在KINS和COCOA数据集上表现优于基线方法，定向遮挡边界方法显著优于先前工作。

Conclusion: OOSIS通过结合遮挡排序和实例分割，提供了一种简单有效的3D信息分析方法，优于传统方法。

Abstract: Standard semantic instance segmentation provides useful, but inherently 2D
information from a single image. To enable 3D analysis, one usually integrates
absolute monocular depth estimation with instance segmentation. However,
monocular depth is a difficult task. Instead, we leverage a simpler
single-image task, occlusion-based relative depth ordering, providing coarser
but useful 3D information. We show that relative depth ordering works more
reliably from occlusions than from absolute depth. We propose to solve the
joint task of relative depth ordering and segmentation of instances based on
occlusions. We call this task Occlusion-Ordered Semantic Instance Segmentation
(OOSIS). We develop an approach to OOSIS that extracts instances and their
occlusion order simultaneously from oriented occlusion boundaries and semantic
segmentation. Unlike popular detect-and-segment framework for instance
segmentation, combining occlusion ordering with instance segmentation allows a
simple and clean formulation of OOSIS as a labeling problem. As a part of our
solution for OOSIS, we develop a novel oriented occlusion boundaries approach
that significantly outperforms prior work. We also develop a new joint OOSIS
metric based both on instance mask accuracy and correctness of their occlusion
order. We achieve better performance than strong baselines on KINS and COCOA
datasets.

</details>

### [73] [Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design](https://arxiv.org/abs/2504.14075)
*Wei Dong,Yan Min,Han Zhou,Jun Chen*

Main category: cs.CV

TLDR: SG-LLIE是一种基于结构先验的多尺度CNN-Transformer混合框架，用于低光图像增强，通过结合CNN和Transformer的优势，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法在极端低光环境下效果不佳，主要由于问题的病态性和语义提取困难。

Method: 提出SG-LLIE框架，利用光照不变边缘检测器提取结构先验，并在UNet架构中引入CNN-Transformer混合模块（HSGFE）和结构引导Transformer块（SGTB）。

Result: 在多个低光图像增强基准测试中取得最优性能，并在NTIRE 2025挑战赛中排名第二。

Conclusion: SG-LLIE通过结合结构先验和多尺度特征提取，显著提升了低光图像增强的效果。

Abstract: Current Low-light Image Enhancement (LLIE) techniques predominantly rely on
either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from
semantic features or illumination maps. Nonetheless, the intrinsic
ill-posedness of LLIE and the difficulty in retrieving robust semantics from
heavily corrupted images hinder their effectiveness in extremely low-light
environments. To tackle this challenge, we present SG-LLIE, a new multi-scale
CNN-Transformer hybrid framework guided by structure priors. Different from
employing pre-trained models for the extraction of semantics or illumination
maps, we choose to extract robust structure priors based on
illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer
Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in
the UNet encoder-decoder architecture. Besides the CNN blocks which excels in
multi-scale feature extraction and fusion, we introduce a Structure-Guided
Transformer Block (SGTB) in each HSGFE that incorporates structural priors to
modulate the enhancement process. Extensive experiments show that our method
achieves state-of-the-art performance on several LLIE benchmarks in both
quantitative metrics and visual quality. Our solution ranks second in the NTIRE
2025 Low-Light Enhancement Challenge. Code is released at
https://github.com/minyan8/imagine.

</details>

### [74] [Retinex-guided Histogram Transformer for Mask-free Shadow Removal](https://arxiv.org/abs/2504.14092)
*Wei Dong,Han Zhou,Seyed Amirreza Mousavi,Jun Chen*

Main category: cs.CV

TLDR: 提出了一种基于混合CNN-Transformer架构的无掩模阴影去除框架ReHiT，通过Retinex理论指导，实现了高效且泛化性强的阴影去除。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖难以获取的阴影掩模，限制了其在真实场景中的泛化能力。

Method: 采用双分支管道分别建模反射和光照分量，结合CNN-Transformer模块（IG-HCT）和光照引导直方图Transformer块（IGHB）处理非均匀光照和复杂阴影。

Result: 在多个基准数据集上表现优于现有无掩模方法，参数少且推理速度快，适用于计算资源有限的实际应用。

Conclusion: ReHiT框架在阴影去除任务中表现出色，兼具高效性和实用性。

Abstract: While deep learning methods have achieved notable progress in shadow removal,
many existing approaches rely on shadow masks that are difficult to obtain,
limiting their generalization to real-world scenes. In this work, we propose
ReHiT, an efficient mask-free shadow removal framework based on a hybrid
CNN-Transformer architecture guided by Retinex theory. We first introduce a
dual-branch pipeline to separately model reflectance and illumination
components, and each is restored by our developed Illumination-Guided Hybrid
CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are
capable of learning residual dense features and performing multi-scale semantic
fusion, multi-scale semantic fusion, we develop the Illumination-Guided
Histogram Transformer Block (IGHB) to effectively handle non-uniform
illumination and spatially complex shadows. Extensive experiments on several
benchmark datasets validate the effectiveness of our approach over existing
mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge
dataset, our solution delivers competitive results with one of the smallest
parameter sizes and fastest inference speeds among top-ranked entries,
highlighting its applicability for real-world applications with limited
computational resources. The code is available at
https://github.com/dongw22/oath.

</details>

### [75] [VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment](https://arxiv.org/abs/2504.14096)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TLDR: VideoPASTA通过偏好优化增强视频语言模型，解决空间、时间和跨帧关系问题，仅需少量数据即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在空间关系、时间顺序和跨帧连续性方面表现不足，需要改进。

Method: 采用VideoPASTA框架，通过对抗样本训练模型区分准确与错误的视频表示，使用Direct Preference Optimization优化偏好。

Result: 在多个基准测试中性能显著提升（如VideoMME提升3.05%），且无需大量数据或复杂架构。

Conclusion: VideoPASTA提供了一种高效、可扩展的解决方案，无需额外标注即可提升模型性能。

Abstract: Video-language models (Video-LLMs) excel at understanding video content but
struggle with spatial relationships, temporal ordering, and cross-frame
continuity. To address these limitations, we introduce VideoPASTA (Preference
Alignment with Spatio-Temporal-Cross Frame Adversaries), a framework that
enhances Video-LLMs through targeted preference optimization. VideoPASTA trains
models to distinguish accurate video representations from carefully generated
adversarial examples that deliberately violate spatial, temporal, or
cross-frame relations. By applying Direct Preference Optimization to just 7,020
preference pairs, VideoPASTA learns robust representations that capture
fine-grained spatial relationships and long-range temporal dynamics.
Experiments on standard video benchmarks show significant relative performance
gains of 3.05% on VideoMME, 1.97% on NeXTQA, and 1.31% on LongVideoBench, over
the baseline Qwen2.5-VL model. These results demonstrate that targeted
alignment, rather than massive pretraining or architectural modifications,
effectively addresses core video-language challenges. Notably, VideoPASTA
achieves these improvements without human annotation or captioning, relying on
just 32-frame sampling, compared to the 96-frame, multi-GPU setups of prior
work. This efficiency makes our approach a scalable, plug-and-play solution
that seamlessly integrates with existing models while preserving their
capabilities.

</details>

### [76] [Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models](https://arxiv.org/abs/2504.14108)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang,Yuelong Xia*

Main category: cs.CV

TLDR: DanceText是一个无需训练的多语言图像文本编辑框架，支持复杂几何变换并实现前景与背景无缝融合。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的生成模型在文本引导的图像合成中缺乏可控性，且难以保持布局一致性，尤其是在旋转、平移、缩放和扭曲等非平凡操作下。

Method: DanceText采用分层编辑策略，将文本与背景分离，以模块化和可控的方式执行几何变换，并通过深度感知模块增强真实感和空间一致性。

Result: 在AnyWord-3M基准测试中，DanceText在视觉质量上表现优异，尤其是在大规模和复杂变换场景下。

Conclusion: DanceText通过无需训练的设计和预训练模块的集成，实现了灵活部署和高性能的文本编辑。

Abstract: We present DanceText, a training-free framework for multilingual text editing
in images, designed to support complex geometric transformations and achieve
seamless foreground-background integration. While diffusion-based generative
models have shown promise in text-guided image synthesis, they often lack
controllability and fail to preserve layout consistency under non-trivial
manipulations such as rotation, translation, scaling, and warping. To address
these limitations, DanceText introduces a layered editing strategy that
separates text from the background, allowing geometric transformations to be
performed in a modular and controllable manner. A depth-aware module is further
proposed to align appearance and perspective between the transformed text and
the reconstructed background, enhancing photorealism and spatial consistency.
Importantly, DanceText adopts a fully training-free design by integrating
pretrained modules, allowing flexible deployment without task-specific
fine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that
our method achieves superior performance in visual quality, especially under
large-scale and complex transformation scenarios.

</details>

### [77] [Lightweight Road Environment Segmentation using Vector Quantization](https://arxiv.org/abs/2504.14113)
*Jiyong Kwag,Alper Yilmaz,Charles Toth*

Main category: cs.CV

TLDR: 论文提出了一种基于向量量化的方法，用于自动驾驶环境中的道路分割，通过离散化特征表示提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于FCN和Transformer的方法依赖连续特征表示，限制了离散信息的表达能力，因此需要一种新方法来提升分割效果。

Method: 结合向量量化与轻量级分割模型MobileUNETR，将连续特征映射为离散向量，优化特征表示。

Result: 在Cityscapes数据集上达到77.0% mIoU，比基线模型提升2.9%，且未增加模型复杂度。

Conclusion: 向量量化能有效提升道路环境分割的准确性和结构化特征表示，为自动驾驶提供更优解决方案。

Abstract: Road environment segmentation plays a significant role in autonomous driving.
Numerous works based on Fully Convolutional Networks (FCNs) and Transformer
architectures have been proposed to leverage local and global contextual
learning for efficient and accurate semantic segmentation. In both
architectures, the encoder often relies heavily on extracting continuous
representations from the image, which limits the ability to represent
meaningful discrete information. To address this limitation, we propose
segmentation of the autonomous driving environment using vector quantization.
Vector quantization offers three primary advantages for road environment
segmentation. (1) Each continuous feature from the encoder is mapped to a
discrete vector from the codebook, helping the model discover distinct features
more easily than with complex continuous features. (2) Since a discrete feature
acts as compressed versions of the encoder's continuous features, they also
compress noise or outliers, enhancing the image segmentation task. (3) Vector
quantization encourages the latent space to form coarse clusters of continuous
features, forcing the model to group similar features, making the learned
representations more structured for the decoding process. In this work, we
combined vector quantization with the lightweight image segmentation model
MobileUNETR and used it as a baseline model for comparison to demonstrate its
efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,
outperforming the baseline by 2.9 % without increasing the model's initial size
or complexity.

</details>

### [78] [BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution](https://arxiv.org/abs/2504.14129)
*Yaning Zhang,Jiahe Zhang,Chunjie Ma,Weili Guan,Tian Gan,Zan Gao*

Main category: cs.CV

TLDR: 提出了一种双模态引导的多视角表示学习框架（BMRL），用于零样本深度伪造溯源（ZS-DFA），通过视觉、解析和语言模态的综合利用，提升了对未见生成器的溯源能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造溯源方法主要关注视觉模态，忽视了文本和面部解析等其他模态的作用，且难以在细粒度上评估对未见生成器的泛化性能。

Method: 设计了多视角视觉编码器（MPVE）探索图像、噪声和边缘三种视角的伪造特征；提出解析编码器和语言编码器，分别通过视觉-解析匹配和视觉-语言对齐学习表示；引入深度伪造对比中心损失（DFACC）优化模型。

Result: 实验表明，该方法在ZS-DFA任务上优于现有技术。

Conclusion: BMRL框架通过多模态和多视角学习，显著提升了深度伪造溯源的泛化能力和准确性。

Abstract: The challenge of tracing the source attribution of forged faces has gained
significant attention due to the rapid advancement of generative models.
However, existing deepfake attribution (DFA) works primarily focus on the
interaction among various domains in vision modality, and other modalities such
as texts and face parsing are not fully explored. Besides, they tend to fail to
assess the generalization performance of deepfake attributors to unseen
generators in a fine-grained manner. In this paper, we propose a novel bi-modal
guided multi-perspective representation learning (BMRL) framework for zero-shot
deepfake attribution (ZS-DFA), which facilitates effective traceability to
unseen generators. Specifically, we design a multi-perspective visual encoder
(MPVE) to explore general deepfake attribution visual characteristics across
three views (i.e., image, noise, and edge). We devise a novel parsing encoder
to focus on global face attribute embeddings, enabling parsing-guided DFA
representation learning via vision-parsing matching. A language encoder is
proposed to capture fine-grained language embeddings, facilitating
language-guided general visual forgery representation learning through
vision-language alignment. Additionally, we present a novel deepfake
attribution contrastive center (DFACC) loss, to pull relevant generators closer
and push irrelevant ones away, which can be introduced into DFA models to
enhance traceability. Experimental results demonstrate that our method
outperforms the state-of-the-art on the ZS-DFA task through various protocols
evaluation.

</details>

### [79] [Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing](https://arxiv.org/abs/2504.14131)
*Ole-Christian Galbo Engstrøm,Michela Albano-Gaglio,Erik Schou Dreier,Yamine Bouzembrak,Maria Font-i-Furnols,Puneet Mishra,Kim Steenstrup Pedersen*

Main category: cs.CV

TLDR: 本文提出了一种基于改进U-Net和自定义损失函数的端到端深度学习方法，用于直接从高光谱图像生成化学图谱，相比传统的PLS回归方法，U-Net在预测精度和空间相关性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统基于PLS回归的化学图谱生成方法忽略了空间上下文且噪声较高，因此需要一种更高效且准确的方法。

Method: 使用改进的U-Net和自定义损失函数，直接从高光谱图像生成化学图谱，跳过传统像素级分析的中间步骤。

Result: U-Net在平均脂肪预测任务上的均方根误差比PLS低9%-13%，且生成的化学图谱99.91%的方差具有空间相关性，而PLS仅为2.53%。

Conclusion: U-Net在化学图谱生成任务上优于PLS回归，能够生成更精确且物理合理的图谱。

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>

### [80] [HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked Autoencoder for 3D Point Cloud Analysis](https://arxiv.org/abs/2504.14132)
*Xuanhua Yin,Dingxin Zhang,Jianhui Yu,Weidong Cai*

Main category: cs.CV

TLDR: HFBRI-MAE是一种改进的自监督学习框架，通过引入旋转不变的手工特征，解决了现有MAE方法在旋转点云处理中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于MAE的自监督学习方法缺乏旋转不变性，导致在现实场景中处理任意旋转的点云时性能显著下降。

Method: HFBRI-MAE结合旋转不变的局部和全局特征进行标记嵌入和位置嵌入，并重新定义重建目标为规范对齐版本。

Result: 在ModelNet40、ScanObjectNN和ShapeNetPart上的实验表明，HFBRI-MAE在分类、分割和少样本学习中优于现有方法。

Conclusion: HFBRI-MAE展示了在真实3D应用中的鲁棒性和强泛化能力。

Abstract: Self-supervised learning (SSL) has demonstrated remarkable success in 3D
point cloud analysis, particularly through masked autoencoders (MAEs). However,
existing MAE-based methods lack rotation invariance, leading to significant
performance degradation when processing arbitrarily rotated point clouds in
real-world scenarios. To address this limitation, we introduce Handcrafted
Feature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel
framework that refines the MAE design with rotation-invariant handcrafted
features to ensure stable feature learning across different orientations. By
leveraging both rotation-invariant local and global features for token
embedding and position embedding, HFBRI-MAE effectively eliminates rotational
dependencies while preserving rich geometric structures. Additionally, we
redefine the reconstruction target to a canonically aligned version of the
input, mitigating rotational ambiguities. Extensive experiments on ModelNet40,
ScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently
outperforms existing methods in object classification, segmentation, and
few-shot learning, highlighting its robustness and strong generalization
ability in real-world 3D applications.

</details>

### [81] [Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach](https://arxiv.org/abs/2504.14137)
*Hangyu Liu,Bo Peng,Pengxiang Ding,Donglin Wang*

Main category: cs.CV

TLDR: 本文提出了一种名为2D-TGAF的多目标对抗攻击框架，通过扩散模型生成二维语义张量指导噪声生成，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法缺乏对目标标签在噪声生成中作用的实践验证和总结，本文填补了这一空白。

Method: 提出2D-TGAF框架，利用扩散模型生成二维语义张量，并设计掩码策略确保噪声保留目标类别的完整语义信息。

Result: 在ImageNet数据集上的实验表明，2D-TGAF在攻击成功率和防御机制下的表现均优于现有方法。

Conclusion: 2D-TGAF通过优化语义特征质量和数量，显著提升了多目标对抗攻击的效果。

Abstract: Compared to single-target adversarial attacks, multi-target attacks have
garnered significant attention due to their ability to generate adversarial
images for multiple target classes simultaneously. Existing generative
approaches for multi-target attacks mainly analyze the effect of the use of
target labels on noise generation from a theoretical perspective, lacking
practical validation and comprehensive summarization. To address this gap, we
first identify and validate that the semantic feature quality and quantity are
critical factors affecting the transferability of targeted attacks: 1) Feature
quality refers to the structural and detailed completeness of the implanted
target features, as deficiencies may result in the loss of key discriminative
information; 2) Feature quantity refers to the spatial sufficiency of the
implanted target features, as inadequacy limits the victim model's attention to
this feature. Based on these findings, we propose the 2D Tensor-Guided
Adversarial Fusion (2D-TGAF) framework, which leverages the powerful generative
capabilities of diffusion models to encode target labels into two-dimensional
semantic tensors for guiding adversarial noise generation. Additionally, we
design a novel masking strategy tailored for the training process, ensuring
that parts of the generated noise retain complete semantic information about
the target class. Extensive experiments on the standard ImageNet dataset
demonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in
attack success rates, both on normally trained models and across various
defense mechanisms.

</details>

### [82] [Segment Any Crack: Deep Semantic Segmentation Adaptation for Crack Detection](https://arxiv.org/abs/2504.14138)
*Ghodsiyeh Rostami,Po-Han Chen,Mahdi S. Hosseini*

Main category: cs.CV

TLDR: 该论文提出了一种选择性微调策略，专注于调整归一化组件，以提高分割模型在裂缝检测中的适应性。该方法在性能和计算效率上优于完全微调和其他常见微调技术。


<details>
  <summary>Details</summary>
Motivation: 现有裂缝检测模型需要大量标注数据和计算成本，限制了其适应性。

Method: 选择性微调归一化参数，应用于SAM和五种分割模型。

Result: 在OmniCrack30k数据集上，SAC模型达到61.22% F1分数和44.13% IoU，零样本数据集中表现最佳且标准差最低。

Conclusion: 该方法显著提高了分割准确性并降低了计算开销。

Abstract: Image-based crack detection algorithms are increasingly in demand in
infrastructure monitoring, as early detection of cracks is of paramount
importance for timely maintenance planning. While deep learning has
significantly advanced crack detection algorithms, existing models often
require extensive labeled datasets and high computational costs for
fine-tuning, limiting their adaptability across diverse conditions. This study
introduces an efficient selective fine-tuning strategy, focusing on tuning
normalization components, to enhance the adaptability of segmentation models
for crack detection. The proposed method is applied to the Segment Anything
Model (SAM) and five well-established segmentation models. Experimental results
demonstrate that selective fine-tuning of only normalization parameters
outperforms full fine-tuning and other common fine-tuning techniques in both
performance and computational efficiency, while improving generalization. The
proposed approach yields a SAM-based model, Segment Any Crack (SAC), achieving
a 61.22\% F1-score and 44.13\% IoU on the OmniCrack30k benchmark dataset, along
with the highest performance across three zero-shot datasets and the lowest
standard deviation. The results highlight the effectiveness of the adaptation
approach in improving segmentation accuracy while significantly reducing
computational overhead.

</details>

### [83] [ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification](https://arxiv.org/abs/2504.14139)
*Hai Pham-Ngoc,De Nguyen-Van,Dung Vu-Tien,Phuong Le-Hong*

Main category: cs.CV

TLDR: 开发了一种高效、可解释的深度学习系统，用于甲状腺细针穿刺活检图像的多类分类，具有高准确性和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺FNAB图像分类中的数据不足、观察者间差异和计算成本高的问题，为临床决策提供支持。

Method: 结合YOLOv10细胞簇检测、课程学习协议、轻量级EfficientNetB0和Transformer模块，进行多尺度多区域分析。

Result: 内部测试集宏F1为89.19%，外部验证AUC分别为0.9495（B2）、0.7436（B5）和0.8396（B6）。系统处理速度快，具有可解释性。

Conclusion: 高准确性、可解释的甲状腺FNAB图像分类在低计算需求下可行，适合临床推广。

Abstract: Background: Automated classification of thyroid fine needle aspiration biopsy
(FNAB) images faces challenges in limited data, inter-observer variability, and
computational cost. Efficient, interpretable models are crucial for clinical
support. Objective: To develop and externally validate a deep learning system
for the multi-class classification of thyroid FNAB images into three key
categories that directly guide post-biopsy treatment decisions in Vietnam:
benign (B2), suspicious for malignancy (B5), and malignant (B6), while
achieving high diagnostic accuracy with low computational overhead. Methods:
Our framework features: (1) YOLOv10-based cell cluster detection for
informative sub-region extraction and noise reduction; (2) a curriculum
learning-inspired protocol sequencing localized crops to full images for
multi-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4
millions parameters) selection balancing performance and efficiency; and (4) a
Transformer-inspired module for multi-scale, multi-region analysis. External
validation used 1,015 independent FNAB images. Results: ThyroidEffi Basic
achieved a macro F1 of 89.19\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)
on the internal test set. External validation yielded AUCs of 0.9495 (B2),
0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\%.
Grad-CAM highlighted key diagnostic regions, confirming interpretability. The
system processed 1000 cases in 30 seconds, demonstrating feasibility on widely
accessible hardware like a 12-core CPU. Conclusions: This work demonstrates
that high-accuracy, interpretable thyroid FNAB image classification is
achievable with minimal computational demands.

</details>

### [84] [Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D](https://arxiv.org/abs/2504.14151)
*Sergio Arnaud,Paul McVay,Ada Martin,Arjun Majumdar,Krishna Murthy Jatavallabhula,Phillip Thomas,Ruslan Partsey,Daniel Dugas,Abha Gejji,Alexander Sax,Vincent-Pierre Berges,Mikael Henaff,Ayush Jain,Ang Cao,Ishita Prasad,Mrinal Kalakrishnan,Michael Rabbat,Nicolas Ballas,Mido Assran,Oleksandr Maksymets,Aravind Rajeswaran,Franziska Meier*

Main category: cs.CV

TLDR: LOCATE 3D是一种通过描述性语言在3D场景中定位物体的模型，结合了3D-JEPA自监督学习算法和语言条件解码器，实现了高性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景中基于语言描述的物体定位问题，并提升模型的泛化能力和实际部署能力。

Method: 使用3D-JEPA自监督学习算法处理3D点云数据，结合2D基础模型（CLIP、DINO）提取特征，通过掩码预测任务学习上下文特征，并联合训练语言条件解码器预测3D掩码和边界框。

Result: 在标准基准测试中达到新最优性能，并展示了强大的泛化能力，适用于机器人和AR设备的实际部署。

Conclusion: LOCATE 3D通过创新的3D-JEPA算法和新的数据集，显著提升了3D物体定位的性能和泛化能力。

Abstract: We present LOCATE 3D, a model for localizing objects in 3D scenes from
referring expressions like "the small coffee table between the sofa and the
lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding
benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D
operates directly on sensor observation streams (posed RGB-D frames), enabling
real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,
a novel self-supervised learning (SSL) algorithm applicable to sensor point
clouds. It takes as input a 3D pointcloud featurized using 2D foundation models
(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a
pretext task to aid the self-supervised learning of contextualized pointcloud
features. Once trained, the 3D-JEPA encoder is finetuned alongside a
language-conditioned decoder to jointly predict 3D masks and bounding boxes.
Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential
grounding, spanning multiple capture setups with over 130K annotations. This
enables a systematic study of generalization capabilities as well as a stronger
model.

</details>

### [85] [Segregation and Context Aggregation Network for Real-time Cloud Segmentation](https://arxiv.org/abs/2504.14178)
*Yijie Li,Hewei Wang,Jiayi Zhang,Jinjiang You,Jinfeng Xu,Puzhen Wu,Yunzhong Xiao,Soumyabrata Dev*

Main category: cs.CV

TLDR: SCANet是一种轻量级云分割模型，通过Segregation and Context Aggregation Module（SCAM）提升分割精度和计算效率，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分割精度和计算效率之间难以平衡，限制了在边缘设备上的实际应用。

Method: SCANet采用SCAM模块，将粗略分割图细化为加权天空和云特征，分别处理。

Result: SCANet-large参数减少70.9%，性能媲美现有方法；SCANet-lite达到1390 fps（FP16），远超实时标准。

Conclusion: SCANet在保持高性能的同时显著降低计算复杂度，适用于实际部署。

Abstract: Cloud segmentation from intensity images is a pivotal task in atmospheric
science and computer vision, aiding weather forecasting and climate analysis.
Ground-based sky/cloud segmentation extracts clouds from images for further
feature analysis. Existing methods struggle to balance segmentation accuracy
and computational efficiency, limiting real-world deployment on edge devices,
so we introduce SCANet, a novel lightweight cloud segmentation model featuring
Segregation and Context Aggregation Module (SCAM), which refines rough
segmentation maps into weighted sky and cloud features processed separately.
SCANet achieves state-of-the-art performance while drastically reducing
computational complexity. SCANet-large (4.29M) achieves comparable accuracy to
state-of-the-art methods with 70.9% fewer parameters. Meanwhile, SCANet-lite
(90K) delivers 1390 fps in FP16, surpassing real-time standards. Additionally,
we propose an efficient pre-training strategy that enhances performance even
without ImageNet pre-training.

</details>

### [86] [Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization](https://arxiv.org/abs/2504.14200)
*Huiyi Chen,Jiawei Peng,Kaihua Tang,Xin Geng,Xu Yang*

Main category: cs.CV

TLDR: 论文提出了一种名为KeCO的新框架，通过利用未使用的数据构建紧凑且信息丰富的核心集，显著提升了图像分类任务的上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有的核心集选择方法在图像分类任务中效率低下且可能导致信息丢失，因此需要一种更高效的方法来优化核心集。

Method: KeCO通过引入视觉特征作为核心集中的关键点，并利用未使用的支持集样本更新这些关键点，从而在低计算成本下构建更有效的核心集。

Result: 在粗粒度和细粒度图像分类基准测试中，KeCO平均提升了20%以上的性能，并在模拟在线场景中表现出色。

Conclusion: KeCO框架在资源受限的实际场景中具有实用价值，能够显著提升图像分类任务的上下文学习效果。

Abstract: In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to
adapt to new tasks without parameter updates, using a few demonstrations from a
large support set. However, selecting informative demonstrations leads to high
computational and memory costs. While some methods explore selecting a small
and representative coreset in the text classification, evaluating all support
set samples remains costly, and discarded samples lead to unnecessary
information loss. These methods may also be less effective for image
classification due to differences in feature spaces. Given these limitations,
we propose Key-based Coreset Optimization (KeCO), a novel framework that
leverages untapped data to construct a compact and informative coreset. We
introduce visual features as keys within the coreset, which serve as the anchor
for identifying samples to be updated through different selection strategies.
By leveraging untapped samples from the support set, we update the keys of
selected coreset samples, enabling the randomly initialized coreset to evolve
into a more informative coreset under low computational cost. Through extensive
experiments on coarse-grained and fine-grained image classification benchmarks,
we demonstrate that KeCO effectively enhances ICL performance for image
classification task, achieving an average improvement of more than 20\%.
Notably, we evaluate KeCO under a simulated online scenario, and the strong
performance in this scenario highlights the practical value of our framework
for resource-constrained real-world scenarios.

</details>

### [87] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/abs/2504.14202)
*Zichuan Liu,Liming Jiang,Qing Yan,Yumin Jia,Hao Kang,Xin Lu*

Main category: cs.CV

TLDR: 提出了一种基于多模态编码策略的ID保留生成框架FaceCLIP，通过联合嵌入空间实现身份与文本的统一输入，结合扩散模型生成身份一致且文本对齐的图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过适配器注入身份特征，限制了生成效果。本文旨在通过多模态编码策略提升身份保留和文本对齐能力。

Method: 引入FaceCLIP多模态编码器，学习身份与文本的联合嵌入空间，结合扩散模型生成图像。提出多模态对齐算法训练FaceCLIP。

Result: FaceCLIP-SDXL在身份保留和文本对齐上优于现有方法，生成更逼真的肖像。

Conclusion: FaceCLIP-SDXL通过多模态编码策略显著提升了ID保留生成的效果，具有定量和定性优势。

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>

### [88] [Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2504.14221)
*Wenbing Zhu,Lidong Wang,Ziqing Zhou,Chengjie Wang,Yurui Pan,Ruoyi Zhang,Zhuhao Chen,Linjie Cheng,Bin-Bin Gao,Jiangning Zhang,Zhenye Gan,Yuxie Wang,Yulong Chen,Shuguang Qian,Mingmin Chi,Bo Peng,Lizhuang Ma*

Main category: cs.CV

TLDR: 论文介绍了Real-IAD D3数据集，这是一个高精度多模态工业异常检测数据集，包含RGB图像、微米级3D点云和通过光度立体生成的伪3D模态，并提出了一个多模态融合方法以提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测（IAD）的多模态方法需求增加，但现有数据集（如MVTec 3D）在规模和分辨率上存在不足，难以满足真实工业环境需求。

Method: 提出Real-IAD D3数据集，包含RGB、3D点云和伪3D模态，并设计了一种融合多模态信息的方法。

Result: 实验表明，多模态融合显著提升了异常检测的鲁棒性和性能。

Conclusion: Real-IAD D3为多模态IAD提供了更具挑战性的基准，数据集和代码已公开。

Abstract: The increasing complexity of industrial anomaly detection (IAD) has
positioned multimodal detection methods as a focal area of machine vision
research. However, dedicated multimodal datasets specifically tailored for IAD
remain limited. Pioneering datasets like MVTec 3D have laid essential
groundwork in multimodal IAD by incorporating RGB+3D data, but still face
challenges in bridging the gap with real industrial environments due to
limitations in scale and resolution. To address these challenges, we introduce
Real-IAD D3, a high-precision multimodal dataset that uniquely incorporates an
additional pseudo3D modality generated through photometric stereo, alongside
high-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3
features finer defects, diverse anomalies, and greater scale across 20
categories, providing a challenging benchmark for multimodal IAD Additionally,
we introduce an effective approach that integrates RGB, point cloud, and
pseudo-3D depth information to leverage the complementary strengths of each
modality, enhancing detection performance. Our experiments highlight the
importance of these modalities in boosting detection robustness and overall IAD
performance. The dataset and code are publicly accessible for research purposes
at https://realiad4ad.github.io/Real-IAD D3

</details>

### [89] [Revisiting CLIP for SF-OSDA: Unleashing Zero-Shot Potential with Adaptive Threshold and Training-Free Feature Filtering](https://arxiv.org/abs/2504.14224)
*Yongguang Li,Jindong Li,Qi Wang,Qianli Xing,Runliang Niu,Shengsheng Wang,Menglin Yang*

Main category: cs.CV

TLDR: CLIPXpert提出了一种新的SF-OSDA方法，通过自适应阈值策略和未知类特征过滤模块，解决了现有方法在CLIP应用中依赖固定阈值和忽略类倾向的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SF-OSDA方法在CLIP应用中存在固定阈值和复杂训练的问题，导致性能受限和部署成本高。

Method: CLIPXpert结合了Box-Cox GMM自适应阈值模块和SVD未知类特征过滤模块，动态优化阈值并减少未知类样本的倾向性。

Result: 在DomainNet数据集上优于UOTA 1.92%，在Office-Home等数据集上达到SOTA水平。

Conclusion: CLIPXpert验证了CLIP在SF-OSDA任务中的零样本潜力，并显著提升了性能。

Abstract: Source-Free Unsupervised Open-Set Domain Adaptation (SF-OSDA) methods using
CLIP face significant issues: (1) while heavily dependent on domain-specific
threshold selection, existing methods employ simple fixed thresholds,
underutilizing CLIP's zero-shot potential in SF-OSDA scenarios; and (2)
overlook intrinsic class tendencies while employing complex training to enforce
feature separation, incurring deployment costs and feature shifts that
compromise CLIP's generalization ability. To address these issues, we propose
CLIPXpert, a novel SF-OSDA approach that integrates two key components: an
adaptive thresholding strategy and an unknown class feature filtering module.
Specifically, the Box-Cox GMM-Based Adaptive Thresholding (BGAT) module
dynamically determines the optimal threshold by estimating sample score
distributions, balancing known class recognition and unknown class sample
detection. Additionally, the Singular Value Decomposition (SVD)-Based
Unknown-Class Feature Filtering (SUFF) module reduces the tendency of unknown
class samples towards known classes, improving the separation between known and
unknown classes. Experiments show that our source-free and training-free method
outperforms state-of-the-art trained approach UOTA by 1.92% on the DomainNet
dataset, achieves SOTA-comparable performance on datasets such as Office-Home,
and surpasses other SF-OSDA methods. This not only validates the effectiveness
of our proposed method but also highlights CLIP's strong zero-shot potential
for SF-OSDA tasks.

</details>

### [90] [Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA in 3D Semantic Segmentation](https://arxiv.org/abs/2504.14231)
*Johannes Spoecklberger,Wei Lin,Pedro Hermosilla,Sivan Doveh,Horst Possegger,M. Jehanzeb Mirza*

Main category: cs.CV

TLDR: 本文探讨了视觉基础模型（VFMs）在LiDAR 3D语义分割任务中的应用，通过融合2D-3D数据提升性能。


<details>
  <summary>Details</summary>
Motivation: VFMs在视觉任务中表现优异，但其在3D任务中的潜力尚未充分挖掘，尤其是在跨模态数据（如图像与点云）的应用中。

Method: 提出一种融合网络，结合2D图像和3D点云数据，利用VFMs的跨域特征训练3D主干网络，动态调整两种模态的贡献。

Result: 在多个实验中显著优于现有方法，平均提升6.5 mIoU。

Conclusion: VFMs在3D语义分割任务中具有显著潜力，跨模态融合能有效提升性能。

Abstract: Vision Foundation Models (VFMs) have become a de facto choice for many
downstream vision tasks, like image classification, image segmentation, and
object localization. However, they can also provide significant utility for
downstream 3D tasks that can leverage the cross-modal information (e.g., from
paired image data). In our work, we further explore the utility of VFMs for
adapting from a labeled source to unlabeled target data for the task of
LiDAR-based 3D semantic segmentation. Our method consumes paired 2D-3D (image
and point cloud) data and relies on the robust (cross-domain) features from a
VFM to train a 3D backbone on a mix of labeled source and unlabeled target
data. At the heart of our method lies a fusion network that is guided by both
the image and point cloud streams, with their relative contributions adjusted
based on the target domain. We extensively compare our proposed methodology
with different state-of-the-art methods in several settings and achieve strong
performance gains. For example, achieving an average improvement of 6.5 mIoU
(over all tasks), when compared with the previous state-of-the-art.

</details>

### [91] [Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network](https://arxiv.org/abs/2504.14238)
*Lu Pan,Yu-Hsuan Huang,Hongxia Xie,Cheng Zhang,Hongwei Zhao,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TLDR: 论文提出DocHR14K数据集和L2HRNet方法，用于文档图像的高光去除，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 文档图像在环境光下易出现高光，影响文本可读性和视觉质量，现有深度学习方法因缺乏专用数据集和针对性设计而表现不佳。

Method: 提出DocHR14K数据集和基于高光位置先验（HLP）的L2HRNet网络，利用残差图估计高光区域，并通过扩散模块恢复细节。

Result: DocHR14K提升了高光去除效果，L2HRNet在多个数据集上达到最优性能，PSNR提升5.01%，RMSE降低13.17%。

Conclusion: DocHR14K和L2HRNet为文档高光去除提供了有效解决方案，显著优于现有方法。

Abstract: Reflective documents often suffer from specular highlights under ambient
lighting, severely hindering text readability and degrading overall visual
quality. Although recent deep learning methods show promise in highlight
removal, they remain suboptimal for document images, primarily due to the lack
of dedicated datasets and tailored architectural designs. To tackle these
challenges, we present DocHR14K, a large-scale real-world dataset comprising
14,902 high-resolution image pairs across six document categories and various
lighting conditions. To the best of our knowledge, this is the first
high-resolution dataset for document highlight removal that captures a wide
range of real-world lighting conditions. Additionally, motivated by the
observation that the residual map between highlighted and clean images
naturally reveals the spatial structure of highlight regions, we propose a
simple yet effective Highlight Location Prior (HLP) to estimate highlight masks
without human annotations. Building on this prior, we present the
Location-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which
effectively removes highlights by leveraging estimated priors and incorporates
diffusion module to restore details. Extensive experiments demonstrate that
DocHR14K improves highlight removal under diverse lighting conditions. Our
L2HRNet achieves state-of-the-art performance across three benchmark datasets,
including a 5.01\% increase in PSNR and a 13.17\% reduction in RMSE on
DocHR14K.

</details>

### [92] [ROI-Guided Point Cloud Geometry Compression Towards Human and Machine Vision](https://arxiv.org/abs/2504.14240)
*Xie Liang,Gao Wei,Zhenghui Ming,Li Ge*

Main category: cs.CV

TLDR: 提出了一种基于ROI引导的点云几何压缩方法（RPCGC），通过双分支并行结构优化压缩性能，同时提升机器视觉任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 点云数据在自动驾驶等领域应用广泛，但大容量存储和传输问题导致语义细节丢失，影响下游任务精度。

Method: 采用双分支结构：基础层简化点云，增强层优化几何细节；通过ROI预测网络生成掩码信息，结合残差信息优化压缩。

Result: 在ScanNet和SUN RGB-D数据集上，RPCGC在高比特率下压缩性能优异，检测精度提升10%。

Conclusion: RPCGC方法在点云压缩和机器视觉任务中表现出色，为高精度压缩提供了新思路。

Abstract: Point cloud data is pivotal in applications like autonomous driving, virtual
reality, and robotics. However, its substantial volume poses significant
challenges in storage and transmission. In order to obtain a high compression
ratio, crucial semantic details usually confront severe damage, leading to
difficulties in guaranteeing the accuracy of downstream tasks. To tackle this
problem, we are the first to introduce a novel Region of Interest (ROI)-guided
Point Cloud Geometry Compression (RPCGC) method for human and machine vision.
Our framework employs a dual-branch parallel structure, where the base layer
encodes and decodes a simplified version of the point cloud, and the
enhancement layer refines this by focusing on geometry details. Furthermore,
the residual information of the enhancement layer undergoes refinement through
an ROI prediction network. This network generates mask information, which is
then incorporated into the residuals, serving as a strong supervision signal.
Additionally, we intricately apply these mask details in the Rate-Distortion
(RD) optimization process, with each point weighted in the distortion
calculation. Our loss function includes RD loss and detection loss to better
guide point cloud encoding for the machine. Experiment results demonstrate that
RPCGC achieves exceptional compression performance and better detection
accuracy (10% gain) than some learning-based compression methods at high
bitrates in ScanNet and SUN RGB-D datasets.

</details>

### [93] [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
*Yikun Ji,Yan Hong,Jiahui Zhan,Haoxing Chen,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TLDR: 论文探讨了利用多模态大语言模型（MLLMs）进行AI生成图像检测的潜力，提出了一种更透明、可解释的检测框架。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成技术带来的公共安全问题，避免传统检测方法的“黑箱”操作，追求更强的泛化能力和透明度。

Method: 设计六种不同的提示词，并整合这些提示词构建一个基于推理的检测框架，同时对比MLLMs与传统方法及人类评估者的表现。

Result: 展示了MLLMs在AI生成图像检测中的优势和局限性，提出的框架更具鲁棒性和可解释性。

Conclusion: MLLMs为AI生成图像检测提供了新的可能性，结合推理驱动的框架可以提升检测的透明度和效果。

Abstract: Progress in image generation raises significant public security concerns. We
argue that fake image detection should not operate as a "black box". Instead,
an ideal approach must ensure both strong generalization and transparency.
Recent progress in Multi-modal Large Language Models (MLLMs) offers new
opportunities for reasoning-based AI-generated image detection. In this work,
we evaluate the capabilities of MLLMs in comparison to traditional detection
methods and human evaluators, highlighting their strengths and limitations.
Furthermore, we design six distinct prompts and propose a framework that
integrates these prompts to develop a more robust, explainable, and
reasoning-driven detection system. The code is available at
https://github.com/Gennadiyev/mllm-defake.

</details>

### [94] [Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation](https://arxiv.org/abs/2504.14249)
*Bin Ren,Eduard Zamfir,Zongwei Wu,Yawei Li,Yidi Li,Danda Pani Paudel,Radu Timofte,Ming-Hsuan Yang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TLDR: AnyIR提出了一种统一的图像修复方法，通过联合嵌入机制高效处理多种退化问题，无需增加模型规模或依赖大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法需为每种退化训练专用模型，效率低且冗余；现有方法要么增加模块规模，要么引入跨模态转换，复杂性高。

Method: 利用退化间的相似性，通过门控机制重加权子潜在空间的关键组件，并采用空间-频率并行融合策略增强修复细节。

Result: 在统一修复设置下，AnyIR性能达到SOTA，模型参数和FLOPs分别减少约82%和85%。

Conclusion: AnyIR提供了一种高效且全面的图像修复解决方案，显著降低了模型复杂性。

Abstract: Restoring any degraded image efficiently via just one model has become
increasingly significant and impactful, especially with the proliferation of
mobile devices. Traditional solutions typically involve training dedicated
models per degradation, resulting in inefficiency and redundancy. More recent
approaches either introduce additional modules to learn visual prompts,
significantly increasing model size, or incorporate cross-modal transfer from
large language models trained on vast datasets, adding complexity to the system
architecture. In contrast, our approach, termed AnyIR, takes a unified path
that leverages inherent similarity across various degradations to enable both
efficient and comprehensive restoration through a joint embedding mechanism,
without scaling up the model or relying on large language models.Specifically,
we examine the sub-latent space of each input, identifying key components and
reweighting them first in a gated manner. To fuse the intrinsic degradation
awareness and the contextualized attention, a spatial-frequency parallel fusion
strategy is proposed for enhancing spatial-aware local-global interactions and
enriching the restoration details from the frequency perspective. Extensive
benchmarking in the all-in-one restoration setting confirms AnyIR's SOTA
performance, reducing model complexity by around 82\% in parameters and 85\% in
FLOPs. Our code will be available at our Project page
(https://amazingren.github.io/AnyIR/)

</details>

### [95] [ColorVein: Colorful Cancelable Vein Biometrics](https://arxiv.org/abs/2504.14253)
*Yifan Wang,Jie Gui,Xinli Shi,Linqing Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TLDR: 本文提出了一种创新的可取消静脉生物特征生成方案ColorVein，通过引入颜色信息增强图像密度，并优化特征提取模型，提升了安全性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对静脉生物特征的可取消模板生成方案，生物信息泄露威胁用户隐私。

Method: 提出ColorVein方案，通过交互式着色将灰度静脉图像转换为动态可控的彩色表示，并引入安全中心损失优化特征提取。

Result: ColorVein在识别性能、不可链接性、不可逆性和可撤销性方面表现优异，安全性分析显示其竞争力。

Conclusion: ColorVein是一种高效且安全的可取消静脉生物特征生成方案，优于现有方法。

Abstract: Vein recognition technologies have become one of the primary solutions for
high-security identification systems. However, the issue of biometric
information leakage can still pose a serious threat to user privacy and
anonymity. Currently, there is no cancelable biometric template generation
scheme specifically designed for vein biometrics. Therefore, this paper
proposes an innovative cancelable vein biometric generation scheme: ColorVein.
Unlike previous cancelable template generation schemes, ColorVein does not
destroy the original biometric features and introduces additional color
information to grayscale vein images. This method significantly enhances the
information density of vein images by transforming static grayscale information
into dynamically controllable color representations through interactive
colorization. ColorVein allows users/administrators to define a controllable
pseudo-random color space for grayscale vein images by editing the position,
number, and color of hint points, thereby generating protected cancelable
templates. Additionally, we propose a new secure center loss to optimize the
training process of the protected feature extraction model, effectively
increasing the feature distance between enrolled users and any potential
impostors. Finally, we evaluate ColorVein's performance on all types of vein
biometrics, including recognition performance, unlinkability, irreversibility,
and revocability, and conduct security and privacy analyses. ColorVein achieves
competitive performance compared with state-of-the-art methods.

</details>

### [96] [Visual Consensus Prompting for Co-Salient Object Detection](https://arxiv.org/abs/2504.14254)
*Jie Wang,Nana Yu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TLDR: 提出了一种参数高效的提示调优架构（VCP），用于共显著目标检测任务，解决了现有方法的两个主要限制，并在性能上超越了13种前沿模型。


<details>
  <summary>Details</summary>
Motivation: 现有共显著目标检测方法存在共识提取不及时和参数效率低的问题，限制了基础模型的表现。

Method: 引入参数高效的提示调优范式，通过共识提示生成器（CPG）和共识提示分散器（CPD）生成任务特定的视觉共识提示（VCP）。

Result: 在最具挑战性的CoCA数据集上，F_m指标提升了6.8%，性能优于13种前沿模型。

Conclusion: VCP架构通过参数高效的方式显著提升了共显著目标检测任务的性能，为未来研究提供了新思路。

Abstract: Existing co-salient object detection (CoSOD) methods generally employ a
three-stage architecture (i.e., encoding, consensus extraction & dispersion,
and prediction) along with a typical full fine-tuning paradigm. Although they
yield certain benefits, they exhibit two notable limitations: 1) This
architecture relies on encoded features to facilitate consensus extraction, but
the meticulously extracted consensus does not provide timely guidance to the
encoding stage. 2) This paradigm involves globally updating all parameters of
the model, which is parameter-inefficient and hinders the effective
representation of knowledge within the foundation model for this task.
Therefore, in this paper, we propose an interaction-effective and
parameter-efficient concise architecture for the CoSOD task, addressing two key
limitations. It introduces, for the first time, a parameter-efficient prompt
tuning paradigm and seamlessly embeds consensus into the prompts to formulate
task-specific Visual Consensus Prompts (VCP). Our VCP aims to induce the frozen
foundation model to perform better on CoSOD tasks by formulating task-specific
visual consensus prompts with minimized tunable parameters. Concretely, the
primary insight of the purposeful Consensus Prompt Generator (CPG) is to
enforce limited tunable parameters to focus on co-salient representations and
generate consensus prompts. The formulated Consensus Prompt Disperser (CPD)
leverages consensus prompts to form task-specific visual consensus prompts,
thereby arousing the powerful potential of pre-trained models in addressing
CoSOD tasks. Extensive experiments demonstrate that our concise VCP outperforms
13 cutting-edge full fine-tuning models, achieving the new state of the art
(with 6.8% improvement in F_m metrics on the most challenging CoCA dataset).
Source code has been available at https://github.com/WJ-CV/VCP.

</details>

### [97] [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
*Liu Xiao,Li Zhiyuan,Lin Yueyu*

Main category: cs.CV

TLDR: CrossWKV是一种新型跨注意力机制，用于增强RWKV-7模型在文本到图像生成中的表达能力，通过线性复杂度的WKV架构和低秩适应技术实现高效跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 提升文本到图像生成的表达能力，同时保持高效的计算和内存使用。

Method: 结合RWKV-7的WKV架构，引入广义delta规则和低秩适应技术（LoRA），实现单次跨模态对齐。

Result: 在DIR-7框架下，CrossWKV在ImageNet 256x256上达到FID 2.88和CLIP分数0.33，性能与最先进模型相当。

Conclusion: CrossWKV在高效计算和高表达能力之间取得平衡，适用于高分辨率生成和动态状态操作任务。

Abstract: We introduce CrossWKV, a novel cross-attention mechanism for the state-based
RWKV-7 model, designed to enhance the expressive power of text-to-image
generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)
architecture, CrossWKV integrates text and image modalities in a single pass,
utilizing a generalized delta rule with vector-valued gating and low-rank
adaptations (LoRA) to achieve superior cross-modal alignment. Unlike
Transformer-based models, CrossWKV's non-diagonal, input-dependent transition
matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$
complexity class, including all regular languages, as demonstrated by its
ability to perform state-tracking tasks like $S_5$ permutation modeling.
Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B
and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and
a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance
while offering robust generalization across diverse prompts. The model's
enhanced expressivity, combined with constant memory usage and linear scaling,
positions it as a powerful solution for advanced cross-modal tasks, with
potential applications in high-resolution generation and dynamic state
manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention

</details>

### [98] [Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction](https://arxiv.org/abs/2504.14267)
*Li Yu,Xuanzhe Sun,Wei Zhou,Moncef Gabbouj*

Main category: cs.CV

TLDR: 本文提出了一种基于文本-音频-视觉条件的扩散模型（TAVDiff），用于视频显著性预测，通过多模态信息融合提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 视频显著性预测在视频压缩和人机交互等应用中至关重要，多模态学习的发展促使研究者探索结合听觉和文本信息的方法。

Method: TAVDiff将视频显著性预测视为基于文本、音频和视觉输入的图像生成任务，通过逐步去噪预测显著性图。使用大型多模态模型生成文本描述，并引入显著性导向的图像-文本响应机制（SITR）。同时提出Saliency-DiT，解耦条件信息与时间步。

Result: 实验表明，TAVDiff在SIM、CC、NSS和AUC-J指标上分别提升1.03%、2.35%、2.71%和0.33%，优于现有方法。

Conclusion: TAVDiff通过多模态信息融合和解耦条件信息，显著提升了视频显著性预测的准确性。

Abstract: Video saliency prediction is crucial for downstream applications, such as
video compression and human-computer interaction. With the flourishing of
multimodal learning, researchers started to explore multimodal video saliency
prediction, including audio-visual and text-visual approaches. Auditory cues
guide the gaze of viewers to sound sources, while textual cues provide semantic
guidance for understanding video content. Integrating these complementary cues
can improve the accuracy of saliency prediction. Therefore, we attempt to
simultaneously analyze visual, auditory, and textual modalities in this paper,
and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video
saliency prediction. TAVDiff treats video saliency prediction as an image
generation task conditioned on textual, audio, and visual inputs, and predicts
saliency maps through stepwise denoising. To effectively utilize text, a large
multimodal model is used to generate textual descriptions for video frames and
introduce a saliency-oriented image-text response (SITR) mechanism to generate
image-text response maps. It is used as conditional information to guide the
model to localize the visual regions that are semantically related to the
textual description. Regarding the auditory modality, it is used as another
conditional information for directing the model to focus on salient regions
indicated by sounds. At the same time, since the diffusion transformer (DiT)
directly concatenates the conditional information with the timestep, which may
affect the estimation of the noise level. To achieve effective conditional
guidance, we propose Saliency-DiT, which decouples the conditional information
from the timestep. Experimental results show that TAVDiff outperforms existing
methods, improving 1.03\%, 2.35\%, 2.71\% and 0.33\% on SIM, CC, NSS and AUC-J
metrics, respectively.

</details>

### [99] [RAMCT: Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking](https://arxiv.org/abs/2504.14278)
*Shang Zhang,Yuke Hou,Guoqiang Gong,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TLDR: RAMCT是一种区域自适应的稀疏相关滤波器跟踪器，通过多通道特征优化和自适应正则化策略，解决了热红外目标跟踪中的低分辨率、遮挡、背景干扰和目标变形等问题。


<details>
  <summary>Details</summary>
Motivation: 现有相关滤波器跟踪器在热红外目标跟踪中面临低分辨率、遮挡、背景干扰和目标变形等挑战，影响了跟踪性能。

Method: 1. 引入空间自适应二值掩码优化相关滤波器学习过程；2. 提出基于广义奇异值分解的区域自适应迭代Tikhonov正则化方法；3. 设计动态差异参数调整的在线优化策略。

Result: 在多个基准测试中，RAMCT在准确性和鲁棒性上优于其他先进跟踪器。

Conclusion: RAMCT通过区域自适应和动态优化策略，显著提升了热红外目标跟踪的性能。

Abstract: Correlation filter (CF)-based trackers have gained significant attention for
their computational efficiency in thermal infrared (TIR) target tracking.
However, ex-isting methods struggle with challenges such as low-resolution
imagery, occlu-sion, background clutter, and target deformation, which severely
impact tracking performance. To overcome these limitations, we propose RAMCT, a
region-adaptive sparse correlation filter tracker that integrates multi-channel
feature opti-mization with an adaptive regularization strategy. Firstly, we
refine the CF learn-ing process by introducing a spatially adaptive binary
mask, which enforces spar-sity in the target region while dynamically
suppressing background interference. Secondly, we introduce generalized
singular value decomposition (GSVD) and propose a novel GSVD-based
region-adaptive iterative Tikhonov regularization method. This enables flexible
and robust optimization across multiple feature channels, improving resilience
to occlusion and background variations. Thirdly, we propose an online
optimization strategy with dynamic discrepancy-based pa-rameter adjustment.
This mechanism facilitates real time adaptation to target and background
variations, thereby improving tracking accuracy and robustness. Ex-tensive
experiments on LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks
demonstrate that RAMCT outperforms other state-of-the-art trackers in terms of
accuracy and robustness.

</details>

### [100] [CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey](https://arxiv.org/abs/2504.14280)
*Jindong Li,Yongguang Li,Yali Fu,Jiahong Liu,Yixin Liu,Menglin Yang,Irwin King*

Main category: cs.CV

TLDR: 本文综述了CLIP在领域泛化（DG）和领域适应（DA）中的应用，填补了现有文献的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习的发展，DG和DA对模型鲁棒性至关重要，但缺乏对CLIP在这些任务中应用的系统综述。

Method: 在DG中，方法分为优化提示学习和利用CLIP作为特征提取主干；在DA中，分为基于源数据的方法和基于目标数据的无源方法。

Result: 综述总结了CLIP在DG和DA中的应用，并指出了关键挑战（如过拟合、领域多样性和计算效率）。

Conclusion: 本文为研究者和实践者提供了有价值的见解，旨在推动更鲁棒的机器学习模型的发展。

Abstract: As machine learning evolves, domain generalization (DG) and domain adaptation
(DA) have become crucial for enhancing model robustness across diverse
environments. Contrastive Language-Image Pretraining (CLIP) plays a significant
role in these tasks, offering powerful zero-shot capabilities that allow models
to perform effectively in unseen domains. However, there remains a significant
gap in the literature, as no comprehensive survey currently exists that
systematically explores the applications of CLIP in DG and DA, highlighting the
necessity for this review. This survey presents a comprehensive review of
CLIP's applications in DG and DA. In DG, we categorize methods into optimizing
prompt learning for task alignment and leveraging CLIP as a backbone for
effective feature extraction, both enhancing model adaptability. For DA, we
examine both source-available methods utilizing labeled source data and
source-free approaches primarily based on target domain data, emphasizing
knowledge transfer mechanisms and strategies for improved performance across
diverse contexts. Key challenges, including overfitting, domain diversity, and
computational efficiency, are addressed, alongside future research
opportunities to advance robustness and efficiency in practical applications.
By synthesizing existing literature and pinpointing critical gaps, this survey
provides valuable insights for researchers and practitioners, proposing
directions for effectively leveraging CLIP to enhance methodologies in domain
generalization and adaptation. Ultimately, this work aims to foster innovation
and collaboration in the quest for more resilient machine learning models that
can perform reliably across diverse real-world scenarios. A more up-to-date
version of the papers is maintained at:
https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.

</details>

### [101] [ISTD-YOLO: A Multi-Scale Lightweight High-Performance Infrared Small Target Detection Algorithm](https://arxiv.org/abs/2504.14289)
*Shang Zhang,Yujie Cui,Ruoyan Xiong,Huanbin Zhang*

Main category: cs.CV

TLDR: ISTD-YOLO是一种基于改进YOLOv7的轻量级红外小目标检测算法，通过轻量化重构、引入无参数注意力机制和优化NWD指标，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 针对红外图像检测中背景复杂、信噪比低、目标尺寸小和亮度弱等难点，提出一种高效检测方法。

Method: 1. 轻量化重构YOLOv7网络结构；2. 用VoV-GSCSP替换ELAN-W模块；3. 引入无参数注意力机制；4. 使用NWD优化IoU指标。

Result: 实验表明，ISTD-YOLO相比YOLOv7和其他主流算法，检测效果显著提升，各项指标均有改善。

Conclusion: ISTD-YOLO能够高质量检测红外小目标，适用于复杂场景。

Abstract: Aiming at the detection difficulties of infrared images such as complex
background, low signal-to-noise ratio, small target size and weak brightness, a
lightweight infrared small target detection algorithm ISTD-YOLO based on
improved YOLOv7 was proposed. Firstly, the YOLOv7 network structure was
lightweight reconstructed, and a three-scale lightweight network architecture
was designed. Then, the ELAN-W module of the model neck network is replaced by
VoV-GSCSP to reduce the computational cost and the complexity of the network
structure. Secondly, a parameter-free attention mechanism was introduced into
the neck network to enhance the relevance of local con-text information.
Finally, the Normalized Wasserstein Distance (NWD) was used to optimize the
commonly used IoU index to enhance the localization and detection accuracy of
small targets. Experimental results show that compared with YOLOv7 and the
current mainstream algorithms, ISTD-YOLO can effectively improve the detection
effect, and all indicators are effectively improved, which can achieve
high-quality detection of infrared small targets.

</details>

### [102] [Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization](https://arxiv.org/abs/2504.14290)
*Shouwei Ruan,Zhenyu Wu,Yao Huang,Ruochen Zhang,Yitong Sun,Caixin Kang,Xingxing Wei*

Main category: cs.CV

TLDR: SC-DPO是一种新的文本到图像生成安全对齐框架，通过整合安全约束和人类偏好校准，有效平衡生成内容的安全性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法完全保证有害概念下的安全性或难以平衡安全性与生成质量，SC-DPO旨在解决这一问题。

Method: SC-DPO引入安全成本模型量化有害程度，采用对比学习和成本锚定目标训练，并构建SCP-10K数据集和动态聚焦机制（DFM）。

Result: 实验表明SC-DPO优于现有方法，能有效防御NSFW内容，保持高质量生成，并对抗恶意提示。

Conclusion: SC-DPO为T2I生成提供了一种高效的安全对齐解决方案，兼顾安全性和生成质量。

Abstract: Ensuring the safety of generated content remains a fundamental challenge for
Text-to-Image (T2I) generation. Existing studies either fail to guarantee
complete safety under potentially harmful concepts or struggle to balance
safety with generation quality. To address these issues, we propose
Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework
for safety alignment in T2I models. SC-DPO integrates safety constraints into
the general human preference calibration, aiming to maximize the likelihood of
generating human-preferred samples while minimizing the safety cost of the
generated outputs. In SC-DPO, we introduce a safety cost model to accurately
quantify harmful levels for images, and train it effectively using the proposed
contrastive learning and cost anchoring objectives. To apply SC-DPO for
effective T2I safety alignment, we constructed SCP-10K, a safety-constrained
preference dataset containing rich harmful concepts, which blends
safety-constrained preference pairs under both harmful and clean instructions,
further mitigating the trade-off between safety and sample quality.
Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO,
promoting the model's learning of difficult preference pair samples. Extensive
experiments demonstrate that SC-DPO outperforms existing methods, effectively
defending against various NSFW content while maintaining optimal sample quality
and human preference alignment. Additionally, SC-DPO exhibits resilience
against adversarial prompts designed to generate harmful content.

</details>

### [103] [From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion](https://arxiv.org/abs/2504.14294)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Huiyu Zhou,Michael Felsberg,Dacheng Tao,Xuelong Li*

Main category: cs.CV

TLDR: ConFill提出了一种新的图像补全框架，通过上下文自适应差异模型（CAD）和动态采样机制，显著提升了生成内容与原始图像的融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像补全任务中难以保持已知与未知区域的连贯性，缺乏显式的空间和语义对齐，导致生成内容与原始图像不协调。

Method: ConFill引入CAD模型，逐步减少扩散过程中生成与原始图像的差异，并结合动态采样机制，在复杂区域自适应提高采样率。

Result: 实验表明，ConFill在图像补全任务中优于现有方法，设定了新的性能基准。

Conclusion: ConFill通过上下文对齐和动态采样，显著提升了图像补全的质量和一致性。

Abstract: Image completion is a challenging task, particularly when ensuring that
generated content seamlessly integrates with existing parts of an image. While
recent diffusion models have shown promise, they often struggle with
maintaining coherence between known and unknown (missing) regions. This issue
arises from the lack of explicit spatial and semantic alignment during the
diffusion process, resulting in content that does not smoothly integrate with
the original image. Additionally, diffusion models typically rely on global
learned distributions rather than localized features, leading to
inconsistencies between the generated and existing image parts. In this work,
we propose ConFill, a novel framework that introduces a Context-Adaptive
Discrepancy (CAD) model to ensure that intermediate distributions of known and
unknown regions are closely aligned throughout the diffusion process. By
incorporating CAD, our model progressively reduces discrepancies between
generated and original images at each diffusion step, leading to contextually
aligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism
that adaptively increases the sampling rate in regions with high reconstruction
complexity. This approach enables precise adjustments, enhancing detail and
integration in restored areas. Extensive experiments demonstrate that ConFill
outperforms current methods, setting a new benchmark in image completion.

</details>

### [104] [Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization](https://arxiv.org/abs/2504.14301)
*Nazia Aslam,Kamal Nasrollahi*

Main category: cs.CV

TLDR: 论文提出了一种隐私保护的图像匿名化技术，通过优化匿名化器以平衡隐私保护和动作识别性能，符合欧盟AI法案和GDPR标准。


<details>
  <summary>Details</summary>
Motivation: 视频监控系统的发展引发了隐私与性能之间的平衡问题，如何在保护隐私的同时不牺牲动作识别性能是一个重要挑战。

Method: 提出了一种基于特征惩罚的方案，通过优化匿名化器来最小化隐私泄漏并保持动作识别性能。

Result: 实验表明，该方法在保持隐私泄漏几乎不变的同时，显著提升了动作识别性能。

Conclusion: 该技术成功解决了隐私与性能之间的权衡问题，为隐私保护提供了新思路。

Abstract: The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.

</details>

### [105] [Exploring Generalizable Pre-training for Real-world Change Detection via Geometric Estimation](https://arxiv.org/abs/2504.14306)
*Yitao Zhao,Sen Lei,Nanqing Liu,Heng-Chao Li,Turgay Celik,Qing Zhu*

Main category: cs.CV

TLDR: 提出了一种名为MatchCD的自监督变化检测框架，通过几何估计解决多时相图像未对齐问题，无需手动配准，可直接处理大尺度图像。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测算法需要手动配准多时相图像，增加了工作流程的复杂性。

Method: 利用自监督对比表示优化编码器，同时处理图像未对齐和目标变化问题，支持直接处理高分辨率图像。

Result: 在多种复杂场景下表现出色，尤其在几何畸变显著的情况下。

Conclusion: MatchCD框架有效解决了传统变化检测的配准和计算效率问题，具有实际应用潜力。

Abstract: As an essential procedure in earth observation system, change detection (CD)
aims to reveal the spatial-temporal evolution of the observation regions. A key
prerequisite for existing change detection algorithms is aligned geo-references
between multi-temporal images by fine-grained registration. However, in the
majority of real-world scenarios, a prior manual registration is required
between the original images, which significantly increases the complexity of
the CD workflow. In this paper, we proposed a self-supervision motivated CD
framework with geometric estimation, called "MatchCD". Specifically, the
proposed MatchCD framework utilizes the zero-shot capability to optimize the
encoder with self-supervised contrastive representation, which is reused in the
downstream image registration and change detection to simultaneously handle the
bi-temporal unalignment and object change issues. Moreover, unlike the
conventional change detection requiring segmenting the full-frame image into
small patches, our MatchCD framework can directly process the original
large-scale image (e.g., 6K*4K resolutions) with promising performance. The
performance in multiple complex scenarios with significant geometric distortion
demonstrates the effectiveness of our proposed framework.

</details>

### [106] [FGSGT: Saliency-Guided Siamese Network Tracker Based on Key Fine-Grained Feature Information for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14309)
*Ruoyan Xiong,Huanbin Zhang,Shentao Wang,Hui He,Yuke Hou,Yue Zhang,Yujie Cui,Huipan Guan,Shang Zhang*

Main category: cs.CV

TLDR: 提出了一种基于显著性引导的Siamese网络跟踪器，通过细粒度特征并行学习和多层融合模块，解决了TIR图像特征提取不足和跟踪漂移问题。


<details>
  <summary>Details</summary>
Motivation: TIR图像特征细节少、对比度低，传统特征提取模型难以捕捉目标特征，导致跟踪器易受干扰和漂移影响。

Method: 设计了细粒度特征并行学习卷积块、多层特征融合模块和Siamese残差细化块，结合显著性损失函数优化预测。

Result: 在PTB-TIR、LSOTB-TIR和VOT-TIR基准测试中取得了最高精度和成功率。

Conclusion: 该方法显著提升了TIR图像跟踪的准确性和鲁棒性。

Abstract: Thermal infrared (TIR) images typically lack detailed features and have low
contrast, making it challenging for conventional feature extraction models to
capture discriminative target characteristics. As a result, trackers are often
affected by interference from visually similar objects and are susceptible to
tracking drift. To address these challenges, we propose a novel saliency-guided
Siamese network tracker based on key fine-grained feature infor-mation. First,
we introduce a fine-grained feature parallel learning convolu-tional block with
a dual-stream architecture and convolutional kernels of varying sizes. This
design captures essential global features from shallow layers, enhances feature
diversity, and minimizes the loss of fine-grained in-formation typically
encountered in residual connections. In addition, we propose a multi-layer
fine-grained feature fusion module that uses bilinear matrix multiplication to
effectively integrate features across both deep and shallow layers. Next, we
introduce a Siamese residual refinement block that corrects saliency map
prediction errors using residual learning. Combined with deep supervision, this
mechanism progressively refines predictions, ap-plying supervision at each
recursive step to ensure consistent improvements in accuracy. Finally, we
present a saliency loss function to constrain the sali-ency predictions,
directing the network to focus on highly discriminative fi-ne-grained features.
Extensive experiment results demonstrate that the pro-posed tracker achieves
the highest precision and success rates on the PTB-TIR and LSOTB-TIR
benchmarks. It also achieves a top accuracy of 0.78 on the VOT-TIR 2015
benchmark and 0.75 on the VOT-TIR 2017 benchmark.

</details>

### [107] [DCFG: Diverse Cross-Channel Fine-Grained Feature Learning and Progressive Fusion Siamese Tracker for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14311)
*Ruoyan Xiong,Yuke Hou,Princess Retor Torboh,Hui He,Huanbin Zhang,Yue Zhang,Yanpin Wang,Huipan Guan,Shang Zhang*

Main category: cs.CV

TLDR: 提出了一种基于跨通道细粒度特征学习和渐进融合的新型Siamese跟踪器，用于解决热红外（TIR）跟踪中高判别性特征捕获的挑战。


<details>
  <summary>Details</summary>
Motivation: 热红外跟踪中难以捕获高判别性特征，现有方法在细节和多样性方面表现不足。

Method: 设计了跨通道细粒度特征学习网络，通过掩码和抑制系数抑制主导特征，引入通道重排和均衡机制减少参数冗余，并结合逐层融合单元优化特征提取。此外，提出了跨通道细粒度损失函数，增强特征多样性和细节捕获能力。

Result: 在VOT-TIR 2015和2017基准测试中分别达到0.81和0.78的准确率，并在LSOTB-TIR和PTB-TIR基准上全面优于其他方法。

Conclusion: 该方法通过细粒度特征学习和渐进融合显著提升了热红外跟踪的性能和鲁棒性。

Abstract: To address the challenge of capturing highly discriminative features in
ther-mal infrared (TIR) tracking, we propose a novel Siamese tracker based on
cross-channel fine-grained feature learning and progressive fusion. First, we
introduce a cross-channel fine-grained feature learning network that employs
masks and suppression coefficients to suppress dominant target features,
en-abling the tracker to capture more detailed and subtle information. The
net-work employs a channel rearrangement mechanism to enhance efficient
in-formation flow, coupled with channel equalization to reduce parameter count.
Additionally, we incorporate layer-by-layer combination units for ef-fective
feature extraction and fusion, thereby minimizing parameter redun-dancy and
computational complexity. The network further employs feature redirection and
channel shuffling strategies to better integrate fine-grained details. Second,
we propose a specialized cross-channel fine-grained loss function designed to
guide feature groups toward distinct discriminative re-gions of the target,
thus improving overall target representation. This loss function includes an
inter-channel loss term that promotes orthogonality be-tween channels,
maximizing feature diversity and facilitating finer detail capture. Extensive
experiments demonstrate that our proposed tracker achieves the highest
accuracy, scoring 0.81 on the VOT-TIR 2015 and 0.78 on the VOT-TIR 2017
benchmark, while also outperforming other methods across all evaluation metrics
on the LSOTB-TIR and PTB-TIR benchmarks.

</details>

### [108] [Visual Prompting for One-shot Controllable Video Editing without Inversion](https://arxiv.org/abs/2504.14335)
*Zhengbo Zhang,Yuxi Zhou,Duo Peng,Joo-Hwee Lim,Zhigang Tu,De Wen Soh,Lin Geng Foo*

Main category: cs.CV

TLDR: 论文提出了一种无需DDIM反演的单次可控视频编辑方法，通过视觉提示和内容一致性采样（CCS）确保编辑帧与源帧的内容一致性，并引入时间一致性采样（TCS）保证编辑帧的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用DDIM反演生成潜在噪声，但误差累积导致内容一致性不足，因此需要一种更高效且一致性更强的视频编辑方法。

Method: 提出基于视觉提示的单次可控视频编辑方法，避免DDIM反演；引入内容一致性采样（CCS）和时间一致性采样（TCS）确保内容与时间一致性。

Result: 实验验证了方法的有效性，能够显著提升编辑帧与源帧的内容一致性及时间一致性。

Conclusion: 该方法通过创新的一致性采样策略，解决了现有视频编辑中的内容与时间一致性问题，为单次可控视频编辑提供了更优解决方案。

Abstract: One-shot controllable video editing (OCVE) is an important yet challenging
task, aiming to propagate user edits that are made -- using any image editing
tool -- on the first frame of a video to all subsequent frames, while ensuring
content consistency between edited frames and source frames. To achieve this,
prior methods employ DDIM inversion to transform source frames into latent
noise, which is then fed into a pre-trained diffusion model, conditioned on the
user-edited first frame, to generate the edited video. However, the DDIM
inversion process accumulates errors, which hinder the latent noise from
accurately reconstructing the source frames, ultimately compromising content
consistency in the generated edited frames. To overcome it, our method
eliminates the need for DDIM inversion by performing OCVE through a novel
perspective based on visual prompting. Furthermore, inspired by consistency
models that can perform multi-step consistency sampling to generate a sequence
of content-consistent images, we propose a content consistency sampling (CCS)
to ensure content consistency between the generated edited frames and the
source frames. Moreover, we introduce a temporal-content consistency sampling
(TCS) based on Stein Variational Gradient Descent to ensure temporal
consistency across the edited frames. Extensive experiments validate the
effectiveness of our approach.

</details>

### [109] [Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms](https://arxiv.org/abs/2504.14337)
*Josef Taher,Eric Hyyppä,Matti Hyyppä,Klaara Salolahti,Xiaowei Yu,Leena Matikainen,Antero Kukko,Matti Lehtomäki,Harri Kaartinen,Sopitta Thurachen,Paula Litkey,Ville Luoma,Markus Holopainen,Gefei Kong,Hongchao Fan,Petri Rönnholm,Antti Polvivaara,Samuli Junttila,Mikko Vastaranta,Stefano Puliti,Rasmus Astrup,Joel Kostensalo,Mari Myllymäki,Maksymilian Kulicki,Krzysztof Stereńczak,Raul de Paula Pires,Ruben Valbuena,Juan Pedro Carbonell-Rivera,Jesús Torralba,Yi-Chen Chen,Lukas Winiwarter,Markus Hollaus,Gottfried Mandlburger,Narges Takhtkeshha,Fabio Remondino,Maciej Lisiewicz,Bartłomiej Kraszewski,Xinlian Liang,Jianchang Chen,Eero Ahokas,Kirsi Karila,Eugeniu Vezeteu,Petri Manninen,Roope Näsi,Heikki Hyyti,Siiri Pyykkönen,Peilun Hu,Juha Hyyppä*

Main category: cs.CV

TLDR: 该研究通过比较机器学习和深度学习方法，发现基于点的深度学习方法（如点变换器模型）在高密度多光谱ALS数据中表现最佳，分类准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 气候智能和生物多样性保护的林业需要精确的森林资源信息，尤其是稀有树种的识别。多光谱ALS技术在自动化点云处理和树种分类中仍有挑战。

Method: 研究收集了高密度多光谱ALS数据，并对比了多种算法（包括点变换器模型、传统机器学习和图像深度学习）在树种分类中的表现。

Result: 点变换器模型在高密度ALS数据中表现最佳，整体准确率达87.9%（宏平均74.5%），光谱信息显著提升了分类准确率。

Conclusion: 基于点的深度学习方法在多光谱ALS数据中优于传统方法，光谱信息对分类性能有重要影响。

Abstract: Climate-smart and biodiversity-preserving forestry demands precise
information on forest resources, extending to the individual tree level.
Multispectral airborne laser scanning (ALS) has shown promise in automated
point cloud processing and tree segmentation, but challenges remain in
identifying rare tree species and leveraging deep learning techniques. This
study addresses these gaps by conducting a comprehensive benchmark of machine
learning and deep learning methods for tree species classification. For the
study, we collected high-density multispectral ALS data (>1000 pts/m$^2$) at
three wavelengths using the FGI-developed HeliALS system, complemented by
existing Optech Titan data (35 pts/m$^2$), to evaluate the species
classification accuracy of various algorithms in a test site located in
Southern Finland. Based on 5261 test segments, our findings demonstrate that
point-based deep learning methods, particularly a point transformer model,
outperformed traditional machine learning and image-based deep learning
approaches on high-density multispectral point clouds. For the high-density ALS
dataset, a point transformer model provided the best performance reaching an
overall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065
segments and 92.0% (85.1%) with 5000 training segments. The best image-based
deep learning method, DetailView, reached an overall (macro-average) accuracy
of 84.3% (63.9%), whereas a random forest (RF) classifier achieved an overall
(macro-average) accuracy of 83.2% (61.3%). Importantly, the overall
classification accuracy of the point transformer model on the HeliALS data
increased from 73.0% with no spectral information to 84.7% with single-channel
reflectance, and to 87.9% with spectral information of all the three channels.

</details>

### [110] [Manipulating Multimodal Agents via Cross-Modal Prompt Injection](https://arxiv.org/abs/2504.14348)
*Le Wang,Zonghao Ying,Tianyuan Zhang,Siyuan Liang,Shengshan Hu,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TLDR: 本文提出了一种针对多模态代理的新型安全漏洞攻击方法CrossInject，通过跨模态提示注入攻击，成功提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的兴起为代理带来了更强的任务执行能力，但也暴露了潜在的安全漏洞，尤其是跨模态提示注入攻击。

Method: CrossInject框架包括视觉潜在对齐和文本引导增强两部分，分别通过优化视觉嵌入空间中的对抗特征和利用大语言模型生成恶意文本指令。

Result: 实验表明，该方法在攻击成功率上比现有方法提高了至少26.4%，并在真实多模态自主代理中验证了有效性。

Conclusion: 研究揭示了多模态代理的安全隐患，强调了在安全关键应用中加强防御的必要性。

Abstract: The emergence of multimodal large language models has redefined the agent
paradigm by integrating language and vision modalities with external data
sources, enabling agents to better interpret human instructions and execute
increasingly complex tasks. However, in this work, we identify a critical yet
previously overlooked security vulnerability in multimodal agents: cross-modal
prompt injection attacks. To exploit this vulnerability, we propose
CrossInject, a novel attack framework in which attackers embed adversarial
perturbations across multiple modalities to align with target malicious
content, allowing external instructions to hijack the agent's decision-making
process and execute unauthorized tasks. Our approach consists of two key
components. First, we introduce Visual Latent Alignment, where we optimize
adversarial features to the malicious instructions in the visual embedding
space based on a text-to-image generative model, ensuring that adversarial
images subtly encode cues for malicious task execution. Subsequently, we
present Textual Guidance Enhancement, where a large language model is leveraged
to infer the black-box defensive system prompt through adversarial meta
prompting and generate an malicious textual command that steers the agent's
output toward better compliance with attackers' requests. Extensive experiments
demonstrate that our method outperforms existing injection attacks, achieving
at least a +26.4% increase in attack success rates across diverse tasks.
Furthermore, we validate our attack's effectiveness in real-world multimodal
autonomous agents, highlighting its potential implications for safety-critical
applications.

</details>

### [111] [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
*Kyle Buettner,Jacob Emmerson,Adriana Kovashka*

Main category: cs.CV

TLDR: 本文提出了一种基于LLM的多模态重述策略，通过修改英文描述后再翻译，提升多语言视觉语言模型对感知多样性的理解，并在德日文本-图像检索任务中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的多语言能力主要依赖英语数据，导致感知偏见和模型灵活性不足。本文旨在通过数据高效的方法增强模型对感知多样性的理解。

Method: 提出基于LLM的多模态重述策略，修改英文描述后再翻译，并结合母语数据引导的多模态机制。通过重述数据增强训练，提升模型性能。

Result: 在德日文本-图像检索任务中，平均召回率提升3.5，非母语错误案例提升4.7。

Conclusion: 该方法有效提升了多语言视觉语言模型对感知多样性的理解，并通过分析对象描述差异，提供了跨数据集和跨语言泛化的见解。

Abstract: There are many ways to describe, name, and group objects when captioning an
image. Differences are evident when speakers come from diverse cultures due to
the unique experiences that shape perception. Machine translation of captions
has pushed multilingual capabilities in vision-language models (VLMs), but data
comes mainly from English speakers, indicating a perceptual bias and lack of
model flexibility. In this work, we address this challenge and outline a
data-efficient framework to instill multilingual VLMs with greater
understanding of perceptual diversity. We specifically propose an LLM-based,
multimodal recaptioning strategy that alters the object descriptions of English
captions before translation. The greatest benefits are demonstrated in a
targeted multimodal mechanism guided by native speaker data. By adding produced
rewrites as augmentations in training, we improve on German and Japanese
text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on
non-native error cases). We further propose a mechanism to analyze the specific
object description differences across datasets, and we offer insights into
cross-dataset and cross-language generalization.

</details>

### [112] [Efficient Spiking Point Mamba for Point Cloud Analysis](https://arxiv.org/abs/2504.14371)
*Peixi Wu,Bosong Chai,Menghua Zheng,Wei Li,Zhangchi Hu,Jie Chen,Zheyu Zhang,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TLDR: SPM是一种基于Mamba的3D脉冲神经网络，通过结合Mamba的序列建模能力和SNN的时间特征提取，显著提升了性能并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D SNN在长程依赖问题上表现不佳，而Mamba的高效计算和序列建模能力为解决这一问题提供了可能。

Method: 提出SPM，包括Hierarchical Dynamic Encoding（HDE）和Spiking Mamba Block（SMB），并采用非对称SNN-ANN架构进行预训练和微调。

Result: 在ScanObjectNN和ShapeNetPart上分别提升了6.2%、6.1%、7.4%和1.9%的性能，能耗比ANN低3.5倍。

Conclusion: SPM在性能和能效上均优于现有SNN模型，为3D时空特征提取提供了新思路。

Abstract: Bio-inspired Spiking Neural Networks (SNNs) provide an energy-efficient way
to extract 3D spatio-temporal features. However, existing 3D SNNs have
struggled with long-range dependencies until the recent emergence of Mamba,
which offers superior computational efficiency and sequence modeling
capability. In this work, we propose Spiking Point Mamba (SPM), the first
Mamba-based SNN in the 3D domain. Due to the poor performance of simply
transferring Mamba to 3D SNNs, SPM is designed to utilize both the sequence
modeling capabilities of Mamba and the temporal feature extraction of SNNs.
Specifically, we first introduce Hierarchical Dynamic Encoding (HDE), an
improved direct encoding method that effectively introduces dynamic temporal
mechanism, thereby facilitating temporal interactions. Then, we propose a
Spiking Mamba Block (SMB), which builds upon Mamba while learning
inter-time-step features and minimizing information loss caused by spikes.
Finally, to further enhance model performance, we adopt an asymmetric SNN-ANN
architecture for spike-based pre-training and finetune. Compared with the
previous state-of-the-art SNN models, SPM improves OA by +6.2%, +6.1%, and
+7.4% on three variants of ScanObjectNN, and boosts instance mIOU by +1.9% on
ShapeNetPart. Meanwhile, its energy consumption is at least 3.5x lower than
that of its ANN counterpart. The code will be made publicly available.

</details>

### [113] [LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers](https://arxiv.org/abs/2504.14386)
*Md Abtahi Majeed Chowdhury,Md Rifat Ur Rahman,Akil Ahmad Taki*

Main category: cs.CV

TLDR: 论文提出了一种可学习的补丁排序方法LOOPE，用于优化视觉变换器（ViT）中的位置嵌入（PE），并通过新基准测试框架验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了补丁排序对位置嵌入的影响，导致2D网格到1D序列映射时的空间信息丢失。

Method: 提出LOOPE方法，通过优化补丁排序来改进空间表示，并引入“三细胞实验”框架评估PE效果。

Result: LOOPE显著提高了ViT的分类准确性，新框架揭示了30-35%的性能差异，远超传统评估的4-6%。

Conclusion: LOOPE能更有效地保留相对和绝对位置信息，为PE优化提供了新思路。

Abstract: Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)
by providing spatial information otherwise lost due to the permutation
invariant nature of self attention. While absolute positional embeddings (APE)
have shown theoretical advantages over relative positional embeddings (RPE),
particularly due to the ability of sinusoidal functions to preserve spatial
inductive biases like monotonicity and shift invariance, a fundamental
challenge arises when mapping a 2D grid to a 1D sequence. Existing methods have
mostly overlooked or never explored the impact of patch ordering in positional
embeddings. To address this, we propose LOOPE, a learnable patch-ordering
method that optimizes spatial representation for a given set of frequencies,
providing a principled approach to patch order optimization. Empirical results
show that our PE significantly improves classification accuracy across various
ViT architectures. To rigorously evaluate the effectiveness of positional
embeddings, we introduce the "Three Cell Experiment", a novel benchmarking
framework that assesses the ability of PEs to retain relative and absolute
positional information across different ViT architectures. Unlike standard
evaluations, which typically report a performance gap of 4 to 6% between models
with and without PE, our method reveals a striking 30 to 35% difference,
offering a more sensitive diagnostic tool to measure the efficacy of PEs. Our
experimental analysis confirms that the proposed LOOPE demonstrates enhanced
effectiveness in retaining both relative and absolute positional information.

</details>

### [114] [How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?](https://arxiv.org/abs/2504.14391)
*Rahul Thapa,Andrew Li,Qingyang Wu,Bryan He,Yuki Sahashi,Christina Binder,Angela Zhang,Ben Athiwaratkun,Shuaiwen Leon Song,David Ouyang,James Zou*

Main category: cs.CV

TLDR: OpenBiomedVi数据集利用教育性生物医学视频训练视觉语言模型，显著提升性能，并引入新基准测试验证模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索非标准化、教育性生物医学视频是否能有效训练通用视觉语言模型。

Method: 构建OpenBiomedVi数据集，包含1031小时视频-字幕和Q/A对，通过多步人工参与流程筛选，并引入新基准测试MIMICEchoQA和SurgeryVideoQA。

Result: Qwen-2-VL模型在视频和图像任务上表现显著提升，2B模型在视频任务上提升98.7%，7B模型在视频任务上提升37.09%。

Conclusion: 教育性生物医学视频为生物医学视觉语言模型提供了有效的训练信号。

Abstract: Publicly available biomedical videos, such as those on YouTube, serve as
valuable educational resources for medical students. Unlike standard machine
learning datasets, these videos are designed for human learners, often mixing
medical imagery with narration, explanatory diagrams, and contextual framing.
In this work, we investigate whether such pedagogically rich, yet
non-standardized and heterogeneous videos can effectively teach general-domain
vision-language models biomedical knowledge. To this end, we introduce
OpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031
hours of video-caption and Q/A pairs, curated through a multi-step
human-in-the-loop pipeline. Diverse biomedical video datasets are rare, and
OpenBiomedVid fills an important gap by providing instruction-style supervision
grounded in real-world educational content. Surprisingly, despite the informal
and heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models
exhibit substantial performance improvements across most benchmarks. The 2B
model achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on
text tasks. The 7B model shows improvements of 37.09% on video and 11.2% on
image tasks, with a slight degradation of 2.7% on text tasks compared to their
respective base models. To address the lack of standardized biomedical video
evaluation datasets, we also introduce two new expert curated benchmarks,
MIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves
gains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%,
respectively, demonstrating the models' ability to generalize and perform
biomedical video understanding on cleaner and more standardized datasets than
those seen during training. These results suggest that educational videos
created for human learning offer a surprisingly effective training signal for
biomedical VLMs.

</details>

### [115] [Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models](https://arxiv.org/abs/2504.14395)
*Chung-En,Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TLDR: Hydra是一个自适应代理框架，通过迭代推理和跨模型验证提升视觉语言模型的对抗鲁棒性和减少幻觉，无需显式对抗防御即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注对抗防御或幻觉后处理，缺乏统一的鲁棒性策略，而Hydra旨在填补这一空白。

Method: Hydra采用Action-Critique循环，结合Chain-of-Thought和In-Context Learning技术，动态优化输出。

Result: 在四个VLM、三个幻觉基准和两种对抗攻击策略上，Hydra表现优于现有插件VLM和去幻觉方法。

Conclusion: Hydra为提升VLM在现实应用中的可靠性提供了可扩展、无需训练的解决方案。

Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to
address adversarial robustness and hallucination mitigation, both of which
impact factual accuracy in high-stakes applications such as defense and
healthcare. Existing methods primarily focus on either adversarial defense or
hallucination post-hoc correction, leaving a gap in unified robustness
strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that
enhances plug-in VLMs through iterative reasoning, structured critiques, and
cross-model verification, improving both resilience to adversarial
perturbations and intrinsic model errors. Hydra employs an Action-Critique
Loop, where it retrieves and critiques visual information, leveraging
Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine
outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to
both adversarial manipulations and intrinsic model errors, making it robust to
malicious perturbations and hallucination-related inaccuracies. We evaluate
Hydra on four VLMs, three hallucination benchmarks, two adversarial attack
strategies, and two adversarial defense methods, assessing performance on both
clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs
and state-of-the-art (SOTA) dehallucination methods, even without explicit
adversarial defenses, demonstrating enhanced robustness and factual
consistency. By bridging adversarial resistance and hallucination mitigation,
Hydra provides a scalable, training-free solution for improving the reliability
of VLMs in real-world applications.

</details>

### [116] [SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation](https://arxiv.org/abs/2504.14396)
*Minho Park,Taewoong Kang,Jooyeol Yun,Sungwon Hwang,Jaegul Choo*

Main category: cs.CV

TLDR: SphereDiff提出了一种无需额外调优的360度全景图像和视频生成方法，通过球形潜在表示和失真感知加权平均技术，解决了传统ERP方法中的失真问题。


<details>
  <summary>Details</summary>
Motivation: AR/VR应用对高质量360度全景内容的需求日益增长，但现有方法因ERP投影的严重失真而难以满足需求。

Method: 定义了球形潜在表示以均匀分布视角，扩展了MultiDiffusion到球形潜在空间，并提出了球形潜在采样方法和失真感知加权平均技术。

Result: SphereDiff在生成360度全景内容时优于现有方法，保持了高保真度。

Conclusion: SphereDiff为沉浸式AR/VR应用提供了一种高效的解决方案，代码已开源。

Abstract: The increasing demand for AR/VR applications has highlighted the need for
high-quality 360-degree panoramic content. However, generating high-quality
360-degree panoramic images and videos remains a challenging task due to the
severe distortions introduced by equirectangular projection (ERP). Existing
approaches either fine-tune pretrained diffusion models on limited ERP datasets
or attempt tuning-free methods that still rely on ERP latent representations,
leading to discontinuities near the poles. In this paper, we introduce
SphereDiff, a novel approach for seamless 360-degree panoramic image and video
generation using state-of-the-art diffusion models without additional tuning.
We define a spherical latent representation that ensures uniform distribution
across all perspectives, mitigating the distortions inherent in ERP. We extend
MultiDiffusion to spherical latent space and propose a spherical latent
sampling method to enable direct use of pretrained diffusion models. Moreover,
we introduce distortion-aware weighted averaging to further improve the
generation quality in the projection process. Our method outperforms existing
approaches in generating 360-degree panoramic content while maintaining high
fidelity, making it a robust solution for immersive AR/VR applications. The
code is available here. https://github.com/pmh9960/SphereDiff

</details>

### [117] [Adversarial Attack for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2504.14423)
*Qiang Chen,Xiao Wang,Haowen Wang,Bo Jiang,Lin Zhu,Dawei Zhang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TLDR: 本文提出了一种针对RGB-Event视觉跟踪的跨模态对抗攻击算法，研究了事件体素和帧两种表示形式，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: RGB-Event流跟踪算法在对抗攻击和防御方面的研究相对稀缺，本文旨在填补这一空白。

Method: 针对RGB-Event体素，通过对抗损失优化扰动生成对抗样本；针对离散事件体素，采用两步攻击策略；针对RGB-Event帧，通过多模态数据梯度信息优化跨模态通用扰动。

Result: 实验表明，该方法在单模态和多模态场景下显著降低了跟踪器的性能。

Conclusion: 本文提出的跨模态对抗攻击算法在RGB-Event视觉跟踪中表现出色，填补了相关研究领域的空白。

Abstract: Visual object tracking is a crucial research topic in the fields of computer
vision and multi-modal fusion. Among various approaches, robust visual tracking
that combines RGB frames with Event streams has attracted increasing attention
from researchers. While striving for high accuracy and efficiency in tracking,
it is also important to explore how to effectively conduct adversarial attacks
and defenses on RGB-Event stream tracking algorithms, yet research in this area
remains relatively scarce. To bridge this gap, in this paper, we propose a
cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because
of the diverse representations of Event streams, and given that Event voxels
and frames are more commonly used, this paper will focus on these two
representations for an in-depth study. Specifically, for the RGB-Event voxel,
we first optimize the perturbation by adversarial loss to generate RGB frame
adversarial examples. For discrete Event voxel representations, we propose a
two-step attack strategy, more in detail, we first inject Event voxels into the
target region as initialized adversarial examples, then, conduct a
gradient-guided optimization by perturbing the spatial location of the Event
voxels. For the RGB-Event frame based tracking, we optimize the cross-modal
universal perturbation by integrating the gradient information from multimodal
data. We evaluate the proposed approach against attacks on three widely used
RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive
experiments show that our method significantly reduces the performance of the
tracker across numerous datasets in both unimodal and multimodal scenarios. The
source code will be released on
https://github.com/Event-AHU/Adversarial_Attack_Defense

</details>

### [118] [ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations](https://arxiv.org/abs/2504.14429)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TLDR: 论文提出了一种解决视频语言模型（ResNetVLLM）中幻觉问题的方法，通过两步策略（检测和缓解）提升模型的事实一致性，并在ActivityNet-QA基准测试中显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和视频语言模型（VideoLLMs）存在幻觉问题，生成的文本可能与实际内容不符，影响多模态任务的可靠性。

Method: 提出两步策略：1）使用改进的Lynx模型检测生成内容与真实视频的语义对齐；2）采用检索增强生成（RAG）动态构建知识库以缓解幻觉。

Result: 改进后的模型ResNetVLLM-2在ActivityNet-QA基准测试中准确率从54.8%提升至65.3%。

Conclusion: 该研究通过检测和缓解幻觉问题，显著提升了视频语言模型的可靠性。

Abstract: Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

</details>

### [119] [ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](https://arxiv.org/abs/2504.14432)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TLDR: ResNetVLLM是一种新型的跨模态框架，结合ResNet视觉编码器和大型语言模型，用于零样本视频理解，无需依赖预训练视频模型，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决零样本视频理解模型中依赖预训练视频模型的挑战，提出一种统一架构学习视觉和语义表示。

Method: 使用非预训练的ResNet提取视觉特征，与大型语言模型结合，生成视频输入的文本描述。

Result: 在MSRVTT-QA、MSVD-QA、TGIF-QA FrameQA和ActivityNet-QA等基准测试中达到最先进性能。

Conclusion: ResNetVLLM通过统一架构学习视觉和语义表示，显著提升了零样本视频理解的性能。

Abstract: In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel
cross-modal framework for zero-shot video understanding that integrates a
ResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM
addresses the challenges associated with zero-shot video models by avoiding
reliance on pre-trained video understanding models and instead employing a
non-pretrained ResNet to extract visual features. This design ensures the model
learns visual and semantic representations within a unified architecture,
enhancing its ability to generate accurate and contextually relevant textual
descriptions from video inputs. Our experimental results demonstrate that
ResNetVLLM achieves state-of-the-art performance in zero-shot video
understanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,
TGIF-QA FrameQA, and ActivityNet-QA.

</details>

### [120] [WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2504.14445)
*Mingya Zhang,Liang Wang,Limei Gu,Tingsheng Ling,Xianping Tao*

Main category: cs.CV

TLDR: 论文提出了一种基于小波变换的双向复制粘贴半监督医学图像分割框架（WT-BCP），通过结合低频和高频信息以及多输入多输出模型（XNet-Plus）提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 解决半监督医学图像分割中标记数据稀缺、分布不匹配、人工扰动导致的训练偏差以及低频和高频信息利用不足的问题。

Method: 采用小波变换提取低频和高频信息，结合双向复制粘贴技术增强未标记数据的理解，并使用XNet-Plus模型进行多输入多输出处理。

Result: 在2D和3D数据集上的实验验证了模型的有效性。

Conclusion: WT-BCP框架通过结合小波变换和多输出一致性训练，显著提升了半监督医学图像分割的性能。

Abstract: Semi-supervised medical image segmentation (SSMIS) shows promise in reducing
reliance on scarce labeled medical data. However, SSMIS field confronts
challenges such as distribution mismatches between labeled and unlabeled data,
artificial perturbations causing training biases, and inadequate use of raw
image information, especially low-frequency (LF) and high-frequency (HF)
components.To address these challenges, we propose a Wavelet Transform based
Bidirectional Copy-Paste SSMIS framework, named WT-BCP, which improves upon the
Mean Teacher approach. Our method enhances unlabeled data understanding by
copying random crops between labeled and unlabeled images and employs WT to
extract LF and HF details.We propose a multi-input and multi-output model named
XNet-Plus, to receive the fused information after WT. Moreover, consistency
training among multiple outputs helps to mitigate learning biases introduced by
artificial perturbations. During consistency training, the mixed images
resulting from WT are fed into both models, with the student model's output
being supervised by pseudo-labels and ground-truth. Extensive experiments
conducted on 2D and 3D datasets confirm the effectiveness of our model.Code:
https://github.com/simzhangbest/WT-BCP.

</details>

### [121] [Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability](https://arxiv.org/abs/2504.14446)
*Carlos Caetano,Gabriel O. dos Santos,Caio Petrucci,Artur Barros,Camila Laranjeira,Leo S. F. Ribeiro,Júlia F. de Mendonça,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.CV

TLDR: 论文探讨了在AI数据集中使用儿童图像的伦理问题，并提出了一种检测和移除这些图像的流程。


<details>
  <summary>Details</summary>
Motivation: 儿童图像的公开使用引发了隐私、同意和数据保护等伦理问题，但目前缺乏有效的解决方案。

Method: 提出了一种基于视觉语言模型的流程，并在Visual Question Answering任务和Open Images V7数据集上进行了测试。

Result: 流程在检测和移除儿童图像方面表现有效，为未来研究提供了基线。

Conclusion: 呼吁研究社区反思并采取行动保护儿童权利，同时鼓励开发更全面的工具。

Abstract: Including children's images in datasets has raised ethical concerns,
particularly regarding privacy, consent, data protection, and accountability.
These datasets, often built by scraping publicly available images from the
Internet, can expose children to risks such as exploitation, profiling, and
tracking. Despite the growing recognition of these issues, approaches for
addressing them remain limited. We explore the ethical implications of using
children's images in AI datasets and propose a pipeline to detect and remove
such images. As a use case, we built the pipeline on a Vision-Language Model
under the Visual Question Answering task and tested it on the #PraCegoVer
dataset. We also evaluate the pipeline on a subset of 100,000 images from the
Open Images V7 dataset to assess its effectiveness in detecting and removing
images of children. The pipeline serves as a baseline for future research,
providing a starting point for more comprehensive tools and methodologies.
While we leverage existing models trained on potentially problematic data, our
goal is to expose and address this issue. We do not advocate for training or
deploying such models, but instead call for urgent community reflection and
action to protect children's rights. Ultimately, we aim to encourage the
research community to exercise - more than an additional - care in creating new
datasets and to inspire the development of tools to protect the fundamental
rights of vulnerable groups, particularly children.

</details>

### [122] [Causal Disentanglement for Robust Long-tail Medical Image Generation](https://arxiv.org/abs/2504.14450)
*Weizhi Nie,Zichun Zhang,Weijie Wang,Bruno Lepri,Anan Liu,Nicu Seb*

Main category: cs.CV

TLDR: 提出一种基于因果解耦和文本引导的医学图像生成框架，解决数据稀缺和类别不平衡问题，生成高质量且多样化的反事实医学图像。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据稀缺且类别分布不平衡，同时需要保持解剖结构稳定性和避免失真，现有方法难以生成高质量且多样化的图像。

Method: 通过因果解耦实现病理和结构特征分离，结合文本引导的扩散模型生成反事实图像，并利用大语言模型提取病灶信息优化生成。

Result: 生成的反事实图像具有临床相关性、结构稳定性和多样性，同时通过噪声优化提升了长尾类别的性能。

Conclusion: 该框架有效解决了医学图像生成中的数据稀缺和类别不平衡问题，提升了生成图像的质量和多样性。

Abstract: Counterfactual medical image generation effectively addresses data scarcity
and enhances the interpretability of medical images. However, due to the
complex and diverse pathological features of medical images and the imbalanced
class distribution in medical data, generating high-quality and diverse medical
images from limited data is significantly challenging. Additionally, to fully
leverage the information in limited data, such as anatomical structure
information and generate more structurally stable medical images while avoiding
distortion or inconsistency. In this paper, in order to enhance the clinical
relevance of generated data and improve the interpretability of the model, we
propose a novel medical image generation framework, which generates independent
pathological and structural features based on causal disentanglement and
utilizes text-guided modeling of pathological features to regulate the
generation of counterfactual images. First, we achieve feature separation
through causal disentanglement and analyze the interactions between features.
Here, we introduce group supervision to ensure the independence of pathological
and identity features. Second, we leverage a diffusion model guided by
pathological findings to model pathological features, enabling the generation
of diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging
a large language model to extract lesion severity and location from medical
reports. Additionally, we improve the performance of the latent diffusion model
on long-tailed categories through initial noise optimization.

</details>

### [123] [Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding](https://arxiv.org/abs/2504.14460)
*Junyan Su,Baozhu Zhao,Xiaohan Zhang,Qi Liu*

Main category: cs.CV

TLDR: Metamon-GS通过方差引导的密集化策略和多级哈希网格，解决了3D高斯泼溅（3DGS）在渲染性能提升中的挑战，显著提高了新视角合成的质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS在渲染性能提升方面仍面临挑战，如特征嵌入难以准确表示不同光照条件下的颜色，以及缺乏有效的密集化策略导致模糊和针状伪影。

Method: 提出Metamon-GS，采用方差引导的密集化策略和多级哈希网格。前者针对高梯度方差的高斯分布区域进行补偿，后者研究全局光照条件以准确解析颜色。

Result: 在公开数据集上的实验表明，Metamon-GS优于基线模型和先前版本，实现了更高质量的新视角渲染。

Conclusion: Metamon-GS通过创新的密集化和光照解析方法，显著提升了3DGS的渲染性能和新视角合成质量。

Abstract: The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view
synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point
features with anchor embeddings has significantly enhanced the performance of
newer 3DGS variants. While significant advances have been made, it is still
challenging to boost rendering performance. Feature embeddings have difficulty
accurately representing colors from different perspectives under varying
lighting conditions, which leads to a washed-out appearance. Another reason is
the lack of a proper densification strategy that prevents Gaussian point growth
in thinly initialized areas, resulting in blurriness and needle-shaped
artifacts. To address them, we propose Metamon-GS, from innovative viewpoints
of variance-guided densification strategy and multi-level hash grid. The
densification strategy guided by variance specifically targets Gaussians with
high gradient variance in pixels and compensates for the importance of regions
with extra Gaussians to improve reconstruction. The latter studies implicit
global lighting conditions and accurately interprets color from different
perspectives and feature embeddings. Our thorough experiments on publicly
available datasets show that Metamon-GS surpasses its baseline model and
previous versions, delivering superior quality in rendering novel views.

</details>

### [124] [LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation](https://arxiv.org/abs/2504.14467)
*Jiachen Li,Qing Xie,Xiaohan Yu,Hongyun Wang,Jinyu Xu,Yongjian Liu,Yongsheng Gao*

Main category: cs.CV

TLDR: LGD框架利用多模态大语言模型生成描述，提升视觉-语言模型在零样本指代图像分割中的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决零样本指代图像分割中因自由形式指代表达的模糊性和多样性导致的错误目标定位问题。

Method: 设计属性提示和周围提示，生成关键属性和周围对象描述，引入三种视觉-文本匹配分数评估相似性。

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上取得新SOTA，oIoU和mIoU最大提升分别为9.97%和11.29%。

Conclusion: LGD通过生成描述显著提升了区域-文本匹配性能，验证了多模态大语言模型在指代分割中的潜力。

Abstract: Zero-shot referring image segmentation aims to locate and segment the target
region based on a referring expression, with the primary challenge of aligning
and matching semantics across visual and textual modalities without training.
Previous works address this challenge by utilizing Vision-Language Models and
mask proposal networks for region-text matching. However, this paradigm may
lead to incorrect target localization due to the inherent ambiguity and
diversity of free-form referring expressions. To alleviate this issue, we
present LGD (Leveraging Generative Descriptions), a framework that utilizes the
advanced language generation capabilities of Multi-Modal Large Language Models
to enhance region-text matching performance in Vision-Language Models.
Specifically, we first design two kinds of prompts, the attribute prompt and
the surrounding prompt, to guide the Multi-Modal Large Language Models in
generating descriptions related to the crucial attributes of the referent
object and the details of surrounding objects, referred to as attribute
description and surrounding description, respectively. Secondly, three
visual-text matching scores are introduced to evaluate the similarity between
instance-level visual features and textual features, which determines the mask
most associated with the referring expression. The proposed method achieves new
state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and
RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU
compared to previous methods.

</details>

### [125] [Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis](https://arxiv.org/abs/2504.14470)
*Jingjing Ren,Wenbo Li,Zhongdao Wang,Haoze Sun,Bangzhen Liu,Haoyu Chen,Jiaqi Xu,Aoxue Li,Shifeng Zhang,Bin Shao,Yong Guo,Lei Zhu*

Main category: cs.CV

TLDR: Turbo2K是一个高效生成2K视频的框架，通过压缩潜在空间和知识蒸馏技术显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着消费者对超清晰视觉的需求增加，2K视频合成的需求上升，但现有扩散变换器（DiTs）在2K分辨率下计算成本过高。

Method: Turbo2K采用压缩潜在空间和知识蒸馏策略，并设计分层两阶段合成框架，以降低计算复杂度。

Result: Turbo2K在生成5秒24fps的2K视频时，推理速度比现有方法快20倍。

Conclusion: Turbo2K通过高效设计和知识蒸馏，实现了2K视频生成的实用化和规模化。

Abstract: Demand for 2K video synthesis is rising with increasing consumer expectations
for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated
remarkable capabilities in high-quality video generation, scaling them to 2K
resolution remains computationally prohibitive due to quadratic growth in
memory and processing costs. In this work, we propose Turbo2K, an efficient and
practical framework for generating detail-rich 2K videos while significantly
improving training and inference efficiency. First, Turbo2K operates in a
highly compressed latent space, reducing computational complexity and memory
footprint, making high-resolution video synthesis feasible. However, the high
compression ratio of the VAE and limited model size impose constraints on
generative quality. To mitigate this, we introduce a knowledge distillation
strategy that enables a smaller student model to inherit the generative
capacity of a larger, more powerful teacher model. Our analysis reveals that,
despite differences in latent spaces and architectures, DiTs exhibit structural
similarities in their internal representations, facilitating effective
knowledge transfer. Second, we design a hierarchical two-stage synthesis
framework that first generates multi-level feature at lower resolutions before
guiding high-resolution video generation. This approach ensures structural
coherence and fine-grained detail refinement while eliminating redundant
encoding-decoding overhead, further enhancing computational efficiency.Turbo2K
achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos
with significantly reduced computational cost. Compared to existing methods,
Turbo2K is up to 20$\times$ faster for inference, making high-resolution video
generation more scalable and practical for real-world applications.

</details>

### [126] [Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space](https://arxiv.org/abs/2504.14471)
*Yichi Zhang,Qianqian Yang*

Main category: cs.CV

TLDR: PICO是一个基于隐式神经表示（INR）的点云压缩框架，通过分解几何和属性压缩任务，并引入LeAFNet网络架构，显著提升了压缩效率和性能。


<details>
  <summary>Details</summary>
Motivation: 点云压缩是3D数据处理中的关键问题，传统方法存在效率不足的问题。PICO旨在通过INR和新型网络架构解决这一问题。

Method: 将点云压缩分为几何和属性压缩两个阶段，使用LeAFNet（基于可学习激活函数的网络）优化隐式函数表示，并通过量化和熵编码提升压缩效率。

Result: LeAFNet在INR点云压缩中优于传统MLP，PICO在几何压缩上比MPEG标准平均提升4.92 dB D1 PSNR，联合压缩中PCQM增益为2.7×10⁻³。

Conclusion: PICO通过INR和LeAFNet实现了高效的点云压缩，性能优于现有标准，为3D数据处理提供了新思路。

Abstract: Implicit Neural Representations (INRs), also known as neural fields, have
emerged as a powerful paradigm in deep learning, parameterizing continuous
spatial fields using coordinate-based neural networks. In this paper, we
propose \textbf{PICO}, an INR-based framework for static point cloud
compression. Unlike prevailing encoder-decoder paradigms, we decompose the
point cloud compression task into two separate stages: geometry compression and
attribute compression, each with distinct INR optimization objectives. Inspired
by Kolmogorov-Arnold Networks (KANs), we introduce a novel network
architecture, \textbf{LeAFNet}, which leverages learnable activation functions
in the latent space to better approximate the target signal's implicit
function. By reformulating point cloud compression as neural parameter
compression, we further improve compression efficiency through quantization and
entropy coding. Experimental results demonstrate that \textbf{LeAFNet}
outperforms conventional MLPs in INR-based point cloud compression.
Furthermore, \textbf{PICO} achieves superior geometry compression performance
compared to the current MPEG point cloud compression standard, yielding an
average improvement of $4.92$ dB in D1 PSNR. In joint geometry and attribute
compression, our approach exhibits highly competitive results, with an average
PCQM gain of $2.7 \times 10^{-3}$.

</details>

### [127] [Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation](https://arxiv.org/abs/2504.14481)
*Guoyi Zhang,Siyang Chen,Guangsheng Xu,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TLDR: 提出了一种轻量级PEFT框架LSR-ST，通过引入形状偏置的归纳先验，提升视觉基础模型在复杂场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在复杂场景（如伪装和红外图像）中表现不佳，主要原因是其固有的纹理偏置在微调过程中被放大，限制了在纹理稀疏环境中的泛化能力。

Method: 提出了LSR-ST框架，通过HDConv Block捕获形状感知特征，结合大核注意力和残差学习，满足形状偏置的三个关键条件：大感受野、多阶特征交互和稀疏连接。

Result: 在17个数据集和6个任务上，仅使用4.719M可训练参数，LSR-ST显著提升了SAM2-UNet的性能。

Conclusion: LSR-ST通过表示效率的概念，为复杂视觉环境中的鲁棒和适应性强的视觉基础模型提供了新思路。

Abstract: Foreground segmentation is crucial for scene understanding, yet
parameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often
fails in complex scenarios, such as camouflage and infrared imagery. We
attribute this challenge to the inherent texture bias in VFMs, which is
exacerbated during fine-tuning and limits generalization in texture-sparse
environments. To address this, we propose Ladder Shape-bias Representation
Side-tuning (LSR-ST), a lightweight PEFT framework that enhances model
robustness by introducing shape-biased inductive priors. LSR-ST captures
shape-aware features using a simple HDConv Block, which integrates large-kernel
attention and residual learning. The method satisfies three key conditions for
inducing shape bias: large receptive fields, multi-order feature interactions,
and sparse connectivity. Our analysis reveals that these improvements stem from
representation efficiency-the ability to extract task-relevant, structurally
grounded features while minimizing redundancy. We formalize this concept via
Information Bottleneck theory and advocate for it as a key PEFT objective.
Unlike traditional NLP paradigms that focus on optimizing parameters and
memory, visual tasks require models that extract task-defined semantics, rather
than just relying on pre-encoded features. This shift enables our approach to
move beyond conventional trade-offs, offering more robust and generalizable
solutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves
consistent improvements across 17 datasets and 6 tasks using only 4.719M
trainable parameters. These results highlight the potential of representation
efficiency for robust and adaptable VFMs within complex visual environments.

</details>

### [128] [STARS: Sparse Learning Correlation Filter with Spatio-temporal Regularization and Super-resolution Reconstruction for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14491)
*Shang Zhang,Xiaobo Ding,Huanbin Zhang,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TLDR: STARS是一种基于稀疏学习的相关滤波器跟踪器，结合时空正则化和超分辨率重建，显著提升了热红外目标跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 热红外图像分辨率低且存在干扰，限制了跟踪器的性能。

Method: 采用自适应稀疏滤波和时域滤波提取目标特征，引入边缘保持稀疏正则化稳定特征，并提出梯度增强超分辨率方法提升图像分辨率。

Result: 在多个基准测试中，STARS表现优于现有最先进的跟踪器。

Conclusion: STARS首次将超分辨率方法集成到稀疏学习框架中，显著提升了热红外目标跟踪的鲁棒性。

Abstract: Thermal infrared (TIR) target tracking methods often adopt the correlation
filter (CF) framework due to its computational efficiency. However, the low
resolution of TIR images, along with tracking interference, significantly
limits the perfor-mance of TIR trackers. To address these challenges, we
introduce STARS, a novel sparse learning-based CF tracker that incorporates
spatio-temporal regulari-zation and super-resolution reconstruction. First, we
apply adaptive sparse filter-ing and temporal domain filtering to extract key
features of the target while reduc-ing interference from background clutter and
noise. Next, we introduce an edge-preserving sparse regularization method to
stabilize target features and prevent excessive blurring. This regularization
integrates multiple terms and employs the alternating direction method of
multipliers to optimize the solution. Finally, we propose a gradient-enhanced
super-resolution method to extract fine-grained TIR target features and improve
the resolution of TIR images, addressing performance degradation in tracking
caused by low-resolution sequences. To the best of our knowledge, STARS is the
first to integrate super-resolution methods within a sparse learning-based CF
framework. Extensive experiments on the LSOTB-TIR, PTB-TIR, VOT-TIR2015, and
VOT-TIR2017 benchmarks demonstrate that STARS outperforms state-of-the-art
trackers in terms of robustness.

</details>

### [129] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye,Miao Hua,Pengze Zhang,Xinghui Li,Qichao Sun,Songtao Zhao,Qian He,Xinglong Wu*

Main category: cs.CV

TLDR: DreamID是一种基于扩散模型的人脸交换方法，通过Triplet ID Group显式监督提升身份相似性和属性保留，结合SD Turbo加速模型实现快速推理。


<details>
  <summary>Details</summary>
Motivation: 传统人脸交换方法依赖隐式监督，效果不佳，DreamID旨在通过显式监督解决这一问题。

Method: 构建Triplet ID Group数据，利用SD Turbo加速模型，提出SwapNet、FaceNet和ID Adapter的架构。

Result: 在身份相似性、姿态和表情保留、图像保真度上优于现有方法，512*512分辨率下仅需0.6秒。

Conclusion: DreamID在复杂场景下表现优异，实现了高质量、快速的人脸交换。

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>

### [130] [Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction](https://arxiv.org/abs/2504.14516)
*Weirong Chen,Ganlin Zhang,Felix Wimbauer,Rui Wang,Nikita Araslanov,Andrea Vedaldi,Daniel Cremers*

Main category: cs.CV

TLDR: BA-Track通过3D点跟踪器分离相机运动与动态物体运动，结合传统SLAM的束调整与学习前端，提升动态场景下的相机位姿估计与3D重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统依赖静态环境假设，难以处理动态场景。现有方法或过滤动态元素或独立建模其运动，但效果有限。

Method: 使用3D点跟踪器分解相机运动与动态物体运动，结合束调整和轻量级后处理（尺度图）确保深度一致性。

Result: 实验表明，BA-Track在相机位姿估计和3D重建精度上显著优于现有方法。

Conclusion: BA-Track通过统一框架有效处理动态场景，提供一致且精确的相机跟踪与重建。

Abstract: Traditional SLAM systems, which rely on bundle adjustment, struggle with
highly dynamic scenes commonly found in casual videos. Such videos entangle the
motion of dynamic elements, undermining the assumption of static environments
required by traditional systems. Existing techniques either filter out dynamic
elements or model their motion independently. However, the former often results
in incomplete reconstructions, whereas the latter can lead to inconsistent
motion estimates. Taking a novel approach, this work leverages a 3D point
tracker to separate the camera-induced motion from the observed motion of
dynamic objects. By considering only the camera-induced component, bundle
adjustment can operate reliably on all scene elements as a result. We further
ensure depth consistency across video frames with lightweight post-processing
based on scale maps. Our framework combines the core of traditional SLAM --
bundle adjustment -- with a robust learning-based 3D tracker front-end.
Integrating motion decomposition, bundle adjustment and depth refinement, our
unified framework, BA-Track, accurately tracks the camera motion and produces
temporally coherent and scale-consistent dense reconstructions, accommodating
both static and dynamic elements. Our experiments on challenging datasets
reveal significant improvements in camera pose estimation and 3D reconstruction
accuracy.

</details>

### [131] [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
*Tong Zeng,Longfeng Wu,Liang Shi,Dawei Zhou,Feng Guo*

Main category: cs.CV

TLDR: DVBench是一个新的基准测试，用于评估视觉大语言模型（VLLMs）在安全关键驾驶场景中的表现，揭示了现有模型的局限性，并通过微调展示了改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有VLLMs在通用视觉任务中表现出色，但在安全关键领域（如自动驾驶）的表现尚未充分探索，亟需专门的评估工具。

Method: 开发了DVBench，包含10,000个人工标注的多选题，基于分层能力分类法，评估VLLMs在复杂驾驶场景中的感知和推理能力。

Result: 测试14个SOTA VLLMs，最高准确率不足40%；微调后模型准确率提升5.24-10.94个百分点，相对改进最高达43.59%。

Conclusion: DVBench为开发满足自动驾驶安全需求的VLLMs提供了评估框架和研究方向，强调针对性适应的重要性。

Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive
capabilities in general visual tasks such as image captioning and visual
question answering. However, their effectiveness in specialized,
safety-critical domains like autonomous driving remains largely unexplored.
Autonomous driving systems require sophisticated scene understanding in complex
environments, yet existing multimodal benchmarks primarily focus on normal
driving conditions, failing to adequately assess VLLMs' performance in
safety-critical scenarios. To address this, we introduce DVBench, a pioneering
benchmark designed to evaluate the performance of VLLMs in understanding
safety-critical driving videos. Built around a hierarchical ability taxonomy
that aligns with widely adopted frameworks for describing driving scenarios
used in assessing highly automated driving systems, DVBench features 10,000
multiple-choice questions with human-annotated ground-truth answers, enabling a
comprehensive evaluation of VLLMs' capabilities in perception and reasoning.
Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal
significant performance gaps, with no model achieving over 40% accuracy,
highlighting critical limitations in understanding complex driving scenarios.
To probe adaptability, we fine-tuned selected models using domain-specific data
from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage
points, with relative improvements of up to 43.59%. This improvement
underscores the necessity of targeted adaptation to bridge the gap between
general-purpose VLLMs and mission-critical driving applications. DVBench
establishes an essential evaluation framework and research roadmap for
developing VLLMs that meet the safety and robustness requirements for
real-world autonomous systems. We released the benchmark toolbox and the
fine-tuned model at: https://github.com/tong-zeng/DVBench.git.

</details>

### [132] [SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization](https://arxiv.org/abs/2504.14534)
*Liang Peng,Boxi Wu,Haoran Cheng,Yibo Zhao,Xiaofei He*

Main category: cs.CV

TLDR: 论文提出了一种名为SUDO的自监督直接偏好优化方法，用于改进文本到图像扩散模型，兼顾像素级细节和全局图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法主要优化像素级MSE损失，忽略了全局图像质量，导致感知质量和结构连贯性不足。

Method: SUDO通过自监督生成偏好图像对，结合直接偏好优化，同时优化像素级和全局图像质量。

Result: 实验表明，SUDO显著提升了Stable Diffusion 1.5和XL等模型的全局和局部图像质量。

Conclusion: SUDO是一种无需昂贵数据标注的有效替代方案，可无缝应用于任何文本到图像扩散模型。

Abstract: Previous text-to-image diffusion models typically employ supervised
fine-tuning (SFT) to enhance pre-trained base models. However, this approach
primarily minimizes the loss of mean squared error (MSE) at the pixel level,
neglecting the need for global optimization at the image level, which is
crucial for achieving high perceptual quality and structural coherence. In this
paper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a
novel paradigm that optimizes both fine-grained details at the pixel level and
global image quality. By integrating direct preference optimization into the
model, SUDO generates preference image pairs in a self-supervised manner,
enabling the model to prioritize global-level learning while complementing the
pixel-level MSE loss. As an effective alternative to supervised fine-tuning,
SUDO can be seamlessly applied to any text-to-image diffusion model.
Importantly, it eliminates the need for costly data collection and annotation
efforts typically associated with traditional direct preference optimization
methods. Through extensive experiments on widely-used models, including Stable
Diffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both
global and local image quality. The codes are provided at
\href{https://github.com/SPengLiang/SUDO}{this link}.

</details>

### [133] [FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models](https://arxiv.org/abs/2504.14535)
*Kuanting Wu,Kei Ota,Asako Kanezaki*

Main category: cs.CV

TLDR: FlowLoss通过直接比较生成视频和真实视频的光流场，结合噪声感知权重方案，提升了视频扩散模型的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（VDMs）在生成高质量视频时常面临时间一致性不足的问题，光流监督是一种潜在解决方案。

Method: 提出FlowLoss方法，直接比较生成视频和真实视频的光流场，并引入噪声感知权重方案以应对高噪声条件下的光流估计不可靠问题。

Result: 实验表明，FlowLoss提高了运动稳定性，并在训练早期加速了收敛。

Conclusion: FlowLoss为噪声条件生成模型中引入运动监督提供了实用见解。

Abstract: Video Diffusion Models (VDMs) can generate high-quality videos, but often
struggle with producing temporally coherent motion. Optical flow supervision is
a promising approach to address this, with prior works commonly employing
warping-based strategies that avoid explicit flow matching. In this work, we
explore an alternative formulation, FlowLoss, which directly compares flow
fields extracted from generated and ground-truth videos. To account for the
unreliability of flow estimation under high-noise conditions in diffusion, we
propose a noise-aware weighting scheme that modulates the flow loss across
denoising steps. Experiments on robotic video datasets suggest that FlowLoss
improves motion stability and accelerates convergence in early training stages.
Our findings offer practical insights for incorporating motion-based
supervision into noise-conditioned generative models.

</details>

### [134] [VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control](https://arxiv.org/abs/2504.14548)
*Lifeng Lin,Rongfeng Lu,Quan Chen,Haofan Ren,Ming Lu,Yaoqi Sun,Chenggang Yan,Anke Xue*

Main category: cs.CV

TLDR: VGNC是一种基于生成式新视角合成模型的验证引导高斯数量控制方法，旨在解决稀疏视角3D高斯泼溅（3DGS）重建中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角3D重建在实际应用中存在过拟合问题，现有方法虽有所进展但仍未完全解决。

Method: VGNC通过生成验证图像并基于此控制高斯数量，以减少过拟合。

Result: 实验表明VGNC不仅减少过拟合，还提升渲染质量并降低高斯点数量，从而减少存储需求和加速训练与渲染。

Conclusion: VGNC是首个利用生成验证图像解决稀疏视角3DGS过拟合问题的方法，效果显著。

Abstract: Sparse-view 3D reconstruction is a fundamental yet challenging task in
practical 3D reconstruction applications. Recently, many methods based on the
3D Gaussian Splatting (3DGS) framework have been proposed to address
sparse-view 3D reconstruction. Although these methods have made considerable
advancements, they still show significant issues with overfitting. To reduce
the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number
Control (VGNC) approach based on generative novel view synthesis (NVS) models.
To the best of our knowledge, this is the first attempt to alleviate the
overfitting issue of sparse-view 3DGS with generative validation images.
Specifically, we first introduce a validation image generation method based on
a generative NVS model. We then propose a Gaussian number control strategy that
utilizes generated validation images to determine the optimal Gaussian numbers,
thereby reducing the issue of overfitting. We conducted detailed experiments on
various sparse-view 3DGS baselines and datasets to evaluate the effectiveness
of VGNC. Extensive experiments show that our approach not only reduces
overfitting but also improves rendering quality on the test set while
decreasing the number of Gaussian points. This reduction lowers storage demands
and accelerates both training and rendering. The code will be released.

</details>

### [135] [Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection](https://arxiv.org/abs/2504.14553)
*Weijun Zhuang,Qizhang Li,Xin Li,Ming Liu,Xiaopeng Hong,Feng Gao,Fan Yang,Wangmeng Zuo*

Main category: cs.CV

TLDR: 论文提出了一种名为Grounding-MD的视频-语言预训练框架，用于开放世界中的时刻检测任务，通过跨模态融合编码器和文本引导的解码器实现高效视频-文本对齐，并在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于封闭场景，无法适应开放世界的需求，因此需要一种能够处理任意自然语言查询的灵活且可扩展的时刻检测框架。

Method: 提出了Grounding-MD框架，结合了跨模态融合编码器和文本引导的解码器，通过大规模预训练实现视频-文本对齐和跨任务协作。

Result: 在ActivityNet、THUMOS14等四个基准数据集上，Grounding-MD在零样本和监督设置下均取得了最先进的性能。

Conclusion: Grounding-MD为开放世界时刻检测提供了一种高效且灵活的解决方案，具有广泛的应用潜力。

Abstract: Temporal Action Detection and Moment Retrieval constitute two pivotal tasks
in video understanding, focusing on precisely localizing temporal segments
corresponding to specific actions or events. Recent advancements introduced
Moment Detection to unify these two tasks, yet existing approaches remain
confined to closed-set scenarios, limiting their applicability in open-world
contexts. To bridge this gap, we present Grounding-MD, an innovative, grounded
video-language pre-training framework tailored for open-world moment detection.
Our framework incorporates an arbitrary number of open-ended natural language
queries through a structured prompt mechanism, enabling flexible and scalable
moment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a
Text-Guided Fusion Decoder to facilitate comprehensive video-text alignment and
enable effective cross-task collaboration. Through large-scale pre-training on
temporal action detection and moment retrieval datasets, Grounding-MD
demonstrates exceptional semantic representation learning capabilities,
effectively handling diverse and complex query conditions. Comprehensive
evaluations across four benchmark datasets including ActivityNet, THUMOS14,
ActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD
establishes new state-of-the-art performance in zero-shot and supervised
settings in open-world moment detection scenarios. All source code and trained
models will be released.

</details>

### [136] [SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14566)
*Shang Zhang,HuiPan Guan,XiaoBo Ding,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TLDR: 提出了一种名为SMTT的新型热红外目标跟踪器，通过多任务学习、联合稀疏表示和自适应图正则化，有效解决了噪声、遮挡和快速目标运动等常见问题。


<details>
  <summary>Details</summary>
Motivation: 热红外目标跟踪在监控、自动驾驶和军事行动中至关重要，但面临噪声、遮挡和快速目标运动等挑战。

Method: 采用多任务学习框架，结合联合稀疏表示和自适应图正则化，使用加权混合范数正则化策略动态捕获空间和特征级相似性，并通过加速近端梯度法实现高效优化。

Result: 在VOT-TIR、PTB-TIR和LSOTB-TIR等基准数据集上的实验表明，SMTT在准确性、鲁棒性和计算效率方面表现优异。

Conclusion: SMTT是一种在复杂环境中可靠且高性能的热红外目标跟踪解决方案。

Abstract: Thermal infrared target tracking is crucial in applications such as
surveillance, autonomous driving, and military operations. In this paper, we
propose a novel tracker, SMTT, which effectively addresses common challenges in
thermal infrared imagery, such as noise, occlusion, and rapid target motion, by
leveraging multi-task learning, joint sparse representation, and adaptive graph
regularization. By reformulating the tracking task as a multi-task learning
problem, the SMTT tracker independently optimizes the representation of each
particle while dynamically capturing spatial and feature-level similarities
using a weighted mixed-norm regularization strategy. To ensure real-time
performance, we incorporate the Accelerated Proximal Gradient method for
efficient optimization. Extensive experiments on benchmark datasets - including
VOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior
accuracy, robustness, and computational efficiency. These results highlight
SMTT as a reliable and high-performance solution for thermal infrared target
tracking in complex environments.

</details>

### [137] [NTIRE 2025 Challenge on Image Super-Resolution ($\times$4): Methods and Results](https://arxiv.org/abs/2504.14582)
*Zheng Chen,Kai Liu,Jue Gong,Jingkai Wang,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Xiangyu Kong,Xiaoxuan Yu,Hyunhee Park,Suejin Han,Hakjae Jeon,Dafeng Zhang,Hyung-Ju Chun,Donghun Ryou,Inju Ha,Bohyung Han,Lu Zhao,Yuyi Zhang,Pengyu Yan,Jiawei Hu,Pengwei Liu,Fengjun Guo,Hongyuan Yu,Pufan Xu,Zhijuan Huang,Shuyuan Cui,Peng Guo,Jiahui Liu,Dongkai Zhang,Heng Zhang,Huiyuan Fu,Huadong Ma,Yanhui Guo,Sisi Tian,Xin Liu,Jinwen Liang,Jie Liu,Jie Tang,Gangshan Wu,Zeyu Xiao,Zhuoyuan Li,Yinxiang Zhang,Wenxuan Cai,Vijayalaxmi Ashok Aralikatti,Nikhil Akalwadi,G Gyaneshwar Rao,Chaitra Desai,Ramesh Ashok Tabib,Uma Mudenagudi,Marcos V. Conde,Alejandro Merino,Bruno Longarela,Javier Abad,Weijun Yuan,Zhan Li,Zhanglu Chen,Boyang Yao,Aagam Jain,Milan Kumar Singh,Ankit Kumar,Shubh Kawa,Divyavardhan Singh,Anjali Sarvaiya,Kishor Upla,Raghavendra Ramachandra,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu,Risheek V Hiremath,Yashaswini Palani,Yuxuan Jiang,Qiang Zhu,Siyue Teng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull,Jingwei Liao,Yuqing Yang,Wenda Shao,Junyi Zhao,Qisheng Xu,Kele Xu,Sunder Ali Khowaja,Ik Hyun Lee,Snehal Singh Tomar,Rajarshi Ray,Klaus Mueller,Sachin Chaudhary,Surya Vashisth,Akshay Dudhane,Praful Hambarde,Satya Naryan Tazi,Prashant Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Bilel Benjdira,Anas M. Ali,Wadii Boulila,Zahra Moammeri,Ahmad Mahmoudi-Aznaveh,Ali Karbasi,Hossein Motamednia,Liangyan Li,Guanhua Zhao,Kevin Le,Yimo Ning,Haoxuan Huang,Jun Chen*

Main category: cs.CV

TLDR: NTIRE 2025图像超分辨率挑战赛旨在通过双三次下采样恢复高分辨率图像，分为恢复和感知两个子赛道，共有286名参与者。


<details>
  <summary>Details</summary>
Motivation: 推动图像超分辨率技术的进步，提供有效的网络设计或解决方案。

Method: 挑战赛包括两个子赛道：恢复赛道（基于PSNR）和感知赛道（基于感知分数）。

Result: 286名参与者注册，25支团队提交有效作品。

Conclusion: 挑战赛作为基准推动了图像超分辨率技术的发展。

Abstract: This paper presents the NTIRE 2025 image super-resolution ($\times$4)
challenge, one of the associated competitions of the 10th NTIRE Workshop at
CVPR 2025. The challenge aims to recover high-resolution (HR) images from
low-resolution (LR) counterparts generated through bicubic downsampling with a
$\times$4 scaling factor. The objective is to develop effective network designs
or solutions that achieve state-of-the-art SR performance. To reflect the dual
objectives of image SR research, the challenge includes two sub-tracks: (1) a
restoration track, emphasizes pixel-wise accuracy and ranks submissions based
on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by
a perceptual score. A total of 286 participants registered for the competition,
with 25 teams submitting valid entries. This report summarizes the challenge
design, datasets, evaluation protocol, the main results, and methods of each
team. The challenge serves as a benchmark to advance the state of the art and
foster progress in image SR.

</details>

### [138] [Using street view imagery and deep generative modeling for estimating the health of urban forests](https://arxiv.org/abs/2504.14583)
*Akshit Gupta,Remko Uijlenhoet*

Main category: cs.CV

TLDR: 本文提出了一种利用街景图像、树木清单数据和气象条件监测城市森林健康的新方法，通过图像到图像转换网络估算NDVI和CTD参数，并与地面实测数据对比验证。


<details>
  <summary>Details</summary>
Motivation: 传统监测方法依赖人工和高成本设备，难以规模化；多光谱遥感技术又受限于部署和分辨率问题，因此需要一种更高效、低成本的方法。

Method: 使用街景图像、树木清单数据和气象条件作为输入，通过图像到图像转换网络估算NDVI和CTD参数，并与地面实测数据对比。

Result: 通过街景图像平台（如Google Street View）扩展，该方法有望实现城市森林的高效规模化监测。

Conclusion: 该方法为城市森林健康监测提供了一种低成本、可扩展的解决方案，有助于城市管理者更有效地管理森林资源。

Abstract: Healthy urban forests comprising of diverse trees and shrubs play a crucial
role in mitigating climate change. They provide several key advantages such as
providing shade for energy conservation, and intercepting rainfall to reduce
flood runoff and soil erosion. Traditional approaches for monitoring the health
of urban forests require instrumented inspection techniques, often involving a
high amount of human labor and subjective evaluations. As a result, they are
not scalable for cities which lack extensive resources. Recent approaches
involving multi-spectral imaging data based on terrestrial sensing and
satellites, are constrained respectively with challenges related to dedicated
deployments and limited spatial resolutions. In this work, we propose an
alternative approach for monitoring the urban forests using simplified inputs:
street view imagery, tree inventory data and meteorological conditions. We
propose to use image-to-image translation networks to estimate two urban forest
health parameters, namely, NDVI and CTD. Finally, we aim to compare the
generated results with ground truth data using an onsite campaign utilizing
handheld multi-spectral and thermal imaging sensors. With the advent and
expansion of street view imagery platforms such as Google Street View and
Mapillary, this approach should enable effective management of urban forests
for the authorities in cities at scale.

</details>

### [139] [NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results](https://arxiv.org/abs/2504.14600)
*Zheng Chen,Jingkai Wang,Kai Liu,Jue Gong,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Jianxing Zhang,Jinlong Wu,Jun Wang,Zheng Xie,Hakjae Jeon,Suejin Han,Hyung-Ju Chun,Hyunhee Park,Zhicun Yin,Junjie Chen,Ming Liu,Xiaoming Li,Chao Zhou,Wangmeng Zuo,Weixia Zhang,Dingquan Li,Kede Ma,Yun Zhang,Zhuofan Zheng,Yuyue Liu,Shizhen Tang,Zihao Zhang,Yi Ning,Hao Jiang,Wenjie An,Kangmeng Yu,Chenyang Wang,Kui Jiang,Xianming Liu,Junjun Jiang,Yingfu Zhang,Gang He,Siqi Wang,Kepeng Xu,Zhenyang Liu,Changxin Zhou,Shanlan Shen,Yubo Duan,Yiang Chen,Jin Guo,Mengru Yang,Jen-Wei Lee,Chia-Ming Lee,Chih-Chung Hsu,Hu Peng,Chunming He*

Main category: cs.CV

TLDR: NTIRE 2025挑战赛聚焦真实世界人脸修复，旨在提升感知质量和真实性，同时保持身份一致性。比赛吸引了141名参与者，最终10支团队在排名中取得有效成绩。


<details>
  <summary>Details</summary>
Motivation: 推动真实世界人脸修复领域的最新进展，提升感知质量和真实性，同时不限制计算资源或训练数据。

Method: 使用加权图像质量评估（IQA）分数和AdaFace模型作为身份检查器，评估参赛模型的性能。

Result: 13支团队提交有效模型，10支团队在最终排名中取得有效成绩。

Conclusion: 该挑战赛推动了真实世界人脸修复的性能提升，并提供了该领域最新趋势的深入概述。

Abstract: This paper provides a review of the NTIRE 2025 challenge on real-world face
restoration, highlighting the proposed solutions and the resulting outcomes.
The challenge focuses on generating natural, realistic outputs while
maintaining identity consistency. Its goal is to advance state-of-the-art
solutions for perceptual quality and realism, without imposing constraints on
computational resources or training data. The track of the challenge evaluates
performance using a weighted image quality assessment (IQA) score and employs
the AdaFace model as an identity checker. The competition attracted 141
registrants, with 13 teams submitting valid models, and ultimately, 10 teams
achieved a valid score in the final ranking. This collaborative effort advances
the performance of real-world face restoration while offering an in-depth
overview of the latest trends in the field.

</details>

### [140] [MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation](https://arxiv.org/abs/2504.14606)
*Siyi Jiao,Wenzheng Zeng,Yerong Li,Huayu Zhang,Changxin Gao,Nong Sang,Mike Zheng Shou*

Main category: cs.CV

TLDR: MP-Mat是一种新颖的3D感知和实例感知的抠图框架，通过多平面表示解决复杂场景中多实例边界分离的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景（如多实例边界交织或毛发细节）中表现不佳，MP-Mat旨在通过3D感知和实例感知的多平面表示提升性能。

Method: 提出两种多平面表示：基于深度的场景几何分割和实例级分割（包括背景），结合特征级和实例级信息。

Result: 实验证明MP-Mat在抠图任务中表现优异，且在图像编辑任务中零样本推理优于专业方法。

Conclusion: MP-Mat通过多平面表示显著提升了复杂场景的抠图效果，并展示了在图像编辑中的潜力。

Abstract: Human instance matting aims to estimate an alpha matte for each human
instance in an image, which is challenging as it easily fails in complex cases
requiring disentangling mingled pixels belonging to multiple instances along
hairy and thin boundary structures. In this work, we address this by
introducing MP-Mat, a novel 3D-and-instance-aware matting framework with
multiplane representation, where the multiplane concept is designed from two
different perspectives: scene geometry level and instance level. Specifically,
we first build feature-level multiplane representations to split the scene into
multiple planes based on depth differences. This approach makes the scene
representation 3D-aware, and can serve as an effective clue for splitting
instances in different 3D positions, thereby improving interpretability and
boundary handling ability especially in occlusion areas. Then, we introduce
another multiplane representation that splits the scene in an instance-level
perspective, and represents each instance with both matte and color. We also
treat background as a special instance, which is often overlooked by existing
methods. Such an instance-level representation facilitates both foreground and
background content awareness, and is useful for other down-stream tasks like
image editing. Once built, the representation can be reused to realize
controllable instance-level image editing with high efficiency. Extensive
experiments validate the clear advantage of MP-Mat in matting task. We also
demonstrate its superiority in image editing tasks, an area under-explored by
existing matting-focused methods, where our approach under zero-shot inference
even outperforms trained specialized image editing techniques by large margins.
Code is open-sourced at https://github.com/JiaoSiyi/MPMat.git}.

</details>

### [141] [VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image](https://arxiv.org/abs/2504.14618)
*Han Bi,Ge Yu,Yu He,Wenzhuo Liu,Zijie Zheng*

Main category: cs.CV

TLDR: 提出VM-BHINet，利用状态空间模型改进双手交互重建，显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡、模糊外观和计算效率方面存在问题，需改进双手交互建模。

Method: 引入状态空间模型（SSMs）和VM-IFEBlock，结合局部与全局特征操作。

Result: 在InterHand2.6M数据集上，MPJPE和MPVPE降低2-3%，优于现有方法。

Conclusion: VM-BHINet通过SSMs有效提升双手交互建模的准确性和效率。

Abstract: Understanding bimanual hand interactions is essential for realistic 3D pose
and shape reconstruction. However, existing methods struggle with occlusions,
ambiguous appearances, and computational inefficiencies. To address these
challenges, we propose Vision Mamba Bimanual Hand Interaction Network
(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to
enhance interaction modeling while improving computational efficiency. The core
component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),
combines SSMs with local and global feature operations, enabling deep
understanding of hand interactions. Experiments on the InterHand2.6M dataset
show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean
per-vertex position error (MPVPE) by 2-3%, significantly surpassing
state-of-the-art methods.

</details>

### [142] [Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts](https://arxiv.org/abs/2504.14621)
*Zhenkui Yang,Zeyi Huang,Ge Wang,Han Ding,Tony Xiao Han,Fei Wang*

Main category: cs.CV

TLDR: 论文提出了一种文本增强的无线传感框架WiTalk，通过三种层次化提示策略整合语义知识，显著提升了无线信号在人类行为识别和定位任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无线信号传感技术虽具备非接触和环境适应性优势，但未能充分利用数据集中的文本信息。

Method: 提出WiTalk框架，采用标签、简要描述和详细动作描述三种提示策略，无需修改架构或增加数据成本。

Result: 在三个公开数据集上验证，性能显著提升，如XRF55上WiFi、RFID和mmWave的准确率分别提高3.9%、2.59%和0.46%。

Conclusion: WiTalk框架通过整合语义知识，有效提升了无线信号传感的性能，为相关应用提供了新思路。

Abstract: Wireless signal-based human sensing technologies, such as WiFi,
millimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),
enable the detection and interpretation of human presence, posture, and
activities, thereby providing critical support for applications in public
security, healthcare, and smart environments. These technologies exhibit
notable advantages due to their non-contact operation and environmental
adaptability; however, existing systems often fail to leverage the textual
information inherent in datasets. To address this, we propose an innovative
text-enhanced wireless sensing framework, WiTalk, that seamlessly integrates
semantic knowledge through three hierarchical prompt strategies-label-only,
brief description, and detailed action description-without requiring
architectural modifications or incurring additional data costs. We rigorously
validate this framework across three public benchmark datasets: XRF55 for human
action recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action
localization (TAL). Experimental results demonstrate significant performance
improvements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,
2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD
improves by 4.98%; and on XRFV2, the mean average precision gains across
various methods range from 4.02% to 13.68%. Our codes have been included in
https://github.com/yangzhenkui/WiTalk.

</details>

### [143] [MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification](https://arxiv.org/abs/2504.14626)
*Santanu Roy,Shweta Singh,Palak Sahu,Ashvath Suresh,Debashish Das*

Main category: cs.CV

TLDR: 提出了一种新型CNN架构MSD-Net，通过密集模块和多尺度特征提取，显著提升了肺癌检测性能，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期检测至关重要，但传统方法依赖人工且效率低。深度学习虽简化了自动检测，但类不平衡问题限制了CNN性能。

Method: 设计了MSD-Net，引入密集模块（含DWSC层和1x1卷积层）以减少复杂度，并采用跳跃连接和平行分支提取多尺度特征。

Result: 实验表明，MSD-Net在性能上显著优于ConvNext-Tiny、ViT、PiT等最新模型。

Conclusion: MSD-Net通过创新架构有效解决了类不平衡问题，为肺癌自动检测提供了高效解决方案。

Abstract: Lung cancer, a severe form of malignant tumor that originates in the tissues
of the lungs, can be fatal if not detected in its early stages. It ranks among
the top causes of cancer-related mortality worldwide. Detecting lung cancer
manually using chest X-Ray image or Computational Tomography (CT) scans image
poses significant challenges for radiologists. Hence, there is a need for
automatic diagnosis system of lung cancers from radiology images. With the
recent emergence of deep learning, particularly through Convolutional Neural
Networks (CNNs), the automated detection of lung cancer has become a much
simpler task. Nevertheless, numerous researchers have addressed that the
performance of conventional CNNs may be hindered due to class imbalance issue,
which is prevalent in medical images. In this research work, we have proposed a
novel CNN architecture ``Multi-Scale Dense Network (MSD-Net)''
(trained-from-scratch). The novelties we bring in the proposed model are (I) We
introduce novel dense modules in the 4th block and 5th block of the CNN model.
We have leveraged 3 depthwise separable convolutional (DWSC) layers, and one
1x1 convolutional layer in each dense module, in order to reduce complexity of
the model considerably. (II) Additionally, we have incorporated one skip
connection from 3rd block to 5th block and one parallel branch connection from
4th block to Global Average Pooling (GAP) layer. We have utilized dilated
convolutional layer (with dilation rate=2) in the last parallel branch in order
to extract multi-scale features. Extensive experiments reveal that our proposed
model has outperformed latest CNN model ConvNext-Tiny, recent trend Vision
Transformer (ViT), Pooling-based ViT (PiT), and other existing models by
significant margins.

</details>

### [144] [NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation](https://arxiv.org/abs/2504.14638)
*Junyuan Fang,Zihan Wang,Yejun Zhang,Shuzhe Wang,Iaroslav Melekhov,Juho Kannala*

Main category: cs.CV

TLDR: 提出了一种基于3D高斯泼溅的硬视觉提示方法，通过相机插值生成多视角图像，无需2D-3D优化或微调，提升了视觉语言模型在3D实例分割任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图像级任务中表现优异，但在需要精确定位和识别的3D实例分割任务中表现不足。

Method: 采用3D高斯泼溅和相机插值生成多视角图像，增强几何一致性，无需额外训练。

Result: 方法有效提升了视觉语言模型在3D实例分割中的鲁棒性和准确性。

Conclusion: 该方法为3D实例分割提供了一种无需训练的高效解决方案。

Abstract: Vision-language models (VLMs) have demonstrated impressive zero-shot transfer
capabilities in image-level visual perception tasks. However, they fall short
in 3D instance-level segmentation tasks that require accurate localization and
recognition of individual objects. To bridge this gap, we introduce a novel 3D
Gaussian Splatting based hard visual prompting approach that leverages camera
interpolation to generate diverse viewpoints around target objects without any
2D-3D optimization or fine-tuning. Our method simulates realistic 3D
perspectives, effectively augmenting existing hard visual prompts by enforcing
geometric consistency across viewpoints. This training-free strategy seamlessly
integrates with prior hard visual prompts, enriching object-descriptive
features and enabling VLMs to achieve more robust and accurate 3D instance
segmentation in diverse 3D scenes.

</details>

### [145] [Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension](https://arxiv.org/abs/2504.14642)
*Lin Li,Wei Chen,Jiahui Li,Long Chen*

Main category: cs.CV

TLDR: Relation-R1是一个统一的关系理解框架，通过认知链式思维（CoT）引导的监督微调（SFT）和群体相对策略优化（GRPO）提升多模态大语言模型（MLLMs）在视觉关系理解上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉关系理解（如场景图生成）上表现不足，尤其是在建模N元关系时缺乏语义依赖，导致输出不可靠和过度依赖语言先验。

Method: Relation-R1结合了CoT引导的SFT和GRPO，首先通过SFT建立基础推理能力，然后通过GRPO优化输出，减少语言偏见并提升泛化能力。

Result: 在PSG和SWiG数据集上的实验表明，Relation-R1在二元和N元关系理解上达到了最先进的性能。

Conclusion: Relation-R1通过显式建模语义依赖和优化视觉-语义基础，显著提升了MLLMs在复杂视觉关系任务中的表现。

Abstract: Recent advances in multi-modal large language models (MLLMs) have
significantly improved object-level grounding and region captioning, but remain
limited in visual relation understanding (\eg, scene graph generation),
particularly in modeling \textit{N}-ary relationships that identify multiple
semantic roles among an action event. Such a lack of \textit{semantic
dependencies} modeling among multi-entities leads to unreliable outputs,
intensifying MLLMs' hallucinations and over-reliance on language priors. To
this end, we propose Relation-R1, the first unified relational comprehension
framework that explicitly integrates cognitive chain-of-thought (CoT)-guided
Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)
within a reinforcement learning (RL) paradigm. Specifically, we first establish
foundational reasoning capabilities via SFT, enforcing structured outputs with
thinking processes. Then, GRPO is utilized to refine these outputs via
multi-reward optimization, prioritizing visual-semantic grounding over
language-induced biases, thereby improving generalization capability. Extensive
experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1
achieves state-of-the-art performance in both binary and \textit{N}-ary
relation understanding.

</details>

### [146] [EmoSEM: Segment and Explain Emotion Stimuli in Visual Art](https://arxiv.org/abs/2504.14658)
*Jing Zhang,Dan Guo,Zhangbin Li,Meng Wang*

Main category: cs.CV

TLDR: 论文提出EmoSEM模型，通过情感提示和轻量级前缀投影器解决艺术图像中像素级情感理解与解释的挑战。


<details>
  <summary>Details</summary>
Motivation: 艺术图像的情感理解面临主观性和抽象性的双重挑战，现有模型难以适应情感导向的分割任务和情感推理。

Method: 引入情感提示和可学习掩码标记，设计情感投影器和前缀投影器，联合视觉、掩码和情感标记生成解释。

Result: 模型实现了从像素特征到情感解释的端到端建模，实验验证了其有效性。

Conclusion: EmoSEM为艺术情感计算提供了首个可解释的细粒度分析框架。

Abstract: This paper focuses on a key challenge in visual art understanding: given an
art image, the model pinpoints pixel regions that trigger a specific human
emotion, and generates linguistic explanations for the emotional arousal.
Despite recent advances in art understanding, pixel-level emotion understanding
still faces a dual challenge: first, the subjectivity of emotion makes it
difficult for general segmentation models like SAM to adapt to emotion-oriented
segmentation tasks; and second, the abstract nature of art expression makes it
difficult for captioning models to balance pixel-level semantic understanding
and emotion reasoning. To solve the above problems, this paper proposes the
Emotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the
segmentation model SAM with emotion comprehension capability. First, to enable
the model to perform segmentation under the guidance of emotional intent well,
we introduce an emotional prompt with a learnable mask token as the conditional
input for segmentation decoding. Then, we design an emotion projector to
establish the association between emotion and visual features. Next, more
importantly, to address emotion-visual stimuli alignment, we develop a
lightweight prefix projector, a module that fuses the learned emotional mask
with the corresponding emotion into a unified representation compatible with
the language model.Finally, we input the joint visual, mask, and emotional
tokens into the language model and output the emotional explanations. It
ensures that the generated interpretations remain semantically and emotionally
coherent with the visual stimuli. The method innovatively realizes end-to-end
modeling from low-level pixel features to high-level emotion interpretation,
providing the first interpretable fine-grained analysis framework for artistic
emotion computing. Extensive experiments validate the effectiveness of our
model.

</details>

### [147] [Frequency-domain Learning with Kernel Prior for Blind Image Deblurring](https://arxiv.org/abs/2504.14664)
*Jixiang Sun,Fei Lei,Jiawei Zhang,Wenxiu Sun,Yujiu Yang*

Main category: cs.CV

TLDR: 论文提出了一种结合核先验和频率域方法的深度学习模型，用于图像去模糊，提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在图像去模糊任务中泛化能力不足，主要依赖特定领域数据集。引入核先验可以解决这一问题。

Method: 提出频率集成模块（FIM），结合核先验和基于频率的去模糊Transformer网络。

Result: 实验表明，该方法在多个盲图像去模糊任务中优于现有技术，具有更强的泛化能力。

Conclusion: 结合核先验和频率域方法能有效提升图像去模糊的泛化性能。

Abstract: While achieving excellent results on various datasets, many deep learning
methods for image deblurring suffer from limited generalization capabilities
with out-of-domain data. This limitation is likely caused by their dependence
on certain domain-specific datasets. To address this challenge, we argue that
it is necessary to introduce the kernel prior into deep learning methods, as
the kernel prior remains independent of the image context. For effective fusion
of kernel prior information, we adopt a rational implementation method inspired
by traditional deblurring algorithms that perform deconvolution in the
frequency domain. We propose a module called Frequency Integration Module (FIM)
for fusing the kernel prior and combine it with a frequency-based deblurring
Transfomer network. Experimental results demonstrate that our method
outperforms state-of-the-art methods on multiple blind image deblurring tasks,
showcasing robust generalization abilities. Source code will be available soon.

</details>

### [148] [DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback Representations](https://arxiv.org/abs/2504.14665)
*A S M Sharifuzzaman Sagar,Yu Chen,Jun Hoong Chan*

Main category: cs.CV

TLDR: 本文提出了一种混合预测误差反馈机制和动态调制的深度预测编码网络，解决了传统方法在局部和全局细节处理上的不足，并设计了专用损失函数以提高性能。实验证明其在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统预测编码网络的误差反馈机制在同时处理局部和全局细节时表现不佳，且难以动态适应输入数据的复杂性。此外，缺乏专用损失函数限制了模型性能。

Method: 提出混合预测误差反馈机制，结合全局上下文和局部细节，并动态调整反馈；设计专用损失函数以优化预测误差最小化。

Result: 在CIFAR-10、CIFAR-100、MNIST和FashionMNIST数据集上，模型表现出更快的收敛速度和更高的预测准确性。

Conclusion: 混合反馈机制和专用损失函数显著提升了预测编码网络的性能，适用于多样化场景。

Abstract: Traditional predictive coding networks, inspired by theories of brain
function, consistently achieve promising results across various domains,
extending their influence into the field of computer vision. However, the
performance of the predictive coding networks is limited by their error
feedback mechanism, which traditionally employs either local or global
recurrent updates, leading to suboptimal performance in processing both local
and broader details simultaneously. In addition, traditional predictive coding
networks face difficulties in dynamically adjusting to the complexity and
context of varying input data, which is crucial for achieving high levels of
performance in diverse scenarios. Furthermore, there is a gap in the
development and application of specific loss functions that could more
effectively guide the model towards optimal performance. To deal with these
issues, this paper introduces a hybrid prediction error feedback mechanism with
dynamic modulation for deep predictive coding networks by effectively combining
global contexts and local details while adjusting feedback based on input
complexity. Additionally, we present a loss function tailored to this framework
to improve accuracy by focusing on precise prediction error minimization.
Experimental results demonstrate the superiority of our model over other
approaches, showcasing faster convergence and higher predictive accuracy in
CIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.

</details>

### [149] [Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens](https://arxiv.org/abs/2504.14666)
*Kaihang Pan,Wang Lin,Zhongqi Yue,Tenglong Ao,Liyu Jia,Wei Zhao,Juncheng Li,Siliang Tang,Hanwang Zhang*

Main category: cs.CV

TLDR: 论文提出了一种基于扩散时间步的视觉语言方法，解决了现有空间视觉标记缺乏递归结构的问题，实现了多模态理解与生成的无缝统一。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖空间视觉标记，但其缺乏语言的递归结构，难以被LLM掌握。

Method: 利用扩散时间步学习离散、递归的视觉标记，补偿噪声图像中的渐进属性损失。

Result: 实验表明，该方法在多模态理解和生成任务上均优于其他MLLMs。

Conclusion: 通过结合LLM的自回归推理和扩散模型的精确图像生成，实现了多模态的统一框架。

Abstract: Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify
visual comprehension and generation by combining LLM and diffusion models, the
state-of-the-art in each task, respectively. Existing approaches rely on
spatial visual tokens, where image patches are encoded and arranged according
to a spatial order (e.g., raster scan). However, we show that spatial tokens
lack the recursive structure inherent to languages, hence form an impossible
language for LLM to master. In this paper, we build a proper visual language by
leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our
proposed tokens recursively compensate for the progressive attribute loss in
noisy images as timesteps increase, enabling the diffusion model to reconstruct
the original image at any timestep. This approach allows us to effectively
integrate the strengths of LLMs in autoregressive reasoning and diffusion
models in precise image generation, achieving seamless multimodal comprehension
and generation within a unified framework. Extensive experiments show that we
achieve superior performance for multimodal comprehension and generation
simultaneously compared with other MLLMs. Project Page:
https://DDT-LLaMA.github.io/.

</details>

### [150] [Seurat: From Moving Points to Depth](https://arxiv.org/abs/2504.14687)
*Seokju Cho,Jiahui Huang,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TLDR: 提出一种通过分析2D轨迹的空间和时间关系来推断相对深度的新方法，利用现有点跟踪模型和时空变换器，在TAPVid-3D基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 单目视频深度估计因缺乏立体视觉等深度线索而具有挑战性，但人类通过观察物体大小和间距变化能直观感知相对深度。

Method: 使用现有点跟踪模型捕获2D轨迹，通过时空变换器处理轨迹并直接推断深度变化。

Result: 在TAPVid-3D基准测试中表现出零样本泛化能力，从合成数据到真实数据均能实现高精度、时间平滑的深度预测。

Conclusion: 该方法通过时空分析2D轨迹，实现了高效且准确的相对深度估计，适用于多样化场景。

Abstract: Accurate depth estimation from monocular videos remains challenging due to
ambiguities inherent in single-view geometry, as crucial depth cues like
stereopsis are absent. However, humans often perceive relative depth
intuitively by observing variations in the size and spacing of objects as they
move. Inspired by this, we propose a novel method that infers relative depth by
examining the spatial relationships and temporal evolution of a set of tracked
2D trajectories. Specifically, we use off-the-shelf point tracking models to
capture 2D trajectories. Then, our approach employs spatial and temporal
transformers to process these trajectories and directly infer depth changes
over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust
zero-shot performance, generalizing effectively from synthetic to real-world
datasets. Results indicate that our approach achieves temporally smooth,
high-accuracy depth predictions across diverse domains.

</details>

### [151] [Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/abs/2504.14693)
*Enxin Song,Wenhao Chai,Weili Xu,Jianwen Xie,Yuxuan Liu,Gaoang Wang*

Main category: cs.CV

TLDR: Video-MMLU是一个用于评估语言多模态模型（LMMs）在多学科讲座理解能力的大规模基准测试，揭示了当前模型在感知与推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 多学科讲座理解是LMMs尚未充分探索的领域，需要评估模型在复杂认知任务中的表现。

Method: 通过Video-MMLU基准测试，评估了90多个开源和专有模型（参数规模从0.5B到40B），并研究了视觉标记数量和大语言模型对性能的影响。

Result: 当前模型在多学科讲座理解任务中表现有限，尤其是在需要感知与推理的任务中。

Conclusion: 研究揭示了LMMs在多学科讲座理解中的不足，并提供了关于多模态感知与推理关系的见解。

Abstract: Recent advancements in language multimodal models (LMMs) for video have
demonstrated their potential for understanding video content, yet the task of
comprehending multi-discipline lectures remains largely unexplored. We
introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities
of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90
open-source and proprietary models, ranging from 0.5B to 40B parameters. Our
results highlight the limitations of current models in addressing the cognitive
challenges presented by these lectures, especially in tasks requiring both
perception and reasoning. Additionally, we explore how the number of visual
tokens and the large language models influence performance, offering insights
into the interplay between multimodal perception and reasoning in lecture
comprehension.

</details>

### [152] [IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays](https://arxiv.org/abs/2504.14699)
*Sascha Jecklin,Aidana Massalimova,Ruyi Zha,Lilian Calvet,Christoph J. Laux,Mazda Farshad,Philipp Fürnstahl*

Main category: cs.CV

TLDR: 该论文提出了一种基于高斯溅射的实例学习方法，用于从稀疏X射线重建3D脊柱解剖结构，无需大量标注数据，并通过风格转移提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量标注数据且泛化能力有限，而高斯溅射方法在稀疏X射线下的应用尚未充分探索。

Method: 扩展了$R^2$-高斯溅射框架，引入基于风格转移的解剖学引导标准化步骤，无需预训练。

Result: 在体外数据集上验证，专家评估确认了重建的临床实用性，定量指标显示标准化步骤的改进效果。

Conclusion: 该方法证明了从稀疏X射线进行实例重建的可行性，推动了术中3D成像的发展。

Abstract: Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
sparse fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to sparse, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.

</details>

### [153] [Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features](https://arxiv.org/abs/2504.14708)
*Parshuram N. Aarotale,Ajita Rattani*

Main category: cs.CV

TLDR: 论文提出了一种基于EMG的手势识别新方法XMANet，通过跨层互注意力结合浅层和深层CNN专家的特征，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 提高EMG手势识别的准确性和鲁棒性，为假肢控制、康复和人机交互提供更优解决方案。

Method: 使用STFT和小波变换生成频谱图和尺度图，结合XMANet模型进行细粒度分类。

Result: 在Grabmyo和FORS EMG数据集上，XMANet显著优于基线模型，性能提升1.46%至9.36%。

Conclusion: XMANet通过细粒度特征提取和跨层注意力机制，证明了其在EMG分类中的高效性和普适性。

Abstract: Electromyography (EMG) based hand gesture recognition converts forearm muscle
activity into control commands for prosthetics, rehabilitation, and human
computer interaction. This paper proposes a novel approach to EMG-based hand
gesture recognition that uses fine-grained classification and presents XMANet,
which unifies low-level local and high level semantic cues through cross layer
mutual attention among shallow to deep CNN experts. Using stacked spectrograms
and scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet
Transform (WT), we benchmark XMANet against ResNet50, DenseNet-121,
MobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset
indicate that, using STFT, the proposed XMANet model outperforms the baseline
ResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement
of approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing
the WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are
observed over the same baselines. Similarly, on the FORS EMG dataset, the
XMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the
baseline ResNet50. In comparison, the XMANet(DenseNet121) and
XMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,
respectively. Moreover, when using WT, the proposed XMANet achieves gains of
around 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,
MobileNetV3, and EfficientNetB0 models, respectively. These results confirm
that XMANet consistently improves performance across various architectures and
signal processing techniques, demonstrating the strong potential of fine
grained features for accurate and robust EMG classification.

</details>

### [154] [Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline](https://arxiv.org/abs/2504.14709)
*Hui Zhou,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TLDR: 论文提出了一种结合模仿学习与强化学习的新框架，以解决模仿学习在自动驾驶规划中的局限性，并开发了闭环模拟器和因果基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在自动驾驶规划中表现优异，但难以验证其是否真正理解驾驶原理，且容易过拟合常见场景，泛化能力不足。

Method: 1) 开发支持模仿学习和强化学习的闭环模拟器；2) 基于Waymo数据集构建因果基准；3) 提出结合模仿学习与强化学习的新框架。

Result: 通过闭环模拟器和因果基准验证了新框架的有效性，解决了模仿学习的局限性。

Conclusion: 结合模仿学习与强化学习的框架能更好地理解驾驶原理并提升泛化能力，代码即将开源。

Abstract: Machine learning (ML)-based planners have recently gained significant
attention. They offer advantages over traditional optimization-based planning
algorithms. These advantages include fewer manually selected parameters and
faster development. Within ML-based planning, imitation learning (IL) is a
common algorithm. It primarily learns driving policies directly from supervised
trajectory data. While IL has demonstrated strong performance on many open-loop
benchmarks, it remains challenging to determine if the learned policy truly
understands fundamental driving principles, rather than simply extrapolating
from the ego-vehicle's initial state. Several studies have identified this
limitation and proposed algorithms to address it. However, these methods often
use original datasets for evaluation. In these datasets, future trajectories
are heavily dependent on initial conditions. Furthermore, IL often overfits to
the most common scenarios. It struggles to generalize to rare or unseen
situations.
  To address these challenges, this work proposes: 1) a novel closed-loop
simulator supporting both imitation and reinforcement learning, 2) a causal
benchmark derived from the Waymo Open Dataset to rigorously assess the impact
of the copycat problem, and 3) a novel framework integrating imitation learning
and reinforcement learning to overcome the limitations of purely imitative
approaches. The code for this work will be released soon.

</details>

### [155] [Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation](https://arxiv.org/abs/2504.14715)
*Md. Sanaullah Chowdhury,Salauddin Tapu,Noyon Kumar Sarkar,Ferdous Bin Ali,Lameya Sabrin*

Main category: cs.CV

TLDR: Med-2D SegNet是一种高效且准确的医学图像分割架构，通过紧凑的Med Block设计，在低计算复杂度下实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对临床诊断和手术规划至关重要，但现有方法在复杂性和准确性之间存在矛盾。

Method: 提出Med-2D SegNet，采用Med Block编码器设计，结合维度扩展和参数减少技术，实现高效特征提取。

Result: 在多个基准数据集上达到89.77%的平均Dice系数，参数仅2.07百万，表现优异。

Conclusion: Med-2D SegNet在准确性和效率之间取得平衡，为医学图像分析设定了新标准。

Abstract: Accurate and efficient medical image segmentation is crucial for advancing
clinical diagnostics and surgical planning, yet remains a complex challenge due
to the variability in anatomical structures and the demand for low-complexity
models. In this paper, we introduced Med-2D SegNet, a novel and highly
efficient segmentation architecture that delivers outstanding accuracy while
maintaining a minimal computational footprint. Med-2D SegNet achieves
state-of-the-art performance across multiple benchmark datasets, including
KVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient
(DSC) of 89.77% across 20 diverse datasets. Central to its success is the
compact Med Block, a specialized encoder design that incorporates dimension
expansion and parameter reduction, enabling precise feature extraction while
keeping model parameters to a low count of just 2.07 million. Med-2D SegNet
excels in cross-dataset generalization, particularly in polyp segmentation,
where it was trained on KVASIR-SEG and showed strong performance on unseen
datasets, demonstrating its robustness in zero-shot learning scenarios, even
though we acknowledge that further improvements are possible. With top-tier
performance in both binary and multi-class segmentation, Med-2D SegNet
redefines the balance between accuracy and efficiency, setting a new benchmark
for medical image analysis. This work paves the way for developing accessible,
high-performance diagnostic tools suitable for clinical environments and
resource-constrained settings, making it a step forward in the democratization
of advanced medical technology.

</details>

### [156] [TAPIP3D: Tracking Any Point in Persistent 3D Geometry](https://arxiv.org/abs/2504.14717)
*Bowei Zhang,Lei Ke,Adam W. Harley,Katerina Fragkiadaki*

Main category: cs.CV

TLDR: TAPIP3D是一种用于单目RGB和RGB-D视频中长期3D点跟踪的新方法，通过相机稳定的时空特征云和局部对注意力机制实现高精度跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有3D点跟踪方法在处理相机运动和3D点分布不规则性时表现不佳，TAPIP3D旨在解决这些问题。

Method: 利用深度和相机运动信息将2D特征提升到3D世界空间，提出局部对注意力机制优化3D轨迹估计。

Result: TAPIP3D显著优于现有3D跟踪方法，并在深度准确时提升2D跟踪精度。

Conclusion: TAPIP3D通过3D上下文建模和相机运动补偿，实现了更鲁棒和准确的3D点跟踪。

Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in
monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized
spatio-temporal feature clouds, leveraging depth and camera motion information
to lift 2D video features into a 3D world space where camera motion is
effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion
estimates within this stabilized representation, enabling robust tracking over
extended periods. To manage the inherent irregularities of 3D point
distributions, we propose a Local Pair Attention mechanism. This 3D
contextualization strategy effectively exploits spatial relationships in 3D,
forming informative feature neighborhoods for precise 3D trajectory estimation.
Our 3D-centric approach significantly outperforms existing 3D point tracking
methods and even enhances 2D tracking accuracy compared to conventional 2D
pixel trackers when accurate depth is available. It supports inference in both
camera coordinates (i.e., unstabilized) and world coordinates, and our results
demonstrate that compensating for camera motion improves tracking performance.
Our approach replaces the conventional 2D square correlation neighborhoods used
in prior 2D and 3D trackers, leading to more robust and accurate results across
various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io

</details>

### [157] [ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant Phenotyping](https://arxiv.org/abs/2504.14736)
*Nicolás Gaggion,Rodrigo Bonazzola,María Florencia Legascue,María Florencia Mammarella,Florencia Sol Rodriguez,Federico Emanuel Aballay,Florencia Belén Catulo,Andana Barrios,Franco Accavallo,Santiago Nahuel Villarreal,Martin Crespi,Martiniano María Ricardi,Ezequiel Petrillo,Thomas Blein,Federico Ariel,Enzo Ferrante*

Main category: cs.CV

TLDR: ChronoRoot 2.0是一个开源平台，结合低成本硬件和人工智能，用于植物表型的时间分析，解决了现有技术在高通量和结构分析上的不足。


<details>
  <summary>Details</summary>
Motivation: 研究植物发育可塑性（如根系结构）对理解植物适应性和农业可持续性至关重要，但现有技术在高通量和时间分析上存在局限。

Method: 开发了ChronoRoot 2.0平台，支持多器官追踪、实时质量控制、全面结构测量和双用户界面设计。

Result: 通过拟南芥的三个案例展示了系统的多功能性，包括昼夜生长模式、重力响应分析和基因型高通量筛选。

Conclusion: ChronoRoot 2.0扩展了功能并保持低成本，其开源特性促进了社区驱动的开发，使复杂表型分析更普及。

Abstract: The analysis of plant developmental plasticity, including root system
architecture, is fundamental to understanding plant adaptability and
development, particularly in the context of climate change and agricultural
sustainability. While significant advances have been made in plant phenotyping
technologies, comprehensive temporal analysis of root development remains
challenging, with most existing solutions providing either limited throughput
or restricted structural analysis capabilities. Here, we present ChronoRoot
2.0, an integrated open-source platform that combines affordable hardware with
advanced artificial intelligence to enable sophisticated temporal plant
phenotyping. The system introduces several major advances, offering an integral
perspective of seedling development: (i) simultaneous multi-organ tracking of
six distinct plant structures, (ii) quality control through real-time
validation, (iii) comprehensive architectural measurements including novel
gravitropic response parameters, and (iv) dual specialized user interfaces for
both architectural analysis and high-throughput screening. We demonstrate the
system's capabilities through three use cases for Arabidopsis thaliana:
characterization of circadian growth patterns under different light conditions,
detailed analysis of gravitropic responses in transgenic plants, and
high-throughput screening of etiolation responses across multiple genotypes.
ChronoRoot 2.0 maintains its predecessor's advantages of low cost and
modularity while significantly expanding its capabilities, making sophisticated
temporal phenotyping more accessible to the broader plant science community.
The system's open-source nature, combined with extensive documentation and
containerized deployment options, ensures reproducibility and enables
community-driven development of new analytical capabilities.

</details>

### [158] [SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training](https://arxiv.org/abs/2504.14737)
*Shuang Zeng,Lei Zhu,Xinliang Zhang,Hangzhou He,Yanye Lu*

Main category: cs.CV

TLDR: 论文提出了一种名为SuperCL的新型对比学习方法，用于医学图像分割预训练，通过利用图像的结构先验和像素相关性，解决了现有方法在对比对生成和特征提取上的不足。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临高质量标注数据稀缺的挑战，现有对比学习方法在实例级或像素级表征提取上存在局限，且对比对生成依赖人工阈值设置，效率低且泛化性差。

Method: SuperCL引入两种新的对比对生成策略：图像内局部对比对（ILCP）和图像间全局对比对（IGCP），并利用超像素图生成伪掩码指导监督对比学习。此外，还提出了ASP和CCL模块以更好地利用结构先验信息。

Result: 在8个医学图像数据集上的实验表明，SuperCL优于12种现有方法，在10%标注数据下，DSC指标分别比之前最佳结果高出3.15%、5.44%和7.89%。

Conclusion: SuperCL通过创新的对比对生成策略和结构先验利用，显著提升了医学图像分割的性能，为数据稀缺问题提供了有效解决方案。

Abstract: Medical image segmentation is a critical yet challenging task, primarily due
to the difficulty of obtaining extensive datasets of high-quality,
expert-annotated images. Contrastive learning presents a potential but still
problematic solution to this issue. Because most existing methods focus on
extracting instance-level or pixel-to-pixel representation, which ignores the
characteristics between intra-image similar pixel groups. Moreover, when
considering contrastive pairs generation, most SOTA methods mainly rely on
manually setting thresholds, which requires a large number of gradient
experiments and lacks efficiency and generalization. To address these issues,
we propose a novel contrastive learning approach named SuperCL for medical
image segmentation pre-training. Specifically, our SuperCL exploits the
structural prior and pixel correlation of images by introducing two novel
contrastive pairs generation strategies: Intra-image Local Contrastive Pairs
(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.
Considering superpixel cluster aligns well with the concept of contrastive
pairs generation, we utilize the superpixel map to generate pseudo masks for
both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also
propose two modules named Average SuperPixel Feature Map Generation (ASP) and
Connected Components Label Generation (CCL) to better exploit the prior
structural information for IGCP. Finally, experiments on 8 medical image
datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL
achieves a superior performance with more precise predictions from
visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best
results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released
after acceptance.

</details>

### [159] [Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches](https://arxiv.org/abs/2504.14753)
*Guodong Shen,Yuqi Ouyang,Junru Lu,Yixuan Yang,Victor Sanchez*

Main category: cs.CV

TLDR: 论文提出了一种基于中间帧预测的混合框架，结合视觉Transformer和ConvLSTM，优化单任务框架以提升视频异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 观察到现有视频异常检测方法在多任务框架中对单任务框架的优化不足，认为优化单任务框架可以同时提升单任务和多任务方法的性能。

Method: 采用中间帧预测作为代理任务，设计了一个双向结构，结合视觉Transformer和ConvLSTM，通过前后向预测分析时间维度，并开发了卷积时间Transformer和层交互ConvLSTM桥。

Result: 在公共基准测试中验证了混合框架的有效性，无论是作为单任务方法还是多任务方法的一部分，均表现出色。

Conclusion: 结合视觉Transformer和ConvLSTM的混合框架显著提升了视频异常检测的稳定性和准确性。

Abstract: Despite the prevailing transition from single-task to multi-task approaches
in video anomaly detection, we observe that many adopt sub-optimal frameworks
for individual proxy tasks. Motivated by this, we contend that optimizing
single-task frameworks can advance both single- and multi-task approaches.
Accordingly, we leverage middle-frame prediction as the primary proxy task, and
introduce an effective hybrid framework designed to generate accurate
predictions for normal frames and flawed predictions for abnormal frames. This
hybrid framework is built upon a bi-directional structure that seamlessly
integrates both vision transformers and ConvLSTMs. Specifically, we utilize
this bi-directional structure to fully analyze the temporal dimension by
predicting frames in both forward and backward directions, significantly
boosting the detection stability. Given the transformer's capacity to model
long-range contextual dependencies, we develop a convolutional temporal
transformer that efficiently associates feature maps from all context frames to
generate attention-based predictions for target frames. Furthermore, we devise
a layer-interactive ConvLSTM bridge that facilitates the smooth flow of
low-level features across layers and time-steps, thereby strengthening
predictions with fine details. Anomalies are eventually identified by
scrutinizing the discrepancies between target frames and their corresponding
predictions. Several experiments conducted on public benchmarks affirm the
efficacy of our hybrid framework, whether used as a standalone single-task
approach or integrated as a branch in a multi-task approach. These experiments
also underscore the advantages of merging vision transformers and ConvLSTMs for
video anomaly detection.

</details>

### [160] [How Effective Can Dropout Be in Multiple Instance Learning ?](https://arxiv.org/abs/2504.14783)
*Wenhui Zhu,Peijie Qiu,Xiwen Chen,Zhangsihao Yang,Aristeidis Sotiras,Abolfazl Razi,Yalin Wang*

Main category: cs.CV

TLDR: 论文提出了一种名为MIL-Dropout的新方法，通过丢弃包中最重要的实例来提升多实例学习（MIL）的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于WSI的高分辨率，MIL在WSI分类中通常采用两阶段训练方案，但这种方法存在特征嵌入噪声和弱监督问题，限制了MIL学习丰富且可泛化特征的能力。

Method: 提出MIL-Dropout方法，通过系统性地丢弃包中最重要的实例来优化MIL训练。

Result: 在五个MIL基准数据集和两个WSI数据集上的实验表明，MIL-Dropout显著提升了现有MIL方法的性能，且计算成本可忽略。

Conclusion: MIL-Dropout是一种简单有效的方法，能够显著提升MIL的性能和泛化能力。

Abstract: Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.

</details>

### [161] [When Cloud Removal Meets Diffusion Model in Remote Sensing](https://arxiv.org/abs/2504.14785)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang*

Main category: cs.CV

TLDR: DC4CR是一种基于多模态扩散的新型云去除框架，通过提示驱动控制和低秩适应等技术，高效去除遥感图像中的云层，无需预生成云掩码。


<details>
  <summary>Details</summary>
Motivation: 云遮挡严重阻碍遥感应用，传统方法依赖预生成云掩码且效率低，亟需更高效、自适应的解决方案。

Method: 提出DC4CR框架，结合提示驱动控制、低秩适应、主题驱动生成和分组学习，实现高效云去除。

Result: 在RICE和CUHK-CR数据集上表现优异，达到最先进的云去除效果。

Conclusion: DC4CR为遥感图像处理提供了一种实用、高效且可扩展的解决方案。

Abstract: Cloud occlusion significantly hinders remote sensing applications by
obstructing surface information and complicating analysis. To address this, we
propose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal
diffusion-based framework for cloud removal in remote sensing imagery. Our
method introduces prompt-driven control, allowing selective removal of thin and
thick clouds without relying on pre-generated cloud masks, thereby enhancing
preprocessing efficiency and model adaptability. Additionally, we integrate
low-rank adaptation for computational efficiency, subject-driven generation for
improved generalization, and grouped learning to enhance performance on small
datasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into
existing cloud removal models, providing a scalable and robust solution.
Extensive experiments on the RICE and CUHK-CR datasets demonstrate
state-of-the-art performance, achieving superior cloud removal across diverse
conditions. This work presents a practical and efficient approach for remote
sensing image processing with broad real-world applications.

</details>

### [162] [Real-Time Sleepiness Detection for Driver State Monitoring System](https://arxiv.org/abs/2504.14807)
*Deepak Ghimire,Sunghwan Jeong,Sunhong Yoon,Sanghyun Park,Juhwan Choi*

Main category: cs.CV

TLDR: 提出了一种基于计算机视觉的实时驾驶员眼睛状态检测方法，结合动态模板匹配和Kalman滤波跟踪，使用SVM分类器判断眼睛开闭状态，并在检测到疲劳时触发警报。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是许多事故的主要原因，实时监测驾驶员眼睛状态可以有效预防疲劳驾驶。

Method: 1. 检测人脸并定位眼睛区域；2. 使用动态模板匹配和Kalman滤波跟踪眼睛位置；3. 基于HOG特征的SVM分类器判断眼睛开闭状态；4. 持续闭眼时触发警报。

Result: 系统能够实时检测驾驶员眼睛状态，并在检测到疲劳时发出警报。

Conclusion: 该方法有效实现了驾驶员疲劳监测，有助于减少疲劳驾驶引发的事故。

Abstract: A driver face monitoring system can detect driver fatigue, which is a
significant factor in many accidents, using computer vision techniques. In this
paper, we present a real-time technique for driver eye state detection. First,
the face is detected, and the eyes are located within the face region for
tracking. A normalized cross-correlation-based online dynamic template matching
technique, combined with Kalman filter tracking, is proposed to track the
detected eye positions in subsequent image frames. A support vector machine
with histogram of oriented gradients (HOG) features is used to classify the
state of the eyes as open or closed. If the eyes remain closed for a specified
period, the driver is considered to be asleep, and an alarm is triggered.

</details>

### [163] [ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages](https://arxiv.org/abs/2504.14825)
*Zhoujie Qian*

Main category: cs.CV

TLDR: ECViT是一种结合CNN和Transformer优势的高效混合架构，通过引入局部性和平移不变性等归纳偏置，解决了ViT的高计算成本和数据需求问题。


<details>
  <summary>Details</summary>
Motivation: ViT因自注意力的二次计算复杂性和大数据需求而受限，ECViT旨在平衡性能与效率。

Method: ECViT通过从低级特征提取补丁并在编码器中加入卷积操作，结合局部注意力和金字塔结构，实现高效多尺度特征提取。

Result: ECViT在图像分类任务中性能优于现有模型，同时保持低计算和存储成本。

Conclusion: ECViT为高效且高性能的应用提供了理想解决方案。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision by leveraging
self-attention to model long-range dependencies. However, ViTs face challenges
such as high computational costs due to the quadratic scaling of self-attention
and the requirement of a large amount of training data. To address these
limitations, we propose the Efficient Convolutional Vision Transformer (ECViT),
a hybrid architecture that effectively combines the strengths of CNNs and
Transformers. ECViT introduces inductive biases such as locality and
translation invariance, inherent to Convolutional Neural Networks (CNNs) into
the Transformer framework by extracting patches from low-level features and
enhancing the encoder with convolutional operations. Additionally, it
incorporates local-attention and a pyramid structure to enable efficient
multi-scale feature extraction and representation. Experimental results
demonstrate that ECViT achieves an optimal balance between performance and
efficiency, outperforming state-of-the-art models on various image
classification tasks while maintaining low computational and storage
requirements. ECViT offers an ideal solution for applications that prioritize
high efficiency without compromising performance.

</details>

### [164] [Distribution-aware Dataset Distillation for Efficient Image Restoration](https://arxiv.org/abs/2504.14826)
*Zhuoran Zheng,Xin Su,Chen Wu,Xiuyi Jia*

Main category: cs.CV

TLDR: 论文提出了一种名为TripleD的数据集蒸馏方法，用于图像修复任务，显著减少了训练时间和计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 随着图像数据的爆炸式增长，训练图像修复模型变得耗时且资源密集。现有的数据集蒸馏技术在图像修复领域尚未应用，因此需要填补这一空白。

Method: TripleD利用预训练的视觉Transformer提取图像特征以评估复杂度，并基于复杂度选择子集。通过轻量级CNN调整图像分布，训练分为两个阶段：早期关注简单样本，后期选择复杂样本。

Result: 在多项图像修复任务中表现优异，包括多任务、一体化及超高清图像修复，仅需一台消费级GPU在8小时内完成训练，节省大量资源。

Conclusion: TripleD为图像修复领域提供了一种高效的数据集蒸馏方法，显著提升了训练效率并降低了资源消耗。

Abstract: With the exponential increase in image data, training an image restoration
model is laborious. Dataset distillation is a potential solution to this
problem, yet current distillation techniques are a blank canvas in the field of
image restoration. To fill this gap, we propose the Distribution-aware Dataset
Distillation method (TripleD), a new framework that extends the principles of
dataset distillation to image restoration. Specifically, TripleD uses a
pre-trained vision Transformer to extract features from images for complexity
evaluation, and the subset (the number of samples is much smaller than the
original training set) is selected based on complexity. The selected subset is
then fed through a lightweight CNN that fine-tunes the image distribution to
align with the distribution of the original dataset at the feature level. To
efficiently condense knowledge, the training is divided into two stages. Early
stages focus on simpler, low-complexity samples to build foundational
knowledge, while later stages select more complex and uncertain samples as the
model matures. Our method achieves promising performance on multiple image
restoration tasks, including multi-task image restoration, all-in-one image
restoration, and ultra-high-definition image restoration tasks. Note that we
can train a state-of-the-art image restoration model on an
ultra-high-definition (4K resolution) dataset using only one consumer-grade GPU
in less than 8 hours (500 savings in computing resources and immeasurable
training time).

</details>

### [165] [Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning](https://arxiv.org/abs/2504.14847)
*Xixi Wan,Aihua Zheng,Zi Wang,Bo Jiang,Jin Tang,Jixin Ma*

Main category: cs.CV

TLDR: 提出了一种名为MGRNet的图推理模型，用于解决多模态ReID任务中局部特征质量差异和模态互补信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视局部特征质量差异，未能充分利用多模态互补信息，特别是在低质量特征情况下。

Method: 构建模态感知图以提取细粒度局部细节，采用选择性图节点交换操作缓解低质量特征影响，并通过局部感知图推理模块传播多模态信息。

Result: 在四个基准测试（RGBNT201、Market1501-MM、RGBNT100、MSVR310）上实现了最先进的性能。

Conclusion: MGRNet通过图推理有效利用多模态信息，提升了ReID任务的性能。

Abstract: Multi-modal data provides abundant and diverse object information, crucial
for effective modal interactions in Re-Identification (ReID) tasks. However,
existing approaches often overlook the quality variations in local features and
fail to fully leverage the complementary information across modalities,
particularly in the case of low-quality features. In this paper, we propose to
address this issue by leveraging a novel graph reasoning model, termed the
Modality-aware Graph Reasoning Network (MGRNet). Specifically, we first
construct modality-aware graphs to enhance the extraction of fine-grained local
details by effectively capturing and modeling the relationships between
patches. Subsequently, the selective graph nodes swap operation is employed to
alleviate the adverse effects of low-quality local features by considering both
local and global information, enhancing the representation of discriminative
information. Finally, the swapped modality-aware graphs are fed into the
local-aware graph reasoning module, which propagates multi-modal information to
yield a reliable feature representation. Another advantage of the proposed
graph reasoning approach is its ability to reconstruct missing modal
information by exploiting inherent structural relationships, thereby minimizing
disparities between different modalities. Experimental results on four
benchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the
proposed method achieves state-of-the-art performance in multi-modal object
ReID. The code for our method will be available upon acceptance.

</details>

### [166] [Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation](https://arxiv.org/abs/2504.14848)
*Yunpu Zhao,Rui Zhang,Junbin Xiao,Ruibo Hou,Jiaming Guo,Zihao Zhang,Yifan Hao,Yunji Chen*

Main category: cs.CV

TLDR: 提出了一种名为CSP的新框架，通过语义扰动校准视觉语言模型的置信度，提升其可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在多模态任务中表现优异，但其置信度校准较差，导致用户信任问题。

Method: 采用高斯噪声扰动关键对象区域模拟视觉不确定性，结合监督微调和偏好优化的两阶段训练。

Result: 实验表明，CSP显著改善了置信度与响应正确性的对齐，同时保持或提升了任务性能。

Conclusion: 语义扰动是提升VLMs可靠性和可解释性的实用工具。

Abstract: Vision-language models (VLMs) excel in various multimodal tasks but
frequently suffer from poor calibration, resulting in misalignment between
their verbalized confidence and response correctness. This miscalibration
undermines user trust, especially when models confidently provide incorrect or
fabricated information. In this work, we propose a novel Confidence Calibration
through Semantic Perturbation (CSP) framework to improve the calibration of
verbalized confidence for VLMs in response to object-centric queries. We first
introduce a perturbed dataset where Gaussian noise is applied to the key object
regions to simulate visual uncertainty at different confidence levels,
establishing an explicit mapping between visual ambiguity and confidence
levels. We further enhance calibration through a two-stage training process
combining supervised fine-tuning on the perturbed dataset with subsequent
preference optimization. Extensive experiments on popular benchmarks
demonstrate that our method significantly improves the alignment between
verbalized confidence and response correctness while maintaining or enhancing
overall task performance. These results highlight the potential of semantic
perturbation as a practical tool for improving the reliability and
interpretability of VLMs.

</details>

### [167] [Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer](https://arxiv.org/abs/2504.14860)
*Ziyi Liu,Yangcen Liu*

Main category: cs.CV

TLDR: PseudoFormer是一个两分支框架，通过生成高质量伪标签和利用不同先验知识，缩小了弱监督与全监督时序动作定位的性能差距。


<details>
  <summary>Details</summary>
Motivation: 弱监督时序动作定位（WTAL）因缺乏时间标注而性能受限，现有方法在伪标签生成、先验知识利用和噪声标签训练方面存在挑战。

Method: 提出PseudoFormer框架，包括RickerFusion生成高质量伪标签，利用片段级和提案级标签训练回归模型，并通过不确定性掩码和迭代优化处理噪声标签。

Result: 在THUMOS14和ActivityNet1.3基准测试中达到最优性能，消融实验验证了各模块的有效性。

Conclusion: PseudoFormer通过创新设计解决了WTAL的关键挑战，显著提升了性能。

Abstract: Weakly-supervised Temporal Action Localization (WTAL) has achieved notable
success but still suffers from a lack of temporal annotations, leading to a
performance and framework gap compared with fully-supervised methods. While
recent approaches employ pseudo labels for training, three key challenges:
generating high-quality pseudo labels, making full use of different priors, and
optimizing training methods with noisy labels remain unresolved. Due to these
perspectives, we propose PseudoFormer, a novel two-branch framework that
bridges the gap between weakly and fully-supervised Temporal Action
Localization (TAL). We first introduce RickerFusion, which maps all predicted
action proposals to a global shared space to generate pseudo labels with better
quality. Subsequently, we leverage both snippet-level and proposal-level labels
with different priors from the weak branch to train the regression-based model
in the full branch. Finally, the uncertainty mask and iterative refinement
mechanism are applied for training with noisy pseudo labels. PseudoFormer
achieves state-of-the-art WTAL results on the two commonly used benchmarks,
THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate
the contribution of each component of our method.

</details>

### [168] [Twin Co-Adaptive Dialogue for Progressive Image Generation](https://arxiv.org/abs/2504.14868)
*Jianhui Wang,Yangfan He,Yan Zhong,Xinyuan Song,Jiayi Su,Yuheng Feng,Hongyang He,Wenyu Zhu,Xinhang Yuan,Kuan Lu,Menghao Huo,Miao Zhang,Keqin Li,Jiaqi Chen,Tianyu Shi,Xueqian Wang*

Main category: cs.CV

TLDR: Twin-Co是一个通过动态对话逐步优化图像生成的框架，解决了文本到图像生成系统中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像生成系统在处理用户提示的模糊性时表现不佳，需要一种更动态的方法来优化生成过程。

Method: Twin-Co采用同步、协同适应的对话机制，通过迭代的用户反馈动态调整图像生成。

Result: 实验表明，Twin-Co减少了试错次数，提升了图像质量，优化了用户体验。

Conclusion: Twin-Co通过动态对话机制有效解决了图像生成中的模糊性问题，提升了生成质量和用户满意度。

Abstract: Modern text-to-image generation systems have enabled the creation of
remarkably realistic and high-quality visuals, yet they often falter when
handling the inherent ambiguities in user prompts. In this work, we present
Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to
progressively refine image generation. Instead of a static generation process,
Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue
agent continuously interacts with the user. Initially, a base image is
generated from the user's prompt. Then, through a series of synchronized
dialogue exchanges, the system adapts and optimizes the image according to
evolving user feedback. The co-adaptive process allows the system to
progressively narrow down ambiguities and better align with user intent.
Experiments demonstrate that Twin-Co not only enhances user experience by
reducing trial-and-error iterations but also improves the quality of the
generated images, streamlining the creative process across various
applications.

</details>

### [169] [ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams](https://arxiv.org/abs/2504.14875)
*Chris Dongjoo Kim,Jihwan Moon,Sangwoo Moon,Heeseung Yun,Sihaeng Lee,Aniruddha Kembhavi,Soonyoung Lee,Gunhee Kim,Sangho Lee,Christopher Clark*

Main category: cs.CV

TLDR: ReSpec是一种基于相关性和特异性的在线数据过滤框架，旨在高效处理视频-文本数据流，仅需少量数据即可实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 视频-文本数据的快速增长带来了存储和计算挑战，而在线学习能够实时处理数据流并快速适应实时需求。

Method: ReSpec通过四个标准选择数据：模态对齐、任务相关性、特异性和效率，利用概率对齐和根嵌入距离作为衡量指标。

Result: 在WebVid2M和VideoCC3M数据集上，ReSpec仅使用5%的数据即可在五个零样本视频检索任务中达到最佳性能。

Conclusion: ReSpec通过实时过滤数据，显著降低了存储和计算需求，同时保持了高性能。

Abstract: The rapid growth of video-text data presents challenges in storage and
computation during training. Online learning, which processes streaming data in
real-time, offers a promising solution to these issues while also allowing
swift adaptations in scenarios demanding real-time responsiveness. One strategy
to enhance the efficiency and effectiveness of learning involves identifying
and prioritizing data that enhances performance on target downstream tasks. We
propose Relevance and Specificity-based online filtering framework (ReSpec)
that selects data based on four criteria: (i) modality alignment for clean
data, (ii) task relevance for target focused data, (iii) specificity for
informative and detailed data, and (iv) efficiency for low-latency processing.
Relevance is determined by the probabilistic alignment of incoming data with
downstream tasks, while specificity employs the distance to a root embedding
representing the least specific data as an efficient proxy for informativeness.
By establishing reference points from target task data, ReSpec filters incoming
data in real-time, eliminating the need for extensive storage and compute.
Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains
state-of-the-art performance on five zeroshot video retrieval tasks, using as
little as 5% of the data while incurring minimal compute. The source code is
available at https://github.com/cdjkim/ReSpec.

</details>

### [170] [Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle Re-identification](https://arxiv.org/abs/2504.14877)
*Aihua Zheng,Yongqi Sun,Zi Wang,Chenglong Li,Jin Tang*

Main category: cs.CV

TLDR: 论文提出了一种协作增强网络（CoEN），用于解决多光谱车辆重识别中因光谱数据质量不均导致的性能下降问题。通过生成高质量代理并动态选择主光谱，协作增强所有光谱特征。


<details>
  <summary>Details</summary>
Motivation: 多光谱车辆重识别中，某些重要光谱线索的丢失会显著降低性能。现有方法依赖主光谱增强低质量数据，但主光谱选择和低质量主光谱的影响问题尚未解决。

Method: 提出协作增强网络（CoEN），包括代理生成器（PG）、动态质量排序模块（DQSM）和协作增强模块（CEM），分别用于生成代理、动态选择主光谱和协作增强光谱特征。

Result: 在三个基准数据集上的实验验证了CoEN的有效性，优于其他多光谱车辆重识别方法。

Conclusion: CoEN通过协作增强和动态主光谱选择，显著提升了多光谱车辆重识别的鲁棒性。

Abstract: The performance of multi-spectral vehicle Re-identification (ReID) is
significantly degraded when some important discriminative cues in visible, near
infrared and thermal infrared spectra are lost. Existing methods generate or
enhance missing details in low-quality spectra data using the high-quality one,
generally called the primary spectrum, but how to justify the primary spectrum
is a challenging problem. In addition, when the quality of the primary spectrum
is low, the enhancement effect would be greatly degraded, thus limiting the
performance of multi-spectral vehicle ReID. To address these problems, we
propose the Collaborative Enhancement Network (CoEN), which generates a
high-quality proxy from all spectra data and leverages it to supervise the
selection of primary spectrum and enhance all spectra features in a
collaborative manner, for robust multi-spectral vehicle ReID. First, to
integrate the rich cues from all spectra data, we design the Proxy Generator
(PG) to progressively aggregate multi-spectral features. Second, we design the
Dynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring
their correlations with the proxy, to accurately select the primary spectra
with the highest correlation. Finally, we design the Collaborative Enhancement
Module (CEM) to effectively compensate for missing contents of all spectra by
collaborating the primary spectra and the proxy, thereby mitigating the impact
of low-quality primary spectra. Extensive experiments on three benchmark
datasets are conducted to validate the efficacy of the proposed approach
against other multi-spectral vehicle ReID methods. The codes will be released
at https://github.com/yongqisun/CoEN.

</details>

### [171] [Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2504.14884)
*Jingyu Xing,Chenwei Tang,Tao Wang,Rong Xiao,Wei Ju,Ji-Zhe Zhou,Liangli Zhen,Jiancheng Lv*

Main category: cs.CV

TLDR: 论文提出了一种名为MDD-Net的方法，通过双解码器网络和类感知记忆模块，解决了多类无监督异常检测中的过泛化和正常模式重建不足问题。


<details>
  <summary>Details</summary>
Motivation: 多类无监督异常检测中，重建方法面临过泛化和正常模式重建不足的挑战，现有方法通常只解决前者而加剧后者。

Method: MDD-Net包含双解码器反向蒸馏网络（DRD-Net）和类感知记忆模块（CMM），前者通过双解码器差异细化异常分数，后者保留类特定正常原型以避免异常重建。

Result: 实验表明，MDD-Net在多类无监督异常检测任务中优于现有方法，减少了假阳性并提高了定位准确性。

Conclusion: MDD-Net通过双解码器和记忆模块的协同作用，有效解决了多类异常检测中的关键问题，性能显著优于现有方法。

Abstract: Recent advances in unsupervised anomaly detection (UAD) have shifted from
single-class to multi-class scenarios. In such complex contexts, the increasing
pattern diversity has brought two challenges to reconstruction-based
approaches: (1) over-generalization: anomalies that are subtle or share
compositional similarities with normal patterns may be reconstructed with high
fidelity, making them difficult to distinguish from normal instances; and (2)
insufficient normality reconstruction: complex normal features, such as
intricate textures or fine-grained structures, may not be faithfully
reconstructed due to the model's limited representational capacity, resulting
in false positives. Existing methods typically focus on addressing the former,
which unintentionally exacerbate the latter, resulting in inadequate
representation of intricate normal patterns. To concurrently address these two
challenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This
network includes two critical components: a Dual-Decoder Reverse Distillation
Network (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the
DRD-Net incorporates a restoration decoder designed to recover normal features
from synthetic abnormal inputs and an identity decoder to reconstruct features
that maintain the anomalous semantics. By exploiting the discrepancy between
features produced by two decoders, our approach refines anomaly scores beyond
the conventional encoder-decoder comparison paradigm, effectively reducing
false positives and enhancing localization accuracy. Furthermore, the CMM
explicitly encodes and preserves class-specific normal prototypes, actively
steering the network away from anomaly reconstruction. Comprehensive
experimental results across several benchmarks demonstrate the superior
performance of our MDD-Net framework over current SoTA approaches in
multi-class UAD tasks.

</details>

### [172] [WMKA-Net: A Weighted Multi-Kernel Attention NetworkMethod for Retinal Vessel Segmentation](https://arxiv.org/abs/2504.14888)
*Xinran Xu,Yuliang Ma,Sifu Cai*

Main category: cs.CV

TLDR: WMKA-Net是一种新型视网膜血管分割网络，通过多核特征融合、渐进特征加权和注意力机制，解决了多尺度特征捕获不足、上下文信息丢失和噪声敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 解决视网膜血管分割中多尺度特征捕获不足、上下文信息丢失和噪声敏感性问题。

Method: 结合MultiKernelFeature Fusion Module (MKDC)、Progressive Feature Weighting Fusion Strategy (UDFF)和Attention Mechanism Module (AttentionBlock)，提升对小血管和低对比度区域的分割能力。

Result: 在多个公共数据集上表现出色，尤其在小血管分割和病理区域处理方面。

Conclusion: WMKA-Net为视网膜血管分割提供了一种高效且鲁棒的新方法。

Abstract: We propose a novel retinal vessel segmentation network, the Weighted
Multi-Kernel Attention Network (WMKA-Net), which aims to address the issues of
insufficient multiscale feature capture, loss of contextual information, and
noise sensitivity in retinal vessel segmentation. WMKA-Net significantly
improves the segmentation performance of small vessels and low-contrast regions
by integrating several innovative components, including the MultiKernelFeature
Fusion Module (MKDC), the Progressive Feature Weighting Fusion Strategy (UDFF),
and the Attention Mechanism Module (AttentionBlock). The MKDC module employs
multiscale parallel convolutional kernels to extract vessel characteristics,
thereby enhancing the ability to capture complex vascular structures. The UDFF
strategy optimizes the transmission of feature information by weighted fusion
of high- and low-level features. The AttentionBlock highlights key regions and
suppresses noise interference through the attention mechanism. Experimental
results demonstrate that WMKA-Net achieves excellent segmentation performance
in multiple public datasets, particularly in segmentation of small vessels and
processing of pathological regions. This work provides a robust and efficient
new method for segmentation of the retinal vessel.

</details>

### [173] [Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation](https://arxiv.org/abs/2504.14899)
*Chenjie Cao,Jingkai Zhou,Shikai Li,Jingyun Liang,Chaohui Yu,Fan Wang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TLDR: Uni3C是一个统一的3D增强框架，用于视频生成中精确控制相机和人体运动，通过点云和SMPL-X字符实现灵活控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独处理相机和人体运动控制，且依赖高质量标注数据，Uni3C旨在克服这些限制。

Method: 提出PCDController模块，利用点云实现相机控制；并设计联合对齐的3D世界指导，统一相机和人体运动信号。

Result: PCDController在相机控制上表现鲁棒，Uni3C在相机可控性和人体运动质量上优于竞争对手。

Conclusion: Uni3C通过模块化设计和联合对齐指导，显著提升了视频生成中相机和人体运动的控制能力。

Abstract: Camera and human motion controls have been extensively studied for video
generation, but existing approaches typically address them separately,
suffering from limited data with high-quality annotations for both aspects. To
overcome this, we present Uni3C, a unified 3D-enhanced framework for precise
control of both camera and human motion in video generation. Uni3C includes two
key contributions. First, we propose a plug-and-play control module trained
with a frozen video generative backbone, PCDController, which utilizes
unprojected point clouds from monocular depth to achieve accurate camera
control. By leveraging the strong 3D priors of point clouds and the powerful
capacities of video foundational models, PCDController shows impressive
generalization, performing well regardless of whether the inference backbone is
frozen or fine-tuned. This flexibility enables different modules of Uni3C to be
trained in specific domains, i.e., either camera control or human motion
control, reducing the dependency on jointly annotated data. Second, we propose
a jointly aligned 3D world guidance for the inference phase that seamlessly
integrates both scenic point clouds and SMPL-X characters to unify the control
signals for camera and human motion, respectively. Extensive experiments
confirm that PCDController enjoys strong robustness in driving camera motion
for fine-tuned backbones of video generation. Uni3C substantially outperforms
competitors in both camera controllability and human motion quality.
Additionally, we collect tailored validation sets featuring challenging camera
movements and human actions to validate the effectiveness of our method.

</details>

### [174] [Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments](https://arxiv.org/abs/2504.14913)
*Kenji Iwata,Eiki Ishidera,Toshifumi Yamaai,Yutaka Satoh,Hiroshi Tanaka,Katsuhiko Takahashi,Akio Furuhata,Yoshihisa Tanabe,Hiroshi Matsumura*

Main category: cs.CV

TLDR: 论文总结了OCR性能下降的外部干扰因素，并整理为指南，以帮助用户正确使用OCR。


<details>
  <summary>Details</summary>
Motivation: 随着OCR应用范围扩大，外部干扰导致性能下降，影响识别精度，需提供解决方案。

Method: 收集真实世界的外部干扰因素及图像退化现象，整理为表格，并制定使用指南。

Result: 形成外部干扰因素表和指南，帮助用户应对性能下降问题。

Conclusion: 通过整理干扰因素和提供指南，确保OCR在复杂环境中仍能有效使用。

Abstract: The performance of OCR has improved with the evolution of AI technology. As
OCR continues to broaden its range of applications, the increased likelihood of
interference introduced by various usage environments can prevent it from
achieving its inherent performance. This results in reduced recognition
accuracy under certain conditions, and makes the quality control of recognition
devices more challenging. Therefore, to ensure that users can properly utilize
OCR, we compiled the real-world external disturbance factors that cause
performance degradation, along with the resulting image degradation phenomena,
into an external disturbance factor table and, by also indicating how to make
use of it, organized them into guidelines.

</details>

### [175] [GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection](https://arxiv.org/abs/2504.14919)
*Donghyeong Kim,Chaewon Park,Suhwan Cho,Hyeonjeong Lim,Minseok Kang,Jungho Lee,Sangyoun Lee*

Main category: cs.CV

TLDR: GenCLIP提出了一种通过多层提示和双分支推理来改进零样本异常检测（ZSAD）的新框架，旨在更有效地学习和利用通用提示。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测（ZSAD）的关键挑战在于如何稳定地学习通用提示并有效利用它们，同时保持泛化性和类别特异性。

Method: GenCLIP采用多层提示和双分支推理策略：多层提示整合不同CLIP层的类别特定视觉线索，双分支推理通过视觉增强分支和仅查询分支平衡特异性和泛化性。

Result: 该方法通过自适应文本提示过滤机制和多层视觉特征结合，提高了对未见类别的异常检测的稳定性和可靠性。

Conclusion: GenCLIP通过创新的提示学习和推理策略，显著提升了零样本异常检测的性能和泛化能力。

Abstract: Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen
categories by leveraging CLIP's zero-shot capabilities to match text prompts
with visual features. A key challenge in ZSAD is learning general prompts
stably and utilizing them effectively, while maintaining both generalizability
and category specificity. Although general prompts have been explored in prior
works, achieving their stable optimization and effective deployment remains a
significant challenge. In this work, we propose GenCLIP, a novel framework that
learns and leverages general prompts more effectively through multi-layer
prompting and dual-branch inference. Multi-layer prompting integrates
category-specific visual cues from different CLIP layers, enriching general
prompts with more comprehensive and robust feature representations. By
combining general prompts with multi-layer visual features, our method further
enhances its generalization capability. To balance specificity and
generalization, we introduce a dual-branch inference strategy, where a
vision-enhanced branch captures fine-grained category-specific features, while
a query-only branch prioritizes generalization. The complementary outputs from
both branches improve the stability and reliability of anomaly detection across
unseen categories. Additionally, we propose an adaptive text prompt filtering
mechanism, which removes irrelevant or atypical class names not encountered
during CLIP's training, ensuring that only meaningful textual inputs contribute
to the final vision-language alignment.

</details>

### [176] [DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding](https://arxiv.org/abs/2504.14920)
*Geng Li,Jinglin Xu,Yunzhen Zhao,Yuxin Peng*

Main category: cs.CV

TLDR: Dyfo是一种无需训练的视觉搜索方法，通过双向交互和MCTS算法模拟人类视觉聚焦，提升多模态模型的细粒度视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 受人类视觉搜索机制启发，旨在解决现有方法需要额外模块或数据的问题。

Method: 利用双向交互和MCTS算法动态调整视觉焦点，无需额外训练或模块。

Result: 显著提升细粒度视觉理解，减少幻觉问题，在固定和动态分辨率模型中表现优异。

Conclusion: Dyfo为多模态模型提供了一种高效、无需训练的视觉聚焦解决方案。

Abstract: Humans can effortlessly locate desired objects in cluttered environments,
relying on a cognitive mechanism known as visual search to efficiently filter
out irrelevant information and focus on task-related regions. Inspired by this
process, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing
visual search method that enhances fine-grained visual understanding in large
multimodal models (LMMs). Unlike existing approaches which require additional
modules or data collection, Dyfo leverages a bidirectional interaction between
LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to
simulate human-like focus adjustments. This enables LMMs to focus on key visual
regions while filtering out irrelevant content, without introducing additional
training caused by vocabulary expansion or the integration of specialized
localization modules. Experimental results demonstrate that Dyfo significantly
improves fine-grained visual understanding and reduces hallucination issues in
LMMs, achieving superior performance across both fixed and dynamic resolution
models. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025

</details>

### [177] [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](https://arxiv.org/abs/2504.14921)
*Songping Wang,Hanqing Liu,Yueming Lyu,Xiantao Hu,Ziwen He,Wei Wang,Caifeng Shan,Liang Wang*

Main category: cs.CV

TLDR: VFAT-WS是一种针对视频数据的快速对抗训练方法，通过时间频率增强和弱到强一致性正则化，显著提升训练效率和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视频对抗训练面临计算成本高和清洁准确性与鲁棒性之间的权衡问题，VFAT-WS旨在解决这些挑战。

Method: 结合时间频率增强（TF-AUG）及其空间-时间增强形式（STF-AUG），以及单步PGD攻击，同时采用弱到强一致性正则化。

Result: 在UCF-101和HMDB-51数据集上，VFAT-WS显著提升了对抗鲁棒性和抗干扰性，同时训练速度加快近490%。

Conclusion: VFAT-WS通过高效设计和一致性正则化，成功平衡了清洁准确性与鲁棒性，为视频对抗训练提供了实用解决方案。

Abstract: Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.

</details>

### [178] [TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models](https://arxiv.org/abs/2504.14933)
*Mazharul Islam Rakib,Showrin Rahman,Joyanta Jyoti Mondal,Xi Xiao,David Lewis,Alessandra Mileo,Meem Arafat Manab*

Main category: cs.CV

TLDR: 提出了一种基于条件扩散模型的两步图像生成方法，通过生成图像分割掩码并避免特定形状，有效减少版权侵权和源复制问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和营销中，AI生成图像常涉及版权侵权问题，传统保护措施无效，需一种低成本解决方案。

Method: 采用两步法：首先生成图像分割掩码，再利用扩散模型重新生成图像，避免特定形状。

Result: 该方法显著降低了与训练图像的结构相似性，避免了源复制问题，且计算成本低。

Conclusion: 提出的方法为基于扩散模型的图像生成提供了一种高效、低成本的版权保护方案。

Abstract: In today's age of social media and marketing, copyright issues can be a major
roadblock to the free sharing of images. Generative AI models have made it
possible to create high-quality images, but concerns about copyright
infringement are a hindrance to their abundant use. As these models use data
from training images to generate new ones, it is often a daunting task to
ensure they do not violate intellectual property rights. Some AI models have
even been noted to directly copy copyrighted images, a problem often referred
to as source copying. Traditional copyright protection measures such as
watermarks and metadata have also proven to be futile in this regard. To
address this issue, we propose a novel two-step image generation model inspired
by the conditional diffusion model. The first step involves creating an image
segmentation mask for some prompt-based generated images. This mask embodies
the shape of the image. Thereafter, the diffusion model is asked to generate
the image anew while avoiding the shape in question. This approach shows a
decrease in structural similarity from the training image, i.e. we are able to
avoid the source copying problem using this approach without expensive
retraining of the model or user-centered prompt generation techniques. This
makes our approach the most computationally inexpensive approach to avoiding
both copyright infringement and source copying for diffusion model-based image
generation.

</details>

### [179] [PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV](https://arxiv.org/abs/2504.14952)
*Qianyu Zhu,Junjie Wang,Jeremiah Hu,Jia Ai,Yong Lee*

Main category: cs.CV

TLDR: 论文提出了一种基于迁移学习的去噪扩散模型（PIV-FlowDiffuser），用于减少粒子图像测速（PIV）中的特殊噪声，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在合成数据集上训练的PIV分析器在实际粒子图像中表现不佳，存在领域差距和特殊噪声问题。

Method: 通过迁移学习策略训练数据密集型的去噪扩散模型：1）在计算机视觉数据集（如Sintel、KITTI）上预训练；2）在合成PIV数据集上微调。

Result: PIV-FlowDiffuser有效抑制了噪声，将平均端点误差（AEE）降低了59.4%，并在未见过的粒子图像上表现出更好的泛化性能。

Conclusion: 研究强调了基于迁移学习的去噪扩散模型在PIV中的潜力，并提供了详细的实现方法。

Abstract: Deep learning algorithms have significantly reduced the computational time
and improved the spatial resolution of particle image velocimetry~(PIV).
However, the models trained on synthetic datasets might have a degraded
performance on practical particle images due to domain gaps. As a result,
special residual patterns are often observed for the vector fields of deep
learning-based estimators. To reduce the special noise step-by-step, we employ
a denoising diffusion model~(FlowDiffuser) for PIV analysis. And the
data-hungry iterative denoising diffusion model is trained via a transfer
learning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)
pre-training a FlowDiffuser model with multiple optical flow datasets of the
computer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the
pre-trained model on synthetic PIV datasets. Note that the PIV images are
upsampled by a factor of two to resolve the small-scale turbulent flow
structures. The visualized results indicate that our PIV-FlowDiffuser
effectively suppresses the noise patterns. Therefore, the denoising diffusion
model reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV
baseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits
enhanced generalization performance on unseen particle images due to transfer
learning. Overall, this study highlights the transfer-learning-based denoising
diffusion models for PIV. And a detailed implementation is recommended for
interested readers in the repository
https://github.com/Zhu-Qianyu/PIV-FlowDiffuser.

</details>

### [180] [3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations](https://arxiv.org/abs/2504.14967)
*Yating Wang,Xuan Wang,Ran Yi,Yanbo Fan,Jichen Hu,Jingcheng Zhu,Lizhuang Ma*

Main category: cs.CV

TLDR: 提出了一种结合3D高斯和3DMM的新方法，通过紧凑的张量表示和动态纹理编码，实现了高质量3D头部虚拟形象的实时渲染和低存储成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态纹理捕捉和运行效率或存储空间上存在不足，无法满足实际需求。

Method: 采用张量格式编码纹理属性，静态三平面存储中性表情外观，动态纹理细节通过轻量级1D特征线表示，并引入自适应截断不透明度惩罚和类平衡采样。

Result: 实验表明，该方法能准确捕捉面部动态细节，保持实时渲染，并显著降低存储成本。

Conclusion: 该方法解决了动态纹理捕捉和资源消耗问题，扩展了3D头部虚拟形象的应用场景。

Abstract: Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to
construct high-quality 3D head avatars. In this line of research, existing
methods either fail to capture the dynamic textures or incur significant
overhead in terms of runtime speed or storage space. To this end, we propose a
novel method that addresses all the aforementioned demands. In specific, we
introduce an expressive and compact representation that encodes texture-related
attributes of the 3D Gaussians in the tensorial format. We store appearance of
neutral expression in static tri-planes, and represents dynamic texture details
for different expressions using lightweight 1D feature lines, which are then
decoded into opacity offset relative to the neutral face. We further propose
adaptive truncated opacity penalty and class-balanced sampling to improve
generalization across different expressions. Experiments show this design
enables accurate face dynamic details capturing while maintains real-time
rendering and significantly reduces storage costs, thus broadening the
applicability to more scenarios.

</details>

### [181] [Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization](https://arxiv.org/abs/2504.14975)
*Hongbin Xu,Chaohui Yu,Feng Xiao,Jiazheng Xing,Hai Ci,Weitao Chen,Ming Li*

Main category: cs.CV

TLDR: 提出了一种名为\name{}的新框架，通过循环一致性增强可控3D生成，显著提升输入条件与生成内容的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D生成中难以保持输入条件（如边缘和深度）与生成内容的准确对齐，导致明显差异。

Method: 采用高效前馈主干网络，通过循环过程（生成3D内容、提取信号、重新生成）实现视图一致性和条件一致性约束。

Result: 在多个基准测试中表现优异，边缘和草图条件的PSNR分别提升14.17%和6.26%。

Conclusion: \name{}显著提升了3D生成的可控性，尤其在细节处理上优于现有方法。

Abstract: Despite the remarkable progress of 3D generation, achieving controllability,
i.e., ensuring consistency between generated 3D content and input conditions
like edge and depth, remains a significant challenge. Existing methods often
struggle to maintain accurate alignment, leading to noticeable discrepancies.
To address this issue, we propose \name{}, a new framework that enhances
controllable 3D generation by explicitly encouraging cyclic consistency between
the second-order 3D content, generated based on extracted signals from the
first-order generation, and its original input controls. Specifically, we
employ an efficient feed-forward backbone that can generate a 3D object from an
input condition and a text prompt. Given an initial viewpoint and a control
signal, a novel view is rendered from the generated 3D content, from which the
extracted condition is used to regenerate the 3D content. This re-generated
output is then rendered back to the initial viewpoint, followed by another
round of control signal extraction, forming a cyclic process with two
consistency constraints. \emph{View consistency} ensures coherence between the
two generated 3D objects, measured by semantic similarity to accommodate
generative diversity. \emph{Condition consistency} aligns the final extracted
signal with the original input control, preserving structural or geometric
details throughout the process. Extensive experiments on popular benchmarks
demonstrate that \name{} significantly improves controllability, especially for
fine-grained details, outperforming existing methods across various conditions
(e.g., +14.17\% PSNR for edge, +6.26\% PSNR for sketch).

</details>

### [182] [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
*Ji Qi,Yuan Yao,Yushi Bai,Bin Xu,Juanzi Li,Zhiyuan Liu,Tat-Seng Chua*

Main category: cs.CV

TLDR: Quicksviewer是一种新型大型多模态模型（LMM），通过动态分区和统一重采样视频帧，显著提高计算效率，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统LMM对视频帧的均匀感知导致计算效率低下，尤其是对信息密度不均匀的视频。

Method: 使用Gumbel Softmax将视频分区为非均匀密度块，并对每个块进行统一重采样，动态压缩视频以减少冗余。

Result: 实现了45倍的压缩率，训练效率高（支持长视频），性能优于固定分区策略基线（最高提升8.72准确率）。

Conclusion: Quicksviewer通过动态感知范式，显著提升了视频理解的效率和性能，并展示了输入帧数增加的潜力。

Abstract: Large Multimodal Models (LMMs) uniformly perceive video frames, creating
computational inefficiency for videos with inherently varying temporal
information density. This paper present \textbf{Quicksviewer}, an LMM with new
perceiving paradigm that partitions a video of nonuniform density into varying
cubes using Gumbel Softmax, followed by a unified resampling for each cube to
achieve efficient video understanding. This simple and intuitive approach
dynamically compress video online based on its temporal density, significantly
reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while
enabling efficient training with large receptive field. We train the model from
a language backbone through three progressive stages, each incorporating
lengthy videos on average of 420s/1fps thanks to the perceiving efficiency.
With only 0.8M total video-text samples for training, our model outperforms the
direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in
accuracy, demonstrating the effectiveness in performance. On Video-MME,
Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\%
of tokens per frame required by baselines. With this paradigm, scaling up the
number of input frames reveals a clear power law of the model capabilities. It
is also empirically verified that the segments generated by the cubing network
can help for analyzing continuous events in videos.

</details>

### [183] [RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild](https://arxiv.org/abs/2504.14977)
*Jingkai Zhou,Yifan Wu,Shikai Li,Min Wei,Chao Fan,Weihua Chen,Wei Jiang,Fan Wang*

Main category: cs.CV

TLDR: 论文提出了一种基于强大基础模型的简单修改方法（RealisDance-DiT），通过灵活的微调策略解决可控角色动画中的挑战，并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决可控角色动画中罕见姿势、风格化角色、角色-物体交互、复杂光照和动态场景等问题，现有方法难以泛化到开放世界场景。

Method: 基于Wan-2.1视频基础模型，提出RealisDance-DiT，通过最小化架构修改和低噪声预热、"大批量小迭代"策略优化微调过程。

Result: RealisDance-DiT在实验中大幅优于现有方法，并在新测试数据集上验证了其鲁棒性。

Conclusion: 强大的基础模型结合简单修改和灵活微调策略，可以有效解决可控角色动画的开放世界挑战。

Abstract: Controllable character animation remains a challenging problem, particularly
in handling rare poses, stylized characters, character-object interactions,
complex illumination, and dynamic scenes. To tackle these issues, prior work
has largely focused on injecting pose and appearance guidance via elaborate
bypass networks, but often struggles to generalize to open-world scenarios. In
this paper, we propose a new perspective that, as long as the foundation model
is powerful enough, straightforward model modifications with flexible
fine-tuning strategies can largely address the above challenges, taking a step
towards controllable character animation in the wild. Specifically, we
introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our
sufficient analysis reveals that the widely adopted Reference Net design is
suboptimal for large-scale DiT models. Instead, we demonstrate that minimal
modifications to the foundation model architecture yield a surprisingly strong
baseline. We further propose the low-noise warmup and "large batches and small
iterations" strategies to accelerate model convergence during fine-tuning while
maximally preserving the priors of the foundation model. In addition, we
introduce a new test dataset that captures diverse real-world challenges,
complementing existing benchmarks such as TikTok dataset and UBC fashion video
dataset, to comprehensively evaluate the proposed method. Extensive experiments
show that RealisDance-DiT outperforms existing methods by a large margin.

</details>

### [184] [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
*Chun-Hsiao Yeh,Chenyu Wang,Shengbang Tong,Ta-Ying Cheng,Rouyu Wang,Tianzhe Chu,Yuexiang Zhai,Yubei Chen,Shenghua Gao,Yi Ma*

Main category: cs.CV

TLDR: 论文提出了All-Angles Bench基准，用于评估多模态大语言模型（MLLMs）在多视角场景理解中的表现，发现当前模型在跨视角一致性和几何对应方面与人类水平存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 多视角理解是MLLMs作为具身代理的核心能力，但现有模型在高层次推理中表现优异，却在几何一致性和跨视角对应上表现不足。

Method: 通过构建包含2,100个人工标注的多视角问答对的基准（All-Angles Bench），涵盖6项任务，测试模型的几何对应能力和跨视角信息对齐能力。

Result: 实验表明，27种代表性MLLMs（如Gemini-2.0-Flash、Claude-3.7-Sonnet和GPT-4o）在跨视角对应和相机姿态估计方面表现不佳，与人类水平差距显著。

Conclusion: 当前MLLMs在多视角理解上仍需改进，特别是针对遮挡视角和相机姿态估计。All-Angles Bench为未来研究提供了重要参考。

Abstract: Multi-view understanding, the ability to reconcile visual information across
diverse viewpoints for effective navigation, manipulation, and 3D scene
comprehension, is a fundamental challenge in Multi-Modal Large Language Models
(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive
advances in high-level reasoning and planning, they frequently fall short when
confronted with multi-view geometric consistency and cross-view correspondence.
To comprehensively evaluate the challenges of MLLMs in multi-view scene
reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human
carefully annotated multi-view question-answer pairs across 90 diverse
real-world scenes. Our six tasks (counting, attribute identification, relative
distance, relative direction, object manipulation, and camera pose estimation)
specifically test model's geometric correspondence and the capacity to align
information consistently across views. Our extensive experiments, benchmark on
27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and
GPT-4o against human evaluators reveals a substantial performance gap,
indicating that current MLLMs remain far from human-level proficiency. Through
in-depth analysis, we show that MLLMs are particularly underperforming under
two aspects: (1) cross-view correspondence for partially occluded views and (2)
establishing the coarse camera poses. These findings highlight the necessity of
domain-specific refinements or modules that embed stronger multi-view
awareness. We believe that our All-Angles Bench offers valuable insights and
contribute to bridging the gap between MLLMs and human-level multi-view
understanding. The project and benchmark are publicly available at
https://danielchyeh.github.io/All-Angles-Bench/.

</details>

### [185] [Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation](https://arxiv.org/abs/2504.14988)
*Hong-Tao Yu,Xiu-Shen Wei,Yuxin Peng,Serge Belongie*

Main category: cs.CV

TLDR: 该论文提出了一个名为FG-BMK的细粒度评估基准，用于评估大型视觉语言模型（LVLMs）在细粒度图像任务中的表现，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LVLMs的评估主要集中在整体和特定任务上，而细粒度图像任务尚未得到充分探索。

Method: 通过构建包含349万问题和332万图像的FG-BMK基准，从人类和机器视角系统评估LVLMs的语义识别和细粒度特征表示能力。

Result: 实验揭示了训练范式、模态对齐、扰动敏感性和细粒度类别推理对任务性能的影响。

Conclusion: 该研究为当前LVLMs的局限性提供了关键见解，并为未来数据构建和模型设计提供了指导。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
remarkable multimodal perception capabilities, garnering significant attention.
While numerous evaluation studies have emerged, assessing LVLMs both
holistically and on specialized tasks, fine-grained image tasks-fundamental to
computer vision-remain largely unexplored. To fill this gap, we introduce a
comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49
million questions and 3.32 million images. Our evaluation systematically
examines LVLMs from both human-oriented and machine-oriented perspectives,
focusing on their semantic recognition and fine-grained feature representation
capabilities. Through extensive experiments on eight representative LVLMs/VLMs,
we uncover key findings regarding the influence of training paradigms, modality
alignment, perturbation susceptibility, and fine-grained category reasoning on
task performance. This work provides critical insights into the limitations of
current LVLMs and offers guidance for future data construction and model design
in the development of more advanced LVLMs. Our code is open-source and
available at https://github.com/SEU-VIPGroup/FG-BMK.

</details>

### [186] [NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study](https://arxiv.org/abs/2504.15003)
*Xin Li,Xijun Wang,Bingchen Li,Kun Yuan,Yizhen Shao,Suhang Yao,Ming Sun,Chao Zhou,Radu Timofte,Zhibo Chen*

Main category: cs.CV

TLDR: KwaiSR是首个针对短用户生成内容（UGC）图像超分辨率的基准数据集，包含合成和真实两部分，旨在推动相关算法研究。


<details>
  <summary>Details</summary>
Motivation: 为短形式UGC平台开发图像超分辨率算法提供数据支持。

Method: 数据集包括合成（1,900对图像）和真实（1,900张图像）两部分，模拟真实低质量UGC图像分布，并用于训练和测试。

Result: KwaiSR数据集对现有超分辨率方法具有挑战性，推动了该领域的新方向。

Conclusion: KwaiSR数据集为图像超分辨率研究提供了重要资源，并通过NTIRE 2025挑战赛吸引了广泛关注。

Abstract: In this work, we build the first benchmark dataset for short-form UGC Image
Super-resolution in the wild, termed KwaiSR, intending to advance the research
on developing image super-resolution algorithms for short-form UGC platforms.
This dataset is collected from the Kwai Platform, which is composed of two
parts, i.e., synthetic and wild parts. Among them, the synthetic dataset,
including 1,900 image pairs, is produced by simulating the degradation
following the distribution of real-world low-quality short-form UGC images,
aiming to provide the ground truth for training and objective comparison in the
validation/testing. The wild dataset contains low-quality images collected
directly from the Kwai Platform, which are filtered using the quality
assessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset
contains 1800 synthetic image pairs and 1900 wild images, which are divided
into training, validation, and testing parts with a ratio of 8:1:1. Based on
the KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form
UGC Video quality assessment and enhancement, which attracts lots of
researchers to develop the algorithm for it. The results of this competition
have revealed that our KwaiSR dataset is pretty challenging for existing Image
SR methods, which is expected to lead to a new direction in the image
super-resolution field. The dataset can be found from
https://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.

</details>

### [187] [Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images](https://arxiv.org/abs/2504.15007)
*David C Wong,Bin Wang,Gorkem Durak,Marouane Tliba,Mohamed Amine Kerkouri,Aladine Chetouani,Ahmet Enis Cetin,Cagdas Topel,Nicolo Gennaro,Camila Vendrami,Tugce Agirlar Trabzonlu,Amir Ali Rahsepar,Laetitia Perronne,Matthew Antalek,Onural Ozturk,Gokcan Okur,Andrew C. Gordon,Ayis Pyrros,Frank H Miller,Amir A Borhani,Hatice Savas,Eric M. Hart*

Main category: cs.CV

TLDR: 该论文通过眼动追踪分析放射科医生在真实与深度学习生成图像中的注意力分配和诊断策略差异。


<details>
  <summary>Details</summary>
Motivation: 研究放射科医生在医疗影像诊断中的视觉注意力模式，以及真实与合成图像对其眼动行为的影响。

Method: 通过测量眼动模式（如扫视方向、幅度及其联合分布）和注视偏差图（包括首次、末次、短时和长时注视）来量化差异。

Result: 揭示了放射科医生在真实与合成图像中的注意力分配和视觉显著性差异。

Conclusion: 眼动追踪分析为理解放射科医生的诊断行为提供了重要工具，并揭示了合成图像可能对其注意力模式的影响。

Abstract: Eye-tracking analysis plays a vital role in medical imaging, providing key
insights into how radiologists visually interpret and diagnose clinical cases.
In this work, we first analyze radiologists' attention and agreement by
measuring the distribution of various eye-movement patterns, including saccades
direction, amplitude, and their joint distribution. These metrics help uncover
patterns in attention allocation and diagnostic strategies. Furthermore, we
investigate whether and how doctors' gaze behavior shifts when viewing
authentic (Real) versus deep-learning-generated (Fake) images. To achieve this,
we examine fixation bias maps, focusing on first, last, short, and longest
fixations independently, along with detailed saccades patterns, to quantify
differences in gaze distribution and visual saliency between authentic and
synthetic images.

</details>

### [188] [Insert Anything: Image Insertion via In-Context Editing in DiT](https://arxiv.org/abs/2504.15009)
*Wensong Song,Hong Jiang,Zongxing Yang,Ruijie Quan,Yi Yang*

Main category: cs.CV

TLDR: Insert Anything是一个统一的框架，用于基于参考的图像插入，支持灵活的、用户指定的控制引导。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要为不同任务训练单独模型的问题，提出一个统一的框架，能够处理多样化的插入场景。

Method: 利用Diffusion Transformer的多模态注意力支持掩码和文本引导编辑，并引入上下文编辑机制。

Result: 在多个基准测试中表现优于现有方法，展示了在创意内容生成、虚拟试穿等实际应用中的潜力。

Conclusion: Insert Anything是一个高效、通用的图像插入框架，具有广泛的实际应用价值。

Abstract: This work presents Insert Anything, a unified framework for reference-based
image insertion that seamlessly integrates objects from reference images into
target scenes under flexible, user-specified control guidance. Instead of
training separate models for individual tasks, our approach is trained once on
our new AnyInsertion dataset--comprising 120K prompt-image pairs covering
diverse tasks such as person, object, and garment insertion--and effortlessly
generalizes to a wide range of insertion scenarios. Such a challenging setting
requires capturing both identity features and fine-grained details, while
allowing versatile local adaptations in style, color, and texture. To this end,
we propose to leverage the multimodal attention of the Diffusion Transformer
(DiT) to support both mask- and text-guided editing. Furthermore, we introduce
an in-context editing mechanism that treats the reference image as contextual
information, employing two prompting strategies to harmonize the inserted
elements with the target scene while faithfully preserving their distinctive
features. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD
benchmarks demonstrate that our method consistently outperforms existing
alternatives, underscoring its great potential in real-world applications such
as creative content generation, virtual try-on, and scene composition.

</details>

### [189] [Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models](https://arxiv.org/abs/2504.15026)
*Zijin Yang,Xin Zhang,Kejiang Chen,Kai Zeng,Qiyi Yao,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TLDR: 论文提出了一种名为Gaussian Shading++的水印方法，解决了扩散模型在实际部署中面临的版权保护和内容验证问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法忽略了实际部署中的关键挑战，如密钥管理复杂性和第三方验证困难。

Method: 采用双通道设计和伪随机纠错码编码种子，结合高斯噪声通道建模和软决策解码策略，并引入公钥签名。

Result: 实验表明，Gaussian Shading++在保持性能无损的同时，鲁棒性优于现有方法。

Conclusion: Gaussian Shading++是一种更实用的现实部署解决方案。

Abstract: Ethical concerns surrounding copyright protection and inappropriate content
generation pose challenges for the practical implementation of diffusion
models. One effective solution involves watermarking the generated images.
Existing methods primarily focus on ensuring that watermark embedding does not
degrade the model performance. However, they often overlook critical challenges
in real-world deployment scenarios, such as the complexity of watermark key
management, user-defined generation parameters, and the difficulty of
verification by arbitrary third parties. To address this issue, we propose
Gaussian Shading++, a diffusion model watermarking method tailored for
real-world deployment. We propose a double-channel design that leverages
pseudorandom error-correcting codes to encode the random seed required for
watermark pseudorandomization, achieving performance-lossless watermarking
under a fixed watermark key and overcoming key management challenges.
Additionally, we model the distortions introduced during generation and
inversion as an additive white Gaussian noise channel and employ a novel soft
decision decoding strategy during extraction, ensuring strong robustness even
when generation parameters vary. To enable third-party verification, we
incorporate public key signatures, which provide a certain level of resistance
against forgery attacks even when model inversion capabilities are fully
disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only
maintains performance losslessness but also outperforms existing methods in
terms of robustness, making it a more practical solution for real-world
deployment.

</details>

### [190] [DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation](https://arxiv.org/abs/2504.15032)
*Weijie He,Mushui Liu,Yunlong Yu,Zhao Wang,Chao Wu*

Main category: cs.CV

TLDR: DyST-XL是一个无需训练的框架，通过帧感知控制提升现有文本到视频模型的性能，解决布局不连续、实体身份漂移和交互动态不合理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本到视频生成模型在处理多实体交互和时空关系时存在布局不连续、实体身份漂移和交互动态不合理的问题。

Method: DyST-XL提出动态布局规划器、双提示控制注意力机制和实体一致性约束策略，通过LLM解析输入提示并生成物理感知关键帧布局，实现精确控制。

Result: 实验表明，DyST-XL在复杂提示下的文本到视频生成中表现优异，显著提升了性能。

Conclusion: DyST-XL填补了无需训练视频合成中的关键空白，为多实体交互场景提供了高效解决方案。

Abstract: Compositional text-to-video generation, which requires synthesizing dynamic
scenes with multiple interacting entities and precise spatial-temporal
relationships, remains a critical challenge for diffusion-based models.
Existing methods struggle with layout discontinuity, entity identity drift, and
implausible interaction dynamics due to unconstrained cross-attention
mechanisms and inadequate physics-aware reasoning. To address these
limitations, we propose DyST-XL, a \textbf{training-free} framework that
enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through
frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic
Layout Planner that leverages large language models (LLMs) to parse input
prompts into entity-attribute graphs and generates physics-aware keyframe
layouts, with intermediate frames interpolated via trajectory optimization; (2)
A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video
alignment through frame-aware attention masking, achieving the precise control
over individual entities; and (3) An Entity-Consistency Constraint strategy
that propagates first-frame feature embeddings to subsequent frames during
denoising, preserving object identity without manual annotation. Experiments
demonstrate that DyST-XL excels in compositional text-to-video generation,
significantly improving performance on complex prompts and bridging a crucial
gap in training-free video synthesis.

</details>

### [191] [Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification](https://arxiv.org/abs/2504.15041)
*Shiben Liu,Huijie Fan,Qiang Wang,Baojie Fan,Yandong Tang,Liangqiong Qu*

Main category: cs.CV

TLDR: 论文提出了一种名为DAFC的新模型，通过文本驱动的提示聚合和分布感知集成，解决了终身行人重识别中的遗忘问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 终身行人重识别（LReID）面临在适应新信息的同时保留旧知识的关键挑战，现有方法（基于排练和无排练）存在遗忘问题。

Method: 提出了DAFC模型，包括文本驱动的提示聚合（TPA）和分布感知与集成（DAI），以及知识巩固机制（KCM）。

Result: 实验结果显示，DAFC在两种训练顺序上的平均mAP/R@1分别至少优于现有方法9.8%/6.6%和6.4%/6.2%。

Conclusion: DAFC通过跨域共享表示学习和领域特定分布集成，有效缓解了遗忘问题，提升了性能。

Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in
preserving old knowledge while adapting to new information. The existing
solutions include rehearsal-based and rehearsal-free methods to address this
challenge. Rehearsal-based approaches rely on knowledge distillation,
continuously accumulating forgetting during the distillation process.
Rehearsal-free methods insufficiently learn the distribution of each domain,
leading to forgetfulness over time. To solve these issues, we propose a novel
Distribution-aware Forgetting Compensation (DAFC) model that explores
cross-domain shared representation learning and domain-specific distribution
integration without using old exemplars or knowledge distillation. We propose a
Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich
prompt elements and guide the prompt model to learn fine-grained
representations for each instance. This can enhance the differentiation of
identity information and establish the foundation for domain distribution
awareness. Then, Distribution-based Awareness and Integration (DAI) is designed
to capture each domain-specific distribution by a dedicated expert network and
adaptively consolidate them into a shared region in high-dimensional space. In
this manner, DAI can consolidate and enhance cross-domain shared representation
learning while alleviating catastrophic forgetting. Furthermore, we develop a
Knowledge Consolidation Mechanism (KCM) that comprises instance-level
discrimination and cross-domain consistency alignment strategies to facilitate
model adaptive learning of new knowledge from the current domain and promote
knowledge consolidation learning between acquired domain-specific
distributions, respectively. Experimental results show that our DAFC outperform
state-of-the-art methods by at least 9.8\%/6.6\% and 6.4\%/6.2\% of average
mAP/R@1 on two training orders.

</details>

### [192] [ScanEdit: Hierarchically-Guided Functional 3D Scan Editing](https://arxiv.org/abs/2504.15049)
*Mohamed el amine Boudjoghra,Ivan Laptev,Angela Dai*

Main category: cs.CV

TLDR: ScanEdit是一种基于指令驱动的3D扫描编辑方法，利用分层场景图和大型语言模型（LLMs）实现高效编辑，结合物理约束生成逼真场景。


<details>
  <summary>Details</summary>
Motivation: 随着3D捕获技术的快速发展，3D数据大量涌现，高效的3D场景编辑成为图形应用的关键需求。

Method: 通过构建分层场景图表示3D扫描对象，利用LLMs将语言指令转化为分层编辑命令，并结合物理约束生成逼真场景。

Result: ScanEdit在实验中表现优异，优于现有技术，适用于多种真实场景和输入指令。

Conclusion: ScanEdit通过分层和LLM驱动的编辑方法，成功实现了高效且逼真的3D场景编辑。

Abstract: With the fast pace of 3D capture technology and resulting abundance of 3D
data, effective 3D scene editing becomes essential for a variety of graphics
applications. In this work we present ScanEdit, an instruction-driven method
for functional editing of complex, real-world 3D scans. To model large and
interdependent sets of ob- jectswe propose a hierarchically-guided approach.
Given a 3D scan decomposed into its object instances, we first construct a
hierarchical scene graph representation to enable effective, tractable editing.
We then leverage reason- ing capabilities of Large Language Models (LLMs) and
translate high-level language instructions into actionable commands applied
hierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based
guidance with ex- plicit physical constraints and generates realistic scenes
where object arrangements obey both physics and common sense. In our extensive
experimental evaluation ScanEdit outperforms state of the art and demonstrates
excellent re- sults for a variety of real-world scenes and input instruc-
tions.

</details>

### [193] [Structure-guided Diffusion Transformer for Low-Light Image Enhancement](https://arxiv.org/abs/2504.15054)
*Xiangchen Yin,Zhenda Yu,Longtao Jiang,Xin Gao,Xiao Sun,Zhi Liu,Xun Yang*

Main category: cs.CV

TLDR: 本文首次将扩散变换器（DiT）引入低光图像增强任务，提出了一种基于结构引导的扩散变换器框架（SDTL），通过小波变换和结构增强模块（SEM）提升图像质量，并在实验中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前低光图像增强方法在恢复细节的同时会放大噪声，导致视觉质量不佳，因此需要一种更有效的方法来解决这一问题。

Method: 提出SDTL框架，结合小波变换压缩特征以提高推理效率，设计SEM模块利用结构先验增强纹理，并引入SAB模块在噪声预测中关注纹理丰富的区域。

Result: 实验表明，SDTL在多个流行数据集上实现了SOTA性能，显著提升了图像质量。

Conclusion: SDTL框架验证了DiT在低光增强任务中的潜力，为相关研究提供了新思路。

Abstract: While the diffusion transformer (DiT) has become a focal point of interest in
recent years, its application in low-light image enhancement remains a blank
area for exploration. Current methods recover the details from low-light images
while inevitably amplifying the noise in images, resulting in poor visual
quality. In this paper, we firstly introduce DiT into the low-light enhancement
task and design a novel Structure-guided Diffusion Transformer based Low-light
image enhancement (SDTL) framework. We compress the feature through wavelet
transform to improve the inference efficiency of the model and capture the
multi-directional frequency band. Then we propose a Structure Enhancement
Module (SEM) that uses structural prior to enhance the texture and leverages an
adaptive fusion strategy to achieve more accurate enhancement effect. In
Addition, we propose a Structure-guided Attention Block (SAB) to pay more
attention to texture-riched tokens and avoid interference from noisy areas in
noise prediction. Extensive qualitative and quantitative experiments
demonstrate that our method achieves SOTA performance on several popular
datasets, validating the effectiveness of SDTL in improving image quality and
the potential of DiT in low-light enhancement tasks.

</details>

### [194] [Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.15085)
*Wangyu Wu,Zhenhong Chen,Siqi Song,Xianglin Qiua,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TLDR: HAF-VT是一种新颖的跨域序列推荐方法，通过结合视觉和文本数据增强认知建模，利用分层注意力机制学习单域和跨域偏好，在四个电商数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨域序列推荐（CDSR）需要更好地建模用户跨域偏好，而现有方法未能充分利用多模态数据（如视觉和文本）来模拟人类认知过程。

Method: 提出HAF-VT方法，使用冻结的CLIP模型生成图像和文本嵌入，通过分层注意力机制联合学习单域和跨域偏好。

Result: 在四个电商数据集上，HAF-VT在捕捉跨域用户兴趣方面优于现有方法。

Conclusion: HAF-VT成功将认知原理与计算模型结合，突显了多模态数据在序列决策中的作用。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences through intra- and inter-sequence item
relationships. Inspired by human cognitive processes, we propose Hierarchical
Attention Fusion of Visual and Textual Representations (HAF-VT), a novel
approach integrating visual and textual data to enhance cognitive modeling.
Using the frozen CLIP model, we generate image and text embeddings, enriching
item representations with multimodal data. A hierarchical attention mechanism
jointly learns single-domain and cross-domain preferences, mimicking human
information integration. Evaluated on four e-commerce datasets, HAF-VT
outperforms existing methods in capturing cross-domain user interests, bridging
cognitive principles with computational models and highlighting the role of
multimodal data in sequential decision-making.

</details>

### [195] [VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation](https://arxiv.org/abs/2504.15095)
*Mingxia Zhan,Li Zhang,XiaoMeng Chu,Beibei Wang*

Main category: cs.CV

TLDR: VistaDepth通过结合频域特征增强和自适应权重平衡机制，改进了基于扩散模型的单目深度估计（MDE），显著提升了远距离深度重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的MDE方法在远距离深度重建上表现不佳，主要由于深度值分布不均衡和对空间域特征的过度依赖。

Method: 提出VistaDepth框架，包含Latent Frequency Modulation（LFM）模块动态优化潜在特征空间的频谱响应，并采用自适应权重策略实时调整扩散损失。

Result: 实验证明VistaDepth在扩散基MDE方法中达到最先进性能，尤其在远距离区域重建上表现突出。

Conclusion: VistaDepth通过频域特征增强和自适应权重机制，显著提升了深度感知性能，特别是在远距离深度重建方面。

Abstract: Monocular depth estimation (MDE) aims to predict per-pixel depth values from
a single RGB image. Recent advancements have positioned diffusion models as
effective MDE tools by framing the challenge as a conditional image generation
task. Despite their progress, these methods often struggle with accurately
reconstructing distant depths, due largely to the imbalanced distribution of
depth values and an over-reliance on spatial-domain features. To overcome these
limitations, we introduce VistaDepth, a novel framework that integrates
adaptive frequency-domain feature enhancements with an adaptive
weight-balancing mechanism into the diffusion process. Central to our approach
is the Latent Frequency Modulation (LFM) module, which dynamically refines
spectral responses in the latent feature space, thereby improving the
preservation of structural details and reducing noisy artifacts. Furthermore,
we implement an adaptive weighting strategy that modulates the diffusion loss
in real-time, enhancing the model's sensitivity towards distant depth
reconstruction. These innovations collectively result in superior depth
perception performance across both distance and detail. Experimental
evaluations confirm that VistaDepth achieves state-of-the-art performance among
diffusion-based MDE techniques, particularly excelling in the accurate
reconstruction of distant regions.

</details>

### [196] [A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae](https://arxiv.org/abs/2504.15105)
*Yurun Wang,Zerong Qi,Shujun Fu,Mingzheng Hu*

Main category: cs.CV

TLDR: 提出了一种名为TBSFNet的三分支空间融合网络，结合MLFGNet提升潜在指纹增强效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在低质量指纹区域恢复上表现不足，需针对不同区域采用不同增强策略。

Method: 提出TBSFNet，结合方向场和细节相关模块，并引入MLFGNet提升泛化能力。

Result: 在MOLF和MUST数据集上，MLFGNet表现优于现有增强算法。

Conclusion: TBSFNet和MLFGNet能有效提升潜在指纹增强效果，尤其针对低质量区域。

Abstract: Latent fingerprint enhancement is a critical step in the process of latent
fingerprint identification. Existing deep learning-based enhancement methods
still fall short of practical application requirements, particularly in
restoring low-quality fingerprint regions. Recognizing that different regions
of latent fingerprints require distinct enhancement strategies, we propose a
Triple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances
different regions of the image using tailored strategies. Furthermore, to
improve the generalization capability of the network, we integrate orientation
field and minutiae-related modules into TBSFNet and introduce a Multi-Level
Feature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST
datasets demonstrate that MLFGNet outperforms existing enhancement algorithms.

</details>

### [197] [Unwarping Screen Content Images via Structure-texture Enhancement Network and Transformation Self-estimation](https://arxiv.org/abs/2504.15108)
*Zhenzhen Xiao,Heng Liu,Bingwen Hu*

Main category: cs.CV

TLDR: 提出了一种结构-纹理增强网络（STEN）用于屏幕内容图像（SCI）的扭曲校正，结合B样条隐式神经表示和变换自估计算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经网络方法在自然图像上表现良好，但难以处理包含大几何扭曲、文本、符号和锐利边缘的SCI。

Method: STEN包含结构估计分支（SEB）和纹理估计分支（TEB），分别增强局部聚合与全局依赖建模及纹理细节合成，并通过变换自估计模块校正坐标变换矩阵。

Result: 在公开SCI数据集上的实验表明，该方法显著优于现有技术，且在自然图像数据集上也显示出潜力。

Conclusion: STEN通过结构-纹理增强和变换自估计，有效解决了SCI扭曲问题，并具有扩展到自然图像的潜力。

Abstract: While existing implicit neural network-based image unwarping methods perform
well on natural images, they struggle to handle screen content images (SCIs),
which often contain large geometric distortions, text, symbols, and sharp
edges. To address this, we propose a structure-texture enhancement network
(STEN) with transformation self-estimation for SCI warping. STEN integrates a
B-spline implicit neural representation module and a transformation error
estimation and self-correction algorithm. It comprises two branches: the
structure estimation branch (SEB), which enhances local aggregation and global
dependency modeling, and the texture estimation branch (TEB), which improves
texture detail synthesis using B-spline implicit neural representation.
Additionally, the transformation self-estimation module autonomously estimates
the transformation error and corrects the coordinate transformation matrix,
effectively handling real-world image distortions. Extensive experiments on
public SCI datasets demonstrate that our approach significantly outperforms
state-of-the-art methods. Comparisons on well-known natural image datasets also
show the potential of our approach for natural image distortion.

</details>

### [198] [Improving Sound Source Localization with Joint Slot Attention on Image and Audio](https://arxiv.org/abs/2504.15118)
*Inho Kim,Youngkil Song,Jicheol Park,Won Hwa Kim,Suha Kwak*

Main category: cs.CV

TLDR: 论文提出了一种基于联合槽注意力的新型声源定位方法，通过分解图像和音频特征为目标和非目标表示，仅使用目标表示进行对比学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有声源定位方法因缺乏定位标签，通常将图像和音频表示为单一嵌入向量，并通过对比学习进行训练，但这种方法容易受到噪声和背景干扰的影响。

Method: 提出联合槽注意力机制，通过两个槽竞争性地关注图像和音频特征，分解为目标和非目标表示，并仅使用目标表示进行对比学习；同时引入跨模态注意力匹配以进一步对齐局部特征。

Result: 在三个公开的声源定位基准测试中，该方法几乎在所有设置下均表现最佳，并在跨模态检索任务中大幅超越先前工作。

Conclusion: 通过联合槽注意力和跨模态注意力匹配，新方法有效解决了噪声和背景干扰问题，显著提升了声源定位和跨模态检索的性能。

Abstract: Sound source localization (SSL) is the task of locating the source of sound
within an image. Due to the lack of localization labels, the de facto standard
in SSL has been to represent an image and audio as a single embedding vector
each, and use them to learn SSL via contrastive learning. To this end, previous
work samples one of local image features as the image embedding and aggregates
all local audio features to obtain the audio embedding, which is far from
optimal due to the presence of noise and background irrelevant to the actual
target in the input. We present a novel SSL method that addresses this chronic
issue by joint slot attention on image and audio. To be specific, two slots
competitively attend image and audio features to decompose them into target and
off-target representations, and only target representations of image and audio
are used for contrastive learning. Also, we introduce cross-modal attention
matching to further align local features of image and audio. Our method
achieved the best in almost all settings on three public benchmarks for SSL,
and substantially outperformed all the prior work in cross-modal retrieval.

</details>

### [199] [Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations](https://arxiv.org/abs/2504.15121)
*Csongor Csanad Kariko,Muhammad Rafi Faisal,Levente Hajder*

Main category: cs.CV

TLDR: 提出了一种基于校正立体图像对的表面法线估计新方法，利用视差值的仿射变换实现快速准确的结果。


<details>
  <summary>Details</summary>
Motivation: 通过校正立体图像对简化表面法线估计过程，降低计算复杂度。

Method: 结合仿射变换和自定义卷积算法处理视差数据，并引入自适应启发式技术检测连通表面组件。

Result: 在Middlebury和Cityscapes数据集上验证，显著提升了实时性能和准确性。

Conclusion: 方法快速准确，生成密集定向点云，代码将公开以促进研究。

Abstract: This work introduces a novel method for surface normal estimation from
rectified stereo image pairs, leveraging affine transformations derived from
disparity values to achieve fast and accurate results. We demonstrate how the
rectification of stereo image pairs simplifies the process of surface normal
estimation by reducing computational complexity. To address noise reduction, we
develop a custom algorithm inspired by convolutional operations, tailored to
process disparity data efficiently. We also introduce adaptive heuristic
techniques for efficiently detecting connected surface components within the
images, further improving the robustness of the method. By integrating these
methods, we construct a surface normal estimator that is both fast and
accurate, producing a dense, oriented point cloud as the final output. Our
method is validated using both simulated environments and real-world stereo
images from the Middlebury and Cityscapes datasets, demonstrating significant
improvements in real-time performance and accuracy when implemented on a GPU.
Upon acceptance, the shader source code will be made publicly available to
facilitate further research and reproducibility.

</details>

### [200] [MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video](https://arxiv.org/abs/2504.15122)
*Minh-Quan Viet Bui,Jongmin Park,Juan Luis Gonzalez Bello,Jaeho Moon,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TLDR: MoBGS是一种新的动态3D高斯泼溅（3DGS）去模糊框架，能够从模糊的单目视频中重建清晰的高质量时空视图。


<details>
  <summary>Details</summary>
Motivation: 现有动态新视角合成（NVS）方法对运动模糊敏感，导致渲染质量下降。MoBGS旨在解决这一问题，专注于动态对象的运动建模。

Method: MoBGS提出Blur-adaptive Latent Camera Estimation（BLCE）和Latent Camera-induced Exposure Estimation（LCEE）方法，分别用于全局相机运动去模糊和局部对象运动去模糊。

Result: 在Stereo Blur数据集和真实模糊视频上的实验表明，MoBGS显著优于现有方法（DyBluRF和Deblur4DGS），在动态NVS任务中达到最佳性能。

Conclusion: MoBGS通过改进的相机轨迹估计和运动分解，实现了动态场景的高质量去模糊和时空一致性。

Abstract: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)
framework capable of reconstructing sharp and high-quality novel
spatio-temporal views from blurry monocular videos in an end-to-end manner.
Existing dynamic novel view synthesis (NVS) methods are highly sensitive to
motion blur in casually captured videos, resulting in significant degradation
of rendering quality. While recent approaches address motion-blurred inputs for
NVS, they primarily focus on static scene reconstruction and lack dedicated
motion modeling for dynamic objects. To overcome these limitations, our MoBGS
introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for
effective latent camera trajectory estimation, improving global camera motion
deblurring. In addition, we propose a physically-inspired Latent Camera-induced
Exposure Estimation (LCEE) method to ensure consistent deblurring of both
global camera and local object motion. Our MoBGS framework ensures the temporal
consistency of unseen latent timestamps and robust motion decomposition of
static and dynamic regions. Extensive experiments on the Stereo Blur dataset
and real-world blurry videos show that our MoBGS significantly outperforms the
very recent advanced methods (DyBluRF and Deblur4DGS), achieving
state-of-the-art performance for dynamic NVS under motion blur.

</details>

### [201] [Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation](https://arxiv.org/abs/2504.15134)
*Xiao Zhang,Lu Zou,Tao Lu,Yuan Yao,Zhangjin Huang,Guoping Wang*

Main category: cs.CV

TLDR: INKL-Pose是一种新的类别级物体姿态估计框架，通过实例自适应关键点学习和局部到全局几何聚合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理复杂几何或非规范形状物体时的泛化能力不足问题。

Method: 提出实例自适应关键点生成器，结合局部和全局特征聚合，并引入双向Mamba和特征序列翻转策略。

Result: 在CAMERA25、REAL275和HouseCat6D数据集上达到最先进性能。

Conclusion: INKL-Pose通过自适应关键点学习和几何聚合，显著提升了类别级物体姿态估计的准确性和鲁棒性。

Abstract: Category-level object pose estimation aims to predict the 6D pose and size of
previously unseen instances from predefined categories, requiring strong
generalization across diverse object instances. Although many previous methods
attempt to mitigate intra-class variations, they often struggle with instances
exhibiting complex geometries or significant deviations from canonical shapes.
To address this challenge, we propose INKL-Pose, a novel category-level object
pose estimation framework that enables INstance-adaptive Keypoint Learning with
local-to-global geometric aggregation. Specifically, our approach first
predicts semantically consistent and geometric informative keypoints through an
Instance-Adaptive Keypoint Generator, then refines them with: (1) a Local
Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global
Keypoint Feature Aggregator using bidirectional Mamba for structural
consistency. To enable bidirectional modeling in Mamba, we introduce a Feature
Sequence Flipping strategy that preserves spatial coherence while constructing
backward feature sequences. Additionally, we design a surface loss and a
separation loss to enforce uniform coverage and spatial diversity in keypoint
distribution. The generated keypoints are finally mapped to a canonical space
for regressing the object's 6D pose and size. Extensive experiments on
CAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves
state-of-the-art performance and significantly outperforms existing methods.

</details>

### [202] ["I Know It When I See It": Mood Spaces for Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2504.15145)
*Huzheng Yang,Katherine Xu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TLDR: 提出了一种名为Mood Board的方法，通过示例传达抽象概念，并构建了一个紧凑的Mood Space，支持图像级操作。


<details>
  <summary>Details</summary>
Motivation: 许多抽象概念难以定义但易于识别，需要一种方法来有效传达这些概念。

Method: 使用Mood Board和Mood Space，通过纤维化计算压缩预训练特征，学习图像标记的成对亲和关系，并在特征向量空间中定义损失。

Result: Mood Space具有局部线性和紧凑性，支持对象平均、视觉类比和姿态转移等操作，计算高效且仅需少量示例。

Conclusion: Mood Board和Mood Space为抽象概念的传达和操作提供了一种高效且紧凑的解决方案。

Abstract: Expressing complex concepts is easy when they can be labeled or quantified,
but many ideas are hard to define yet instantly recognizable. We propose a Mood
Board, where users convey abstract concepts with examples that hint at the
intended direction of attribute changes. We compute an underlying Mood Space
that 1) factors out irrelevant features and 2) finds the connections between
images, thus bringing relevant concepts closer. We invent a fibration
computation to compress/decompress pre-trained features into/from a compact
space, 50-100x smaller. The main innovation is learning to mimic the pairwise
affinity relationship of the image tokens across exemplars. To focus on the
coarse-to-fine hierarchical structures in the Mood Space, we compute the top
eigenvector structure from the affinity matrix and define a loss in the
eigenvector space. The resulting Mood Space is locally linear and compact,
allowing image-level operations, such as object averaging, visual analogy, and
pose transfer, to be performed as a simple vector operation in Mood Space. Our
learning is efficient in computation without any fine-tuning, needs only a few
(2-20) exemplars, and takes less than a minute to learn.

</details>

### [203] [Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection](https://arxiv.org/abs/2504.15152)
*Jun Zhou,Bingchen Gao,Kai Wang,Jialun Pei,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TLDR: 提出了一种基于自监督学习的无标记术前到术中肝脏配准框架，解决了传统方法依赖解剖标记和术中信息不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统肝脏配准方法依赖解剖标记，存在标记定义模糊和术中形状变形建模不足的问题。

Method: 提出了一种3D-3D配准框架，分为刚性和非刚性配准子任务，使用特征解耦变换器和结构正则化变形网络。

Result: 在合成和真实数据集上的实验表明，该方法具有优越性和临床适用性。

Conclusion: 该方法显著提升了肝脏配准的准确性和效率，具有临床潜力。

Abstract: Liver registration by overlaying preoperative 3D models onto intraoperative
2D frames can assist surgeons in perceiving the spatial anatomy of the liver
clearly for a higher surgical success rate. Existing registration methods rely
heavily on anatomical landmark-based workflows, which encounter two major
limitations: 1) ambiguous landmark definitions fail to provide efficient
markers for registration; 2) insufficient integration of intraoperative liver
visual information in shape deformation modeling. To address these challenges,
in this paper, we propose a landmark-free preoperative-to-intraoperative
registration framework utilizing effective self-supervised learning, termed
\ourmodel. This framework transforms the conventional 3D-2D workflow into a
3D-3D registration pipeline, which is then decoupled into rigid and non-rigid
registration subtasks. \ourmodel~first introduces a feature-disentangled
transformer to learn robust correspondences for recovering rigid
transformations. Further, a structure-regularized deformation network is
designed to adjust the preoperative model to align with the intraoperative
liver surface. This network captures structural correlations through geometry
similarity modeling in a low-rank transformer network. To facilitate the
validation of the registration performance, we also construct an in-vivo
registration dataset containing liver resection videos of 21 patients, called
\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the
liver together with liver mask annotations and calibrated camera intrinsic
parameters. Extensive experiments and user studies on both synthetic and
in-vivo datasets demonstrate the superiority and potential clinical
applicability of our method.

</details>

### [204] [Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15155)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TLDR: KANet基于改进的3D-DenseNet模型，通过引入可学习的B样条函数和动态网格调整机制，高效处理高光谱图像分类中的高维数据和冗余问题，显著提升模型精度和参数效率。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维数据、地物稀疏分布和光谱冗余等挑战，导致过拟合和泛化能力受限。

Method: 提出KANet，结合3D KAN Conv和自适应网格更新机制，利用B样条函数替代传统卷积核的固定权重，动态调整网格点位置以匹配数据特征。

Result: 在IN、UP和KSC数据集上表现优于主流方法，提升了高维数据建模精度和参数效率。

Conclusion: KANet通过动态专家卷积系统增强模型表示能力，无需增加网络深度或宽度，有效缓解维度灾难和过拟合风险。

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more efficiently adapt to
ground object distributions while extracting image features without introducing
excessive parameters and skipping redundant information, this paper proposes
KANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an
adaptive grid update mechanism. By introducing learnable univariate B-spline
functions on network edges, specifically by flattening three-dimensional
neighborhoods into vectors and applying B-spline-parameterized nonlinear
activation functions to replace the fixed linear weights of traditional 3D
convolutional kernels, we precisely capture complex spectral-spatial nonlinear
relationships in hyperspectral data. Simultaneously, through a dynamic grid
adjustment mechanism, we adaptively update the grid point positions of
B-splines based on the statistical characteristics of input data, optimizing
the resolution of spline functions to match the non-uniform distribution of
spectral features, significantly improving the model's accuracy in
high-dimensional data modeling and parameter efficiency, effectively
alleviating the curse of dimensionality. This characteristic demonstrates
superior neural scaling laws compared to traditional convolutional neural
networks and reduces overfitting risks in small-sample and high-noise
scenarios. KANet enhances model representation capability through a 3D dynamic
expert convolution system without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>

### [205] [Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration](https://arxiv.org/abs/2504.15159)
*Junyuan Deng,Xinyi Wu,Yongxing Yang,Congchao Zhu,Song Wang,Zhenyao Wu*

Main category: cs.CV

TLDR: 论文提出了一种名为FluxGen的数据生成管道和轻量级适配器FluxIR，用于利用预训练的大规模文本到图像模型（如Flux）进行图像恢复，显著降低了训练成本和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模文本到图像模型在图像恢复任务中需要大量高质量图像和计算资源，成本高且不隐私友好。

Method: 提出FluxGen管道（包括无条件图像生成、图像选择和退化图像模拟）和轻量级适配器FluxIR，用于控制基于DiT的T2I模型。

Result: 实验表明，该方法在合成和真实退化数据集上均取得了优异的恢复效果，且训练成本仅为现有方法的8.5%。

Conclusion: FluxGen和FluxIR有效解决了大规模模型在图像恢复中的成本和隐私问题，同时保持了高质量的恢复效果。

Abstract: Recently, pre-trained text-to-image (T2I) models have been extensively
adopted for real-world image restoration because of their powerful generative
prior. However, controlling these large models for image restoration usually
requires a large number of high-quality images and immense computational
resources for training, which is costly and not privacy-friendly. In this
paper, we find that the well-trained large T2I model (i.e., Flux) is able to
produce a variety of high-quality images aligned with real-world distributions,
offering an unlimited supply of training samples to mitigate the above issue.
Specifically, we proposed a training data construction pipeline for image
restoration, namely FluxGen, which includes unconditional image generation,
image selection, and degraded image simulation. A novel light-weighted adapter
(FluxIR) with squeeze-and-excitation layers is also carefully designed to
control the large Diffusion Transformer (DiT)-based T2I model so that
reasonable details can be restored. Experiments demonstrate that our proposed
method enables the Flux model to adapt effectively to real-world image
restoration tasks, achieving superior scores and visual quality on both
synthetic and real-world degradation datasets - at only about 8.5\% of the
training cost compared to current approaches.

</details>

### [206] [An Efficient Aerial Image Detection with Variable Receptive Fields](https://arxiv.org/abs/2504.15165)
*Liu Wenbin*

Main category: cs.CV

TLDR: VRF-DETR是一种基于Transformer的检测器，通过动态调整感受野和高效架构设计，解决了无人机目标检测中的小目标、遮挡和计算限制问题。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临小目标（小于10像素）、密集遮挡和严格计算限制的挑战，现有检测器难以平衡精度与效率。

Method: VRF-DETR包含三个关键模块：1）多尺度上下文融合模块（MSCF），2）门控卷积层（GConv），3）门控多尺度融合瓶颈（GMCF），通过动态感受野和高效参数设计提升性能。

Result: 在VisDrone2019数据集上，VRF-DETR达到51.4% mAP50和31.8% mAP50:95，仅需13.5M参数。

Conclusion: VRF-DETR为无人机目标检测任务建立了新的效率-精度平衡基准。

Abstract: Aerial object detection using unmanned aerial vehicles (UAVs) faces critical
challenges including sub-10px targets, dense occlusions, and stringent
computational constraints. Existing detectors struggle to balance accuracy and
efficiency due to rigid receptive fields and redundant architectures. To
address these limitations, we propose Variable Receptive Field DETR (VRF-DETR),
a transformer-based detector incorporating three key components: 1) Multi-Scale
Context Fusion (MSCF) module that dynamically recalibrates features through
adaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution
(GConv) layer enabling parameter-efficient local-context modeling via depthwise
separable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)
Bottleneck that hierarchically disentangles occluded objects through cascaded
global-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR
achieves 51.4\% mAP\textsubscript{50} and 31.8\% mAP\textsubscript{50:95} with
only 13.5M parameters. This work establishes a new efficiency-accuracy Pareto
frontier for UAV-based detection tasks.

</details>

### [207] [HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change Detection](https://arxiv.org/abs/2504.15170)
*Chengxi Han,Xiaoyu Su,Zhiqiang Wei,Meiqi Hu,Yichu Xu*

Main category: cs.CV

TLDR: HSANet提出了一种基于分层卷积和混合注意力机制的遥感图像变化检测网络，提升多尺度特征提取和全局信息融合能力。


<details>
  <summary>Details</summary>
Motivation: 遥感图像变化检测在大规模监测中至关重要，但现有方法在多尺度特征提取和全局信息融合方面存在不足。

Method: HSANet采用分层卷积提取多尺度特征，结合混合自注意力和跨注意力机制，学习并融合全局及跨尺度信息。

Result: HSANet能够捕捉不同尺度的全局上下文，整合跨尺度特征，优化边缘细节，提升检测性能。

Conclusion: HSANet通过多尺度特征和注意力机制的结合，显著提升了遥感图像变化检测的效果，代码已开源。

Abstract: The remote sensing image change detection task is an essential method for
large-scale monitoring. We propose HSANet, a network that uses hierarchical
convolution to extract multi-scale features. It incorporates hybrid
self-attention and cross-attention mechanisms to learn and fuse global and
cross-scale information. This enables HSANet to capture global context at
different scales and integrate cross-scale features, refining edge details and
improving detection performance. We will also open-source our model code:
https://github.com/ChengxiHAN/HSANet.

</details>

### [208] [DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution](https://arxiv.org/abs/2504.15176)
*Miaomiao Cai,Simiao Li,Wei Li,Xudong Huang,Hanting Chen,Jie Hu,Yunhe Wang*

Main category: cs.CV

TLDR: 该论文提出了一种名为DSPO的方法，将人类偏好对齐引入Real-ISR任务，通过语义指导解决像素级重建与图像级偏好的冲突。


<details>
  <summary>Details</summary>
Motivation: 现有Real-ISR方法缺乏人类反馈整合，可能导致生成结果与人类偏好不一致，甚至产生伪影和有害内容。

Method: 提出Direct Semantic Preference Optimization (DSPO)，结合语义实例对齐策略和用户描述反馈策略，实现实例级人类偏好对齐。

Result: DSPO在单步和多步超分辨率框架中均表现出高效性。

Conclusion: DSPO是一种即插即用的解决方案，有效提升了Real-ISR任务中生成结果与人类偏好的一致性。

Abstract: Recent advances in diffusion models have improved Real-World Image
Super-Resolution (Real-ISR), but existing methods lack human feedback
integration, risking misalignment with human preference and may leading to
artifacts, hallucinations and harmful content generation. To this end, we are
the first to introduce human preference alignment into Real-ISR, a technique
that has been successfully applied in Large Language Models and Text-to-Image
tasks to effectively enhance the alignment of generated outputs with human
preferences. Specifically, we introduce Direct Preference Optimization (DPO)
into Real-ISR to achieve alignment, where DPO serves as a general alignment
technique that directly learns from the human preference dataset. Nevertheless,
unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR
are difficult to reconcile with the image-level preferences of DPO, which can
lead to the DPO being overly sensitive to local anomalies, leading to reduced
generation quality. To resolve this dichotomy, we propose Direct Semantic
Preference Optimization (DSPO) to align instance-level human preferences by
incorporating semantic guidance, which is through two strategies: (a) semantic
instance alignment strategy, implementing instance-level alignment to ensure
fine-grained perceptual consistency, and (b) user description feedback
strategy, mitigating hallucinations through semantic textual feedback on
instance-level images. As a plug-and-play solution, DSPO proves highly
effective in both one-step and multi-step SR frameworks.

</details>

### [209] [FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image](https://arxiv.org/abs/2504.15179)
*Fei Yin,Mallikarjun B R,Chun-Han Yao,Rafał Mantiuk,Varun Jampani*

Main category: cs.CV

TLDR: 提出了一种从单张图像生成高质量可动画4D头像的新框架，解决了现有方法对多视图数据依赖或形状精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有4D头像生成方法需要大量多视图数据或难以保持形状准确性和身份一致性，因此提出一种结合形状、图像和视频先验的系统。

Method: 通过3D-GAN反演获取初始粗糙形状，利用深度引导变形信号增强多视图纹理一致性，并结合视频先验处理表情动画，引入一致性-非一致性训练优化4D重建。

Result: 实验结果表明，该方法在质量和多视图、表情一致性上优于现有技术。

Conclusion: 该方法成功实现了从单张图像生成高质量、可动画的4D头像，解决了现有方法的局限性。

Abstract: We present a novel framework for generating high-quality, animatable 4D
avatar from a single image. While recent advances have shown promising results
in 4D avatar creation, existing methods either require extensive multiview data
or struggle with shape accuracy and identity consistency. To address these
limitations, we propose a comprehensive system that leverages shape, image, and
video priors to create full-view, animatable avatars. Our approach first
obtains initial coarse shape through 3D-GAN inversion. Then, it enhances
multiview textures using depth-guided warping signals for cross-view
consistency with the help of the image diffusion model. To handle expression
animation, we incorporate a video prior with synchronized driving signals
across viewpoints. We further introduce a Consistent-Inconsistent training to
effectively handle data inconsistencies during 4D reconstruction. Experimental
results demonstrate that our method achieves superior quality compared to the
prior art, while maintaining consistency across different viewpoints and
expressions.

</details>

### [210] [Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform](https://arxiv.org/abs/2504.15182)
*Xianpan Zhou*

Main category: cs.CV

TLDR: Tiger200K是一个手动标注的高质量视频数据集，旨在解决开源文本到视频生成模型对专有训练数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集质量不足，无法满足高级视频生成模型的微调需求，因此需要高质量、人工标注的数据集。

Method: 通过人工筛选用户生成内容（UGC）平台视频，结合镜头边界检测、OCR、边框检测、运动过滤和双语字幕等技术构建高质量数据集。

Result: Tiger200K提供了视觉保真度高、时间一致的视频-文本对，适用于视频生成模型的优化。

Conclusion: Tiger200K将作为开源项目持续扩展，推动视频生成模型的研究和应用。

Abstract: The recent surge in open-source text-to-video generation models has
significantly energized the research community, yet their dependence on
proprietary training datasets remains a key constraint. While existing open
datasets like Koala-36M employ algorithmic filtering of web-scraped videos from
early platforms, they still lack the quality required for fine-tuning advanced
video generation models. We present Tiger200K, a manually curated high visual
quality video dataset sourced from User-Generated Content (UGC) platforms. By
prioritizing visual fidelity and aesthetic quality, Tiger200K underscores the
critical role of human expertise in data curation, and providing high-quality,
temporally consistent video-text pairs for fine-tuning and optimizing video
generation architectures through a simple but effective pipeline including shot
boundary detection, OCR, border detecting, motion filter and fine bilingual
caption. The dataset will undergo ongoing expansion and be released as an
open-source initiative to advance research and applications in video generative
models. Project page: https://tinytigerpan.github.io/tiger200k/

</details>

### [211] [Breast density in MRI: an AI-based quantification and relationship to assessment in mammography](https://arxiv.org/abs/2504.15192)
*Yaqian Chen,Lin Li,Hanxue Gu,Haoyu Dong,Derek L. Nguyen,Allan D. Kirk,Maciej A. Mazurowski,E. Shelley Hwang*

Main category: cs.CV

TLDR: 该论文研究了通过MRI评估乳腺密度的机器学习方法，发现其与乳腺X线摄影密度相关，但MRI能捕捉到某些独特成分，未来可能用于改进乳腺癌风险评估。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度是乳腺癌的重要风险因素，MRI作为一种补充工具，能提供更全面的乳腺组织评估，但其3D特性带来了分析挑战。

Method: 使用内部开发的机器学习算法，在三个MRI数据集中评估正常乳腺的密度。

Result: 乳腺密度在不同数据集中表现一致（0.104 - 0.114），且随年龄下降；MRI密度与乳腺X线摄影密度相关，但存在差异。

Conclusion: MRI乳腺密度可能补充现有工具，未来研究将探索如何整合以改进乳腺癌风险预测。

Abstract: Mammographic breast density is a well-established risk factor for breast
cancer. Recently there has been interest in breast MRI as an adjunct to
mammography, as this modality provides an orthogonal and highly quantitative
assessment of breast tissue. However, its 3D nature poses analytic challenges
related to delineating and aggregating complex structures across slices. Here,
we applied an in-house machine-learning algorithm to assess breast density on
normal breasts in three MRI datasets. Breast density was consistent across
different datasets (0.104 - 0.114). Analysis across different age groups also
demonstrated strong consistency across datasets and confirmed a trend of
decreasing density with age as reported in previous studies. MR breast density
was correlated with mammographic breast density, although some notable
differences suggest that certain breast density components are captured only on
MRI. Future work will determine how to integrate MR breast density with current
tools to improve future breast cancer risk prediction.

</details>

### [212] [Automated Measurement of Eczema Severity with Self-Supervised Learning](https://arxiv.org/abs/2504.15193)
*Neelesh Kumar,Oya Aran*

Main category: cs.CV

TLDR: 提出了一种基于自监督学习的湿疹自动诊断框架，在有限标注数据下表现优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有湿疹自动诊断方法依赖大量标注数据，而标注数据难以获取，因此需要一种在有限数据下仍能高效工作的方法。

Method: 采用两阶段框架：1) 使用SegGPT进行少量样本的湿疹区域分割；2) 提取DINO特征并通过MLP进行湿疹严重程度的4分类。

Result: 在真实湿疹图像数据集上，加权F1得分为0.67±0.01，优于Resnet-18（0.44±0.16）和Vision Transformer（0.40±0.22）。

Conclusion: 自监督学习在标注数据稀缺的皮肤自动诊断中具有潜力。

Abstract: Automated diagnosis of eczema using images acquired from digital camera can
enable individuals to self-monitor their recovery. The process entails first
segmenting out the eczema region from the image and then measuring the severity
of eczema in the segmented region. The state-of-the-art methods for automated
eczema diagnosis rely on deep neural networks such as convolutional neural
network (CNN) and have shown impressive performance in accurately measuring the
severity of eczema. However, these methods require massive volume of annotated
data to train which can be hard to obtain. In this paper, we propose a
self-supervised learning framework for automated eczema diagnosis under limited
training data regime. Our framework consists of two stages: i) Segmentation,
where we use an in-context learning based algorithm called SegGPT for few-shot
segmentation of eczema region from the image; ii) Feature extraction and
classification, where we extract DINO features from the segmented regions and
feed it to a multi-layered perceptron (MLP) for 4-class classification of
eczema severity. When evaluated on a dataset of annotated "in-the-wild" eczema
images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the
state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted
F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our
results show that self-supervised learning can be a viable solution for
automated skin diagnosis where labeled data is scarce.

</details>

### [213] [Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning](https://arxiv.org/abs/2504.15199)
*Yassir Benhammou,Alessandro Tiberio,Gabriel Trautmann,Suman Kalyan*

Main category: cs.CV

TLDR: MILS框架声称无需训练即可实现多模态任务，但其迭代优化过程带来高计算成本，而BLIP-2和GPT-4V等单次处理模型表现更高效。


<details>
  <summary>Details</summary>
Motivation: 揭示MILS框架在零样本任务中的隐藏计算成本，挑战其无需训练即可高效完成的说法。

Method: 通过对比MILS与BLIP-2、GPT-4V的性能和计算成本，量化其迭代优化过程的资源消耗。

Result: MILS的高性能伴随显著计算开销，单次处理模型在效率和性能上更具竞争力。

Conclusion: MILS的迭代设计虽有效但资源密集，未来多模态模型需平衡性能与效率。

Abstract: MILS (Multimodal Iterative LLM Solver) is a recently published framework that
claims "LLMs can see and hear without any training" by leveraging an iterative,
LLM-CLIP based approach for zero-shot image captioning. While this MILS
approach demonstrates good performance, our investigation reveals that this
success comes at a hidden, substantial computational cost due to its expensive
multi-step refinement process. In contrast, alternative models such as BLIP-2
and GPT-4V achieve competitive results through a streamlined, single-pass
approach. We hypothesize that the significant overhead inherent in MILS's
iterative process may undermine its practical benefits, thereby challenging the
narrative that zero-shot performance can be attained without incurring heavy
resource demands. This work is the first to expose and quantify the trade-offs
between output quality and computational cost in MILS, providing critical
insights for the design of more efficient multimodal models.

</details>

### [214] [Shape-Guided Clothing Warping for Virtual Try-On](https://arxiv.org/abs/2504.15232)
*Xiaoyu Han,Shunyuan Zheng,Zonglin Li,Chenyang Wang,Xin Sun,Quanling Meng*

Main category: cs.CV

TLDR: SCW-VTON提出了一种新的形状引导的服装变形方法，通过全局形状约束和肢体纹理增强虚拟试穿的逼真度和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在服装变形时缺乏对细节的精确控制，导致服装与人体形状不一致以及肢体区域变形。

Method: 采用双路径服装变形模块（形状路径和流路径）和肢体重建网络，结合全局形状约束和肢体细节引导。

Result: 实验表明，SCW-VTON在服装形状一致性和细节控制上优于现有方法。

Conclusion: SCW-VTON通过形状约束和肢体细节增强，显著提升了虚拟试穿的逼真度和一致性。

Abstract: Image-based virtual try-on aims to seamlessly fit in-shop clothing to a
person image while maintaining pose consistency. Existing methods commonly
employ the thin plate spline (TPS) transformation or appearance flow to deform
in-shop clothing for aligning with the person's body. Despite their promising
performance, these methods often lack precise control over fine details,
leading to inconsistencies in shape between clothing and the person's body as
well as distortions in exposed limb regions. To tackle these challenges, we
propose a novel shape-guided clothing warping method for virtual try-on, dubbed
SCW-VTON, which incorporates global shape constraints and additional limb
textures to enhance the realism and consistency of the warped clothing and
try-on results. To integrate global shape constraints for clothing warping, we
devise a dual-path clothing warping module comprising a shape path and a flow
path. The former path captures the clothing shape aligned with the person's
body, while the latter path leverages the mapping between the pre- and
post-deformation of the clothing shape to guide the estimation of appearance
flow. Furthermore, to alleviate distortions in limb regions of try-on results,
we integrate detailed limb guidance by developing a limb reconstruction network
based on masked image modeling. Through the utilization of SCW-VTON, we are
able to generate try-on results with enhanced clothing shape consistency and
precise control over details. Extensive experiments demonstrate the superiority
of our approach over state-of-the-art methods both qualitatively and
quantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON.

</details>

### [215] [Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation](https://arxiv.org/abs/2504.15259)
*Yunxuan Cai,Sitao Xiang,Zongjian Li,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TLDR: 论文提出了一种基于生成网络的语义可控数字人脸建模方法，通过扩散模型生成高质量3D人脸数据，并开发了一个高效的GAN生成器，支持语义属性输入和后期编辑。


<details>
  <summary>Details</summary>
Motivation: 当前数字人脸建模受限于数据采集设备、人工成本和合适的演员，导致模型多样性、表现力和可控性不足。

Method: 使用预训练扩散模型生成高质量3D人脸数据，通过归一化模块将其转换为扫描数据；开发GAN生成器，支持语义属性输入和潜在空间编辑；最后通过资产细化组件生成物理基础的面部资产。

Result: 生成了44,000个人脸模型，开发了高效的GAN生成器和交互式工具，实验和评估表明模型性能优越。

Conclusion: 提出的系统显著提升了数字人脸建模的多样性和可控性，未来将公开交互工具。

Abstract: Digital modeling and reconstruction of human faces serve various
applications. However, its availability is often hindered by the requirements
of data capturing devices, manual labor, and suitable actors. This situation
restricts the diversity, expressiveness, and control over the resulting models.
This work aims to demonstrate that a semantically controllable generative
network can provide enhanced control over the digital face modeling process. To
enhance diversity beyond the limited human faces scanned in a controlled
setting, we introduce a novel data generation pipeline that creates a
high-quality 3D face database using a pre-trained diffusion model. Our proposed
normalization module converts synthesized data from the diffusion model into
high-quality scanned data. Using the 44,000 face models we obtained, we further
developed an efficient GAN-based generator. This generator accepts semantic
attributes as input, and generates geometry and albedo. It also allows
continuous post-editing of attributes in the latent space. Our asset refinement
component subsequently creates physically-based facial assets. We introduce a
comprehensive system designed for creating and editing high-quality face
assets. Our proposed model has undergone extensive experiment, comparison and
evaluation. We also integrate everything into a web-based interactive tool. We
aim to make this tool publicly available with the release of the paper.

</details>

### [216] [Diffusion Bridge Models for 3D Medical Image Translation](https://arxiv.org/abs/2504.15267)
*Shaorong Zhang,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Jose-Luis Ambite,Paul M. Thompson,Greg Ver Steeg*

Main category: cs.CV

TLDR: 提出了一种扩散桥模型，用于T1w MRI和DTI模态之间的3D脑图像转换，减少DTI采集需求。


<details>
  <summary>Details</summary>
Motivation: DTI成像耗时，而T1w MRI更易获取，需要一种方法实现跨模态数据增强。

Method: 使用扩散桥模型从T1w图像生成高质量DTI FA图像，反之亦然。

Result: 模型在感知相似性、像素级一致性和分布一致性上表现优异，生成图像在性别和阿尔茨海默病分类任务中与真实数据相当。

Conclusion: 扩散桥模型为神经影像数据集改进和临床决策提供了潜在解决方案。

Abstract: Diffusion tensor imaging (DTI) provides crucial insights into the
microstructure of the human brain, but it can be time-consuming to acquire
compared to more readily available T1-weighted (T1w) magnetic resonance imaging
(MRI). To address this challenge, we propose a diffusion bridge model for 3D
brain image translation between T1w MRI and DTI modalities. Our model learns to
generate high-quality DTI fractional anisotropy (FA) images from T1w images and
vice versa, enabling cross-modality data augmentation and reducing the need for
extensive DTI acquisition. We evaluate our approach using perceptual
similarity, pixel-level agreement, and distributional consistency metrics,
demonstrating strong performance in capturing anatomical structures and
preserving information on white matter integrity. The practical utility of the
synthetic data is validated through sex classification and Alzheimer's disease
classification tasks, where the generated images achieve comparable performance
to real data. Our diffusion bridge model offers a promising solution for
improving neuroimaging datasets and supporting clinical decision-making, with
the potential to significantly impact neuroimaging research and clinical
practice.

</details>

### [217] [Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](https://arxiv.org/abs/2504.15271)
*Guo Chen,Zhiqi Li,Shihao Wang,Jindong Jiang,Yicheng Liu,Lidong Lu,De-An Huang,Wonmin Byeon,Matthieu Le,Tuomas Rintamaki,Tyler Poon,Max Ehrlich,Tuomas Rintamaki,Tyler Poon,Tong Lu,Limin Wang,Bryan Catanzaro,Jan Kautz,Andrew Tao,Zhiding Yu,Guilin Liu*

Main category: cs.CV

TLDR: Eagle 2.5是一个前沿的视觉语言模型家族，专注于长上下文多模态学习，解决了长视频理解和高分辨率图像识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在长上下文多模态任务中存在局限性，如长视频理解和高质量图像处理。

Method: 提出了包含自动降级采样和图像区域保护技术的训练框架，并优化了长上下文数据训练的效率。

Result: Eagle 2.5在长上下文多模态基准测试中表现优异，其最佳模型Eagle 2.5-8B在512输入帧下达到72.4%的Video-MME分数。

Conclusion: Eagle 2.5为长上下文多模态任务提供了高效解决方案，性能媲美顶级商业和开源模型。

Abstract: We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)
for long-context multimodal learning. Our work addresses the challenges in long
video comprehension and high-resolution image understanding, introducing a
generalist framework for both tasks. The proposed training framework
incorporates Automatic Degrade Sampling and Image Area Preservation, two
techniques that preserve contextual integrity and visual details. The framework
also includes numerous efficiency optimizations in the pipeline for
long-context data training. Finally, we propose Eagle-Video-110K, a novel
dataset that integrates both story-level and clip-level annotations,
facilitating long-video understanding. Eagle 2.5 demonstrates substantial
improvements on long-context multimodal benchmarks, providing a robust solution
to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B
achieves 72.4% on Video-MME with 512 input frames, matching the results of
top-tier commercial model such as GPT-4o and large-scale open-source models
like Qwen2.5-VL-72B and InternVL2.5-78B.

</details>

### [218] [DRAWER: Digital Reconstruction and Articulation With Environment Realism](https://arxiv.org/abs/2504.15278)
*Hongchi Xia,Entong Su,Marius Memmel,Arhan Jain,Raymond Yu,Numfor Mbiziwo-Tiapo,Ali Farhadi,Abhishek Gupta,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.CV

TLDR: DRAWER框架将静态室内场景视频转换为逼真、交互式的数字环境，适用于游戏和机器人模拟。


<details>
  <summary>Details</summary>
Motivation: 通过虚拟数字复制品释放真实世界数据在游戏和机器人等领域的潜力。

Method: 基于双重场景表示的重建模块和关节模块，分别处理几何细节重建与交互功能实现。

Result: 生成的虚拟环境逼真、交互性强，实时运行，兼容游戏引擎和机器人模拟平台。

Conclusion: DRAWER展示了在游戏自动生成和机器人仿真中的潜力。

Abstract: Creating virtual digital replicas from real-world data unlocks significant
potential across domains like gaming and robotics. In this paper, we present
DRAWER, a novel framework that converts a video of a static indoor scene into a
photorealistic and interactive digital environment. Our approach centers on two
main contributions: (i) a reconstruction module based on a dual scene
representation that reconstructs the scene with fine-grained geometric details,
and (ii) an articulation module that identifies articulation types and hinge
positions, reconstructs simulatable shapes and appearances and integrates them
into the scene. The resulting virtual environment is photorealistic,
interactive, and runs in real time, with compatibility for game engines and
robotic simulation platforms. We demonstrate the potential of DRAWER by using
it to automatically create an interactive game in Unreal Engine and to enable
real-to-sim-to-real transfer for robotics applications.

</details>

### [219] [VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2504.15279)
*Weiye Xu,Jiahao Wang,Weiyun Wang,Zhe Chen,Wengang Zhou,Aijun Yang,Lewei Lu,Houqiang Li,Xiaohua Wang,Xizhou Zhu,Wenhai Wang,Jifeng Dai,Jinguo Zhu*

Main category: cs.CV

TLDR: VisuLogic是一个包含1000个人工验证问题的视觉推理基准，用于评估多模态大语言模型（MLLMs）的真实视觉推理能力，发现当前模型表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs的推理评估依赖文本描述，存在语言推理捷径，无法真正衡量视觉推理能力。

Method: 提出VisuLogic基准，包含六类问题，评估MLLMs的视觉推理能力，并提供训练数据和强化学习基线。

Result: 主流MLLMs准确率低于30%，远低于人类的51.4%，揭示了视觉推理能力的显著差距。

Conclusion: VisuLogic为视觉推理评估提供了新标准，并支持未来研究改进模型表现。

Abstract: Visual reasoning is a core component of human intelligence and a critical
capability for advanced multimodal models. Yet current reasoning evaluations of
multimodal large language models (MLLMs) often rely on text descriptions and
allow language-based reasoning shortcuts, failing to measure genuine
vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark
of 1,000 human-verified problems across six categories (e.g., quantitative
shifts, spatial relations, attribute comparisons). These various types of
questions can be evaluated to assess the visual reasoning capabilities of MLLMs
from multiple perspectives. We evaluate leading MLLMs on this benchmark and
analyze their results to identify common failure modes. Most models score below
30% accuracy-only slightly above the 25% random baseline and far below the
51.4% achieved by humans-revealing significant gaps in visual reasoning.
Furthermore, we provide a supplementary training dataset and a
reinforcement-learning baseline to support further progress.

</details>

### [220] [StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians](https://arxiv.org/abs/2504.15281)
*Cailin Zhuang,Yaoqi Hu,Xuanyang Zhang,Wei Cheng,Jiacheng Bao,Shengqi Liu,Yiying Yang,Xianfang Zeng,Gang Yu,Ming Li*

Main category: cs.CV

TLDR: StyleMe3D提出了一种用于3D高斯泼溅（3DGS）风格转移的整体框架，解决了现有方法在风格化场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 3DGS在真实场景重建中表现出色，但在风格化场景（如卡通、游戏）中表现不佳，主要问题包括纹理碎片化、语义不对齐和对抽象美学的适应性不足。

Method: StyleMe3D整合了多模态风格条件、多级语义对齐和感知质量增强，包括动态风格分数蒸馏（DSSD）、对比风格描述符（CSD）、同时优化尺度（SOS）和3D高斯质量评估（3DG-QA）。

Result: 在NeRF合成数据集和tandt db数据集上，StyleMe3D在保留几何细节和确保风格一致性方面优于现有方法，同时保持实时渲染。

Conclusion: 该研究填补了真实3DGS与艺术风格化之间的空白，为游戏、虚拟世界和数字艺术提供了新的应用可能。

Abstract: 3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction
but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented
textures, semantic misalignment, and limited adaptability to abstract
aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer
that integrates multi-modal style conditioning, multi-level semantic alignment,
and perceptual quality enhancement. Our key insights include: (1) optimizing
only RGB attributes preserves geometric integrity during stylization; (2)
disentangling low-, medium-, and high-level semantics is critical for coherent
style transfer; (3) scalability across isolated objects and complex scenes is
essential for practical deployment. StyleMe3D introduces four novel components:
Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent
space for semantic alignment; Contrastive Style Descriptor (CSD) for localized,
content-aware texture transfer; Simultaneously Optimized Scale (SOS) to
decouple style details and structural coherence; and 3D Gaussian Quality
Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated
data to suppress artifacts and enhance visual harmony. Evaluated on NeRF
synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D
outperforms state-of-the-art methods in preserving geometric details (e.g.,
carvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,
coherent lighting in landscapes), while maintaining real-time rendering. This
work bridges photorealistic 3D GS and artistic stylization, unlocking
applications in gaming, virtual worlds, and digital art.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [221] [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
*Natalia Tomashenko,Xiaoxiao Miao,Emmanuel Vincent,Junichi Yamagishi*

Main category: eess.AS

TLDR: ICASSP 2025 SP Grand Challenge评估攻击者系统对语音匿名化系统的效果，最佳攻击系统将EER降低了25-44%。


<details>
  <summary>Details</summary>
Motivation: 评估攻击者系统对语音匿名化系统的性能，推动隐私保护技术的发展。

Method: 参与者开发自动说话人验证系统作为攻击者系统，并在开发和评估数据集上测试。

Result: 最佳攻击系统将EER相对基线降低了25-44%。

Conclusion: 挑战赛展示了攻击者系统的有效性，为语音隐私保护提供了重要参考。

Abstract: The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand
Challenge which focuses on evaluating attacker systems against a set of voice
anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,
development, and evaluation datasets were provided along with a baseline
attacker. Participants developed their attacker systems in the form of
automatic speaker verification systems and submitted their scores on the
development and evaluation data. The best attacker systems reduced the equal
error rate (EER) by 25-44% relative w.r.t. the baseline.

</details>

### [222] [Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training](https://arxiv.org/abs/2504.14409)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François G. Germain,Jonathan Le Roux*

Main category: eess.AS

TLDR: MERL提出了一种基于神经声场的RIR估计系统，用于ICASSP 2025的数据增强任务，包括RIR数据增强和扬声器距离估计。


<details>
  <summary>Details</summary>
Motivation: 解决RIR数据增强和扬声器距离估计问题，通过预训练和适应目标房间的方法提高准确性。

Method: 预训练神经声场，适应目标房间数据，预测RIR并用于训练距离估计模型。

Result: 系统能够生成目标房间的RIR数据，并用于改进扬声器距离估计。

Conclusion: 该方法在数据增强和距离估计任务中表现出有效性。

Abstract: This report details MERL's system for room impulse response (RIR) estimation
submitted to the Generative Data Augmentation Workshop at ICASSP 2025 for
Augmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task
2). We first pre-train a neural acoustic field conditioned by room geometry on
an external large-scale dataset in which pairs of RIRs and the geometries are
provided. The neural acoustic field is then adapted to each target room by
using the enrollment data, where we leverage either the provided room
geometries or geometries retrieved from the external dataset, depending on
availability. Lastly, we predict the RIRs for each pair of source and receiver
locations specified by Task 1, and use these RIRs to train the speaker distance
estimation model in Task 2.

</details>

### [223] [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://arxiv.org/abs/2504.14906)
*Huadai Liu,Tianyi Luo,Qikai Jiang,Kaicheng Luo,Peiwen Sun,Jialei Wan,Rongjie Huang,Qian Chen,Wen Wang,Xiangtai Li,Shiliang Zhang,Zhijie Yan,Zhou Zhao,Wei Xue*

Main category: eess.AS

TLDR: 论文提出了一种新任务360V2SA，从360度视频生成空间音频，并开发了OmniAudio框架和Sphere360数据集，实现了先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频到音频生成技术缺乏对3D环境中声音源空间线索的捕捉，限制了音频的真实感。

Method: 提出了OmniAudio框架，利用自监督预训练和双分支结构处理全景和FoV视频输入，生成First-order Ambisonics音频。

Result: OmniAudio在Sphere360数据集上实现了最先进的性能。

Conclusion: 该研究为360度视频生成空间音频提供了有效解决方案，并开源了代码和数据集。

Abstract: Traditional video-to-audio generation techniques primarily focus on
field-of-view (FoV) video and non-spatial audio, often missing the spatial cues
necessary for accurately representing sound sources in 3D environments. To
address this limitation, we introduce a novel task, 360V2SA, to generate
spatial audio from 360-degree videos, specifically producing First-order
Ambisonics (FOA) audio - a standard format for representing 3D spatial audio
that captures sound directionality and enables realistic 3D audio reproduction.
We first create Sphere360, a novel dataset tailored for this task that is
curated from real-world data. We also design an efficient semi-automated
pipeline for collecting and cleaning paired video-audio data. To generate
spatial audio from 360-degree video, we propose a novel framework OmniAudio,
which leverages self-supervised pre-training using both spatial audio data (in
FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a
dual-branch framework that utilizes both panoramic and FoV video inputs to
capture comprehensive local and global information from 360-degree videos.
Experimental results demonstrate that OmniAudio achieves state-of-the-art
performance across both objective and subjective metrics on Sphere360. Code and
datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo
page is available at https://OmniAudio-360V2SA.github.io.

</details>

<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [224] [Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation](https://arxiv.org/abs/2504.14541)
*Yi Yu,Song Xia,Xun Lin,Chenqi Kong,Wenhan Yang,Shijian Lu,Yap-Peng Tan,Alex C. Kot*

Main category: cs.CR

TLDR: 本文提出了一种新的训练范式，通过触发器激活模型来增强对抗可转移攻击的鲁棒性，同时保持对干净数据的随机猜测行为。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的可转移性对深度神经网络构成威胁，现有防御方法效率低且效果有限。

Method: 提出触发器激活模型，通过恒定触发器生成准确预测，并通过联合优化触发器与模型提升鲁棒性。

Result: 实验证明该方法在多种数据集和攻击方法下有效且优于现有方法。

Conclusion: 触发器激活模型为对抗可转移攻击提供了一种高效且有效的防御方案。

Abstract: Adversarial examples, characterized by imperceptible perturbations, pose
significant threats to deep neural networks by misleading their predictions. A
critical aspect of these examples is their transferability, allowing them to
deceive {unseen} models in black-box scenarios. Despite the widespread
exploration of defense methods, including those on transferability, they show
limitations: inefficient deployment, ineffective defense, and degraded
performance on clean images. In this work, we introduce a novel training
paradigm aimed at enhancing robustness against transferable adversarial
examples (TAEs) in a more efficient and effective way. We propose a model that
exhibits random guessing behavior when presented with clean data
$\boldsymbol{x}$ as input, and generates accurate predictions when with
triggered data $\boldsymbol{x}+\boldsymbol{\tau}$. Importantly, the trigger
$\boldsymbol{\tau}$ remains constant for all data instances. We refer to these
models as \textbf{models with trigger activation}. We are surprised to find
that these models exhibit certain robustness against TAEs. Through the
consideration of first-order gradients, we provide a theoretical analysis of
this robustness. Moreover, through the joint optimization of the learnable
trigger and the model, we achieve improved robustness to transferable attacks.
Extensive experiments conducted across diverse datasets, evaluating a variety
of attacking methods, underscore the effectiveness and superiority of our
approach.

</details>

### [225] [REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.14554)
*Chongye Guo,Jinhu Fu,Junfeng Fang,Kun Wang,Guorui Feng*

Main category: cs.CR

TLDR: 论文提出了一种基于模型编辑的无训练后门投毒方法REDEditing，揭示了模型编辑技术对图像生成模型的安全风险，并提出了等效属性对齐和隐蔽投毒原则，攻击成功率提升11%，隐蔽性提高24%。


<details>
  <summary>Details</summary>
Motivation: 生成AI快速发展，文本到图像（T2I）模型的安全问题日益突出，尤其是后门投毒威胁。及时披露和缓解T2I模型的安全漏洞对安全部署至关重要。

Method: 通过模型编辑实现无训练后门投毒，提出等效属性对齐和隐蔽投毒原则，开发等效关系检索和联合属性转移方法，确保通过概念重绑定生成一致的后门图像，并提出知识隔离约束保护良性生成完整性。

Result: REDEditing方法攻击成功率比现有方法高11%，仅添加一行代码即可提高输出自然度和后门隐蔽性24%。

Conclusion: 该研究旨在提高对可编辑图像生成模型中安全漏洞的认识，揭示了模型编辑技术的潜在风险。

Abstract: The rapid advancement of generative AI highlights the importance of
text-to-image (T2I) security, particularly with the threat of backdoor
poisoning. Timely disclosure and mitigation of security vulnerabilities in T2I
models are crucial for ensuring the safe deployment of generative models. We
explore a novel training-free backdoor poisoning paradigm through model
editing, which is recently employed for knowledge updating in large language
models. Nevertheless, we reveal the potential security risks posed by model
editing techniques to image generation models. In this work, we establish the
principles for backdoor attacks based on model editing, and propose a
relationship-driven precise backdoor poisoning method, REDEditing. Drawing on
the principles of equivalent-attribute alignment and stealthy poisoning, we
develop an equivalent relationship retrieval and joint-attribute transfer
approach that ensures consistent backdoor image generation through concept
rebinding. A knowledge isolation constraint is proposed to preserve benign
generation integrity. Our method achieves an 11\% higher attack success rate
compared to state-of-the-art approaches. Remarkably, adding just one line of
code enhances output naturalness while improving backdoor stealthiness by 24\%.
This work aims to heighten awareness regarding this security vulnerability in
editable image generation models.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [226] [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
*Theo Jaffrelot Inizan,Sherry Yang,Aaron Kaplan,Yen-hsu Lin,Jian Yin,Saber Mirzaei,Mona Abdelgaid,Ali H. Alawadhi,KwangHwan Cho,Zhiling Zheng,Ekin Dogus Cubuk,Christian Borgs,Jennifer T. Chayes,Kristin A. Persson,Omar M. Yaghi*

Main category: cond-mat.mtrl-sci

TLDR: MOFGen是一个由多个AI代理组成的系统，用于加速MOF材料的发现，成功生成了大量新MOF结构并验证了其可合成性。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型和机器学习在MOF材料发现中面临的化学空间广阔和可合成性挑战。

Method: MOFGen系统包括语言模型、扩散模型、量子力学代理和合成可行性代理，结合实验数据和计算数据库。

Result: 生成了数十万种新MOF结构和可合成有机连接体，并通过实验验证了五种AI设计的MOF。

Conclusion: MOFGen为自动化可合成材料发现迈出了重要一步。

Abstract: Generative models and machine learning promise accelerated material discovery
in MOFs for CO2 capture and water harvesting but face significant challenges
navigating vast chemical spaces while ensuring synthetizability. Here, we
present MOFGen, a system of Agentic AI comprising interconnected agents: a
large language model that proposes novel MOF compositions, a diffusion model
that generates crystal structures, quantum mechanical agents that optimize and
filter candidates, and synthetic-feasibility agents guided by expert rules and
machine learning. Trained on all experimentally reported MOFs and computational
databases, MOFGen generated hundreds of thousands of novel MOF structures and
synthesizable organic linkers. Our methodology was validated through
high-throughput experiments and the successful synthesis of five "AI-dreamt"
MOFs, representing a major step toward automated synthesizable material
discovery.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [227] [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
*Zhe Liu*

Main category: cs.HC

TLDR: 该研究提出了一种名为Interview AI-ssistant的系统，旨在通过AI辅助提升访谈中的实时协作，并通过四项研究验证其设计和效果。


<details>
  <summary>Details</summary>
Motivation: 访谈在获取深度见解时面临认知挑战，如实时信息处理和问题调整，LLM的发展为AI辅助提供了新机会。

Method: 通过四项研究：需求调研、AI辅助访谈准备原型开发、实时AI辅助实验评估及实地部署。

Result: 研究为智能访谈支持系统提供了实践指导，并推动了人机协作界面的理解。

Conclusion: 该工作不仅为智能访谈工具提供了设计指南，还为复杂社交任务中的人机协作界面研究做出了贡献。

Abstract: Recent advances in large language models (LLMs) offer unprecedented
opportunities to enhance human-AI collaboration in qualitative research
methods, including interviews. While interviews are highly valued for gathering
deep, contextualized insights, interviewers often face significant cognitive
challenges, such as real-time information processing, question adaptation, and
rapport maintenance. My doctoral research introduces Interview AI-ssistant, a
system designed for real-time interviewer-AI collaboration during both the
preparation and execution phases. Through four interconnected studies, this
research investigates the design of effective human-AI collaboration in
interviewing contexts, beginning with a formative study of interviewers' needs,
followed by a prototype development study focused on AI-assisted interview
preparation, an experimental evaluation of real-time AI assistance during
interviews, and a field study deploying the system in a real-world research
setting. Beyond informing practical implementations of intelligent interview
support systems, this work contributes to the Intelligent User Interfaces (IUI)
community by advancing the understanding of human-AI collaborative interfaces
in complex social tasks and establishing design guidelines for AI-enhanced
qualitative research tools.

</details>

### [228] [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
*Ivan Sviridov,Amina Miftakhova,Artemiy Tereshchenko,Galina Zubkova,Pavel Blinov,Andrey Savchenko*

Main category: cs.HC

TLDR: 3MDBench是一个开源评估框架，用于评估大型视觉语言模型在医疗咨询中的表现，通过模拟多样化患者行为和整合多模态数据提升诊断效果。


<details>
  <summary>Details</summary>
Motivation: 探索大型视觉语言模型在远程医疗中与多样化患者互动的能力，并提供一个可复现的评估框架。

Method: 开发3MDBench框架，包含四种性格驱动的患者代理和评估代理，整合文本和图像数据，评估不同诊断策略下的模型表现。

Result: 对话和多模态输入显著提升诊断效果，结合CNN模型的训练进一步将F1分数提升至70.3。

Conclusion: 3MDBench为AI驱动的医疗助手提供了可扩展的评估工具，强调了患者性格、对话策略和多模态推理对诊断质量的影响。

Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for
applications in telemedicine, yet their ability to engage with diverse patient
behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal
Multi-agent Dialogue Benchmark), an open-source evaluation framework designed
to assess LLM-driven medical consultations. Unlike existing benchmarks,
3MDBench simulates real-world patient variability by incorporating four
temperament-driven Patient Agents and an Assessor Agent that evaluates
diagnostic accuracy and dialogue quality. The benchmark integrates textual and
image-based patient data across 34 common diagnoses, mirroring real-world
telemedicine interactions. Under different diagnostic strategies, we evaluate
state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue
improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,
underscoring the value of context-driven, information-seeking questioning.
Additionally, we demonstrate that multimodal inputs enhance diagnostic
efficiency. Image-supported models outperform text-only counterparts by raising
the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.
Finally, we suggest an approach that improves the diagnostic F1-score to 70.3
by training the CNN model on the diagnosis prediction task and incorporating
its top-3 predictions into the LVLM context. 3MDBench provides a reproducible
and extendable evaluation framework for AI-driven medical assistants. It offers
insights into how patient temperament, dialogue strategies, and multimodal
reasoning influence diagnosis quality. By addressing real-world complexities in
telemedicine, our benchmark paves the way for more empathetic, reliable, and
context-aware AI-driven healthcare solutions. The source code of our benchmark
is publicly available: https://github.com/univanxx/3mdbench

</details>

### [229] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
*Fei Tang,Haolei Xu,Hang Zhang,Siqi Chen,Xingyu Wu,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Zeqi Tan,Yuchen Yan,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.HC

TLDR: 本文综述了基于LLM的GUI代理的发展，分析了其架构、技术组件和评估方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索GUI代理从规则脚本到AI驱动系统的演变，以及其在人机交互中的潜力。

Method: 系统分析了GUI代理的四大核心组件：感知系统、探索机制、规划框架和交互系统。

Result: 揭示了LLM和多模态学习对GUI自动化的革命性影响，并指出了当前评估框架的局限性。

Conclusion: 为研究人员提供了GUI代理领域的全面概述，并提出了未来增强其能力的研究方向。

Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative
paradigm in human-computer interaction, evolving from rule-based automation
scripts to sophisticated AI-driven systems capable of understanding and
executing complex interface operations. This survey provides a comprehensive
examination of the rapidly advancing field of LLM-based GUI Agents,
systematically analyzing their architectural foundations, technical components,
and evaluation methodologies. We identify and analyze four fundamental
components that constitute modern GUI Agents: (1) perception systems that
integrate text-based parsing with multimodal understanding for comprehensive
interface comprehension; (2) exploration mechanisms that construct and maintain
knowledge bases through internal modeling, historical experience, and external
information retrieval; (3) planning frameworks that leverage advanced reasoning
methodologies for task decomposition and execution; and (4) interaction systems
that manage action generation with robust safety controls. Through rigorous
analysis of these components, we reveal how recent advances in large language
models and multimodal learning have revolutionized GUI automation across
desktop, mobile, and web platforms. We critically examine current evaluation
frameworks, highlighting methodological limitations in existing benchmarks
while proposing directions for standardization. This survey also identifies key
technical challenges, including accurate element localization, effective
knowledge retrieval, long-horizon planning, and safety-aware execution control,
while outlining promising research directions for enhancing GUI Agents'
capabilities. Our systematic review provides researchers and practitioners with
a thorough understanding of the field's current state and offers insights into
future developments in intelligent interface automation.

</details>

### [230] [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
*Megan Gu,Chloe Qianhui Zhao,Claire Liu,Nikhil Patel,Jahnvi Shah,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.HC

TLDR: 研究利用大型语言模型（LLM）自动评估五种关键辅导策略的有效性，结果显示模型在排除错误分类方面表现良好，但在准确识别策略上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在辅导策略分析中的应用潜力，提升辅导对话的自动化评估能力。

Method: 使用GPT-3.5和少量示例提示，对公开数据集中的辅导对话进行分类，评估五种策略的使用情况。

Result: 模型在排除错误分类（TNR 0.655-0.738）方面表现良好，但准确识别策略（Recall 0.327-0.432）仍有挑战。其中“帮助学生管理不平等”策略表现最佳。

Conclusion: LLM在辅导策略分析中具有潜力，未来可通过更先进的模型提升反馈的细致度。

Abstract: Our study introduces an automated system leveraging large language models
(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving
effective praise, 2. reacting to errors, 3. determining what students know, 4.
helping students manage inequity, and 5. responding to negative self-talk.
Using a public dataset from the Teacher-Student Chatroom Corpus, our system
classifies each tutoring strategy as either being employed as desired or
undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use
of these strategies and analyze tutoring dialogues. The results show that for
the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to
0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is
effective at excluding incorrect classifications but struggles to consistently
identify the correct strategy. The strategy \textit{helping students manage
inequity} showed the highest performance with a TNR of 0.738 and Recall of
0.432. The study highlights the potential of LLMs in tutoring strategy analysis
and outlines directions for future improvements, including incorporating more
advanced models for more nuanced feedback.

</details>

### [231] [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
*Isabel Villanueva,Tara Bobinac,Binwei Yao,Junjie Hu,Kaiping Chen*

Main category: cs.HC

TLDR: 研究探讨了AI聊天机器人在跨文化对话中培养共情能力的效果，发现不同文化群体对AI互动的反应存在差异，凸显了AI在文化代表性上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在公共对话中的普及，其是否能促进跨文化共情尚缺乏实证证据。

Method: 通过随机对话实验，比较了不同类型（审议式与非审议式、文化对齐与非对齐）的AI互动对跨文化共情的影响。

Result: 审议式对话提升了美国参与者的跨文化共情，但对拉丁美洲参与者无效，因AI未能准确反映其文化背景。实时分析显示，差异源于大语言模型的文化知识缺陷。

Conclusion: 研究强调了设计具备文化真实性的AI系统的重要性，并为审议理论和AI对齐研究提供了新视角。

Abstract: Despite the growing integration of AI chatbots as conversational agents in
public discourse, empirical evidence regarding their capacity to foster
intercultural empathy remains limited. Using a randomized dialogue experiment,
we examined how different types of AI chatbot interaction, i.e., deliberative
versus non-deliberative and culturally aligned versus non-aligned, affect
intercultural empathy across cultural groups. Results show that deliberative
conversations increased intercultural empathy among American participants but
not Latin American participants, who perceived AI responses as culturally
inaccurate and failing to represent their cultural contexts and perspectives
authentically. Real-time interaction analyses reveal that these differences
stem from cultural knowledge gaps inherent in Large Language Models. Despite
explicit prompting and instruction to represent cultural perspectives in
participants' native languages, AI systems still exhibit significant
disparities in cultural representation. This highlights the importance of
designing AI systems capable of culturally authentic engagement in deliberative
conversations. Our study contributes to deliberation theory and AI alignment
research by underscoring AI's role in intercultural dialogue and the persistent
challenge of representational asymmetry in democratic discourse.

</details>

### [232] [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
*Paul Taele,Jung In Koh,Tracy Hammond*

Main category: cs.HC

TLDR: Kanji Workbook是一个智能辅导系统，旨在通过模拟教师反馈帮助学生学习日语汉字书写，提高课程成绩。


<details>
  <summary>Details</summary>
Motivation: 英语流利的学生在学习日语汉字书写时面临困难，现有教育应用缺乏教师模拟反馈。

Method: 开发了一个基于书写的智能辅导系统，提供多样化的书写评估指标和视觉动画反馈。

Result: 在为期一学年的大学课程中，使用该系统的学生平均成绩更高，并对系统功能反应积极。

Conclusion: Kanji Workbook通过智能评估和反馈有效提升了学生的汉字书写能力和课程表现。

Abstract: Kanji script writing is a skill that is often introduced to novice Japanese
foreign language students for achieving Japanese writing mastery, but often
poses difficulties to students with primarily English fluency due to their its
vast differences with written English. Instructors often introduce various
pedagogical methods -- such as visual structure and written techniques -- to
assist students in kanji study, but may lack availability providing direct
feedback on students' writing outside of class. Current educational
applications are also limited due to lacking richer instructor-emulated
feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring
system for students to receive intelligent assessment that emulates human
instructor feedback. Our interface not only leverages students' computing
devices for allowing them to learn, practice, and review the writing of
prompted characters from their course's kanji script lessons, but also provides
a diverse set of writing assessment metrics -- derived from instructor
interviews and classroom observation insights -- through intelligent scoring
and visual animations. We deployed our interface onto novice- and
intermediate-level university courses over an entire academic year, and
observed that interface users on average achieved higher course grades than
their peers and also reacted positively to our interface's various features.

</details>

### [233] [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
*Chen Shani,Elizabeth C. Stade*

Main category: cs.HC

TLDR: 论文指出当前计算心理健康研究中使用的心理病理学测量方法存在三个主要问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 解决计算心理健康研究中因使用不合适的心理病理学测量方法而导致的有效性问题。

Method: 识别并分析了三个关键问题：依赖未验证的测量方法、将心理健康构念视为分类而非维度、关注特定障碍而非跨诊断构念。

Result: 提出了使用已验证、维度和跨诊断测量方法的优势，并为实践者提供了实用建议。

Conclusion: 使用反映心理病理学本质和结构的有效测量方法对计算心理健康研究至关重要。

Abstract: Computational mental health research develops models to predict and
understand psychological phenomena, but often relies on inappropriate measures
of psychopathology constructs, undermining validity. We identify three key
issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)
over validated ones (e.g., diagnosis by clinician); (2) treating mental health
constructs as categorical rather than dimensional; and (3) focusing on
disorder-specific constructs instead of transdiagnostic ones. We outline the
benefits of using validated, dimensional, and transdiagnostic measures and
offer practical recommendations for practitioners. Using valid measures that
reflect the nature and structure of psychopathology is essential for
computational mental health research.

</details>

### [234] [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
*Stefano De Paoli,Alex Fawzi*

Main category: cs.HC

TLDR: 本文介绍了一种基于大型语言模型（LLMs）的图形用户界面（GUI）应用，用于辅助研究者进行主题分析（TA），无需编程技能即可操作。


<details>
  <summary>Details</summary>
Motivation: 为社会科学和人文学科等缺乏编程技能的研究者提供一种简便的主题分析方法。

Method: 开发了一个基于streamlit框架的GUI应用，结合LLMs的API，支持用户上传文本数据并生成初始代码和主题。

Result: 应用支持用户通过迭代优化代码和主题，同时保持方法学严谨性。

Conclusion: 该应用为定性研究提供了便捷工具，未来可进一步扩展功能。

Abstract: Thematic analysis (TA) is a widely used qualitative research method for
identifying and interpreting patterns within textual data, such as qualitative
interviews. Recent research has shown that it is possible to satisfactorily
perform TA using Large Language Models (LLMs). This paper presents a novel
application using LLMs to assist researchers in conducting TA. The application
enables users to upload textual data, generate initial codes and themes. All of
this is possible through a simple Graphical User Interface, (GUI) based on the
streamlit framework, working with python scripts for the analysis, and using
Application Program Interfaces of LLMs. Having a GUI is particularly important
for researchers in fields where coding skills may not be prevalent, such as
social sciences or humanities. With the app, users can iteratively refine codes
and themes adopting a human-in-the-loop process, without the need to work with
programming and scripting. The paper describes the application key features,
highlighting its potential for qualitative research while preserving
methodological rigor. The paper discusses the design and interface of the app
and outlines future directions for this work.

</details>

### [235] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
*Donghuo Zeng,Roberto Legaspi,Yuewen Sun,Xinshuai Dong,Kazushi Ikeda,Peter Spirtes,Kun Zhang*

Main category: cs.HC

TLDR: 论文提出了一种基于因果和反事实知识的自适应策略，通过因果发现和反事实推理优化系统响应，显著提升了说服性对话系统的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过因果和反事实知识优化系统响应，尤其是在说服性对话系统中，以提升交互效果。

Method: 方法包括因果发现以识别策略级因果关系，反事实推理生成个性化对话，并将用户策略建模为因果因素，优化系统响应策略。

Result: 实验结果表明，该方法在真实数据集上显著提升了说服性对话系统的累积奖励，验证了其有效性。

Conclusion: 结论指出，因果发现和反事实推理的结合能够有效指导个性化对话生成和策略优化，提升系统性能。

Abstract: We hypothesize that optimal system responses emerge from adaptive strategies
grounded in causal and counterfactual knowledge. Counterfactual inference
allows us to create hypothetical scenarios to examine the effects of
alternative system responses. We enhance this process through causal discovery,
which identifies the strategies informed by the underlying causal structure
that govern system behaviors. Moreover, we consider the psychological
constructs and unobservable noises that might be influencing user-system
interactions as latent factors. We show that these factors can be effectively
estimated. We employ causal discovery to identify strategy-level causal
relationships among user and system utterances, guiding the generation of
personalized counterfactual dialogues. We model the user utterance strategies
as causal factors, enabling system strategies to be treated as counterfactual
actions. Furthermore, we optimize policies for selecting system responses based
on counterfactual data. Our results using a real-world dataset on social good
demonstrate significant improvements in persuasive system outcomes, with
increased cumulative rewards validating the efficacy of causal discovery in
guiding personalized counterfactual inference and optimizing dialogue policies
for a persuasive dialogue system.

</details>

### [236] [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
*Fan Gao,Xinjie Zhao,Ding Xia,Zhongyi Zhou,Rui Yang,Jinghui Lu,Hang Jiang,Chanjun Park,Irene Li*

Main category: cs.HC

TLDR: HealthGenie结合知识图谱（KG）和大语言模型（LLM），提供个性化饮食建议和可视化信息，减少用户交互负担和认知负荷。


<details>
  <summary>Details</summary>
Motivation: 解决用户在获取饮食建议时需处理复杂专业知识和个体健康条件的挑战。

Method: 通过查询优化从预构建的KG中检索信息，结合LLM生成解释性建议，并提供交互式调整功能。

Result: 实验表明HealthGenie能有效支持个性化饮食建议，降低用户认知负担。

Conclusion: LLM与KG结合在可解释和可视化信息支持决策方面具有潜力。

Abstract: Seeking dietary guidance often requires navigating complex professional
knowledge while accommodating individual health conditions. Knowledge Graphs
(KGs) offer structured and interpretable nutritional information, whereas Large
Language Models (LLMs) naturally facilitate conversational recommendation
delivery. In this paper, we present HealthGenie, an interactive system that
combines the strengths of LLMs and KGs to provide personalized dietary
recommendations along with hierarchical information visualization for a quick
and intuitive overview. Upon receiving a user query, HealthGenie performs query
refinement and retrieves relevant information from a pre-built KG. The system
then visualizes and highlights pertinent information, organized by defined
categories, while offering detailed, explainable recommendation rationales.
Users can further tailor these recommendations by adjusting preferences
interactively. Our evaluation, comprising a within-subject comparative
experiment and an open-ended discussion, demonstrates that HealthGenie
effectively supports users in obtaining personalized dietary guidance based on
their health conditions while reducing interaction effort and cognitive load.
These findings highlight the potential of LLM-KG integration in supporting
decision-making through explainable and visualized information. We examine the
system's usefulness and effectiveness with an N=12 within-subject study and
provide design considerations for future systems that integrate conversational
LLM and KG.

</details>

### [237] [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
*Rui Qiu,Shijie Chen,Yu Su,Po-Yin Yen,Han-Wei Shen*

Main category: cs.HC

TLDR: InsightAgent是一种基于大型语言模型的人机交互AI代理，通过语义分割和多代理设计显著提升系统综述的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 系统综述在医疗等高要求领域至关重要，但传统方法耗时且依赖专家知识，自动摘要方法效果不佳。

Method: InsightAgent采用语义分割和多代理设计处理文献，并提供可视化界面供用户实时监控和反馈。

Result: 用户研究表明，InsightAgent将系统综述质量提升27.2%，接近人工水平，用户满意度提高34.4%，完成时间从数月缩短至1.5小时。

Conclusion: InsightAgent通过人机交互和高效处理，显著提升了系统综述的质量和效率，具有实际应用价值。

Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes
disciplines, such as healthcare, but are often impeded by intensive labors and
lengthy processes that can take months to complete. Due to the high demand for
domain expertise, existing automatic summarization methods fail to accurately
identify relevant studies and generate high-quality summaries. To that end, we
introduce InsightAgent, a human-centered interactive AI agent powered by large
language models that revolutionize this workflow. InsightAgent partitions a
large literature corpus based on semantics and employs a multi-agent design for
more focused processing of literature, leading to significant improvement in
the quality of generated SRs. InsightAgent also provides intuitive
visualizations of the corpus and agent trajectories, allowing users to
effortlessly monitor the actions of the agent and provide real-time feedback
based on their expertise. Our user studies with 9 medical professionals
demonstrate that the visualization and interaction mechanisms can effectively
improve the quality of synthesized SRs by 27.2%, reaching 79.7% of
human-written quality. At the same time, user satisfaction is improved by
34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather
than months, to complete a high-quality systematic review.

</details>

### [238] [Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises](https://arxiv.org/abs/2504.13866)
*Aleksa Marusic,Sao Mai Nguyen,Adriana Tapus*

Main category: cs.HC

TLDR: 提出了一种基于Transformer的算法，用于康复训练中的错误分类，并提供关节重要性反馈，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 患者在没有直接监督的情况下参与度下降，现有评估系统仅提供二元或连续评分，无法提供足够反馈。

Method: 采用基于骨架的运动评估，结合Transformer模型（受HyperFormer启发），并在KERAAL数据集上评估。

Result: 模型在KERAAL数据集上显著优于现有方法，并提供关节重要性分析。

Conclusion: 该算法为患者提供更详细的反馈，是康复训练评估的重要进步。

Abstract: Physical rehabilitation exercises suggested by healthcare professionals can
help recovery from various musculoskeletal disorders and prevent re-injury.
However, patients' engagement tends to decrease over time without direct
supervision, which is why there is a need for an automated monitoring system.
In recent years, there has been great progress in quality assessment of
physical rehabilitation exercises. Most of them only provide a binary
classification if the performance is correct or incorrect, and a few provide a
continuous score. This information is not sufficient for patients to improve
their performance. In this work, we propose an algorithm for error
classification of rehabilitation exercises, thus making the first step toward
more detailed feedback to patients. We focus on skeleton-based exercise
assessment, which utilizes human pose estimation to evaluate motion. Inspired
by recent algorithms for quality assessment during rehabilitation exercises, we
propose a Transformer-based model for the described classification. Our model
is inspired by the HyperFormer method for human action recognition, and adapted
to our problem and dataset. The evaluation is done on the KERAAL dataset, as it
is the only medical dataset with clear error labels for the exercises, and our
model significantly surpasses state-of-the-art methods. Furthermore, we bridge
the gap towards better feedback to the patients by presenting a way to
calculate the importance of joints for each exercise.

</details>

### [239] [Towards a Multimodal Document-grounded Conversational AI System for Education](https://arxiv.org/abs/2504.13884)
*Karan Taneja,Anjali Singh,Ashok K. Goel*

Main category: cs.HC

TLDR: MuDoC是一个基于GPT-4o的多模态对话AI系统，结合文本和图像提升学习效果，验证功能增强信任，但未显著提高学习成绩。


<details>
  <summary>Details</summary>
Motivation: 探索多模态对话在教育中的应用，解决现有文本AI系统缺乏视觉支持和内容验证的问题。

Method: 基于GPT-4o开发MuDoC系统，结合文档中的文本和图像生成多模态响应，并支持内容验证。与纯文本系统对比实验。

Result: 多模态和验证功能提高了学习者的参与度和信任感，但对学习成绩无显著影响。

Conclusion: 多模态对话AI在教育中有潜力，需进一步研究以优化学习效果。

Abstract: Multimedia learning using text and images has been shown to improve learning
outcomes compared to text-only instruction. But conversational AI systems in
education predominantly rely on text-based interactions while multimodal
conversations for multimedia learning remain unexplored. Moreover, deploying
conversational AI in learning contexts requires grounding in reliable sources
and verifiability to create trust. We present MuDoC, a Multimodal
Document-grounded Conversational AI system based on GPT-4o, that leverages both
text and visuals from documents to generate responses interleaved with text and
images. Its interface allows verification of AI generated content through
seamless navigation to the source. We compare MuDoC to a text-only system to
explore differences in learner engagement, trust in AI system, and their
performance on problem-solving tasks. Our findings indicate that both visuals
and verifiability of content enhance learner engagement and foster trust;
however, no significant impact in performance was observed. We draw upon
theories from cognitive and learning sciences to interpret the findings and
derive implications, and outline future directions for the development of
multimodal conversational AI systems in education.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [240] [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
*Jiakai Tang,Jingsen Zhang,Zihang Tian,Xueyang Feng,Lei Wang,Xu Chen*

Main category: cs.IR

TLDR: 提出了一种基于人类反馈优化的可解释推荐框架，利用大语言模型模拟人类反馈，结合多目标优化提升解释性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因依赖稀疏交互数据和传统监督学习，无法提供有效的反馈信号以优化解释生成。

Method: 采用动态交互优化机制，利用大语言模型模拟人类反馈，结合定制化奖励评分和多目标优化。

Result: 在四个数据集上的实验证明了该方法的优越性。

Conclusion: 该框架通过高效的数据利用和模型泛化能力，显著提升了可解释推荐的性能。

Abstract: Recent advancements in explainable recommendation have greatly bolstered user
experience by elucidating the decision-making rationale. However, the existing
methods actually fail to provide effective feedback signals for potentially
better or worse generated explanations due to their reliance on traditional
supervised learning paradigms in sparse interaction data. To address these
issues, we propose a novel human-like feedback-driven optimization framework.
This framework employs a dynamic interactive optimization mechanism for
achieving human-centered explainable requirements without incurring high labor
costs. Specifically, we propose to utilize large language models (LLMs) as
human simulators to predict human-like feedback for guiding the learning
process. To enable the LLMs to deeply understand the task essence and meet
user's diverse personalized requirements, we introduce a human-induced
customized reward scoring method, which helps stimulate the language
understanding and logical reasoning capabilities of LLMs. Furthermore,
considering the potential conflicts between different perspectives of
explanation quality, we introduce a principled Pareto optimization that
transforms the multi-perspective quality enhancement task into a
multi-objective optimization problem for improving explanation performance. At
last, to achieve efficient model training, we design an off-policy optimization
pipeline. By incorporating a replay buffer and addressing the data distribution
biases, we can effectively improve data utilization and enhance model
generality. Extensive experiments on four datasets demonstrate the superiority
of our approach.

</details>

### [241] [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
*Ronak Pradeep,Nandan Thakur,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.IR

TLDR: 提出了一种基于LLMs的自动评估框架AutoNuggetizer，用于评估RAG系统，验证了其与人工标注的一致性。


<details>
  <summary>Details</summary>
Motivation: RAG系统的评估是当前研究的瓶颈，需要一种自动化的方法来提升效率和准确性。

Method: 采用TREC QA Track的nugget评估方法，通过LLMs自动生成和分配nuggets，并与人工方法对比。

Result: 全自动评估与人工评估在运行级别上表现出一致性，尤其在独立自动化组件时效果更佳。

Conclusion: 该框架为RAG系统开发提供了效率与质量的权衡，但需进一步研究以优化每主题一致性。

Abstract: Large Language Models (LLMs) have significantly enhanced the capabilities of
information access systems, especially with retrieval-augmented generation
(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to
continued progress, a challenge we tackle in this work by proposing an
automatic evaluation framework that is validated against human annotations. We
believe that the nugget evaluation methodology provides a solid foundation for
evaluating RAG systems. This approach, originally developed for the TREC
Question Answering (QA) Track in 2003, evaluates systems based on atomic facts
that should be present in good answers. Our efforts focus on "refactoring" this
methodology, where we describe the AutoNuggetizer framework that specifically
applies LLMs to both automatically create nuggets and automatically assign
nuggets to system answers. In the context of the TREC 2024 RAG Track, we
calibrate a fully automatic approach against strategies where nuggets are
created manually or semi-manually by human assessors and then assigned manually
to system answers. Based on results from a community-wide evaluation, we
observe strong agreement at the run level between scores derived from fully
automatic nugget evaluation and human-based variants. The agreement is stronger
when individual framework components such as nugget assignment are automated
independently. This suggests that our evaluation framework provides tradeoffs
between effort and quality that can be used to guide the development of future
RAG systems. However, further research is necessary to refine our approach,
particularly in establishing robust per-topic agreement to diagnose system
failures effectively.

</details>

### [242] [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
*Juyeon Kim,Geon Lee,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TLDR: KGMEL是一个利用知识图谱三元组增强多模态实体链接的新框架，通过生成、检索和重排名三个阶段显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法忽略了知识图谱的结构信息，KGMEL旨在利用这些信息减少歧义并提高对齐准确性。

Method: KGMEL分三阶段：生成高质量三元组、通过对比学习整合多模态信息检索候选实体、利用大语言模型重排名。

Result: 在基准数据集上，KGMEL表现优于现有方法。

Conclusion: KGMEL通过结合知识图谱三元组和多模态信息，显著提升了实体链接的准确性。

Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities
in a knowledge base, facilitating various applications such as semantic search
and question answering. Recent advances in multimodal entity linking (MEL) have
shown that combining text and images can reduce ambiguity and improve alignment
accuracy. However, most existing MEL methods overlook the rich structural
information available in the form of knowledge-graph (KG) triples. In this
paper, we propose KGMEL, a novel framework that leverages KG triples to enhance
MEL. Specifically, it operates in three stages: (1) Generation: Produces
high-quality triples for each mention by employing vision-language models based
on its text and images. (2) Retrieval: Learns joint mention-entity
representations, via contrastive learning, that integrate text, images, and
(generated or KG) triples to retrieve candidate entities for each mention. (3)
Reranking: Refines the KG triples of the candidate entities and employs large
language models to identify the best-matching entity for the mention. Extensive
experiments on benchmark datasets demonstrate that KGMEL outperforms existing
methods. Our code and datasets are available at:
https://github.com/juyeonnn/KGMEL.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [243] [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
*Xingyu Lu,Tianke Zhang,Chang Meng,Xiaobei Wang,Jinpeng Wang,YiFan Zhang,Shisong Tang,Changyi Liu,Haojie Ding,Kaiyu Jiang,Kaiyu Tang,Bin Wen,Hai-Tao Zheng,Fan Yang,Tingting Gao,Di Zhang,Kun Gai*

Main category: cs.SI

TLDR: 论文提出了KuaiMod框架，用于解决短视频平台内容审核的挑战，结合视觉语言模型和链式推理，实现了高效动态的内容审核。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容审核存在人工偏见、自动化方法准确性不足及法规更新滞后等问题，亟需改进。

Method: 提出KuaiMod框架，包含训练数据构建、离线适应和在线部署与优化三部分，利用视觉语言模型和链式推理。

Result: KuaiMod在基准测试中表现最佳，用户举报率降低20%，并提升了平台活跃度和使用时长。

Conclusion: KuaiMod通过动态审核策略和高效更新，显著提升了内容审核的准确性和适应性。

Abstract: Exponentially growing short video platforms (SVPs) face significant
challenges in moderating content detrimental to users' mental health,
particularly for minors. The dissemination of such content on SVPs can lead to
catastrophic societal consequences. Although substantial efforts have been
dedicated to moderating such content, existing methods suffer from critical
limitations: (1) Manual review is prone to human bias and incurs high
operational costs. (2) Automated methods, though efficient, lack nuanced
content understanding, resulting in lower accuracy. (3) Industrial moderation
regulations struggle to adapt to rapidly evolving trends due to long update
cycles. In this paper, we annotate the first SVP content moderation benchmark
with authentic user/reviewer feedback to fill the absence of benchmark in this
field. Then we evaluate various methods on the benchmark to verify the
existence of the aforementioned limitations. We further propose our common-law
content moderation framework named KuaiMod to address these challenges. KuaiMod
consists of three components: training data construction, offline adaptation,
and online deployment & refinement. Leveraging large vision language model
(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video
toxicity based on sparse user feedback and fosters dynamic moderation policy
with rapid update speed and high accuracy. Offline experiments and large-scale
online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the
best moderation performance on our benchmark. The deployment of KuaiMod reduces
the user reporting rate by 20% and its application in video recommendation
increases both Daily Active User (DAU) and APP Usage Time (AUT) on several
Kuaishou scenarios. We have open-sourced our benchmark at
https://kuaimod.github.io.

</details>

### [244] [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
*Yulong Li,Zhixiang Lu,Feilong Tang,Simin Lai,Ming Hu,Yuxuan Zhang,Haochen Xue,Zhaodong Wu,Imran Razzak,Qingxia Li,Jionglong Su*

Main category: cs.SI

TLDR: 论文提出了一种结合多维霍克斯过程和图神经网络的方法，用于建模社交网络中意见传播的动态，并引入了一个新数据集VISTA。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以有效捕捉社交媒体中复杂的公共意见动态，因此需要一种新方法来建模多维度互动和层级关系。

Method: 整合多维霍克斯过程与图神经网络，捕捉层级结构和多维度互动，同时引入VISTA数据集支持研究。

Result: 提出的方法能够有效建模意见传播动态，VISTA数据集提供了高质量的数据支持。

Conclusion: 该方法为未来研究提供了强有力的基线，并具有较高的可解释性。

Abstract: The rapid development of social media has significantly reshaped the dynamics
of public opinion, resulting in complex interactions that traditional models
fail to effectively capture. To address this challenge, we propose an
innovative approach that integrates multi-dimensional Hawkes processes with
Graph Neural Network, modeling opinion propagation dynamics among nodes in a
social network while considering the intricate hierarchical relationships
between comments. The extended multi-dimensional Hawkes process captures the
hierarchical structure, multi-dimensional interactions, and mutual influences
across different topics, forming a complex propagation network. Moreover,
recognizing the lack of high-quality datasets capable of comprehensively
capturing the evolution of public opinion dynamics, we introduce a new dataset,
VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015
second-level comments, and 29,578 third-level comments, covering diverse
domains such as politics, entertainment, sports, health, and medicine. The
dataset is annotated with detailed sentiment labels across 11 categories and
clearly defined hierarchical relationships. When combined with our method, it
offers strong interpretability by linking sentiment propagation to the comment
hierarchy and temporal evolution. Our approach provides a robust baseline for
future research.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [245] [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
*Yuheng Huang,Lei Ma,Keizaburo Nishikino,Takumi Akazaki*

Main category: cs.SE

TLDR: PtTrust是一个基于内部状态预训练的两阶段风险评估框架，旨在提升代码LLM的可信度，通过无监督预训练和有监督训练实现跨任务和语言的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM在生成代码时存在不可信问题（如错误、不安全或不可靠代码），现有方法局限于窄领域且缺乏行业级实用性。

Method: PtTrust采用两阶段方法：1）无监督预训练学习LLM状态的通用表示；2）用小规模标注数据训练风险预测器。

Result: PtTrust在代码行级别风险评估中表现有效，具有跨任务和语言的通用性，并提供直观可解释的特征。

Conclusion: PtTrust为代码LLM的可扩展和可信保障迈出了重要一步。

Abstract: The pre-training paradigm plays a key role in the success of Large Language
Models (LLMs), which have been recognized as one of the most significant
advancements of AI recently. Building on these breakthroughs, code LLMs with
advanced coding capabilities bring huge impacts on software engineering,
showing the tendency to become an essential part of developers' daily routines.
However, the current code LLMs still face serious challenges related to
trustworthiness, as they can generate incorrect, insecure, or unreliable code.
Recent exploratory studies find that it can be promising to detect such risky
outputs by analyzing LLMs' internal states, akin to how the human brain
unconsciously recognizes its own mistakes. Yet, most of these approaches are
limited to narrow sub-domains of LLM operations and fall short of achieving
industry-level scalability and practicability. To address these challenges, in
this paper, we propose PtTrust, a two-stage risk assessment framework for code
LLM based on internal state pre-training, designed to integrate seamlessly with
the existing infrastructure of software companies. The core idea is that the
risk assessment framework could also undergo a pre-training process similar to
LLMs. Specifically, PtTrust first performs unsupervised pre-training on
large-scale unlabeled source code to learn general representations of LLM
states. Then, it uses a small, labeled dataset to train a risk predictor. We
demonstrate the effectiveness of PtTrust through fine-grained, code line-level
risk assessment and demonstrate that it generalizes across tasks and different
programming languages. Further experiments also reveal that PtTrust provides
highly intuitive and interpretable features, fostering greater user trust. We
believe PtTrust makes a promising step toward scalable and trustworthy
assurance for code LLMs.

</details>

### [246] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Main category: cs.SE

TLDR: CRUST-Bench是一个用于评估C到Rust转译的数据集，包含100个C仓库及其对应的安全Rust接口和测试用例，旨在解决复杂项目的转译挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法评估C到安全Rust的转译能力，CRUST-Bench填补了这一空白，支持现代Rust生态系统的安全性和互操作性。

Method: CRUST-Bench包含100个C仓库，每个仓库配有手动编写的安全Rust接口和测试用例，以验证转译的正确性和安全性。

Result: 实验表明，即使是先进的LLM（如OpenAI o1）在单次尝试中仅能完成15个任务，生成安全且符合习惯的Rust代码仍具挑战性。

Conclusion: CRUST-Bench为改进转译系统提供了基准，有助于从C迁移到内存安全的Rust，未来需进一步提升复杂场景的处理能力。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>

<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [247] [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
*Jon Kleinberg,Fan Wei*

Main category: math.CO

TLDR: 论文提出了一种抽象的语言生成框架（语言生成极限），研究了算法在生成新字符串时的有效性与广度之间的权衡，并通过密度度量量化了这种权衡。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在语言生成中的有效性与广度之间的根本性权衡，尤其是避免幻觉和模式崩溃的问题。

Method: 提出了一种抽象的语言生成极限框架，将生成过程视为对抗性游戏，并开发了一种算法，其输出在目标语言中具有严格正密度。

Result: 证明了现有算法的输出密度可能为零，但新算法能够实现严格正密度，并分析了算法内部表示的特性。

Conclusion: 通过引入新的语言家族拓扑结构，揭示了实现最佳广度可能需要在高密度和低密度表示之间无限振荡的复杂性。

Abstract: The recent successes of large language models (LLMs) have led to a surge of
theoretical research into language generation. A recent line of work proposes
an abstract view, called language generation in the limit, where generation is
seen as a game between an adversary and an algorithm: the adversary generates
strings from an unknown language $K$, chosen from a countable collection of
candidate languages, and after seeing a finite set of these strings, the
algorithm must generate new strings from $K$ that it has not seen before. This
formalism highlights a key tension: the trade-off between validity (the
algorithm should only produce strings from the language) and breadth (it should
be able to produce many strings from the language). This trade-off is central
in applied language generation as well, where it appears as a balance between
hallucination (generating invalid utterances) and mode collapse (generating
only a restricted set of outputs). Despite its importance, this trade-off has
been challenging to study quantitatively. We develop ways to quantify this
trade-off by formalizing breadth using measures of density. Existing algorithms
for language generation in the limit produce output sets that can have zero
density in the true language, and this important failure of breadth might seem
unavoidable. We show, however, that such a failure is not necessary: we provide
an algorithm for language generation in the limit whose outputs have strictly
positive density in $K$. We also study the internal representations built by
these algorithms, specifically the sequence of hypothesized candidate languages
they consider, and show that achieving the strongest form of breadth may
require oscillating indefinitely between high- and low-density representations.
Our analysis introduces a novel topology on language families, with notions of
convergence and limit points playing a key role.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [248] [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
*Deyu Cao,Samin Aref*

Main category: cs.LG

TLDR: 论文探讨了大型语言模型的量化方法，提出了一种基于ApiQ的改进方法，通过显著性感知正则化提升超低比特量化性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算需求和规模带来实际挑战，量化方法虽能压缩模型，但现有方法在精度和资源消耗之间存在权衡。

Method: 结合ApiQ的部分训练与量化感知训练技术，提出显著性感知正则化方法，优先保留关键参数。

Result: 实验表明，新方法在LLaMA系列模型上提升了精度，缩小了量化模型与全精度模型的差距，且开销极小。

Conclusion: 提出的方法在超低比特量化中表现优异，未来将公开以促进相关研究。

Abstract: Large language models offer remarkable capabilities, but their size and
computational demands pose practical challenges. Quantization methods compress
their size through replacing their high-precision parameters by quantized
values of lower precision. Post-training quantization reduces model size
efficiently at the cost of decreased accuracy, while quantization-aware
training better preserves accuracy but is resource-intensive. Among existing
post-training quantization algorithms, the ApiQ method achieves superior
accuracy preservation at minimal memory and time overhead. We investigate two
ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.
First, we look into combining existing quantization-aware training techniques
with ApiQ's partial training. We show that this does not outperform the
baseline ApiQ method with limited training data and frozen weights. This leads
to two key insights: (1) The substantial representational capacity that is
gained through full retraining may not be feasible through partial training.
(2) This gain seems to depend on using a large and diverse dataset in
quantization-aware training. Second, through a novel approach informed by the
two insights, we propose an ultra-low-bit quantization method that builds upon
ApiQ and extends its performance without the need for full retraining. It
relies on a saliency-aware regularization term that prioritizes preserving the
most impactful parameters during quantization. Our experiments on benchmark
language models from the LLaMA family show that our proposed approach boosts
accuracy and tightens the gap between the quantized model and the
full-precision model, with minimal overhead. Our method will be made publicly
available to facilitate future developments in ultra-low-bit quantization of
large language models.

</details>

### [249] [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
*Cheng Qian,Emre Can Acikgoz,Qi He,Hongru Wang,Xiusi Chen,Dilek Hakkani-Tür,Gokhan Tur,Heng Ji*

Main category: cs.LG

TLDR: 本文研究了强化学习中奖励设计对大型语言模型（LLMs）工具使用能力的影响，提出了一种新的奖励设计方法，并在实验中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前监督微调（SFT）方法在复杂工具使用场景中泛化能力不足，而强化学习（RL）的奖励设计面临多样性和细粒度反馈的挑战。

Method: 系统探索了多种奖励策略的类型、规模、粒度和时间动态，提出了一种针对工具使用任务的奖励设计方法，并采用GRPO训练LLMs。

Result: 实验表明，该方法在多个基准测试中表现优异，比基础模型提升17%，比SFT模型提升15%。

Conclusion: 研究表明，精心设计的奖励策略对提升LLMs的工具使用能力和泛化性能至关重要，并开源代码以促进未来研究。

Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.

</details>

### [250] [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
*Amrit Diggavi Seshadri*

Main category: cs.LG

TLDR: 提出了一种名为OJFA的低秩捷径方法，显著减少了推理时的参数成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少大型语言模型推理的时间和计算成本，研究参数高效的早期退出方法。

Method: 提出了一种单一的低秩捷径（OJFA），替代了传统的多捷径方法。

Result: OJFA方法在GPT2-XL、Phi3-Mini和Llama2-7B模型上表现稳定，参数成本减少了30倍。

Conclusion: OJFA方法在显著降低参数成本的同时，性能与多捷径方法相当。

Abstract: To reduce the time and computational costs of inference of large language
models, there has been interest in parameter-efficient low-rank early-exit
casting of transformer hidden-representations to final-representations. Such
low-rank short-cutting has been shown to outperform identity shortcuts at early
model stages while offering parameter-efficiency in shortcut jumps. However,
current low-rank methods maintain a separate early-exit shortcut jump to
final-representations for each transformer intermediate block-level during
inference. In this work, we propose selection of a single One-Jump-Fits-All
(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter
costs during inference. We show that despite this extreme reduction, our OJFA
choice largely matches the performance of maintaining multiple shortcut jumps
during inference and offers stable precision from all transformer block-levels
for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.

</details>

### [251] [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.LG

TLDR: 本文提出了一种基于Hadamard矩阵的量化方法，有效减少大语言模型中的异常值，实现了3位量化，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在边缘设备上部署时，由于参数规模庞大和激活中的异常值问题，量化面临挑战。

Method: 利用Hadamard矩阵的理论优势，结合渐进式二分搜索方法，实现了对权重、激活和KV缓存的3位量化。

Result: 在多个模型家族（如Mistral、LLaMA和Qwen）上实验，性能优于现有方法，准确率提升40%。

Conclusion: Hadamard矩阵在量化中表现优越，为低比特量化提供了实用解决方案。

Abstract: Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of quantization
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving low-bit quantization.
Our method based on a gradual binary search enables 3-bit quantization for
weights, activations, and key-value (KV) caches, resulting in a 40\% increase
in accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit quantization for weights, activations, and KV cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
quantization.

</details>

### [252] [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
*Till Rossner,Ziteng Li,Jonas Balke,Nikoo Salehfard,Tom Seifert,Ming Tang*

Main category: cs.LG

TLDR: 提出了一种基于scGPT和DeepCDR的创新方法，用于预测癌症药物反应（CDR），实验结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高癌症药物反应预测的准确性，探索scGPT在基因表达数据中的应用潜力。

Method: 结合scGPT生成基因表达数据的嵌入表示，作为DeepCDR的输入数据。

Result: scGPT方法在CDR预测中表现优于原始DeepCDR和scFoundation模型。

Conclusion: scGPT嵌入能显著提升CDR预测精度，为现有方法提供了有前景的替代方案。

Abstract: In this study, we propose an innovative methodology for predicting Cancer
Drug Response (CDR) through the integration of the scGPT foundation model
within the DeepCDR model. Our approach utilizes scGPT to generate embeddings
from gene expression data, which are then used as gene expression input data
for DeepCDR. The experimental findings demonstrate the efficacy of this
scGPT-based method in outperforming previous related works, including the
original DeepCDR model and the scFoundation-based model. This study highlights
the potential of scGPT embeddings to enhance the accuracy of CDR predictions
and offers a promising alternative to existing approaches.

</details>

### [253] [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
*Shihan Dou,Muling Wu,Jingwen Xu,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TLDR: 论文提出了一种名为RRL的新算法，通过动态重放机制解决强化学习中早期探索不足的问题，显著提升了大型语言模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型的强化学习训练中，早期探索阶段的有价值解决方案可能因模型能力不足而被抑制，导致后期无法有效解决复杂问题。

Method: 提出Retrospective Replay-based Reinforcement Learning (RRL)算法，动态重放早期探索阶段的潜在有价值状态。

Result: 实验表明，RRL在复杂推理任务（如数学推理和代码生成）中显著提升了探索效率和模型性能，同时优化了RLHF的效果。

Conclusion: RRL通过动态重放机制有效解决了强化学习中的探索问题，为大型语言模型的优化提供了新思路。

Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in
the post-training of large language models (LLMs). The effective exploration of
the output space is essential for the success of RL. We observe that for
complex problems, during the early stages of training, the model exhibits
strong exploratory capabilities and can identify promising solution ideas.
However, its limited capability at this stage prevents it from successfully
solving these problems. The early suppression of these potentially valuable
solution ideas by the policy gradient hinders the model's ability to revisit
and re-explore these ideas later. Consequently, although the LLM's capabilities
improve in the later stages of training, it still struggles to effectively
address these complex problems. To address this exploration issue, we propose a
novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),
which introduces a dynamic replay mechanism throughout the training process.
RRL enables the model to revisit promising states identified in the early
stages, thereby improving its efficiency and effectiveness in exploration. To
evaluate the effectiveness of RRL, we conduct extensive experiments on complex
reasoning tasks, including mathematical reasoning and code generation, and
general dialogue tasks. The results indicate that RRL maintains high
exploration efficiency throughout the training period, significantly enhancing
the effectiveness of RL in optimizing LLMs for complicated reasoning tasks.
Moreover, it also improves the performance of RLHF, making the model both safer
and more helpful.

</details>

### [254] [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
*Avinandan Bose,Zhihan Xiong,Yuejie Chi,Simon Shaolei Du,Lin Xiao,Maryam Fazel*

Main category: cs.LG

TLDR: 提出了一种基于低秩偏好建模的新框架，用于个性化大型语言模型，以更好地适应多样化的用户偏好。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法依赖单一价值表示，难以适应个体偏好，因此需要一种更灵活的方法。

Method: 通过低维子空间表示奖励函数，并将个体偏好建模为共享基函数的加权组合，实现可扩展性和少样本适应。

Result: 在多个偏好数据集上验证了方法的有效性，表现出对未见用户的优越泛化能力和偏好预测准确性。

Conclusion: 该框架为个性化LLMs提供了一种高效且灵活的方法，避免了僵化的用户分类。

Abstract: Personalizing large language models (LLMs) to accommodate diverse user
preferences is essential for enhancing alignment and user satisfaction.
Traditional reinforcement learning from human feedback (RLHF) approaches often
rely on monolithic value representations, limiting their ability to adapt to
individual preferences. We introduce a novel framework that leverages low-rank
preference modeling to efficiently learn and generalize user-specific reward
functions. By representing reward functions in a low-dimensional subspace and
modeling individual preferences as weighted combinations of shared basis
functions, our approach avoids rigid user categorization while enabling
scalability and few-shot adaptation. We validate our method on multiple
preference datasets, demonstrating superior generalization to unseen users and
improved accuracy in preference prediction tasks.

</details>

### [255] [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
*Yunhui Xia,Wei Shen,Yan Wang,Jason Klein Liu,Huifeng Sun,Siyue Wu,Jian Hu,Xiaolong Xu*

Main category: cs.LG

TLDR: LeetCodeDataset是一个高质量的代码生成模型评估和训练基准，解决了LLM研究中缺乏推理导向的编码基准和自包含训练测试的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM研究中缺乏推理导向的编码基准和自包含训练测试的问题。

Method: 通过整理LeetCode Python问题，提供丰富的元数据、广泛覆盖、每个问题100+测试用例和时间分割（2024年7月前后），支持无污染评估和高效监督微调（SFT）。

Result: 实验显示推理模型显著优于非推理模型，仅用2.6K模型生成的解决方案进行SFT即可达到与110K样本相当的性能。

Conclusion: LeetCodeDataset及其评估框架已在Hugging Face和Github上公开。

Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
training code-generation models, addressing two key challenges in LLM research:
the lack of reasoning-focused coding benchmarks and self-contained training
testbeds. By curating LeetCode Python problems with rich metadata, broad
coverage, 100+ test cases per problem, and temporal splits (pre/post July
2024), our dataset enables contamination-free evaluation and efficient
supervised fine-tuning (SFT). Experiments show reasoning models significantly
outperform non-reasoning counterparts, while SFT with only 2.6K model-generated
solutions achieves performance comparable to 110K-sample counterparts. The
dataset and evaluation framework are available on Hugging Face and Github.

</details>

### [256] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
*Jianhao Yan,Yafu Li,Zican Hu,Zhi Wang,Ganqu Cui,Xiaoye Qu,Yu Cheng,Yue Zhang*

Main category: cs.LG

TLDR: LUFFY框架通过结合离策略推理轨迹和策略内训练，显著提升了数学推理任务的性能，并在泛化能力上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有零强化学习方法局限于策略内学习，无法超越初始推理能力，需要一种新方法来结合离策略指导。

Method: LUFFY框架动态平衡模仿与探索，利用正则化重要性采样避免浅层模仿，结合离策略演示和策略内训练。

Result: 在六个数学基准测试中平均提升7.0分，在分布外任务中优势达6.2分，显著优于监督微调。

Conclusion: LUFFY不仅有效模仿，还能超越演示进行探索，为训练泛化推理模型提供了可扩展路径。

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning (RL) with simple rule-based rewards. However,
existing zero-RL approaches are inherently ``on-policy'', limiting learning to
a model's own outputs and failing to acquire reasoning abilities beyond its
initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY
guidance), a framework that augments zero-RL with off-policy reasoning traces.
LUFFY dynamically balances imitation and exploration by combining off-policy
demonstrations with on-policy rollouts during training. Notably, we propose
policy shaping via regularized importance sampling to avoid superficial and
rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an
over +7.0 average gain across six math benchmarks and an advantage of over +6.2
points in out-of-distribution tasks. It also substantially surpasses
imitation-based supervised fine-tuning (SFT), particularly in generalization.
Analysis shows LUFFY not only imitates effectively but also explores beyond
demonstrations, offering a scalable path to train generalizable reasoning
models with off-policy guidance.

</details>

### [257] [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
*Vaishnavh Nagarajan,Chen Henry Wu,Charles Ding,Aditi Raghunathan*

Main category: cs.LG

TLDR: 论文设计了一套最小化算法任务，用于量化语言模型的创造性限制，并提出多令牌方法和输入层噪声注入优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在开放式任务中的创造性表现，探索其局限性并提出改进方法。

Method: 设计抽象任务，比较单令牌与多令牌方法（如无教师训练和扩散模型），并测试输入层噪声注入（hash-conditioning）。

Result: 多令牌方法在多样性和原创性上表现更优，输入层噪声注入能更好地平衡随机性与连贯性。

Conclusion: 论文为分析开放式创造力提供了测试框架，并支持超越单令牌学习和基于softmax采样的方法。

Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction
of open-ended real-world tasks. This allows us to cleanly and controllably
quantify the creative limits of the present-day language model. Much like
real-world tasks that require a creative, far-sighted leap of thought, our
tasks require an implicit, open-ended stochastic planning step that either (a)
discovers new connections in an abstract knowledge graph (like in wordplay,
drawing analogies, or research) or (b) constructs new patterns (like in
designing math problems or new proteins). In these tasks, we empirically and
conceptually argue how next-token learning is myopic and memorizes excessively;
comparatively, multi-token approaches, namely teacherless training and
diffusion models, excel in producing diverse and original output. Secondly, in
our tasks, we find that to elicit randomness from the Transformer without
hurting coherence, it is better to inject noise right at the input layer (via a
method we dub hash-conditioning) rather than defer to temperature sampling from
the output layer. Thus, our work offers a principled, minimal test-bed for
analyzing open-ended creative skills, and offers new arguments for going beyond
next-token learning and softmax-based sampling. We make part of the code
available under https://github.com/chenwu98/algorithmic-creativity

</details>

### [258] [Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing](https://arxiv.org/abs/2504.13975)
*Mehmet Yamaç,Muhammad Numan Yousaf,Serkan Kiranyaz,Moncef Gabbouj*

Main category: cs.LG

TLDR: 论文提出了一种名为多尺度张量求和（MTS）分解的新型神经网络算子，通过多尺度张量求和提升效率，优于传统密集层和卷积层。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机（MLP）和卷积层在计算机视觉任务中存在高维输入输出对和有限感受野的问题，需要更高效的算子。

Method: 引入MTS分解，通过Tucker分解类模式乘积实现多尺度张量求和，作为新的神经网络层。

Result: MTS减少了参数数量并优化了权重效率，实验证明其在分类、压缩和信号恢复等任务中优于MLP和CNN。

Conclusion: MTSNet结合现代非线性单元（如MHG），在计算机视觉应用中表现出优于现有Transformer的复杂度-性能权衡。

Abstract: Multilayer perceptrons (MLP), or fully connected artificial neural networks,
are known for performing vector-matrix multiplications using learnable weight
matrices; however, their practical application in many machine learning tasks,
especially in computer vision, can be limited due to the high dimensionality of
input-output pairs at each layer. To improve efficiency, convolutional
operators have been utilized to facilitate weight sharing and local
connections, yet they are constrained by limited receptive fields. In this
paper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel
neural network operator that implements tensor summation at multiple scales,
where each tensor to be summed is obtained through Tucker-decomposition-like
mode products. Unlike other tensor decomposition methods in the literature, MTS
is not introduced as a network compression tool; instead, as a new backbone
neural layer. MTS not only reduces the number of parameters required while
enhancing the efficiency of weight optimization compared to traditional dense
layers (i.e., unfactorized weight matrices in MLP layers), but it also
demonstrates clear advantages over convolutional layers. The proof-of-concept
experimental comparison of the proposed MTS networks with MLPs and
Convolutional Neural Networks (CNNs) demonstrates their effectiveness across
various tasks, such as classification, compression, and signal restoration.
Additionally, when integrated with modern non-linear units such as the
multi-head gate (MHG), also introduced in this study, the corresponding neural
network, MTSNet, demonstrates a more favorable complexity-performance tradeoff
compared to state-of-the-art transformers in various computer vision
applications. The software implementation of the MTS layer and the
corresponding MTS-based networks, MTSNets, is shared at
https://github.com/mehmetyamac/MTSNet.

</details>

### [259] [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://arxiv.org/abs/2504.14662)
*Yeoreum Lee,Jinwook Jung,Sungyong Baik*

Main category: cs.LG

TLDR: 论文提出了一种新的微调目标函数，旨在减少参数干扰并提升单任务性能，类似于锐度感知最小化（SAM），实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型的预训练-微调范式导致了许多任务特定模型的出现，但合并这些模型时存在参数干扰问题。现有方法在减少干扰的同时牺牲了性能，因此需要一种新的微调方法。

Method: 设计了一种新的微调目标函数，类似于SAM，通过锐度感知最小化来减少参数干扰并提升性能。

Result: 实验和理论结果表明，该方法在合并和微调任务中表现优于现有方法。

Conclusion: 通过锐度感知最小化的微调方法有效减少了参数干扰并提升了合并模型的性能。

Abstract: Large-scale deep learning models with a pretraining-finetuning paradigm have
led to a surge of numerous task-specific models fine-tuned from a common
pre-trained model. Recently, several research efforts have been made on merging
these large models into a single multi-task model, particularly with simple
arithmetic on parameters. Such merging methodology faces a central challenge:
interference between model parameters fine-tuned on different tasks. Few recent
works have focused on designing a new fine-tuning scheme that can lead to small
parameter interference, however at the cost of the performance of each
task-specific fine-tuned model and thereby limiting that of a merged model. To
improve the performance of a merged model, we note that a fine-tuning scheme
should aim for (1) smaller parameter interference and (2) better performance of
each fine-tuned model on the corresponding task. In this work, we aim to design
a new fine-tuning objective function to work towards these two goals. In the
course of this process, we find such objective function to be strikingly
similar to sharpness-aware minimization (SAM) objective function, which aims to
achieve generalization by finding flat minima. Drawing upon our observation, we
propose to fine-tune pre-trained models via sharpness-aware minimization. The
experimental and theoretical results showcase the effectiveness and
orthogonality of our proposed approach, improving performance upon various
merging and fine-tuning methods. Our code is available at
https://github.com/baiklab/SAFT-Merge.

</details>

### [260] [Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning](https://arxiv.org/abs/2504.14727)
*Geng Liu,Fei Zhu,Rong Feng,Zhiqiang Yi,Shiqi Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.LG

TLDR: 本文提出了一种受人类记忆和学习系统启发的生物模拟持续学习框架，解决了深度神经网络在连续任务训练中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在连续任务学习中容易遗忘已学知识，而人类和动物具有持续学习的能力，因此需要模仿生物智能来解决这一问题。

Method: 结合半参数记忆和醒睡巩固机制，提出了一种生物模拟持续学习框架。

Result: 该方法在真实世界的持续学习场景（如ImageNet上的类增量学习）中，能够保持新任务的高性能并保留先验知识。

Conclusion: 模仿生物智能是实现深度神经网络持续学习能力的有效途径。

Abstract: Humans and most animals inherently possess a distinctive capacity to
continually acquire novel experiences and accumulate worldly knowledge over
time. This ability, termed continual learning, is also critical for deep neural
networks (DNNs) to adapt to the dynamically evolving world in open
environments. However, DNNs notoriously suffer from catastrophic forgetting of
previously learned knowledge when trained on sequential tasks. In this work,
inspired by the interactive human memory and learning system, we propose a
novel biomimetic continual learning framework that integrates semi-parametric
memory and the wake-sleep consolidation mechanism. For the first time, our
method enables deep neural networks to retain high performance on novel tasks
while maintaining prior knowledge in real-world challenging continual learning
scenarios, e.g., class-incremental learning on ImageNet. This study
demonstrates that emulating biological intelligence provides a promising path
to enable deep neural networks with continual learning capabilities.

</details>

### [261] [Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](https://arxiv.org/abs/2504.14798)
*Hao Xuan,Xingyu Li*

Main category: cs.LG

TLDR: 论文提出了一种名为“鲁棒性遗忘”的概念，并通过“遗忘映射攻击”（UMA）框架验证现有遗忘技术的安全性，发现其仍存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术无法完全消除遗忘信息的痕迹，导致隐私泄露风险，需要更严格的验证标准。

Method: 提出“鲁棒性遗忘”概念，并设计UMA框架，通过对抗性查询主动探测模型中的遗忘痕迹。

Result: 实验表明，现有遗忘技术即使通过现有验证指标，仍存在信息泄露风险。

Conclusion: UMA为机器遗忘安全性评估设立了新标准，并揭示了现有技术的不足。

Abstract: Machine Unlearning (MUL) is crucial for privacy protection and content
regulation, yet recent studies reveal that traces of forgotten information
persist in unlearned models, enabling adversaries to resurface removed
knowledge. Existing verification methods only confirm whether unlearning was
executed, failing to detect such residual information leaks. To address this,
we introduce the concept of Robust Unlearning, ensuring models are
indistinguishable from retraining and resistant to adversarial recovery. To
empirically evaluate whether unlearning techniques meet this security standard,
we propose the Unlearning Mapping Attack (UMA), a post-unlearning verification
framework that actively probes models for forgotten traces using adversarial
queries. Extensive experiments on discriminative and generative tasks show that
existing unlearning techniques remain vulnerable, even when passing existing
verification metrics. By establishing UMA as a practical verification tool,
this study sets a new standard for assessing and enhancing machine unlearning
security.

</details>

### [262] [A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions](https://arxiv.org/abs/2504.14800)
*Shuxian Zhao,Jie Gui,Minjing Dong,Baosheng Yu,Zhipeng Gui,Lu Dong,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.LG

TLDR: 本文提出了一种系统性分析框架，用于解决小样本不平衡（S&I）问题，总结了不平衡度量和复杂性分析方法，并探讨了现有解决方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 小样本不平衡问题是机器学习和数据分析中的主要挑战，现有方法多依赖启发式算法，缺乏对数据特性的深入分析。

Method: 提出系统性分析框架，总结不平衡度量和复杂性分析方法，并实验验证重采样方法的局限性。

Result: 实验表明，分类器性能差异显著超过重采样带来的改进。

Conclusion: 本文强调了未解决的问题，并讨论了未来趋势，呼吁从数据角度深入分析。

Abstract: The small sample imbalance (S&I) problem is a major challenge in machine
learning and data analysis. It is characterized by a small number of samples
and an imbalanced class distribution, which leads to poor model performance. In
addition, indistinct inter-class feature distributions further complicate
classification tasks. Existing methods often rely on algorithmic heuristics
without sufficiently analyzing the underlying data characteristics. We argue
that a detailed analysis from the data perspective is essential before
developing an appropriate solution. Therefore, this paper proposes a systematic
analytical framework for the S\&I problem. We first summarize imbalance metrics
and complexity analysis methods, highlighting the need for interpretable
benchmarks to characterize S&I problems. Second, we review recent solutions for
conventional, complexity-based, and extreme S&I problems, revealing
methodological differences in handling various data distributions. Our summary
finds that resampling remains a widely adopted solution. However, we conduct
experiments on binary and multiclass datasets, revealing that classifier
performance differences significantly exceed the improvements achieved through
resampling. Finally, this paper highlights open questions and discusses future
trends.

</details>

### [263] [What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale](https://arxiv.org/abs/2504.14815)
*Xiaoyong Yuan,Xiaolong Ma,Linke Guo,Lan Zhang*

Main category: cs.LG

TLDR: PAIA是一种无需提示或生成图像的扩散模型概念审计框架，能高效检测模型是否学习特定概念。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的广泛共享引发伦理和法律问题，现有审计方法存在局限性。

Method: 提出Prompt-Agnostic Image-Free Auditing (PAIA)，直接分析模型内部行为。

Result: 在320个控制模型和690个社区模型上，PAIA检测准确率超90%，审计时间减少18-40倍。

Conclusion: PAIA是首个可扩展的扩散模型预部署审计方案，为模型共享提供更安全透明的工具。

Abstract: Diffusion models (DMs) have revolutionized text-to-image generation, enabling
the creation of highly realistic and customized images from text prompts. With
the rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users
can now customize powerful pre-trained models using minimal computational
resources. However, the widespread sharing of fine-tuned DMs on open platforms
raises growing ethical and legal concerns, as these models may inadvertently or
deliberately generate sensitive or unauthorized content, such as copyrighted
material, private individuals, or harmful content. Despite the increasing
regulatory attention on generative AI, there are currently no practical tools
for systematically auditing these models before deployment. In this paper, we
address the problem of concept auditing: determining whether a fine-tuned DM
has learned to generate a specific target concept. Existing approaches
typically rely on prompt-based input crafting and output-based image
classification but suffer from critical limitations, including prompt
uncertainty, concept drift, and poor scalability. To overcome these challenges,
we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric
concept auditing framework. By treating the DM as the object of inspection,
PAIA enables direct analysis of internal model behavior, bypassing the need for
optimized prompts or generated images. We evaluate PAIA on 320 controlled model
and 690 real-world community models sourced from a public DM sharing platform.
PAIA achieves over 90% detection accuracy while reducing auditing time by
18-40x compared to existing baselines. To our knowledge, PAIA is the first
scalable and practical solution for pre-deployment concept auditing of
diffusion models, providing a practical foundation for safer and more
transparent diffusion model sharing.

</details>

### [264] [Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness](https://arxiv.org/abs/2504.14882)
*Mojtaba Kolahdouzi,Hatice Gunes,Ali Etemad*

Main category: cs.LG

TLDR: 研究优化算法选择对深度神经网络群体公平性的影响，发现自适应优化器（如RMSProp）比随机优化器（如SGD）更易收敛到公平解，并通过理论和实验验证。


<details>
  <summary>Details</summary>
Motivation: 探讨优化算法如何影响深度学习的群体公平性，特别是在数据不平衡的情况下。

Method: 通过随机微分方程分析优化动态，比较自适应优化器（如RMSProp）与随机优化器（如SGD），并在多个数据集（CelebA、FairFace、MS-COCO）上进行实验验证。

Result: 自适应优化器在多种公平性定义（如均等机会、人口均等）下表现优于SGD，同时保持预测准确性。

Conclusion: 自适应优化器是实现公平性的关键机制，未来研究应关注其作用。

Abstract: We study whether and how the choice of optimization algorithm can impact
group fairness in deep neural networks. Through stochastic differential
equation analysis of optimization dynamics in an analytically tractable setup,
we demonstrate that the choice of optimization algorithm indeed influences
fairness outcomes, particularly under severe imbalance. Furthermore, we show
that when comparing two categories of optimizers, adaptive methods and
stochastic methods, RMSProp (from the adaptive category) has a higher
likelihood of converging to fairer minima than SGD (from the stochastic
category). Building on this insight, we derive two new theoretical guarantees
showing that, under appropriate conditions, RMSProp exhibits fairer parameter
updates and improved fairness in a single optimization step compared to SGD. We
then validate these findings through extensive experiments on three publicly
available datasets, namely CelebA, FairFace, and MS-COCO, across different
tasks as facial expression recognition, gender classification, and multi-label
classification, using various backbones. Considering multiple fairness
definitions including equalized odds, equal opportunity, and demographic
parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD
in terms of group fairness, while maintaining comparable predictive accuracy.
Our results highlight the role of adaptive updates as a crucial yet overlooked
mechanism for promoting fair outcomes.

</details>

### [265] [VeLU: Variance-enhanced Learning Unit for Deep Neural Networks](https://arxiv.org/abs/2504.15051)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicolè,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TLDR: VeLU是一种基于输入方差动态调整的激活函数，通过ArcTan-Sin变换和Wasserstein-2正则化，解决了ReLU等传统激活函数的梯度消失和适应性不足问题，并在多个视觉任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数（如ReLU）存在梯度消失和缺乏动态适应性的问题，而其他替代方案（如Swish和GELU）未能根据输入统计动态调整。

Method: VeLU结合了ArcTan-Sin变换和Wasserstein-2正则化，动态调整输入方差，以缓解协变量偏移和优化稳定性问题。

Result: 在ViT_B16、VGG19等六种视觉模型上的实验表明，VeLU优于ReLU、Swish和GELU。

Conclusion: VeLU通过动态调整输入方差，显著提升了激活函数的性能，适用于多种视觉任务。

Abstract: Activation functions are fundamental in deep neural networks and directly
impact gradient flow, optimization stability, and generalization. Although ReLU
remains standard because of its simplicity, it suffers from vanishing gradients
and lacks adaptability. Alternatives like Swish and GELU introduce smooth
transitions, but fail to dynamically adjust to input statistics. We propose
VeLU, a Variance-enhanced Learning Unit as an activation function that
dynamically scales based on input variance by integrating ArcTan-Sin
transformations and Wasserstein-2 regularization, effectively mitigating
covariate shifts and stabilizing optimization. Extensive experiments on
ViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm
VeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.
The codes of VeLU are publicly available on GitHub.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [266] [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
*Akash V. Maharaj,David Arbour,Daniel Lee,Uttaran Bhattacharya,Anup Rao,Austin Zane,Avi Feller,Kun Qian,Yunyao Li*

Main category: cs.AI

TLDR: 本文提出了一种全面的框架，用于监控、基准测试和持续改进企业AI助手，以确保其在关键环境中的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 企业AI助手在准确性至关重要的领域部署，错误输出可能导致重大事故，因此需要系统化的改进方法。

Method: 框架包含三个关键部分：分层严重性框架、可扩展的基准测试方法以及多维评估的持续改进策略。

Result: 通过该框架，组织可以系统提升AI助手的可靠性和性能。

Conclusion: 这种多维度评估方法为更强大和可信的AI系统开辟了道路。

Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy
is paramount, making each erroneous output a potentially significant incident.
This paper presents a comprehensive framework for monitoring, benchmarking, and
continuously improving such complex, multi-component systems under active
development by multiple teams. Our approach encompasses three key elements: (1)
a hierarchical ``severity'' framework for incident detection that identifies
and categorizes errors while attributing component-specific error rates,
facilitating targeted improvements; (2) a scalable and principled methodology
for benchmark construction, evaluation, and deployment, designed to accommodate
multiple development teams, mitigate overfitting risks, and assess the
downstream impact of system modifications; and (3) a continual improvement
strategy leveraging multidimensional evaluation, enabling the identification
and implementation of diverse enhancement opportunities. By adopting this
holistic framework, organizations can systematically enhance the reliability
and performance of their AI Assistants, ensuring their efficacy in critical
enterprise environments. We conclude by discussing how this multifaceted
evaluation approach opens avenues for various classes of enhancements, paving
the way for more robust and trustworthy AI systems.

</details>

### [267] [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
*Jennifer Hu,Michael A. Lepori,Michael Franke*

Main category: cs.AI

TLDR: 研究探讨Transformer模型的内部动态（层时间动态）是否能预测人类实时处理过程，发现其比模型输出更能解释人类认知。


<details>
  <summary>Details</summary>
Motivation: 探索AI模型（如Transformer）的内部处理策略是否与人类认知过程相似，超越传统的黑箱输入输出映射研究。

Method: 通过五个跨领域和模态的研究，比较Transformer模型的层时间动态与人类实时处理数据。

Result: 层时间动态比模型输出更能预测人类处理特征，表明两者处理策略相似。

Conclusion: AI模型可作为人类认知的显式处理模型，而不仅是黑箱工具。

Abstract: Modern AI models are increasingly being used as theoretical tools to study
human cognition. One dominant approach is to evaluate whether human-derived
measures (such as offline judgments or real-time processing) are predicted by a
model's output: that is, the end-product of forward pass(es) through the
network. At the same time, recent advances in mechanistic interpretability have
begun to reveal the internal processes that give rise to model outputs, raising
the question of whether models and humans might arrive at outputs using similar
"processing strategies". Here, we investigate the link between real-time
processing in humans and "layer-time" dynamics in Transformer models. Across
five studies spanning domains and modalities, we test whether the dynamics of
computation in a single forward pass of pre-trained Transformers predict
signatures of processing in humans, above and beyond properties of the model's
output probability distribution. We consistently find that layer-time dynamics
provide additional predictive power on top of output measures. Our results
suggest that Transformer processing and human processing may be facilitated or
impeded by similar properties of an input stimulus, and this similarity has
emerged through general-purpose objectives such as next-token prediction or
image recognition. Our work suggests a new way of using AI models to study
human cognition: not just as a black box mapping stimuli to responses, but
potentially also as explicit processing models.

</details>

### [268] [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
*Mingyu Kim,Jongwoo Ko,Mijung Park*

Main category: cs.AI

TLDR: 提出了一种基于贝叶斯学习原理的新训练目标函数，以解决提示学习中过拟合问题，平衡适应性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法易过拟合，泛化性差，需改进。

Method: 通过贝叶斯学习原理，设计先验和后验分布，平衡预训练模型和微调模型的关系。

Result: 新目标函数在适应下游任务的同时保持接近预训练模型，提升泛化性。

Conclusion: 该方法有效平衡了适应性和泛化性，解决了过拟合问题。

Abstract: Prompt learning is a popular fine-tuning method for vision-language models
due to its efficiency. It requires a small number of additional learnable
parameters while significantly enhancing performance on target tasks. However,
most existing methods suffer from overfitting to fine-tuning data, yielding
poor generalizability. To address this, we propose a new training objective
function based on a Bayesian learning principle to balance adaptability and
generalizability. We derive a prior over the logits, where the mean function is
parameterized by the pre-trained model, while the posterior corresponds to the
fine-tuned model. This objective establishes a balance by allowing the
fine-tuned model to adapt to downstream tasks while remaining close to the
pre-trained model.

</details>

### [269] [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
*Saad Hameed,Basheer Qolomany,Samir Brahim Belhaouari,Mohamed Abdallah,Junaid Qadir,Ala Al-Fuqaha*

Main category: cs.AI

TLDR: 提出了一种结合LLM（如ChatGPT-3.5和Llama3）与PSO的方法，用于深度学习超参数调优，显著提高了收敛速度并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型架构设计依赖人工调优或高成本优化方法，现有PSO和LLM的结合研究不足，需要更高效的优化方法。

Method: 将LLM集成到PSO中，通过LLM替换表现不佳的粒子位置，加速搜索空间探索。

Result: 实验表明，该方法在三个场景中显著提升收敛速度，计算复杂度降低20%-60%，同时保持准确性。

Conclusion: 该方法为深度学习模型优化提供了高效解决方案，适用于多种应用场景。

Abstract: Determining the ideal architecture for deep learning models, such as the
number of layers and neurons, is a difficult and resource-intensive process
that frequently relies on human tuning or computationally costly optimization
approaches. While Particle Swarm Optimization (PSO) and Large Language Models
(LLMs) have been individually applied in optimization and deep learning, their
combined use for enhancing convergence in numerical optimization tasks remains
underexplored. Our work addresses this gap by integrating LLMs into PSO to
reduce model evaluations and improve convergence for deep learning
hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the
difficulties of efficiency and convergence by using LLMs (particularly
ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster
achievement of target objectives. Our method speeds up search space exploration
by substituting underperforming particle placements with best suggestions
offered by LLMs. Comprehensive experiments across three scenarios -- (1)
optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)
networks for time series regression, and (3) using Convolutional Neural
Networks (CNNs) for material classification -- show that the method
significantly improves convergence rates and lowers computational costs.
Depending on the application, computational complexity is lowered by 20% to 60%
compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in
model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by
60% for both regression and classification tasks, all while preserving accuracy
and error rates. This groundbreaking methodology offers a very efficient and
effective solution for optimizing deep learning models, leading to substantial
computational performance improvements across a wide range of applications.

</details>

### [270] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui,Xingdi Yuan,Zhang Xiao,Prithviraj Ammanabrolu,Marc-Alexandre Côté*

Main category: cs.AI

TLDR: TALES是一个多样化的文本冒险游戏集合，用于挑战和评估大型语言模型（LLMs）的推理能力。尽管在合成游戏中表现良好，但LLMs在人类设计的游戏中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂化，需要更复杂的推理能力来支持顺序决策，因此需要评估LLMs的多样化推理能力。

Method: 使用TALES（合成和人类编写的文本冒险游戏集合）对多种LLMs进行测试，并进行定性分析。

Result: LLMs在合成游戏中表现良好，但在人类设计的游戏中表现不佳（成功率低于15%）。

Conclusion: TALES为评估LLMs的推理能力提供了有效工具，但LLMs在人类设计的复杂任务中仍有改进空间。

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>

### [271] [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
*Li He,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.AI

TLDR: 论文提出了一种名为DAR的简单对齐算法，利用在线AI奖励优化策略改进，替代传统的RLHF方法，减少了实现复杂度并提高了学习效率。


<details>
  <summary>Details</summary>
Motivation: 在线AI反馈（OAIF）虽然是一种有前景的替代RLHF的方法，但直接替换人类反馈会限制语言模型学习更细粒度的AI监督信号。

Method: 提出Direct Advantage Regression（DAR），一种基于在线AI奖励的加权监督微调算法，无需强化学习。

Result: 实验表明，AI奖励比AI偏好更能提高人机一致性，DAR在GPT-4-Turbo和MT-bench评估中优于OAIF和在线RLHF基线。

Conclusion: DAR是一种高效且理论一致的替代方案，适用于语言模型对齐任务。

Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement
Learning from Human Feedback (RLHF) by utilizing online AI preference in
aligning language models (LLMs). However, the straightforward replacement of
humans with AI deprives LLMs from learning more fine-grained AI supervision
beyond binary signals. In this paper, we propose Direct Advantage Regression
(DAR), a simple alignment algorithm using online AI reward to optimize policy
improvement through weighted supervised fine-tuning. As an RL-free approach,
DAR maintains theoretical consistency with online RLHF pipelines while
significantly reducing implementation complexity and improving learning
efficiency. Our empirical results underscore that AI reward is a better form of
AI supervision consistently achieving higher human-AI agreement as opposed to
AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show
that DAR outperforms both OAIF and online RLHF baselines.

</details>

### [272] [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
*Yansheng Qiu,Haoquan Zhang,Zhaopan Xu,Ming Li,Diping Song,Zheng Wang,Kaipeng Zhang*

Main category: cs.AI

TLDR: AI Idea Bench 2025是一个评估LLMs生成AI研究想法的框架，解决了现有评估方法的不足，如知识泄漏和缺乏开放基准。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在生成新想法时存在评估不足的问题，如知识泄漏和缺乏开放基准，限制了突破性研究的发现。

Method: 提出了AI Idea Bench 2025框架，包含3,495篇AI论文数据集和评估方法，从原创性和参考性两个维度评估想法质量。

Result: 该框架能定量评估LLMs生成的想法，为自动化科学发现提供支持。

Conclusion: AI Idea Bench 2025是评估和比较想法生成技术的重要工具，有助于推动科学发现自动化。

Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction
and achieved significant success in the generation of novel ideas. However,
current assessments of idea generation overlook crucial factors such as
knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded
truth, and the limited scope of feasibility analysis constrained by prompt
design. These limitations hinder the potential of uncovering groundbreaking
research ideas. In this paper, we present AI Idea Bench 2025, a framework
designed to quantitatively evaluate and compare the ideas generated by LLMs
within the domain of AI research from diverse perspectives. The framework
comprises a comprehensive dataset of 3,495 AI papers and their associated
inspired works, along with a robust evaluation methodology. This evaluation
system gauges idea quality in two dimensions: alignment with the ground-truth
content of the original papers and judgment based on general reference
material. AI Idea Bench 2025's benchmarking system stands to be an invaluable
resource for assessing and comparing idea-generation techniques, thereby
facilitating the automation of scientific discovery.

</details>

### [273] [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
*Antoun Yaacoub,Jérôme Da-Rugna,Zainab Assaghir*

Main category: cs.AI

TLDR: 研究评估了将布鲁姆分类法整合到AI驱动的Moodle插件OneClickQuiz中的效果，发现高级模型（如DistilBERT）能显著提升问题分类准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨布鲁姆分类法是否能提升AI生成问题与认知目标的匹配度。

Method: 使用多种分类模型（如逻辑回归、Naive Bayes、SVC和DistilBERT）对3691个问题进行分类。

Result: 高级布鲁姆层级的问题复杂度更高，DistilBERT表现最佳，整体验证准确率达91%。

Conclusion: 整合布鲁姆分类法可提升AI教育工具效果，高级模型如DistilBERT更具优势。

Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,
an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice
Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured
framework for categorizing educational objectives into hierarchical cognitive
levels. Our research investigates whether incorporating this taxonomy can
improve the alignment of AI-generated questions with specific cognitive
objectives. We developed a dataset of 3691 questions categorized according to
Bloom's levels and employed various classification models-Multinomial Logistic
Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a
Transformer-based model (DistilBERT)-to evaluate their effectiveness in
categorizing questions. Our results indicate that higher Bloom's levels
generally correlate with increased question length, Flesch-Kincaid Grade Level
(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher
cognitive demands. Multinomial Logistic Regression showed varying accuracy
across Bloom's levels, performing best for "Knowledge" and less accurately for
higher-order levels. Merging higher-level categories improved accuracy for
complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective
classification for lower levels but struggled with higher-order tasks.
DistilBERT achieved the highest performance, significantly improving
classification of both lower and higher-order cognitive levels, achieving an
overall validation accuracy of 91%. This study highlights the potential of
integrating Bloom's Taxonomy into AI-driven assessment tools and underscores
the advantages of advanced models like DistilBERT for enhancing educational
content generation.

</details>

### [274] [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
*Yuhang Liu,Pengxiang Li,Congkai Xie,Xavier Hu,Xiaotian Han,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TLDR: 论文提出了InfiGUI-R1，一种基于MLLM的GUI代理，通过两阶段训练框架Actor2Reasoner，从反应式执行者演化为深思熟虑的推理者。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理依赖手动设计的推理模板或隐式推理，缺乏对复杂环境的鲁棒性和适应性，需要转向基于深思熟虑的推理。

Method: 采用两阶段训练：1. 推理注入（通过空间推理蒸馏提升跨模态推理能力）；2. 深思熟虑增强（通过强化学习优化推理，包括子目标引导和错误恢复场景构建）。

Result: InfiGUI-R1在GUI基础和轨迹任务中表现优异。

Conclusion: 通过两阶段训练框架，实现了从反应式执行者到深思熟虑推理者的转变，提升了GUI代理的性能。

Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User
Interface (GUI) Agents, showing promise in automating tasks on computing
devices. Recent works have begun exploring reasoning in GUI tasks with
encouraging results. However, many current approaches rely on manually designed
reasoning templates, which may result in reasoning that is not sufficiently
robust and adaptive for complex GUI environments. Meanwhile, some existing
agents continue to operate as Reactive Actors, relying primarily on implicit
reasoning that may lack sufficient depth for GUI tasks demanding planning and
error recovery. We argue that advancing these agents requires a shift from
reactive acting towards acting based on deliberate reasoning. To facilitate
this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed
through our Actor2Reasoner framework, a reasoning-centric, two-stage training
approach designed to progressively evolve agents from Reactive Actors to
Deliberative Reasoners. The first stage, Reasoning Injection, focuses on
establishing a basic reasoner. We employ Spatial Reasoning Distillation to
transfer cross-modal spatial reasoning capabilities from teacher models to
MLLMs through trajectories with explicit reasoning steps, enabling models to
integrate GUI visual-spatial information with logical reasoning before action
generation. The second stage, Deliberation Enhancement, refines the basic
reasoner into a deliberative one using Reinforcement Learning. This stage
introduces two approaches: Sub-goal Guidance, which rewards models for
generating accurate intermediate sub-goals, and Error Recovery Scenario
Construction, which creates failure-and-recovery training scenarios from
identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves
strong performance in GUI grounding and trajectory tasks. Resources at
https://github.com/Reallm-Labs/InfiGUI-R1.

</details>

### [275] [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
*Ahsan Bilal,Muhammad Ahmed Mohsin,Muhammad Umer,Muhammad Awais Khan Bangash,Muhammad Ali Jamshed*

Main category: cs.AI

TLDR: 本文探讨了从多智能体强化学习（MARL）角度提升大语言模型（LLM）的元思考能力，包括自我反思、评估和控制，以提高其可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM存在幻觉和缺乏自我评估机制等问题，需通过元思考能力增强其复杂任务处理能力。

Method: 分析了RLHF、自蒸馏和思维链提示等方法，并探讨了多智能体架构（如监督-代理层次、代理辩论和心理理论框架）的应用。

Result: 提出了通过MARL的奖励机制、自我对抗和持续学习构建自省、自适应和可信赖LLM的路线图。

Conclusion: 讨论了评估指标、数据集及未来研究方向，如神经科学启发架构和混合符号推理。

Abstract: This survey explores the development of meta-thinking capabilities in Large
Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)
perspective. Meta-thinking self-reflection, assessment, and control of thinking
processes is an important next step in enhancing LLM reliability, flexibility,
and performance, particularly for complex or high-stakes tasks. The survey
begins by analyzing current LLM limitations, such as hallucinations and the
lack of internal self-assessment mechanisms. It then talks about newer methods,
including RL from human feedback (RLHF), self-distillation, and
chain-of-thought prompting, and each of their limitations. The crux of the
survey is to talk about how multi-agent architectures, namely supervisor-agent
hierarchies, agent debates, and theory of mind frameworks, can emulate
human-like introspective behavior and enhance LLM robustness. By exploring
reward mechanisms, self-play, and continuous learning methods in MARL, this
survey gives a comprehensive roadmap to building introspective, adaptive, and
trustworthy LLMs. Evaluation metrics, datasets, and future research avenues,
including neuroscience-inspired architectures and hybrid symbolic reasoning,
are also discussed.

</details>

### [276] [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
*Haoming Li,Zhaoliang Chen,Jonathan Zhang,Fei Liu*

Main category: cs.AI

TLDR: 本文综述了现有规划基准的分类与适用性，为算法选择和未来基准开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 规划能力对智能体至关重要，但目前缺乏对规划基准的全面理解，导致算法比较和新场景应用困难。

Method: 通过分析现有规划基准，将其分类为具身环境、网络导航、调度、游戏与谜题及日常任务自动化，并评估其适用性。

Result: 研究推荐了适合不同算法的最优基准，并指出了未来基准开发的潜在方向。

Conclusion: 本文为规划算法的比较和选择提供了实用指南，同时为未来基准设计提供了参考。

Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g.,
creating travel itineraries within a budget, holds immense potential in both
scientific and commercial contexts. Moreover, optimal plans tend to require
fewer resources compared to ad-hoc methods. To date, a comprehensive
understanding of existing planning benchmarks appears to be lacking. Without
it, comparing planning algorithms' performance across domains or selecting
suitable algorithms for new scenarios remains challenging. In this paper, we
examine a range of planning benchmarks to identify commonly used testbeds for
algorithm development and highlight potential gaps. These benchmarks are
categorized into embodied environments, web navigation, scheduling, games and
puzzles, and everyday task automation. Our study recommends the most
appropriate benchmarks for various algorithms and offers insights to guide
future benchmark development.

</details>

### [277] [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
*Jiaqi Wei,Hao Zhou,Xiang Zhang,Di Zhang,Zijie Qiu,Wei Wei,Jinzhe Li,Wanli Ouyang,Siqi Sun*

Main category: cs.AI

TLDR: AlignRAG提出了一种新的测试时框架，通过迭代的批判驱动对齐（CDA）步骤解决检索增强生成（RAG）中的推理不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在推理轨迹与检索证据的对齐上存在不足，导致推理不对齐问题。

Method: AlignRAG通过构建上下文丰富的训练语料、生成对比性批判、训练专用的批判语言模型（CLM）以及应用CDA步骤迭代优化推理轨迹。

Result: 实验表明，AlignRAG优于所有基线方法，并可无缝集成到现有RAG流程中。

Conclusion: AlignRAG为检索感知生成提供了实用的进展，重新定义了RAG的结构化推理轨迹。

Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm
for knowledge-grounded text generation. However, existing RAG pipelines often
fail to ensure that the reasoning trajectories align with the evidential
constraints imposed by retrieved content. In this paper, we reframe RAG as a
problem of retrieval-aware reasoning and identify a core challenge: reasoning
misalignment-the mismatch between a model's reasoning trajectory and the
retrieved evidence. To address this challenge, we propose AlignRAG, a novel
test-time framework that mitigates reasoning misalignment through iterative
Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that
rely on static training or post-hoc selection, AlignRAG actively refines
reasoning trajectories during inference by enforcing fine-grained alignment
with evidence. Our framework introduces a new paradigm for retrieval-aware
reasoning by: (1) constructing context-rich training corpora; (2) generating
contrastive critiques from preference-aware reasoning trajectories; (3)
training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning
misalignments; and (4) applying CDA steps to optimize reasoning trajectories
iteratively. Empirical results demonstrate that AlignRAG consistently
outperforms all baselines and could integrate as a plug-and-play module into
existing RAG pipelines without further changes. By reconceptualizing RAG as a
structured reasoning trajectory and establishing the test-time framework for
correcting reasoning misalignments in RAG, AlignRAG provides practical
advancements for retrieval-aware generation.

</details>

### [278] [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
*Hongru Wang,Cheng Qian,Wanjun Zhong,Xiusi Chen,Jiahao Qiu,Shijue Huang,Bowen Jin,Mengdi Wang,Kam-Fai Wong,Heng Ji*

Main category: cs.AI

TLDR: 论文提出了一种名为OTC-PO的强化学习框架，通过优化工具调用效率，在保持答案准确性的同时减少工具使用。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法在优化答案正确性时忽视了工具使用的效率和成本，导致工具调用过多或不足的问题。

Method: 提出了OTC-PO框架，结合正确性和工具效率的奖励机制，并在PPO和GRPO中实现。

Result: 实验显示，该方法减少了73.1%的工具调用，提高了229.4%的工具效率，同时保持答案准确性。

Conclusion: OTC-PO是首个明确优化工具使用效率的强化学习框架，为工具集成推理提供了高效解决方案。

Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with
the ability to invoke external tools, such as search engines and code
interpreters, to solve tasks beyond the capabilities of language-only
reasoning. While reinforcement learning (RL) has shown promise in improving TIR
by optimizing final answer correctness, existing approaches often overlook the
efficiency and cost associated with tool usage. This can lead to suboptimal
behavior, including excessive tool calls that increase computational and
financial overhead, or insufficient tool use that compromises answer quality.
In this work, we propose Optimal Tool Call-controlled Policy Optimization
(OTC-PO), a simple yet effective RL-based framework that encourages models to
produce accurate answers with minimal tool calls. Our method introduces a
tool-integrated reward that jointly considers correctness and tool efficiency,
promoting high tool productivity. We instantiate this framework within both
Proximal Policy Optimization (PPO) and Group Relative Preference Optimization
(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and
Qwen-Math across multiple QA benchmarks show that our approach reduces tool
calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while
maintaining comparable answer accuracy. To the best of our knowledge, this is
the first RL-based framework that explicitly optimizes tool-use efficiency in
TIR.

</details>

### [279] [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
*Yao Shi,Rongkeng Liang,Yong Xu*

Main category: cs.AI

TLDR: EducationQ框架通过多代理对话评估LLMs的教学能力，发现教学效果与模型规模无关，小模型可能优于大模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs教学能力评估缺乏对互动教学的关注，需要更高效的评估方法。

Method: 引入EducationQ框架，模拟动态教育场景，测试14个LLMs的1,498个问题，结合定量与定性分析。

Result: 教学效果与模型规模无关，小模型可能更优；78%专家评估与自动分析一致。

Conclusion: LLMs教学需针对性优化，而非简单扩展规模。

Abstract: Large language models (LLMs) increasingly serve as educational tools, yet
evaluating their teaching capabilities remains challenging due to the
resource-intensive, context-dependent, and methodologically complex nature of
teacher-student interactions. We introduce EducationQ, a multi-agent dialogue
framework that efficiently assesses teaching capabilities through simulated
dynamic educational scenarios, featuring specialized agents for teaching,
learning, and evaluation. Testing 14 LLMs across major AI Organizations
(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13
disciplines and 10 difficulty levels reveals that teaching effectiveness does
not correlate linearly with model scale or general reasoning capabilities -
with some smaller open-source models outperforming larger commercial
counterparts in teaching contexts. This finding highlights a critical gap in
current evaluations that prioritize knowledge recall over interactive pedagogy.
Our mixed-methods evaluation, combining quantitative metrics with qualitative
analysis and expert case studies, identifies distinct pedagogical strengths
employed by top-performing models (e.g., sophisticated questioning strategies,
adaptive feedback mechanisms). Human expert evaluations show 78% agreement with
our automated qualitative analysis of effective teaching behaviors, validating
our methodology. EducationQ demonstrates that LLMs-as-teachers require
specialized optimization beyond simple scaling, suggesting next-generation
educational AI prioritize targeted enhancement of specific pedagogical
effectiveness.

</details>

### [280] [SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam](https://arxiv.org/abs/2504.15252)
*Tue Vo,Lakshay Sharma,Tuan Dinh,Khuong Dinh,Trang Nguyen,Trung Phan,Minh Do,Duong Vu*

Main category: cs.AI

TLDR: SuoiAI是一个端到端流程，用于构建越南水生无脊椎动物数据集，并利用机器学习进行物种分类。


<details>
  <summary>Details</summary>
Motivation: 理解和监测水生生物多样性对生态健康和保护至关重要。

Method: 通过半监督学习减少标注工作量，利用先进的目标检测和分类模型进行数据收集、标注和模型训练。

Result: 解决了数据稀缺、细粒度分类和多样化环境部署等挑战。

Conclusion: SuoiAI为水生生物多样性监测提供了一种高效且实用的解决方案。

Abstract: Understanding and monitoring aquatic biodiversity is critical for ecological
health and conservation efforts. This paper proposes SuoiAI, an end-to-end
pipeline for building a dataset of aquatic invertebrates in Vietnam and
employing machine learning (ML) techniques for species classification. We
outline the methods for data collection, annotation, and model training,
focusing on reducing annotation effort through semi-supervised learning and
leveraging state-of-the-art object detection and classification models. Our
approach aims to overcome challenges such as data scarcity, fine-grained
classification, and deployment in diverse environmental conditions.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [281] [Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions](https://arxiv.org/abs/2504.14078)
*M-Mahdi Naddaf-Sh,Andrew Lee,Kin Yen,Eemon Amini,Iman Soltani*

Main category: cs.RO

TLDR: 研究探讨红外（IR）摄像头技术在低能见度条件下提升紧急车辆驾驶员安全的潜力，特别是在夜间和浓雾中。


<details>
  <summary>Details</summary>
Motivation: 低能见度环境（如夜间和浓雾）显著增加碰撞风险，尤其是拖车和扫雪车等紧急车辆。传统辅助系统因能见度限制效果不佳。

Method: 结合实验室实验、实地测试和紧急车辆操作员调查，评估IR摄像头的检测性能及在现有交通部门车队中经济性改造的可行性。

Result: IR技术显著提升驾驶员警觉性，并提供了可扩展部署的数据支持建议。

Conclusion: IR摄像头技术是提升紧急车辆在低能见度条件下安全性的有效解决方案，适合大规模推广。

Abstract: This study investigates the potential of infrared (IR) camera technology to
enhance driver safety for emergency vehicles operating in low-visibility
conditions, particularly at night and in dense fog. Such environments
significantly increase the risk of collisions, especially for tow trucks and
snowplows that must remain operational in challenging conditions. Conventional
driver assistance systems often struggle under these conditions due to limited
visibility. In contrast, IR cameras, which detect the thermal signatures of
obstacles, offer a promising alternative. The evaluation combines controlled
laboratory experiments, real-world field tests, and surveys of emergency
vehicle operators. In addition to assessing detection performance, the study
examines the feasibility of retrofitting existing Department of Transportation
(DoT) fleets with cost-effective IR-based driver assistance systems. Results
underscore the utility of IR technology in enhancing driver awareness and
provide data-driven recommendations for scalable deployment across legacy
emergency vehicle fleets.

</details>

### [282] [Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering](https://arxiv.org/abs/2504.14135)
*Jonathan Embley-Riches,Jianwei Liu,Simon Julier,Dimitrios Kanoulas*

Main category: cs.RO

TLDR: 论文提出了一种名为Unreal Robotics Lab (URL)的新型仿真框架，结合了Unreal Engine的高质量渲染和MuJoCo的高精度物理模拟，用于机器人研究。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真对机器人研究至关重要，但目前同时实现高质量渲染和精确物理模拟仍具挑战性。

Method: 通过整合Unreal Engine的渲染能力和MuJoCo的物理模拟，创建了URL框架，支持复杂环境效果（如烟雾、火焰和水动力学）。

Result: 该框架成功用于视觉导航和SLAM方法的基准测试，展示了其在多样化场景中测试机器人性能的能力。

Conclusion: URL框架填补了物理精度与真实感渲染之间的鸿沟，为机器人研究和仿真到现实的迁移提供了强大工具。

Abstract: High-fidelity simulation is essential for robotics research, enabling safe
and efficient testing of perception, control, and navigation algorithms.
However, achieving both photorealistic rendering and accurate physics modeling
remains a challenge. This paper presents a novel simulation framework--the
Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced
rendering capabilities with MuJoCo's high-precision physics simulation. Our
approach enables realistic robotic perception while maintaining accurate
physical interactions, facilitating benchmarking and dataset generation for
vision-based robotics applications. The system supports complex environmental
effects, such as smoke, fire, and water dynamics, which are critical for
evaluating robotic performance under adverse conditions. We benchmark visual
navigation and SLAM methods within our framework, demonstrating its utility for
testing real-world robustness in controlled yet diverse scenarios. By bridging
the gap between physics accuracy and photorealistic rendering, our framework
provides a powerful tool for advancing robotics research and sim-to-real
transfer.

</details>

### [283] [SG-Reg: Generalizable and Efficient Scene Graph Registration](https://arxiv.org/abs/2504.14440)
*Chuhao Liu,Zhijian Qiao,Jieqi Shi,Ke Wang,Peize Liu,Shaojie Shen*

Main category: cs.RO

TLDR: 提出了一种基于多模态语义节点特征融合的场景图网络，用于刚性语义场景图的注册，减少了对GPU资源和通信带宽的需求，并在实验中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统语义辅助注册方法中手工特征描述符或学习依赖真实标注的问题，以适应实际环境中的需求。

Method: 设计了一个场景图网络，融合开放集语义特征、局部拓扑与空间感知以及形状特征，采用粗到细的匹配策略，后端使用鲁棒的姿态估计器。

Result: 在两智能体SLAM基准测试中显著优于手工基线方法，注册召回率略高于视觉闭环网络，且每次查询帧仅需52 KB通信带宽。

Conclusion: 该方法通过多模态特征融合和高效的数据生成方式，实现了更高效的语义场景图注册，适用于实际应用场景。

Abstract: This paper addresses the challenges of registering two rigid semantic scene
graphs, an essential capability when an autonomous agent needs to register its
map against a remote agent, or against a prior map. The hand-crafted
descriptors in classical semantic-aided registration, or the ground-truth
annotation reliance in learning-based scene graph registration, impede their
application in practical real-world environments. To address the challenges, we
design a scene graph network to encode multiple modalities of semantic nodes:
open-set semantic feature, local topology with spatial awareness, and shape
feature. These modalities are fused to create compact semantic node features.
The matching layers then search for correspondences in a coarse-to-fine manner.
In the back-end, we employ a robust pose estimator to decide transformation
according to the correspondences. We manage to maintain a sparse and
hierarchical scene representation. Our approach demands fewer GPU resources and
fewer communication bandwidth in multi-agent tasks. Moreover, we design a new
data generation approach using vision foundation models and a semantic mapping
module to reconstruct semantic scene graphs. It differs significantly from
previous works, which rely on ground-truth semantic annotations to generate
data. We validate our method in a two-agent SLAM benchmark. It significantly
outperforms the hand-crafted baseline in terms of registration success rate.
Compared to visual loop closure networks, our method achieves a slightly higher
registration recall while requiring only 52 KB of communication bandwidth for
each query frame. Code available at:
\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.

</details>

### [284] [Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction](https://arxiv.org/abs/2504.14588)
*Wenke Xia,Ruoxuan Feng,Dong Wang,Di Hu*

Main category: cs.RO

TLDR: 论文提出Phoenix框架，通过运动指令连接高层语义反思与底层机器人动作修正，结合多任务运动条件扩散策略实现精细动作修正，并在仿真和真实场景中验证其泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从失败中恢复的通用性问题，特别是如何将语义反思转化为精细动作修正的挑战。

Method: 采用双过程运动调整机制将语义反思转化为粗粒度运动指令，再通过多任务运动条件扩散策略结合视觉观察实现高频动作修正。

Result: 在RoboMimic仿真和真实场景中验证了框架的泛化性和鲁棒性。

Conclusion: Phoenix框架通过运动指令和扩散策略实现了高效的机器人动作修正，并通过终身学习方法持续提升模型能力。

Abstract: Building a generalizable self-correction system is crucial for robots to
recover from failures. Despite advancements in Multimodal Large Language Models
(MLLMs) that empower robots with semantic reflection ability for failure,
translating semantic reflection into how to correct fine-grained robotic
actions remains a significant challenge. To address this gap, we build the
Phoenix framework, which leverages motion instruction as a bridge to connect
high-level semantic reflection with low-level robotic action correction. In
this motion-based self-reflection framework, we start with a dual-process
motion adjustment mechanism with MLLMs to translate the semantic reflection
into coarse-grained motion instruction adjustment. To leverage this motion
instruction for guiding how to correct fine-grained robotic actions, a
multi-task motion-conditioned diffusion policy is proposed to integrate visual
observations for high-frequency robotic action correction. By combining these
two models, we could shift the demand for generalization capability from the
low-level manipulation policy to the MLLMs-driven motion adjustment model and
facilitate precise, fine-grained robotic action correction. Utilizing this
framework, we further develop a lifelong learning method to automatically
improve the model's capability from interactions with dynamic environments. The
experiments conducted in both the RoboMimic simulation and real-world scenarios
prove the superior generalization and robustness of our framework across a
variety of manipulation tasks. Our code is released at
\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.

</details>

### [285] [Latent Representations for Visual Proprioception in Inexpensive Robots](https://arxiv.org/abs/2504.14634)
*Sahara Sheikholeslami,Ladislau Bölöni*

Main category: cs.RO

TLDR: 本文探讨了如何通过单次快速回归架构从单个外部摄像头图像实现视觉本体感知，并比较了多种潜在表示方法。


<details>
  <summary>Details</summary>
Motivation: 低成本机器人在非结构化环境中通常缺乏精确的本体感知能力，本文旨在探索视觉替代方案的可行性。

Method: 研究了多种潜在表示（CNN、VAE、ViT和无标定标记点），并采用适应有限数据的微调技术。

Result: 在低成本6自由度机器人上的实验验证了方法的准确性。

Conclusion: 视觉本体感知在简单操作环境中具有潜力，但仍需进一步优化。

Abstract: Robotic manipulation requires explicit or implicit knowledge of the robot's
joint positions. Precise proprioception is standard in high-quality industrial
robots but is often unavailable in inexpensive robots operating in unstructured
environments. In this paper, we ask: to what extent can a fast, single-pass
regression architecture perform visual proprioception from a single external
camera image, available even in the simplest manipulation settings? We explore
several latent representations, including CNNs, VAEs, ViTs, and bags of
uncalibrated fiducial markers, using fine-tuning techniques adapted to the
limited data available. We evaluate the achievable accuracy through experiments
on an inexpensive 6-DoF robot.

</details>

### [286] [A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment](https://arxiv.org/abs/2504.15129)
*Kangyao Huang,Hao Wang,Yu Luo,Jingyu Chen,Jintao Chen,Xiangkui Zhang,Xiangyang Ji,Huaping Liu*

Main category: cs.RO

TLDR: 提出一个平台，实现端到端深度强化学习策略的无缝转移，支持四旋翼无人机从零训练到现实部署。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机在非结构化户外环境中应用学习方法的挑战，如大量模拟数据需求、实时处理要求和模拟与现实的差距。

Method: 整合训练环境、飞行动力学控制、DRL算法、MAVROS中间件和硬件，形成完整工作流和架构。

Result: 平台支持多种环境任务（如悬停、避障、轨迹跟踪等），并通过实验验证了高效性和鲁棒性。

Conclusion: 该平台为四旋翼无人机的学习策略提供了从模拟到现实的快速部署解决方案。

Abstract: Deploying robot learning methods to a quadrotor in unstructured outdoor
environments is an exciting task. Quadrotors operating in real-world
environments by learning-based methods encounter several challenges: a large
amount of simulator generated data required for training, strict demands for
real-time processing onboard, and the sim-to-real gap caused by dynamic and
noisy conditions. Current works have made a great breakthrough in applying
learning-based methods to end-to-end control of quadrotors, but rarely mention
the infrastructure system training from scratch and deploying to reality, which
makes it difficult to reproduce methods and applications. To bridge this gap,
we propose a platform that enables the seamless transfer of end-to-end deep
reinforcement learning (DRL) policies. We integrate the training environment,
flight dynamics control, DRL algorithms, the MAVROS middleware stack, and
hardware into a comprehensive workflow and architecture that enables
quadrotors' policies to be trained from scratch to real-world deployment in
several minutes. Our platform provides rich types of environments including
hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and
planning in unknown environments, as a physical experiment benchmark. Through
extensive empirical validation, we demonstrate the efficiency of proposed
sim-to-real platform, and robust outdoor flight performance under real-world
perturbations. Details can be found from our website
https://emnavi.tech/AirGym/.

</details>

<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [287] [Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields](https://arxiv.org/abs/2504.15262)
*Brandon Zhao,Aviad Levis,Liam Connor,Pratul P. Srinivasan,Katherine L. Bouman*

Main category: astro-ph.CO

TLDR: 该论文提出了一种利用神经场和物理前向模型从二维弱引力透镜信号中重建三维暗物质分布的方法，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 暗物质的三维分布对理解宇宙结构至关重要，但传统方法因单视角观测和噪声问题难以准确重建。

Method: 采用引力约束的神经场和可微物理前向模型，通过分析-合成方法优化神经网络权重以匹配观测信号。

Result: 在模拟实验中，该方法不仅优于传统方法，还能恢复潜在的意外暗物质结构。

Conclusion: 该方法为暗物质三维重建提供了灵活且高效的解决方案，适用于未来望远镜观测数据。

Abstract: Weak gravitational lensing is the slight distortion of galaxy shapes caused
primarily by the gravitational effects of dark matter in the universe. In our
work, we seek to invert the weak lensing signal from 2D telescope images to
reconstruct a 3D map of the universe's dark matter field. While inversion
typically yields a 2D projection of the dark matter field, accurate 3D maps of
the dark matter distribution are essential for localizing structures of
interest and testing theories of our universe. However, 3D inversion poses
significant challenges. First, unlike standard 3D reconstruction that relies on
multiple viewpoints, in this case, images are only observed from a single
viewpoint. This challenge can be partially addressed by observing how galaxy
emitters throughout the volume are lensed. However, this leads to the second
challenge: the shapes and exact locations of unlensed galaxies are unknown, and
can only be estimated with a very large degree of uncertainty. This introduces
an overwhelming amount of noise which nearly drowns out the lensing signal
completely. Previous approaches tackle this by imposing strong assumptions
about the structures in the volume. We instead propose a methodology using a
gravitationally-constrained neural field to flexibly model the continuous
matter distribution. We take an analysis-by-synthesis approach, optimizing the
weights of the neural network through a fully differentiable physical forward
model to reproduce the lensing signal present in image measurements. We
showcase our method on simulations, including realistic simulated measurements
of dark matter distributions that mimic data from upcoming telescope surveys.
Our results show that our method can not only outperform previous methods, but
importantly is also able to recover potentially surprising dark matter
structures.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [288] [PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling](https://arxiv.org/abs/2504.14219)
*Alara Dirik,Tuanfeng Wang,Duygu Ceylan,Stefanos Zafeiriou,Anna Frühstück*

Main category: cs.GR

TLDR: PRISM是一个统一的框架，能够在单一基础模型中实现多种图像生成和编辑任务，通过联合生成RGB图像和内在层（X层）来保持一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要单独推断内在属性或使用不同模型进行分解和条件生成的问题，提供一种更高效且一致的多任务处理方式。

Method: 基于预训练的文本到图像扩散模型，提出一种有效的微调策略，同时生成RGB图像和内在层（X层）。

Result: 实验表明PRISM在内在图像分解和条件图像生成任务中表现优异，同时保留了基础模型的文本到图像生成能力。

Conclusion: PRISM通过联合生成内在层和RGB图像，实现了多任务的高效处理，并在性能和一致性上优于现有方法。

Abstract: We present PRISM, a unified framework that enables multiple image generation
and editing tasks in a single foundational model. Starting from a pre-trained
text-to-image diffusion model, PRISM proposes an effective fine-tuning strategy
to produce RGB images along with intrinsic maps (referred to as X layers)
simultaneously. Unlike previous approaches, which infer intrinsic properties
individually or require separate models for decomposition and conditional
generation, PRISM maintains consistency across modalities by generating all
intrinsic layers jointly. It supports diverse tasks, including text-to-RGBX
generation, RGB-to-X decomposition, and X-to-RGBX conditional generation.
Additionally, PRISM enables both global and local image editing through
conditioning on selected intrinsic layers and text prompts. Extensive
experiments demonstrate the competitive performance of PRISM both for intrinsic
image decomposition and conditional image generation while preserving the base
model's text-to-image generation capability.

</details>

### [289] [HoLa: B-Rep Generation using a Holistic Latent Representation](https://arxiv.org/abs/2504.14257)
*Yilin Liu,Duoteng Xu,Xingyao Yu,Xiang Xu,Daniel Cohen-Or,Hao Zhang,Hui Huang*

Main category: cs.GR

TLDR: 提出了一种新的CAD模型表示方法，通过统一的HoLa空间结合几何与拓扑关系，显著提升了生成模型的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CAD模型表示方法存在几何与拓扑关系分离的问题，导致生成模型时存在冗余和不一致性。

Method: 利用HoLa空间统一几何与拓扑关系，通过神经交叉网络学习曲线几何，简化了拓扑学习为几何重建问题。

Result: 生成模型的准确性显著提升（82% vs. 50%），同时减少了冗余和训练复杂度。

Conclusion: 提出的方法在CAD模型生成中表现出色，为多模态输入提供了高效解决方案。

Abstract: We introduce a novel representation for learning and generating
Computer-Aided Design (CAD) models in the form of $\textit{boundary
representations}$ (B-Reps). Our representation unifies the continuous geometric
properties of B-Rep primitives in different orders (e.g., surfaces and curves)
and their discrete topological relations in a $\textit{holistic latent}$ (HoLa)
space. This is based on the simple observation that the topological connection
between two surfaces is intrinsically tied to the geometry of their
intersecting curve. Such a prior allows us to reformulate topology learning in
B-Reps as a geometric reconstruction problem in Euclidean space. Specifically,
we eliminate the presence of curves, vertices, and all the topological
connections in the latent space by learning to distinguish and derive curve
geometries from a pair of surface primitives via a neural intersection network.
To this end, our holistic latent space is only defined on surfaces but encodes
a full B-Rep model, including the geometry of surfaces, curves, vertices, and
their topological relations. Our compact and holistic latent space facilitates
the design of a first diffusion-based generator to take on a large variety of
inputs including point clouds, single/multi-view images, 2D sketches, and text
prompts. Our method significantly reduces ambiguities, redundancies, and
incoherences among the generated B-Rep primitives, as well as training
complexities inherent in prior multi-step B-Rep learning pipelines, while
achieving greatly improved validity rate over current state of the art: 82% vs.
$\approx$50%.

</details>

### [290] [SEGA: Drivable 3D Gaussian Head Avatar from a Single Image](https://arxiv.org/abs/2504.14373)
*Chen Guo,Zhuo Su,Jian Wang,Shuang Li,Xu Chang,Zhaohu Li,Yang Zhao,Guidong Wang,Ruqi Huang*

Main category: cs.GR

TLDR: SEGA提出了一种基于单图像的3D可驱动高斯头像生成方法，结合广义先验模型和分层UV空间高斯泼溅框架，实现了对未见身份的鲁棒泛化和实时动画渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多图像或多视角输入，限制了实际应用；SEGA旨在通过单图像输入实现高质量3D头像生成。

Method: 结合2D和3D先验模型，采用分层UV空间高斯泼溅框架，动态和静态分支分别处理表情细节和不变区域，支持个性化微调。

Result: 实验表明，SEGA在泛化能力、身份保持和表情真实性上优于现有方法，实现了实时性能。

Conclusion: SEGA推动了单图像头像生成的实际应用，为虚拟现实等领域提供了高效解决方案。

Abstract: Creating photorealistic 3D head avatars from limited input has become
increasingly important for applications in virtual reality, telepresence, and
digital entertainment. While recent advances like neural rendering and 3D
Gaussian splatting have enabled high-quality digital human avatar creation and
animation, most methods rely on multiple images or multi-view inputs, limiting
their practicality for real-world use. In this paper, we propose SEGA, a novel
approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that
combines generalized prior models with a new hierarchical UV-space Gaussian
Splatting framework. SEGA seamlessly combines priors derived from large-scale
2D datasets with 3D priors learned from multi-view, multi-expression, and
multi-ID data, achieving robust generalization to unseen identities while
ensuring 3D consistency across novel viewpoints and expressions. We further
present a hierarchical UV-space Gaussian Splatting framework that leverages
FLAME-based structural priors and employs a dual-branch architecture to
disentangle dynamic and static facial components effectively. The dynamic
branch encodes expression-driven fine details, while the static branch focuses
on expression-invariant regions, enabling efficient parameter inference and
precomputation. This design maximizes the utility of limited 3D data and
achieves real-time performance for animation and rendering. Additionally, SEGA
performs person-specific fine-tuning to further enhance the fidelity and
realism of the generated avatars. Experiments show our method outperforms
state-of-the-art approaches in generalization ability, identity preservation,
and expression realism, advancing one-shot avatar creation for practical
applications.

</details>

### [291] [A Controllable Appearance Representation for Flexible Transfer and Editing](https://arxiv.org/abs/2504.15028)
*Santiago Jimenez-Navarro,Julia Guerrero-Viu,Belen Masia*

Main category: cs.GR

TLDR: 提出一种自监督学习方法，通过FactorVAE构建紧凑且解耦的潜在空间，用于材料外观的表示，并用于指导扩散模型实现外观迁移和编辑。


<details>
  <summary>Details</summary>
Motivation: 解决材料外观表示中的解耦和可解释性问题，避免人工标注带来的偏差。

Method: 使用FactorVAE自监督学习紧凑解耦的潜在空间，结合IP-Adapter指导扩散模型进行外观迁移和编辑。

Result: 模型在无显式监督下实现了强解耦和可解释性，支持用户直观编辑外观属性。

Conclusion: 该方法提供了细粒度的外观控制，适用于材料外观的迁移和编辑任务。

Abstract: We present a method that computes an interpretable representation of material
appearance within a highly compact, disentangled latent space. This
representation is learned in a self-supervised fashion using an adapted
FactorVAE. We train our model with a carefully designed unlabeled dataset,
avoiding possible biases induced by human-generated labels. Our model
demonstrates strong disentanglement and interpretability by effectively
encoding material appearance and illumination, despite the absence of explicit
supervision. Then, we use our representation as guidance for training a
lightweight IP-Adapter to condition a diffusion pipeline that transfers the
appearance of one or more images onto a target geometry, and allows the user to
further edit the resulting appearance. Our approach offers fine-grained control
over the generated results: thanks to the well-structured compact latent space,
users can intuitively manipulate attributes such as hue or glossiness in image
space to achieve the desired final appearance.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [292] [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CY

TLDR: 论文提出了一个名为“Thousand Voices of Trauma”的合成数据集，包含3,000个基于PTSD治疗的对话，用于填补心理健康数据空白。


<details>
  <summary>Details</summary>
Motivation: AI系统在心理健康支持中的应用受到治疗对话数据不足的限制，尤其是创伤治疗领域。

Method: 通过确定性和概率性生成方法，创建了包含500个独特案例的数据集，每个案例有6种对话视角，涵盖多样化的创伤类型和行为。

Result: 数据集展示了创伤类型和症状的合理分布，并通过临床专家验证了其治疗真实性。

Conclusion: 该隐私保护数据集为创伤治疗AI应用和临床培训提供了宝贵资源。

Abstract: The advancement of AI systems for mental health support is hindered by
limited access to therapeutic conversation data, particularly for trauma
treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset
of 3,000 therapy conversations based on Prolonged Exposure therapy protocols
for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique
cases, each explored through six conversational perspectives that mirror the
progression of therapy from initial anxiety to peak distress to emotional
processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,
49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10
trauma-related behaviors using deterministic and probabilistic generation
methods. Analysis reveals realistic distributions of trauma types (witnessing
violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse
20.8%). Clinical experts validated the dataset's therapeutic fidelity,
highlighting its emotional depth while suggesting refinements for greater
authenticity. We also developed an emotional trajectory benchmark with
standardized metrics for evaluating model responses. This privacy-preserving
dataset addresses critical gaps in trauma-focused mental health data, offering
a valuable resource for advancing both patient-facing applications and
clinician training tools.

</details>

### [293] [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
*Sanchaita Hazra,Bodhisattwa Prasad Majumder,Tuhin Chakrabarty*

Main category: cs.CY

TLDR: 论文探讨了当前AI安全研究中忽视的人类中心问题，特别是AI对工作未来的影响，并提出支持劳动力转型和公平补偿机制的建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究过于关注技术风险，忽视了AI对社会结构和人类生计的长期影响，尤其是劳动力市场和收入不平等问题。

Method: 通过经济理论分析AI对劳动力市场的结构性影响，并提出国际版权框架和集体许可机制以保障数据使用的公平补偿。

Result: 研究发现AI的发展可能导致收入不平等加剧和创造性劳动的平庸化，封闭式开发模式加剧了资源垄断。

Conclusion: 建议建立以工人为中心的全球AI治理框架，促进经济公平和共享繁荣，同时减少技术债务。

Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing
manipulation of human behavior, and eliminating existential risks in
cybersecurity or biosecurity. While pressing, this narrow focus overlooks
critical human-centric considerations that shape the long-term trajectory of a
society. In this position paper, we identify the risks of overlooking the
impact of AI on the future of work and recommend comprehensive transition
support towards the evolution of meaningful labor with human agency. Through
the lens of economic theories, we highlight the intertemporal impacts of AI on
human livelihood and the structural changes in labor markets that exacerbate
income inequality. Additionally, the closed-source approach of major
stakeholders in AI development resembles rent-seeking behavior through
exploiting resources, breeding mediocrity in creative labor, and monopolizing
innovation. To address this, we argue in favor of a robust international
copyright anatomy supported by implementing collective licensing that ensures
fair compensation mechanisms for using data to train AI models. We strongly
recommend a pro-worker framework of global AI governance to enhance shared
prosperity and economic justice while reducing technical debt.

</details>

### [294] [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
*Ali Safari*

Main category: cs.CY

TLDR: 研究探讨Airbnb评论的情感极性（正面/负面）对房源接受率和租金的影响，发现正面评论占比高（>90%）且情感质量比评论数量更重要。


<details>
  <summary>Details</summary>
Motivation: 探究Airbnb评论的情感内容如何影响房源的接受率和定价策略。

Method: 收集数千条评论，使用NLP分类情感，并通过t检验和相关分析验证影响。

Result: 正面评论占比高，但情感极性对接受率影响更大；预算房源评论多但价格稳定，高端房源评论少但价格高。

Conclusion: 在高度正面的评论环境中，情感质量比数量更能影响客户行为和定价策略。

Abstract: This research examines whether Airbnb guests' positive and negative comments
influence acceptance rates and rental prices across six U.S. regions: Rhode
Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of
reviews were collected and analyzed using Natural Language Processing (NLP) to
classify sentiments as positive or negative, followed by statistical testing
(t-tests and basic correlations) on the average scores. The findings reveal
that over 90 percent of reviews in each region are positive, indicating that
having additional reviews does not significantly enhance prices. However,
listings with predominantly positive feedback exhibit slightly higher
acceptance rates, suggesting that sentiment polarity, rather than the sheer
volume of reviews, is a more critical factor for host success. Additionally,
budget listings often gather extensive reviews while maintaining competitive
pricing, whereas premium listings sustain higher prices with fewer but highly
positive reviews. These results underscore the importance of sentiment quality
over quantity in shaping guest behavior and pricing strategies in an
overwhelmingly positive review environment.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [295] [Segmentation with Noisy Labels via Spatially Correlated Distributions](https://arxiv.org/abs/2504.14795)
*Ryu Tadokoro,Tsukasa Takagi,Shin-ichi Maeda*

Main category: eess.IV

TLDR: 提出了一种基于贝叶斯估计的概率模型，用于处理语义分割中因空间相关性导致的标注错误，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在语义分割中，高质量标注对模型准确性至关重要，但实际场景（如医学影像和遥感）中标注错误常见且具有空间相关性，传统方法难以处理。

Method: 采用近似贝叶斯估计，假设标注错误存在空间相关性，并通过高斯分布和KMS矩阵建模这种相关性，解决计算难题。

Result: 实验表明，利用标注错误的空间相关性显著提升了性能，在某些任务（如肺部分割）中，性能接近使用干净标注的水平。

Conclusion: 提出的方法有效解决了标注错误的空间相关问题，为实际应用中的语义分割提供了可靠解决方案。

Abstract: In semantic segmentation, the accuracy of models heavily depends on the
high-quality annotations. However, in many practical scenarios such as medical
imaging and remote sensing, obtaining true annotations is not straightforward
and usually requires significant human labor. Relying on human labor often
introduces annotation errors, including mislabeling, omissions, and
inconsistency between annotators. In the case of remote sensing, differences in
procurement time can lead to misaligned ground truth annotations. These label
errors are not independently distributed, and instead usually appear in
spatially connected regions where adjacent pixels are more likely to share the
same errors. To address these issues, we propose an approximate Bayesian
estimation based on a probabilistic model that assumes training data includes
label errors, incorporating the tendency for these errors to occur with spatial
correlations between adjacent pixels. Bayesian inference requires computing the
posterior distribution of label errors, which becomes intractable when spatial
correlations are present. We represent the correlation of label errors between
adjacent pixels through a Gaussian distribution whose covariance is structured
by a Kac-Murdock-Szeg\"{o} (KMS) matrix, solving the computational challenges.
Through experiments on multiple segmentation tasks, we confirm that leveraging
the spatial correlation of label errors significantly improves performance.
Notably, in specific tasks such as lung segmentation, the proposed method
achieves performance comparable to training with clean labels under moderate
noise levels. Code is available at
https://github.com/pfnet-research/Bayesian_SpatialCorr.

</details>
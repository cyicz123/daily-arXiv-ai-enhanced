<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.CV](#cs.CV) [Total: 52]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.OH](#cs.OH) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
*Bithiah Yuan*

Main category: cs.CL

TLDR: 提出了一种基于BERT的金融QA系统，用于非事实性答案选择，通过检索和重排序方法提升效率，并在FiQA数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 金融行业对大规模非结构化和结构化数据的自动分析需求迫切，QA系统可为金融顾问提供决策支持。

Method: 结合BM25检索和BERT变体重排序，研究多种BERT学习、预训练和微调方法。

Result: FinBERT-QA模型在FiQA数据集任务2上，MRR提升16%，NDCG提升17%，Precision@1提升21%。

Conclusion: 提出的FinBERT-QA系统在金融非事实性答案选择任务中表现优异，显著超越现有方法。

Abstract: Motivated by the emerging demand in the financial industry for the automatic
analysis of unstructured and structured data at scale, Question Answering (QA)
systems can provide lucrative and competitive advantages to companies by
facilitating the decision making of financial advisers. Consequently, we
propose a novel financial QA system using the transformer-based pre-trained
BERT language model to address the limitations of data scarcity and language
specificity in the financial domain. Our system focuses on financial
non-factoid answer selection, which retrieves a set of passage-level texts and
selects the most relevant as the answer. To increase efficiency, we formulate
the answer selection task as a re-ranking problem, in which our system consists
of an Answer Retriever using BM25, a simple information retrieval approach, to
first return a list of candidate answers, and an Answer Re-ranker built with
variants of pre-trained BERT language models to re-rank and select the most
relevant answers. We investigate various learning, further pre-training, and
fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a
model built from applying the Transfer and Adapt further fine-tuning and
pointwise learning approach, is the most effective, improving the
state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on
NDCG, and 21% on Precision@1.

</details>

### [2] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Yankai Chen,Chunyu Miao,Hoang Nguyen,Yue Zhou,Weizhi Zhang,Liancheng Fang,Langzhou He,Yangning Li,Yuwei Cao,Dongyuan Li,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TLDR: 本文综述了基于大语言模型（LLM）的人机协作系统（LLM-HAS），旨在解决完全自主LLM代理的局限性，如幻觉、复杂任务处理困难及安全伦理风险。


<details>
  <summary>Details</summary>
Motivation: 完全自主的LLM代理存在可靠性、复杂任务处理及安全伦理问题，LLM-HAS通过引入人类信息与反馈提升系统性能、可靠性和安全性。

Method: 通过系统化梳理LLM-HAS的核心组件（环境与画像、人类反馈、交互类型、编排与通信），探索应用场景，并讨论挑战与机遇。

Result: 提供了首个LLM-HAS的全面结构化综述，明确了基础概念，并整理了相关资源。

Conclusion: LLM-HAS有望推动这一跨学科领域的进一步研究与创新。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>

### [3] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
*Alessandro Raganato,Rafael Peñaloza,Marco Viviani,Gabriella Pasi*

Main category: cs.CL

TLDR: 论文分析了大型语言模型（LLMs）在简单推理任务中的表现，重点关注其对提示的依赖性，并引入了一个新的基准数据集进行测试。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多领域自然语言处理中表现出色，但其简单推理能力仍受质疑，因此需要系统评估其推理能力。

Method: 通过零样本和少样本提示测试24种不同规模的LLMs，并使用链式思维提示测试22种模型，以评估其表现。

Result: 超过700亿参数的LLMs在零样本设置中表现更好，但仍有改进空间；链式思维提示的效果取决于提示的时机。

Conclusion: LLMs在简单推理任务中仍有局限性，提示方式对其表现有显著影响。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
manipulating natural language across multiple applications, but their ability
to handle simple reasoning tasks is often questioned. In this work, we aim to
provide a comprehensive analysis of LLMs' reasoning competence, specifically
focusing on their prompt dependency. In particular, we introduce a new
benchmark dataset with a series of simple reasoning questions demanding shallow
logical reasoning. Aligned with cognitive psychology standards, the questions
are confined to a basic domain revolving around geometric figures, ensuring
that responses are independent of any pre-existing intuition about the world
and rely solely on deduction. An empirical analysis involving zero-shot and
few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs
with over 70 billion parameters perform better in the zero-shot setting, there
is still a large room for improvement. An additional test with chain-of-thought
prompting over 22 LLMs shows that this additional prompt can aid or damage the
performance of models, depending on whether the rationale is required before or
after the answer.

</details>

### [4] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
*Mario Sänger,Ulf Leser*

Main category: cs.CL

TLDR: 研究评估了预训练语言模型（PLMs）在生物医学关系抽取中的表现，重点比较了不同上下文信息增强方法的效果。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献中关系抽取（RE）对管理大量科学知识至关重要，但现有研究因模型、数据和评估方法差异难以直接比较。

Method: 在五个数据集上评估三种基线PLMs，并进行超参数优化，随后用实体描述、知识图谱和分子结构编码增强最佳模型。

Result: 发现语言模型选择和超参数优化对性能至关重要，上下文信息对小模型有显著提升，但对整体改进有限。

Conclusion: 研究强调了模型选择和超参数优化的重要性，同时指出上下文信息对小模型的潜在价值。

Abstract: Automatic relationship extraction (RE) from biomedical literature is critical
for managing the vast amount of scientific knowledge produced each year. In
recent years, utilizing pre-trained language models (PLMs) has become the
prevalent approach in RE. Several studies report improved performance when
incorporating additional context information while fine-tuning PLMs for RE.
However, variations in the PLMs applied, the databases used for augmentation,
hyper-parameter optimization, and evaluation methods complicate direct
comparisons between studies and raise questions about the generalizability of
these findings. Our study addresses this research gap by evaluating PLMs
enhanced with contextual information on five datasets spanning four relation
scenarios within a consistent evaluation framework. We evaluate three baseline
PLMs and first conduct extensive hyperparameter optimization. After selecting
the top-performing model, we enhance it with additional data, including textual
entity descriptions, relational information from knowledge graphs, and
molecular structure encodings. Our findings illustrate the importance of i) the
choice of the underlying language model and ii) a comprehensive hyperparameter
optimization for achieving strong extraction performance. Although inclusion of
context information yield only minor overall improvements, an ablation study
reveals substantial benefits for smaller PLMs when such external data was
included during fine-tuning.

</details>

### [5] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
*Timur Jaganov,John Blake,Julián Villegas,Nicholas Carr*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLMs）在动态评估（DA）中的扩展潜力，开发了DynaWrite应用，测试了21种LLMs，发现GPT-4o和Neural Chat表现最佳，GPT-4o在反馈质量上更优。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何扩展动态评估，以支持更大规模的语言学习群体。

Method: 开发DynaWrite应用，测试21种LLMs，筛选出GPT-4o和Neural Chat进行进一步评估。

Result: GPT-4o在反馈质量和系统性能上优于Neural Chat，适合扩展动态评估。

Conclusion: LLMs（尤其是GPT-4o）能够有效扩展动态评估，适用于大规模语言学习场景。

Abstract: This study investigates the potential for Large Language Models (LLMs) to
scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first
developed DynaWrite-a modular, microservices-based grammatical tutoring
application which supports multiple LLMs to generate dynamic feedback to
learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural
chat to have the most potential to scale-up DA in the language learning
classroom. Further testing of these two candidates found both models performed
similarly in their ability to accurately identify grammatical errors in user
sentences. However, GPT-4o consistently outperformed neural chat in the quality
of its DA by generating clear, consistent, and progressively explicit hints.
Real-time responsiveness and system stability were also confirmed through
detailed performance testing, with GPT-4o exhibiting sufficient speed and
stability. This study shows that LLMs can be used to scale-up dynamic
assessment and thus enable dynamic assessment to be delivered to larger groups
than possible in traditional teacher-learner settings.

</details>

### [6] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
*Akhiad Bercovich,Itay Levy,Izik Golan,Mohammad Dabbah,Ran El-Yaniv,Omri Puny,Ido Galil,Zach Moshe,Tomer Ronen,Najeeb Nabwani,Ido Shahaf,Oren Tropp,Ehud Karpas,Ran Zilberstein,Jiaqi Zeng,Soumye Singhal,Alexander Bukharin,Yian Zhang,Tugrul Konuk,Gerald Shen,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Yoshi Suhara,Olivier Delalleau,Zijia Chen,Zhilin Wang,David Mosallanezhad,Adi Renduchintala,Haifeng Qian,Dima Rekesh,Fei Jia,Somshubra Majumdar,Vahid Noroozi,Wasi Uddin Ahmad,Sean Narenthiran,Aleksander Ficek,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Igor Gitman,Ivan Moshkov,Wei Du,Shubham Toshniwal,George Armstrong,Branislav Kisacanin,Matvei Novikov,Daria Gitman,Evelina Bakhturina,Jane Polak Scowcroft,John Kamalu,Dan Su,Kezhi Kong,Markus Kliegl,Rabeeh Karimi,Ying Lin,Sanjeev Satheesh,Jupinder Parmar,Pritam Gundecha,Brandon Norick,Joseph Jennings,Shrimai Prabhumoye,Syeda Nahida Akter,Mostofa Patwary,Abhinav Khattar,Deepak Narayanan,Roger Waleffe,Jimmy Zhang,Bor-Yiing Su,Guyue Huang,Terry Kong,Parth Chadha,Sahil Jain,Christine Harvey,Elad Segal,Jining Huang,Sergey Kashirsky,Robert McQueen,Izzy Putterman,George Lam,Arun Venkatesan,Sherry Wu,Vinh Nguyen,Manoj Kilaru,Andrew Wang,Anna Warno,Abhilash Somasamudramath,Sandip Bhaskar,Maka Dong,Nave Assaf,Shahar Mor,Omer Ullman Argov,Scot Junkin,Oleksandr Romanenko,Pedro Larroy,Monika Katariya,Marco Rovinelli,Viji Balas,Nicholas Edelman,Anahita Bhiwandiwalla,Muthu Subramaniam,Smita Ithape,Karthik Ramamoorthy,Yuting Wu,Suguna Varshini Velury,Omri Almog,Joyjit Daw,Denys Fridman,Erick Galinkin,Michael Evans,Katherine Luna,Leon Derczynski,Nikki Pope,Eileen Long,Seth Schneider,Guillermo Siman,Tomasz Grzegorzek,Pablo Ribalta,Monika Katariya,Joey Conway,Trisha Saar,Ann Guan,Krzysztof Pawelec,Shyamala Prayaga,Oleksii Kuchaiev,Boris Ginsburg,Oluwatobi Olabiyi,Kari Briski,Jonathan Cohen,Bryan Catanzaro,Jonah Alben,Yonatan Geifman,Eric Chung*

Main category: cs.CL

TLDR: Llama-Nemotron系列模型是一个开源的异构推理模型家族，提供卓越的推理能力、高效的推理速度和商业友好的许可证。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个高性能、高效且开源的推理模型家族，支持动态推理切换，促进开放研究和模型开发。

Method: 通过神经架构搜索、知识蒸馏和持续预训练优化模型，随后进行监督微调和大规模强化学习的后训练阶段。

Result: 模型在推理能力和效率上表现优异，支持动态切换推理模式，并提供了完整的开源资源。

Conclusion: Llama-Nemotron系列模型为开源社区和企业提供了高性能的推理工具，推动了开放研究的发展。

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>

### [7] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
*Yingquan Chen,Qianmu Li,Xiaocong Wu,Huifeng Li,Qing Chang*

Main category: cs.CL

TLDR: 本文提出了一种新的嵌入算法CDEA，结合XLNet模型，显著提升了隐写文本的质量，特别是在感知不可察觉性方面。


<details>
  <summary>Details</summary>
Motivation: 现有模型在文本生成能力上有限，且嵌入算法难以有效减少敏感信息属性（如语义内容或随机性）的负面影响，导致隐写文本质量下降。

Method: 提出基于字符的扩散嵌入算法（CDEA），利用敏感信息的属性，通过字符级统计特性和幂律分布分组方法，优化候选词选择频率。同时引入XLNet模型处理长序列。

Result: 实验表明，CDEA与XLNet的结合显著提升了隐写文本的质量，尤其在感知不可察觉性方面表现突出。

Conclusion: CDEA通过优化候选词选择，结合XLNet模型，有效解决了隐写文本生成中的质量问题。

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>

### [8] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
*Xuhui Jiang,Shengjie Ma,Chengjin Xu,Cehao Yang,Liyu Zhang,Jian Guo*

Main category: cs.CL

TLDR: 论文提出了一种名为Synthetic-on-Graph (SoG)的合成数据生成框架，通过跨文档知识关联提升数据多样性和深度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在数据效率上仍有不足，尤其是在处理小规模、专业化语料时。现有合成数据生成方法忽略了跨文档知识关联，限制了数据多样性和深度。

Method: SoG构建上下文图，提取实体和概念表示跨文档关联，采用图游走策略进行知识关联采样，并结合Chain-of-Thought (CoT)和Contrastive Clarifying (CC)提升数据质量。

Result: 实验表明，SoG在多跳文档问答数据集上优于现有方法，在阅读理解任务中表现相当，展示了更好的泛化能力。

Conclusion: SoG推动了合成数据生成，为LLMs在数据有限领域的高效知识获取提供了实用解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success but remain
data-inefficient, especially when learning from small, specialized corpora with
limited and proprietary data. Existing synthetic data generation methods for
continue pre-training focus on intra-document content and overlook
cross-document knowledge associations, limiting content diversity and depth. We
propose Synthetic-on-Graph (SoG), a synthetic data generation framework that
incorporates cross-document knowledge associations for efficient corpus
expansion. SoG constructs a context graph by extracting entities and concepts
from the original corpus, representing cross-document associations, and
employing a graph walk strategy for knowledge-associated sampling. This
enhances synthetic data diversity and coherence, enabling models to learn
complex knowledge structures and handle rare knowledge. To further improve
synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive
Clarifying (CC) synthetic, enhancing reasoning processes and discriminative
power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method
in a multi-hop document Q&A dataset while performing comparably to the SOTA
method on the reading comprehension task datasets, which also underscores the
better generalization capability of SoG. Our work advances synthetic data
generation and provides practical solutions for efficient knowledge acquisition
in LLMs, especially in domains with limited data availability.

</details>

### [9] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
*Ayan Sengupta,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TLDR: 论文主张从神经扩展定律转向降尺度开发大型语言模型（LLMs），以解决计算效率、环境影响和部署限制等问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩展方法在计算效率、环境影响和部署方面存在显著局限性，需要更可持续和高效的替代方案。

Method: 提出一个全面的降尺度框架，旨在保持性能的同时大幅减少资源需求。

Result: 论文提供了从传统扩展范式转向降尺度的实用策略。

Conclusion: 降尺度是LLM开发中更可持续、高效和可访问的方向。

Abstract: We challenge the dominant focus on neural scaling laws and advocate for a
paradigm shift toward downscaling in the development of large language models
(LLMs). While scaling laws have provided critical insights into performance
improvements through increasing model and dataset size, we emphasize the
significant limitations of this approach, particularly in terms of
computational inefficiency, environmental impact, and deployment constraints.
To address these challenges, we propose a holistic framework for downscaling
LLMs that seeks to maintain performance while drastically reducing resource
demands. This paper outlines practical strategies for transitioning away from
traditional scaling paradigms, advocating for a more sustainable, efficient,
and accessible approach to LLM development.

</details>

### [10] [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
*Sijin Sun,Liangbin Zhao,Ming Deng,Xiuju Fu*

Main category: cs.CL

TLDR: 本文提出VTS-LLM Agent，首个针对VTS操作的交互式决策支持的领域自适应大型LLM代理，通过知识增强的Text-to-SQL任务识别风险船舶，并在多语言风格查询中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有VTS系统在时空推理和直观人机交互方面存在局限，需应对日益复杂的交通和多模态数据。

Method: 结合结构化船舶数据库与外部海事知识，构建定制数据集，采用NER关系推理、领域知识注入、语义代数中间表示和查询重思机制。

Result: VTS-LLM在命令式、操作式和正式自然语言查询中均优于通用和SQL专用基线，并首次实证语言风格变化对Text-to-SQL建模的系统性挑战。

Conclusion: 为VTS自然语言接口奠定基础，推动LLM驱动的主动海事实时交通管理。

Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and
regulatory compliance through real-time traffic management. However, with
increasing traffic complexity and the prevalence of heterogeneous, multimodal
data, existing VTS systems face limitations in spatiotemporal reasoning and
intuitive human interaction. In this work, we propose VTS-LLM Agent, the first
domain-adaptive large LLM agent tailored for interactive decision support in
VTS operations. We formalize risk-prone vessel identification as a
knowledge-augmented Text-to-SQL task, combining structured vessel databases
with external maritime knowledge. To support this, we construct a curated
benchmark dataset consisting of a custom schema, domain-specific corpus, and a
query-SQL test set in multiple linguistic styles. Our framework incorporates
NER-based relational reasoning, agent-based domain knowledge injection,
semantic algebra intermediate representation, and query rethink mechanisms to
enhance domain grounding and context-aware understanding. Experimental results
show that VTS-LLM outperforms both general-purpose and SQL-focused baselines
under command-style, operational-style, and formal natural language queries,
respectively. Moreover, our analysis provides the first empirical evidence that
linguistic style variation introduces systematic performance challenges in
Text-to-SQL modeling. This work lays the foundation for natural language
interfaces in vessel traffic services and opens new opportunities for
proactive, LLM-driven maritime real-time traffic management.

</details>

### [11] [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
*Sumit Mamtani,Maitreya Sonawane,Kanika Agarwal,Nishanth Sanjeev*

Main category: cs.CL

TLDR: 论文评估了两种无标记模型（ByT5和CANINE）在讽刺检测任务中的表现，发现它们优于基于标记的模型，并在新闻标题和社交媒体数据集上取得了新的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理中标记化带来的词汇不匹配和词汇外问题，探索无标记模型在噪声和非正式领域（如社交媒体）的潜力。

Method: 对ByT5和CANINE进行微调，并在讽刺检测任务中与基于标记的基准模型和最新方法进行对比。

Result: ByT5-small和CANINE在新闻标题和Twitter讽刺数据集上的准确率分别提高了0.77%和0.49%，表现优于基于标记的模型。

Conclusion: 无标记模型在噪声和非正式领域的自然语言处理中具有潜力，能够提供更稳健的性能。

Abstract: Tokenization is a foundational step in most natural language processing (NLP)
pipelines, yet it introduces challenges such as vocabulary mismatch and
out-of-vocabulary issues. Recent work has shown that models operating directly
on raw text at the byte or character level can mitigate these limitations. In
this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of
sarcasm detection in both social media (Twitter) and non-social media (news
headlines) domains. We fine-tune and benchmark these models against token-based
baselines and state-of-the-art approaches. Our results show that ByT5-small and
CANINE outperform token-based counterparts and achieve new state-of-the-art
performance, improving accuracy by 0.77% and 0.49% on the News Headlines and
Twitter Sarcasm datasets, respectively. These findings underscore the potential
of token-free models for robust NLP in noisy and informal domains such as
social media.

</details>

### [12] [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
*Jongwook Han,Dongmin Choi,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TLDR: 提出了Value Portrait基准，用于评估语言模型的价值取向，具有生态效度和心理测量验证的特点。


<details>
  <summary>Details</summary>
Motivation: 现有基准易受价值偏见影响且与真实场景脱节，需更可靠的方法评估模型价值取向。

Method: 设计包含真实用户-模型交互的基准项目，通过人类评分与价值分数的相关性验证项目可靠性。

Result: 评估27个模型发现其更重视Benevolence、Security和Self-Direction，较少关注Tradition、Power和Achievement，并存在对某些人群的偏见。

Conclusion: Value Portrait基准为模型价值评估提供了更真实、可靠的框架，揭示了模型的价值取向和潜在偏见。

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage and thus ecological validity.
Second, each item is rated by human subjects based on its similarity to their
own thoughts, and correlations between these ratings and the subjects' actual
value scores are derived. This psychometrically validated approach ensures that
items strongly correlated with specific values serve as reliable items for
assessing those values. Through evaluating 27 LLMs with our benchmark, we find
that these models prioritize Benevolence, Security, and Self-Direction values
while placing less emphasis on Tradition, Power, and Achievement values. Also,
our analysis reveals biases in how LLMs perceive various demographic groups,
deviating from real human data.

</details>

### [13] [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
*Lui Yoshida*

Main category: cs.CL

TLDR: 研究探讨了在自动作文评分（AES）中使用详细评分标准的必要性和影响，发现简化标准在多数情况下不影响评分准确性，且能显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 详细评分标准在基于大语言模型（LLM）的AES中虽常见，但创建和维护成本高，研究旨在验证简化标准的可行性。

Method: 使用TOEFL11数据集，比较了四种LLM（Claude 3.5 Haiku、Gemini 1.5 Flash、GPT-4o-mini和Llama 3 70B Instruct）在三种评分标准条件下的表现：完整标准、简化标准和无标准。

Result: 四分之三的模型在简化标准下保持了与完整标准相似的评分准确性，且显著减少了计算资源消耗；但Gemini 1.5 Flash在详细标准下表现下降。

Conclusion: 简化评分标准对多数LLM的AES应用足够高效且不影响准确性，但需针对不同模型进行特定评估。

Abstract: This study investigates the necessity and impact of a detailed rubric in
automated essay scoring (AES) using large language models (LLMs). While using
rubrics are standard in LLM-based AES, creating detailed rubrics requires
substantial ef-fort and increases token usage. We examined how different levels
of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11
dataset. Our experiments compared three conditions: a full rubric, a simplified
rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5
Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of
four models maintained similar scoring accuracy with the simplified rubric
compared to the detailed one, while significantly reducing token usage.
However, one model (Gemini 1.5 Flash) showed decreased performance with more
detailed rubrics. The findings suggest that simplified rubrics may be
sufficient for most LLM-based AES applications, offering a more efficient
alternative without compromis-ing scoring accuracy. However, model-specific
evaluation remains crucial as per-formance patterns vary across different LLMs.

</details>

### [14] [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
*Yijie Jin,Junjie Peng,Xuanchao Lin,Haochen Yuan,Lan Wang,Cangzhi Zheng*

Main category: cs.CL

TLDR: 论文提出了一种基于图结构的模态间异构图（HMHGs）的多模态Transformer（GsiT），通过交错掩码机制（IM）实现高效参数共享，性能优于传统多模态Transformer。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析（MSA）中的多模态融合是核心挑战，现有方法（如多模态Transformer）存在效率问题。论文从效率优化角度出发，提出多模态Transformer本质上是层次化的模态间异构图（HMHGs），并基于此设计了GsiT。

Method: 提出GsiT，通过交错掩码机制（IM）实现高效的权重共享，避免信息混乱，并引入分解核（Decomposition）以减少计算开销。

Result: GsiT仅需传统多模态Transformer1/3的参数，性能显著提升，并在多个MSA数据集上验证了其有效性。

Conclusion: GsiT和HMHG概念在多模态情感分析中具有高效性和性能优势，为多模态融合提供了新思路。

Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that
integrates multimodal information to recognize sentiments, and existing models
have made significant progress in this area. The central challenge in MSA is
multimodal fusion, which is predominantly addressed by Multimodal Transformers
(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.
In this work, from the perspective of efficiency optimization, we propose and
prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and
we introduce the graph-structured representation pattern of MulTs. Based on
this pattern, we propose an Interlaced Mask (IM) mechanism to design the
Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is
formally equivalent to MulTs which achieves an efficient weight-sharing
mechanism without information disorder through IM, enabling All-Modal-In-One
fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called
Decomposition is implemented to ensure avoiding additional computational
overhead. Moreover, it achieves significantly higher performance than
traditional MulTs. To further validate the effectiveness of GsiT itself and the
HMHG concept, we integrate them into multiple state-of-the-art models and
demonstrate notable performance improvements and parameter reduction on widely
used MSA datasets.

</details>

### [15] [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
*Murtadha Ahmed,Wenbo,Liu yunfeng*

Main category: cs.CL

TLDR: MateICL通过分割上下文窗口和重新校准注意力权重，解决了大规模ICL中的注意力分散问题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的固定位置长度限制和注意力分散问题限制了ICL的演示示例数量。

Method: 将上下文分割为多个窗口并单独处理，引入额外层重新校准注意力权重。

Result: MateICL能有效利用更大上下文提升ICL性能，优于基于检索的方法。

Conclusion: MateICL在计算资源受限环境下仍具优势，代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
In-Context Learning (ICL). However, the fixed position length constraints in
pre-trained models limit the number of demonstration examples. Recent efforts
to extend context suffer from attention dispersion as the number of
demonstrations increases. In this paper, we introduce Mitigating Attention
Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective
self-attention as the context size grows. We first split the context into
multiple windows, each filled to the model's context capacity, which are
processed separately. Then, we introduce an additional layer to recalibrate the
attention weights, prioritizing the query tokens as the number of
demonstrations increases. Our empirical results show that MateICL can
effectively leverage larger contexts to improve ICL performance. Compared to
retrieval-based baselines, MateICL consistently achieves better performance
without requiring an externally trained retrieval model. Despite recent
advances in inference strategies (e.g., 32k token contexts), our results
demonstrate that MateICL remains beneficial in computationally
resource-constrained settings. The code is publicly available at
https://github.com/amurtadha/MateICL.

</details>

### [16] [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
*Chebrolu Niranjan,Kokil Jaidka,Gerard Christopher Yeo*

Main category: cs.CL

TLDR: 本文评估了导向向量作为语言模型对齐机制的局限性，发现其在特定任务（如价值观对齐）中有效，但在复杂场景下可能不足。


<details>
  <summary>Details</summary>
Motivation: 研究导向向量在语言模型推理时对齐行为的有效性及其局限性。

Method: 使用变压器钩干预和反义词功能向量框架，评估提示结构和上下文复杂性对导向效果的影响。

Result: 导向向量在特定对齐任务中表现良好，但在复杂场景中缺乏鲁棒性。

Conclusion: 导向向量适用于特定对齐任务，但需进一步研究其在推理模型中的通用能力。

Abstract: Steering vectors are a promising approach to aligning language model behavior
at inference time. In this paper, we propose a framework to assess the
limitations of steering vectors as alignment mechanisms. Using a framework of
transformer hook interventions and antonym-based function vectors, we evaluate
the role of prompt structure and context complexity in steering effectiveness.
Our findings indicate that steering vectors are promising for specific
alignment tasks, such as value alignment, but may not provide a robust
foundation for general-purpose alignment in LLMs, particularly in complex
scenarios. We establish a methodological foundation for future investigations
into steering capabilities of reasoning models.

</details>

### [17] [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
*Mahdi Dhaini,Ege Erdogan,Nils Feldhus,Gjergji Kasneci*

Main category: cs.CL

TLDR: 研究发现，广泛使用的后验特征归因方法在性别上存在显著差异，影响解释的忠实性、鲁棒性和复杂性，且与训练数据无关。


<details>
  <summary>Details</summary>
Motivation: 探讨解释方法在公平性方面的不足，尤其是在不同子群体中的性能差异。

Method: 通过三个任务和五个语言模型，评估后验特征归因方法的性别差异。

Result: 发现这些方法在性别上存在显著差异，且与训练数据的偏见无关。

Conclusion: 强调在开发和应用解释方法时需关注公平性，并将其纳入监管框架。

Abstract: While research on applications and evaluations of explanation methods
continues to expand, fairness of the explanation methods concerning disparities
in their performance across subgroups remains an often overlooked aspect. In
this paper, we address this gap by showing that, across three tasks and five
language models, widely used post-hoc feature attribution methods exhibit
significant gender disparity with respect to their faithfulness, robustness,
and complexity. These disparities persist even when the models are pre-trained
or fine-tuned on particularly unbiased datasets, indicating that the
disparities we observe are not merely consequences of biased training data. Our
results highlight the importance of addressing disparities in explanations when
developing and applying explainability methods, as these can lead to biased
outcomes against certain subgroups, with particularly critical implications in
high-stakes contexts. Furthermore, our findings underscore the importance of
incorporating the fairness of explanations, alongside overall model fairness
and explainability, as a requirement in regulatory frameworks.

</details>

### [18] [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
*Mahdi Dhaini,Kafaite Zahra Hussain,Efstratios Zaradoukas,Gjergji Kasneci*

Main category: cs.CL

TLDR: EvalxNLP是一个用于评估NLP模型特征归因方法的Python框架，整合了八种XAI技术，支持生成和评估解释，并提供交互式文本解释。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型在高风险应用中的普及，确保其可解释性成为关键挑战，需要为不同利益相关者提供定制化解释框架。

Method: EvalxNLP整合了八种XAI技术，支持基于忠实性、合理性和复杂性等属性的解释生成与评估，并提供交互式文本解释。

Result: 用户评估显示对EvalxNLP满意度高，表明其适用于跨用户群体的解释方法基准测试。

Conclusion: EvalxNLP旨在通过用户友好且可扩展的平台，推动XAI技术在NLP中的系统比较与发展。

Abstract: As Natural Language Processing (NLP) models continue to evolve and become
integral to high-stakes applications, ensuring their interpretability remains a
critical challenge. Given the growing variety of explainability methods and
diverse stakeholder requirements, frameworks that help stakeholders select
appropriate explanations tailored to their specific use cases are increasingly
important. To address this need, we introduce EvalxNLP, a Python framework for
benchmarking state-of-the-art feature attribution methods for transformer-based
NLP models. EvalxNLP integrates eight widely recognized explainability
techniques from the Explainable AI (XAI) literature, enabling users to generate
and evaluate explanations based on key properties such as faithfulness,
plausibility, and complexity. Our framework also provides interactive,
LLM-based textual explanations, facilitating user understanding of the
generated explanations and evaluation outcomes. Human evaluation results
indicate high user satisfaction with EvalxNLP, suggesting it is a promising
framework for benchmarking explanation methods across diverse user groups. By
offering a user-friendly and extensible platform, EvalxNLP aims at
democratizing explainability tools and supporting the systematic comparison and
advancement of XAI techniques in NLP.

</details>

### [19] [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
*Wei Han,Hui Chen,Soujanya Poria*

Main category: cs.CL

TLDR: PREMISE是一种基于匹配的多模态学习架构，用于多模态评论有用性任务，通过多尺度多领域表示和匹配分数提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统融合方法在多模态任务中因重复语义和低效表示导致的性能瓶颈。

Method: 计算多尺度和多领域表示，过滤重复语义，生成匹配分数作为特征向量。

Result: 在两个公开数据集上表现优异，计算成本更低。

Conclusion: PREMISE在多模态任务中显著优于现有融合方法。

Abstract: We present PREMISE (PREdict with Matching ScorEs), a new architecture for the
matching-based learning in the multimodal fields for the multimodal review
helpfulness (MRHP) task. Distinct to previous fusion-based methods which
obtains multimodal representations via cross-modal attention for downstream
tasks, PREMISE computes the multi-scale and multi-field representations,
filters duplicated semantics, and then obtained a set of matching scores as
feature vectors for the downstream recommendation task. This new architecture
significantly boosts the performance for such multimodal tasks whose context
matching content are highly correlated to the targets of that task, compared to
the state-of-the-art fusion-based methods. Experimental results on two publicly
available datasets show that PREMISE achieves promising performance with less
computational cost.

</details>

### [20] [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
*Xuan Li,Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.CL

TLDR: PromptObfus是一种通过反对抗学习扰动隐私词的方法，用于在保护用户提示隐私的同时保持模型预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛使用，用户提示中的隐私保护变得至关重要，传统方法因计算成本高和用户参与需求而受限。

Method: PromptObfus将提示脱敏视为掩码语言建模任务，用[MASK]替换隐私词，并通过梯度反馈选择候选替换词。

Result: 在三个NLP任务中，PromptObfus有效防止了隐私泄露，同时保持了任务性能。

Conclusion: PromptObfus提供了一种高效且实用的LLM提示隐私保护方法。

Abstract: With the widespread use of LLMs, preserving privacy in user prompts has
become crucial, as prompts risk exposing privacy and sensitive data to the
cloud LLMs. Traditional techniques like homomorphic encryption, secure
multi-party computation, and federated learning face challenges due to heavy
computational costs and user participation requirements, limiting their
applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel
method for desensitizing LLM prompts. The core idea of PromptObfus is
"anti-adversarial" learning, which perturbs privacy words in the prompt to
obscure sensitive information while retaining the stability of model
predictions. Specifically, PromptObfus frames prompt desensitization as a
masked language modeling task, replacing privacy-sensitive terms with a [MASK]
token. A desensitization model is trained to generate candidate replacements
for each masked position. These candidates are subsequently selected based on
gradient feedback from a surrogate model, ensuring minimal disruption to the
task output. We demonstrate the effectiveness of our approach on three NLP
tasks. Results show that PromptObfus effectively prevents privacy inference
from remote LLMs while preserving task performance.

</details>

### [21] [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
*Svenja Kenneweg,Jörg Deigmöller,Julian Eggert,Philipp Cimiano*

Main category: cs.CL

TLDR: 论文提出了一种因子化模型，用于捕捉模糊时间副词的语义，并将其与事件特定分布结合，生成上下文相关的解释。


<details>
  <summary>Details</summary>
Motivation: 模糊时间副词（如“最近”、“刚刚”）描述事件与当前时间的时间距离，但未明确具体时长，需要一种模型来捕捉其语义。

Method: 引入因子化模型，将时间副词的语义建模为概率分布，并与事件特定分布结合。模型参数通过母语者对副词适用性的判断数据拟合。

Result: 与基于单高斯分布的非因子化模型相比，因子化模型预测能力相似，但更简单且扩展性更好。

Conclusion: 因子化模型在奥卡姆剃刀原则下更优，因其简洁性和扩展性。

Abstract: Vague temporal adverbials, such as recently, just, and a long time ago,
describe the temporal distance between a past event and the utterance time but
leave the exact duration underspecified. In this paper, we introduce a
factorized model that captures the semantics of these adverbials as
probabilistic distributions. These distributions are composed with
event-specific distributions to yield a contextualized meaning for an adverbial
applied to a specific event. We fit the model's parameters using existing data
capturing judgments of native speakers regarding the applicability of these
vague temporal adverbials to events that took place a given time ago. Comparing
our approach to a non-factorized model based on a single Gaussian distribution
for each pair of event and temporal adverbial, we find that while both models
have similar predictive power, our model is preferable in terms of Occam's
razor, as it is simpler and has better extendability.

</details>

### [22] [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.CL

TLDR: 提出了一种基于Transformer架构的神经架构搜索方法，通过多目标遗传算法优化网络结构，结合BLEU分数和困惑度作为评价指标，实验结果表明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了在神经机器翻译任务中搜索出性能更好的网络结构，需要更全面的评价指标。

Method: 基于Transformer架构，通过多目标遗传算法搜索多头注意力的计算方式，结合BLEU分数和困惑度作为评价指标。

Result: 搜索出的网络结构优于所有基线模型，且结合困惑度作为辅助指标能发现更优模型。

Conclusion: 多目标评价指标（BLEU和困惑度）能更有效地指导神经架构搜索，提升翻译性能。

Abstract: This paper presents a neural architecture search method based on Transformer
architecture, searching cross multihead attention computation ways for
different number of encoder and decoder combinations. In order to search for
neural network structures with better translation results, we considered
perplexity as an auxiliary evaluation metric for the algorithm in addition to
BLEU scores and iteratively improved each individual neural network within the
population by a multi-objective genetic algorithm. Experimental results show
that the neural network structures searched by the algorithm outperform all the
baseline models, and that the introduction of the auxiliary evaluation metric
can find better models than considering only the BLEU score as an evaluation
metric.

</details>

### [23] [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
*Sheikh Samit Muhaimin,Spyridon Mastorakis*

Main category: cs.CL

TLDR: 提出了一种无需重新训练LLM的防御框架，通过提示过滤和上下文总结模块，有效识别和抵御恶意输入，实验显示成功率达98.71%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型易受对抗性攻击和恶意输入影响，现有防御方法需重新训练模型，成本高且不实用。

Method: 框架包含两部分：1) 提示过滤模块，利用NLP技术检测和分类恶意输入；2) 总结模块，提供上下文防御知识。

Result: 实验显示该方法识别恶意模式的成功率达98.71%，并显著提升模型对恶意输入的抵抗能力。

Conclusion: 该框架是一种高效、无需重新训练的防御方案，显著增强LLM的安全性。

Abstract: The recent growth in the use of Large Language Models has made them
vulnerable to sophisticated adversarial assaults, manipulative prompts, and
encoded malicious inputs. Existing countermeasures frequently necessitate
retraining models, which is computationally costly and impracticable for
deployment. Without the need for retraining or fine-tuning, this study presents
a unique defense paradigm that allows LLMs to recognize, filter, and defend
against adversarial or malicious inputs on their own. There are two main parts
to the suggested framework: (1) A prompt filtering module that uses
sophisticated Natural Language Processing (NLP) techniques, including zero-shot
classification, keyword analysis, and encoded content detection (e.g. base64,
hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and
(2) A summarization module that processes and summarizes adversarial research
literature to give the LLM context-aware defense knowledge. This approach
strengthens LLMs' resistance to adversarial exploitation by fusing text
extraction, summarization, and harmful prompt analysis. According to
experimental results, this integrated technique has a 98.71% success rate in
identifying harmful patterns, manipulative language structures, and encoded
prompts. By employing a modest amount of adversarial research literature as
context, the methodology also allows the model to react correctly to harmful
inputs with a larger percentage of jailbreak resistance and refusal rate. While
maintaining the quality of LLM responses, the framework dramatically increases
LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and
easy substitute for time-consuming, retraining-based defenses.

</details>

### [24] [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
*Svenja Kenneweg,Jörg Deigmöller,Philipp Cimiano,Julian Eggert*

Main category: cs.CL

TLDR: TRAVELER是一个新的合成基准数据集，用于评估模型在解决显式、隐式和模糊时间引用方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准对特定时间引用的系统性评估有限，TRAVELER旨在填补这一空白。

Method: 通过问答范式构建数据集，包含3300个问题，评估四种最先进的大语言模型。

Result: 模型在少量事件和显式时间引用下表现良好，但随着事件数量增加和时间引用模糊化，性能显著下降。

Conclusion: TRAVELER为时间引用解析提供了系统性评估工具，揭示了当前模型的局限性。

Abstract: Understanding and resolving temporal references is essential in Natural
Language Understanding as we often refer to the past or future in daily
communication. Although existing benchmarks address a system's ability to
reason about and resolve temporal references, systematic evaluation of specific
temporal references remains limited. Towards closing this gap, we introduce
TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering
paradigm and consists of questions involving temporal references with the
corresponding correct answers. TRAVELER assesses models' abilities to resolve
explicit, implicit relative to speech time, and vague temporal references.
Beyond investigating the performance of state-of-the-art LLMs depending on the
type of temporal reference, our benchmark also allows evaluation of performance
in relation to the length of the set of events. For the category of vague
temporal references, ground-truth answers were established via human surveys on
Prolific, following a procedure similar to the one from Kenneweg et al. To
demonstrate the benchmark's applicability, we evaluate four state-of-the-art
LLMs using a question-answering task encompassing 3,300 questions. Our findings
show that while the benchmarked LLMs can answer questions over event sets with
a handful of events and explicit temporal references successfully, performance
clearly deteriorates with larger event set length and when temporal references
get less explicit. Notably, the vague question category exhibits the lowest
performance across all models.
  The benchmark is publicly available at:
https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [25] [Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes](https://arxiv.org/abs/2505.00734)
*Neil Joshi,Joshua Carney,Nathanael Kuo,Homer Li,Cheng Peng,Myron Brown*

Main category: cs.CV

TLDR: 论文提出首个公开基准数据集，用于解决多视角3D重建和新视角合成的现实挑战，包括图像数量有限、相机异构等问题。


<details>
  <summary>Details</summary>
Motivation: 为灾害救援或执法等场景提供逼真的3D场景模型，但现有图像数据不足且质量不均。

Method: 开发基于多类型校准相机的数据集，评估未标定相机的校准和新视角渲染质量。

Result: 展示了现有方法的基线性能，并指出进一步研究的挑战。

Conclusion: 数据集为3D重建和新视角合成研究提供了重要基准。

Abstract: Production of photorealistic, navigable 3D site models requires a large
volume of carefully collected images that are often unavailable to first
responders for disaster relief or law enforcement. Real-world challenges
include limited numbers of images, heterogeneous unposed cameras, inconsistent
lighting, and extreme viewpoint differences for images collected from varying
altitudes. To promote research aimed at addressing these challenges, we have
developed the first public benchmark dataset for 3D reconstruction and novel
view synthesis based on multiple calibrated ground-level, security-level, and
airborne cameras. We present datasets that pose real-world challenges,
independently evaluate calibration of unposed cameras and quality of novel
rendered views, demonstrate baseline performance using recent state-of-practice
methods, and identify challenges for further research.

</details>

### [26] [MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection](https://arxiv.org/abs/2505.00739)
*Qiushi Yang,Yuan Yao,Miaomiao Cui,Liefeng Bo*

Main category: cs.CV

TLDR: MoSAM通过引入运动引导提示和时空记忆选择机制，解决了SAM2在视频分割中依赖固定帧记忆的问题，提升了对象跟踪能力和分割准确性。


<details>
  <summary>Details</summary>
Motivation: SAM2在视频分割中仅依赖过去六帧的掩码记忆，导致对象消失或遮挡时性能下降，需要改进以提升长程跟踪和分割可靠性。

Method: 提出MoSAM，结合运动引导提示（MGP）和时空记忆选择（ST-MS）机制，动态整合运动信息和优化记忆特征。

Result: 在多个视频对象分割和实例分割基准测试中，MoSAM取得了最先进的性能。

Conclusion: MoSAM通过运动信息和动态记忆优化，显著提升了视频分割的准确性和鲁棒性。

Abstract: The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional
capabilities in interactive object segmentation for both images and videos.
However, as a foundational model on interactive segmentation, SAM2 performs
segmentation directly based on mask memory from the past six frames, leading to
two significant challenges. Firstly, during inference in videos, objects may
disappear since SAM2 relies solely on memory without accounting for object
motion information, which limits its long-range object tracking capabilities.
Secondly, its memory is constructed from fixed past frames, making it
susceptible to challenges associated with object disappearance or occlusion,
due to potentially inaccurate segmentation results in memory. To address these
problems, we present MoSAM, incorporating two key strategies to integrate
object motion cues into the model and establish more reliable feature memory.
Firstly, we propose Motion-Guided Prompting (MGP), which represents the object
motion in both sparse and dense manners, then injects them into SAM2 through a
set of motion-guided prompts. MGP enables the model to adjust its focus towards
the direction of motion, thereby enhancing the object tracking capabilities.
Furthermore, acknowledging that past segmentation results may be inaccurate, we
devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically
identifies frames likely to contain accurate segmentation in both pixel- and
frame-level. By eliminating potentially inaccurate mask predictions from
memory, we can leverage more reliable memory features to exploit similar
regions for improving segmentation results. Extensive experiments on various
benchmarks of video object segmentation and video instance segmentation
demonstrate that our MoSAM achieves state-of-the-art results compared to other
competitors.

</details>

### [27] [Fast2comm:Collaborative perception combined with prior knowledge](https://arxiv.org/abs/2505.00740)
*Zhengbin Zhang,Yan Wu,Hongkun Zhang*

Main category: cs.CV

TLDR: Fast2comm是一个基于先验知识的协作感知框架，通过生成高区分度置信特征和优化带宽效率，解决协作感知中的性能与带宽平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的协作感知面临性能与带宽平衡以及定位误差的挑战，需要一种高效且适应性强的解决方案。

Method: 提出先验监督的置信特征生成方法、基于GT边界框的空间先验特征选择策略，并解耦训练和测试阶段的特征融合策略。

Result: 在真实和模拟数据集上的实验验证了Fast2comm的优越性能和所提方法的必要性。

Conclusion: Fast2comm通过优化特征选择和动态带宽适应，显著提升了协作感知的准确性和效率。

Abstract: Collaborative perception has the potential to significantly enhance
perceptual accuracy through the sharing of complementary information among
agents. However, real-world collaborative perception faces persistent
challenges, particularly in balancing perception performance and bandwidth
limitations, as well as coping with localization errors. To address these
challenges, we propose Fast2comm, a prior knowledge-based collaborative
perception framework. Specifically, (1)we propose a prior-supervised confidence
feature generation method, that effectively distinguishes foreground from
background by producing highly discriminative confidence features; (2)we
propose GT Bounding Box-based spatial prior feature selection strategy to
ensure that only the most informative prior-knowledge features are selected and
shared, thereby minimizing background noise and optimizing bandwidth efficiency
while enhancing adaptability to localization inaccuracies; (3)we decouple the
feature fusion strategies between model training and testing phases, enabling
dynamic bandwidth adaptation. To comprehensively validate our framework, we
conduct extensive experiments on both real-world and simulated datasets. The
results demonstrate the superior performance of our model and highlight the
necessity of the proposed methods. Our code is available at
https://github.com/Zhangzhengbin-TJ/Fast2comm.

</details>

### [28] [Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models](https://arxiv.org/abs/2505.00741)
*Srinivas Kanakala,Sneha Ningappa*

Main category: cs.CV

TLDR: 该研究利用CNN和LSTM模型对植物叶片疾病进行分类，CNN模型表现更优，验证准确率达96.4%。


<details>
  <summary>Details</summary>
Motivation: 植物病害严重影响农业产量和食品质量，早期检测和分类对减少损失至关重要。

Method: 使用CNN和LSTM模型，基于包含70,295张训练图像和17,572张验证图像的数据集进行分类。

Result: CNN模型训练准确率99.1%，验证准确率96.4%；LSTM验证准确率93.43%。

Conclusion: 深度学习模型（尤其是CNN）为植物病害分类提供了准确且可扩展的解决方案。

Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield
and affecting food quality. Early detection and classification of these
diseases are essential for minimising losses and improving crop management
practices. This study applies Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset
containing 70,295 training images and 17,572 validation images across 38
disease classes. The CNN model was trained using the Adam optimiser with a
learning rate of 0.0001 and categorical cross-entropy as the loss function.
After 10 training epochs, the model achieved a training accuracy of 99.1% and a
validation accuracy of 96.4%. The LSTM model reached a validation accuracy of
93.43%. Performance was evaluated using precision, recall, F1-score, and
confusion matrix, confirming the reliability of the CNN-based approach. The
results suggest that deep learning models, particularly CNN, enable an
effective solution for accurate and scalable plant disease classification,
supporting practical applications in agricultural monitoring.

</details>

### [29] [Zoomer: Adaptive Image Focus Optimization for Black-box MLLM](https://arxiv.org/abs/2505.00742)
*Jiaxu Qian,Chendong Wang,Yifan Yang,Chaoyun Zhang,Huiqiang Jiang,Xufang Luo,Yu Kang,Qingwei Lin,Anlan Zhang,Shiqi Jiang,Ting Cao,Tianjun Mao,Suman Banerjee,Guyue Liu,Saravan Rajmohan,Dongmei Zhang,Yuqing Yang,Qi Zhang,Lili Qiu*

Main category: cs.CV

TLDR: 论文提出了一种名为\SysName的新型视觉提示机制，旨在提升多模态大语言模型（MLLMs）在视觉任务中的性能，同时保留关键视觉细节。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在处理视觉数据时存在精度不足和关键信息丢失的问题，主要受限于严格的token限制。

Method: \SysName包含三个创新点：动态突出相关图像区域的提示感知策略、保持对象完整性的空间保留编排模式，以及平衡全局上下文与关键视觉细节的预算感知提示方法。

Result: 在多个数据集上的评估表明，\SysName显著优于基线方法，准确率提升高达26.9%，同时显著减少了token消耗。

Conclusion: \SysName通过创新的视觉提示机制有效解决了MLLMs在视觉任务中的局限性，提升了性能并优化了资源使用。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
broadened the scope of vision-language tasks, excelling in applications like
image captioning and interactive question-answering. However, these models
struggle with accurately processing visual data, particularly in tasks
requiring precise object recognition and fine visual details. Stringent token
limits often result in the omission of critical information, hampering
performance. To address these limitations, we introduce \SysName, a novel
visual prompting mechanism designed to enhance MLLM performance while
preserving essential visual details within token limits. \SysName features
three key innovations: a prompt-aware strategy that dynamically highlights
relevant image regions, a spatial-preserving orchestration schema that
maintains object integrity, and a budget-aware prompting method that balances
global context with crucial visual details. Comprehensive evaluations across
multiple datasets demonstrate that \SysName consistently outperforms baseline
methods, achieving up to a $26.9\%$ improvement in accuracy while significantly
reducing token consumption.

</details>

### [30] [DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation](https://arxiv.org/abs/2505.00743)
*Yinfeng Yu,Dongsheng Yang*

Main category: cs.CV

TLDR: 论文提出了一种DOPE网络，通过增强文本和图像中的对象感知能力，解决了VLN任务中语言理解不足和跨模态对象关系建模缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法未能充分利用指令中的细节信息，且忽视跨模态对象关系建模，导致导航决策不准确。

Method: 设计了TSE模块提取关键文本信息，并通过TOPA和IOPA模块增强文本和图像中的对象感知能力。

Result: 在R2R和REVERIE数据集上的实验验证了方法的有效性。

Conclusion: DOPE网络显著提升了导航性能，解决了现有方法的局限性。

Abstract: Vision-and-Language Navigation (VLN) is a challenging task where an agent
must understand language instructions and navigate unfamiliar environments
using visual cues. The agent must accurately locate the target based on visual
information from the environment and complete tasks through interaction with
the surroundings. Despite significant advancements in this field, two major
limitations persist: (1) Many existing methods input complete language
instructions directly into multi-layer Transformer networks without fully
exploiting the detailed information within the instructions, thereby limiting
the agent's language understanding capabilities during task execution; (2)
Current approaches often overlook the modeling of object relationships across
different modalities, failing to effectively utilize latent clues between
objects, which affects the accuracy and robustness of navigation decisions. We
propose a Dual Object Perception-Enhancement Network (DOPE) to address these
issues to improve navigation performance. First, we design a Text Semantic
Extraction (TSE) to extract relatively essential phrases from the text and
input them into the Text Object Perception-Augmentation (TOPA) to fully
leverage details such as objects and actions within the instructions. Second,
we introduce an Image Object Perception-Augmentation (IOPA), which performs
additional modeling of object information across different modalities, enabling
the model to more effectively utilize latent clues between objects in images
and text, enhancing decision-making accuracy. Extensive experiments on the R2R
and REVERIE datasets validate the efficacy of the proposed approach.

</details>

### [31] [Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering](https://arxiv.org/abs/2505.00744)
*Dung Nguyen,Minh Khoi Ho,Huy Ta,Thanh Tam Nguyen,Qi Chen,Kumar Rav,Quy Duong Dang,Satwik Ramchandre,Son Lam Phung,Zhibin Liao,Minh-Son To,Johan Verjans,Phi Le Nguyen,Vu Minh Hieu Phan*

Main category: cs.CV

TLDR: 论文揭示了当前医学大型多模态模型（LMMs）在定位推理上的不足，导致幻觉问题，并提出HEAL-MedVQA基准和LobA框架以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学LMMs在解释医学数据时表现出色，但常因定位推理不足而生成与源证据矛盾的幻觉回答，需解决这一问题。

Method: 提出HEAL-MedVQA基准，包含两种评估协议和67K VQA对数据集；设计LobA框架，通过定位目标区域并自提示生成可靠答案。

Result: 实验表明，LobA框架在HEAL-MedVQA基准上显著优于现有生物医学LMMs，提升了医学VQA的鲁棒性。

Conclusion: HEAL-MedVQA和LobA框架有效解决了医学LMMs的幻觉问题，推动了医学视觉问答的可靠性。

Abstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable
capabilities in medical data interpretation. However, these models frequently
generate hallucinations contradicting source evidence, particularly due to
inadequate localization reasoning. This work reveals a critical limitation in
current medical LMMs: instead of analyzing relevant pathological regions, they
often rely on linguistic patterns or attend to irrelevant image areas when
responding to disease-related queries. To address this, we introduce
HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive
benchmark designed to evaluate LMMs' localization abilities and hallucination
robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to
assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA
pairs, with doctor-annotated anatomical segmentation masks for pathological
regions. To improve visual reasoning, we propose the Localize-before-Answer
(LobA) framework, which trains LMMs to localize target regions of interest and
self-prompt to emphasize segmented pathological areas, generating grounded and
reliable answers. Experimental results demonstrate that our approach
significantly outperforms state-of-the-art biomedical LMMs on the challenging
HEAL-MedVQA benchmark, advancing robustness in medical VQA.

</details>

### [32] [Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations](https://arxiv.org/abs/2505.00745)
*Maozhe Zhao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.CV

TLDR: MOCHA框架通过移动与云资源的层次协作优化连续模型适应的响应速度，提升准确性并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 现有云中心模型适应框架在环境变化时性能下降且反应延迟，需更高效的适应方案。

Method: MOCHA通过设备端模型重用与快速微调、结构化专家模型检索及本地模型缓存优化响应速度。

Result: 实验显示MOCHA提升适应准确性6.8%，减少响应延迟35.5倍和重训练时间3倍。

Conclusion: MOCHA显著优化移动视频分析系统的模型适应效率与性能。

Abstract: Mobile video analysis systems often encounter various deploying environments,
where environment shifts present greater demands for responsiveness in
adaptations of deployed "expert DNN models". Existing model adaptation
frameworks primarily operate in a cloud-centric way, exhibiting degraded
performance during adaptation and delayed reactions to environment shifts.
Instead, this paper proposes MOCHA, a novel framework optimizing the
responsiveness of continuous model adaptation through hierarchical
collaborations between mobile and cloud resources. Specifically, MOCHA (1)
reduces adaptation response delays by performing on-device model reuse and fast
fine-tuning before requesting cloud model retrieval and end-to-end retraining;
(2) accelerates history expert model retrieval by organizing them into a
structured taxonomy utilizing domain semantics analyzed by a cloud foundation
model as indices; (3) enables efficient local model reuse by maintaining
onboard expert model caches for frequent scenes, which proactively prefetch
model weights from the cloud model database. Extensive evaluations with
real-world videos on three DNN tasks show MOCHA improves the model accuracy
during adaptation by up to 6.8% while saving the response delay and retraining
time by up to 35.5x and 3.0x respectively.

</details>

### [33] [Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis](https://arxiv.org/abs/2505.00746)
*Alexei Kaltchenko*

Main category: cs.CV

TLDR: 论文提出了一种基于熵热图的方法，通过滑动窗口分析GPT-4o的逐词置信度，定位OCR错误。实验表明，高熵区域与真实错误高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如GPT-4o）在转录数学文档时，其逐词置信度信号未被充分利用以定位识别错误。

Method: 提出熵热图概念，将逐词Shannon熵转化为视觉化的“不确定性景观”，并通过滑动窗口检测高熵热点区域。

Result: 实验表明，大多数真实错误集中在高熵区域内。

Conclusion: 滑动窗口熵分析可作为轻量级工具，辅助GPT-OCR的后编辑工作。

Abstract: Vision-language models such as OpenAI GPT-4o can transcribe mathematical
documents directly from images, yet their token-level confidence signals are
seldom used to pinpoint local recognition mistakes. We present an
entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into
a visual ''uncertainty landscape''. By scanning the entropy sequence with a
fixed-length sliding window, we obtain hotspots that are likely to contain OCR
errors such as missing symbols, mismatched braces, or garbled prose. Using a
small, curated set of scanned research pages rendered at several resolutions,
we compare the highlighted hotspots with the actual transcription errors
produced by GPT-4o. Our analysis shows that the vast majority of true errors
are indeed concentrated inside the high-entropy regions. This study
demonstrates--in a minimally engineered setting--that sliding-window entropy
can serve as a practical, lightweight aid for post-editing GPT-based OCR. All
code, sample data, and annotation guidelines are released to encourage
replication and further research.

</details>

### [34] [InstructAttribute: Fine-grained Object Attributes editing with Instruction](https://arxiv.org/abs/2505.00751)
*Xingxi Yin,Jingfeng Zhang,Zhi Li,Yicheng Li,Yin Zhang*

Main category: cs.CV

TLDR: 论文提出了一种名为SPAA的无训练方法，通过编辑自注意力图和交叉注意力值，实现对物体颜色和材质的精确控制，并构建了Attribute Dataset和InstructAttribute模型，用于细粒度编辑。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑技术难以精确控制细粒度属性或保持物体结构一致性，SPAA旨在解决这些问题。

Method: SPAA通过编辑自注意力图和交叉注意力值实现精确控制，并利用MLLM构建Attribute Dataset，训练InstructAttribute模型。

Result: 实验表明，SPAA在物体级颜色和材质编辑上优于现有基于指令的图像编辑方法。

Conclusion: SPAA提供了一种无训练的高效方法，实现了对颜色和材质的精确控制，同时保持图像结构一致性。

Abstract: Text-to-image (T2I) diffusion models, renowned for their advanced generative
abilities, are extensively utilized in image editing applications,
demonstrating remarkable effectiveness. However, achieving precise control over
fine-grained attributes still presents considerable challenges. Existing image
editing techniques either fail to modify the attributes of an object or
struggle to preserve its structure and maintain consistency in other areas of
the image. To address these challenges, we propose the Structure-Preserving and
Attribute Amplification (SPAA), a training-free method which enables precise
control over the color and material transformations of objects by editing the
self-attention maps and cross-attention values. Furthermore, we constructed the
Attribute Dataset, which encompasses nearly all colors and materials associated
with various objects, by integrating multimodal large language models (MLLM) to
develop an automated pipeline for data filtering and instruction labeling.
Training on this dataset, we present our InstructAttribute, an
instruction-based model designed to facilitate fine-grained editing of color
and material attributes. Extensive experiments demonstrate that our method
achieves superior performance in object-level color and material editing,
outperforming existing instruction-based image editing approaches.

</details>

### [35] [DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking](https://arxiv.org/abs/2505.00752)
*Xuzhao Li,Xuchen Li,Shiyu Hu*

Main category: cs.CV

TLDR: DARTer是一种用于夜间无人机跟踪的端到端框架，通过动态特征融合和自适应激活机制，显著提升了跟踪性能和效率。


<details>
  <summary>Details</summary>
Motivation: 夜间无人机跟踪因光照变化和视角变化导致性能下降，现有方法计算成本高或未能充分利用动态特征。

Method: 提出DARTer框架，包含动态特征混合器（DFB）和动态特征激活器（DFA），优化特征融合和计算效率。

Result: 在多个夜间无人机跟踪基准测试中表现优于现有方法，平衡了准确性和效率。

Conclusion: DARTer为实际夜间无人机跟踪应用提供了高效且准确的解决方案。

Abstract: Nighttime UAV tracking presents significant challenges due to extreme
illumination variations and viewpoint changes, which severely degrade tracking
performance. Existing approaches either rely on light enhancers with high
computational costs or introduce redundant domain adaptation mechanisms,
failing to fully utilize the dynamic features in varying perspectives. To
address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic
\textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end
tracking framework designed for nighttime UAV scenarios. DARTer leverages a
Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime
features from static and dynamic templates, enhancing representation
robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates
Vision Transformer layers based on extracted features, significantly improving
efficiency by reducing redundant computations. Our model eliminates the need
for complex multi-task loss functions, enabling a streamlined training process.
Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate
the superiority of DARTer over state-of-the-art trackers. These results confirm
that DARTer effectively balances tracking accuracy and efficiency, making it a
promising solution for real-world nighttime UAV tracking applications.

</details>

### [36] [P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors](https://arxiv.org/abs/2505.00755)
*Atsuya Watanabe,Ratna Aisuwarya,Lei Jing*

Main category: cs.CV

TLDR: P2P-Insole是一种低成本方法，通过集成IMU的鞋垫传感器估计和可视化3D人体骨骼数据，适用于大规模生产。


<details>
  <summary>Details</summary>
Motivation: 解决现有商业解决方案成本高的问题，提供轻量、低侵入性和隐私保护的解决方案。

Method: 使用Transformer模型提取时间特征，结合脚压分布、加速度和旋转数据，并利用多模态信息提高准确性。

Result: 实验证明该方法在复杂运动模式识别和姿态估计任务中具有鲁棒性。

Conclusion: P2P-Insole为康复、伤害预防和健康监测提供了低成本实用基础，并可通过传感器优化和数据集扩展进一步发展。

Abstract: This work presents P2P-Insole, a low-cost approach for estimating and
visualizing 3D human skeletal data using insole-type sensors integrated with
IMUs. Each insole, fabricated with e-textile garment techniques, costs under
USD 1, making it significantly cheaper than commercial alternatives and ideal
for large-scale production. Our approach uses foot pressure distribution,
acceleration, and rotation data to overcome limitations, providing a
lightweight, minimally intrusive, and privacy-aware solution. The system
employs a Transformer model for efficient temporal feature extraction, enriched
by first and second derivatives in the input stream. Including multimodal
information, such as accelerometers and rotational measurements, improves the
accuracy of complex motion pattern recognition. These facts are demonstrated
experimentally, while error metrics show the robustness of the approach in
various posture estimation tasks. This work could be the foundation for a
low-cost, practical application in rehabilitation, injury prevention, and
health monitoring while enabling further development through sensor
optimization and expanded datasets.

</details>

### [37] [Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L](https://arxiv.org/abs/2505.00757)
*Woong-Chan Byun,Dong-Hee Paek,Seung-Hyun Song,Seung-Hyun Kong*

Main category: cs.CV

TLDR: 论文提出了一种在Hailo-8L AI加速器上实现4D雷达3D物体检测的芯片级方法，通过张量变换解决5D输入与4D支持的兼容性问题，实现了实时处理。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在自动驾驶中具有鲁棒性，但需在低功耗嵌入式环境中实现实时处理，因此需要解决5D输入与4D支持的兼容性问题。

Method: 引入张量变换方法，在编译过程中将5D输入重塑为4D格式，无需改变模型结构。

Result: 系统达到46.47% AP_3D和52.75% AP_BEV，推理速度为13.76 Hz，与GPU模型精度相当。

Conclusion: 该方法证明了4D雷达感知技术在自动驾驶系统中的实用性。

Abstract: 4D radar has attracted attention in autonomous driving due to its ability to
enable robust 3D object detection even under adverse weather conditions. To
practically deploy such technologies, it is essential to achieve real-time
processing within low-power embedded environments. Addressing this, we present
the first on-chip implementation of a 4D radar-based 3D object detection model
on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural
network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D
tensors, posing a significant challenge. To overcome this limitation, we
introduce a tensor transformation method that reshapes 5D inputs into 4D
formats during the compilation process, enabling direct deployment without
altering the model structure. The proposed system achieves 46.47% AP_3D and
52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while
achieving an inference speed of 13.76 Hz. These results demonstrate the
applicability of 4D radar-based perception technologies to autonomous driving
systems.

</details>

### [38] [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
*Jiahui Chen,Candace Ross,Reyhane Askari-Hemmat,Koustuv Sinha,Melissa Hall,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TLDR: MT2IE是一种利用多模态大语言模型（MLLMs）评估文本到图像（T2I）生成模型的新框架，通过动态生成提示词并评分，显著减少评估所需的提示词数量，且评分与人类判断更一致。


<details>
  <summary>Details</summary>
Motivation: 由于T2I模型的持续改进，依赖静态数据集的自动评估基准逐渐失效，需要新的评估方法。

Method: 提出MT2IE框架，利用MLLMs动态生成提示词并评估T2I模型的生成一致性和图像美学。

Result: MT2IE仅需1/80的提示词即可达到与现有基准相同的模型排名，且其评分与人类判断相关性更高。

Conclusion: MT2IE为T2I模型提供了一种高效、动态的评估方法，优于传统静态基准。

Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow
deprecation of automatic evaluation benchmarks that rely on static datasets,
motivating researchers to seek alternative ways to evaluate the T2I progress.
In this paper, we explore the potential of multi-modal large language models
(MLLMs) as evaluator agents that interact with a T2I model, with the objective
of assessing prompt-generation consistency and image aesthetics. We present
Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively
generates prompts for evaluation, scores generated images and matches T2I
evaluation of existing benchmarks with a fraction of the prompts used in
existing static benchmarks. Moreover, we show that MT2IE's prompt-generation
consistency scores have higher correlation with human judgment than scores
previously introduced in the literature. MT2IE generates prompts that are
efficient at probing T2I model performance, producing the same relative T2I
model rankings as existing benchmarks while using only 1/80th the number of
prompts for evaluation.

</details>

### [39] [Person detection and re-identification in open-world settings of retail stores and public spaces](https://arxiv.org/abs/2505.00772)
*Branko Brkljač,Milan Brkljač*

Main category: cs.CV

TLDR: 论文讨论了在开放世界环境中智能城市计算机视觉的实际应用，特别是行人重识别任务，涉及多摄像头、多行人场景的复杂系统设计。


<details>
  <summary>Details</summary>
Motivation: 智能城市中的行人重识别任务面临开放世界环境的挑战，如多摄像头、光照变化等，需要高效的系统架构和模型。

Method: 探讨了现有系统设计架构的挑战，提出了基于计算机视觉技术的解决方案，并在零售店和公共空间进行了应用测试。

Result: 通过实验展示了接近实时解决方案的性能，分析了不同环境下的敏感性。

Conclusion: 指出了进一步的研究方向和系统改进的可能性。

Abstract: Practical applications of computer vision in smart cities usually assume
system integration and operation in challenging open-world environments. In the
case of person re-identification task the main goal is to retrieve information
whether the specific person has appeared in another place at a different time
instance of the same video, or over multiple camera feeds. This typically
assumes collecting raw data from video surveillance cameras in different places
and under varying illumination conditions. In the considered open-world setting
it also requires detection and localization of the person inside the analyzed
video frame before the main re-identification step. With multi-person and
multi-camera setups the system complexity becomes higher, requiring
sophisticated tracking solutions and re-identification models. In this work we
will discuss existing challenges in system design architectures, consider
possible solutions based on different computer vision techniques, and describe
applications of such systems in retail stores and public spaces for improved
marketing analytics. In order to analyse sensitivity of person
re-identification task under different open-world environments, a performance
of one close to real-time solution will be demonstrated over several video
captures and live camera feeds. Finally, based on conducted experiments we will
indicate further research directions and possible system improvements.

</details>

### [40] [AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring](https://arxiv.org/abs/2505.00786)
*Oluwanisola Ibikunle,Hara Talasila,Debvrat Varshney,Jilu Li,John Paden,Maryam Rahnemoonfar*

Main category: cs.CV

TLDR: 该研究首次提供了一个标准化的雷达回波图数据集，用于深度学习算法测试和比较，推动了冰层追踪技术的发展。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化和标注完善的雷达回波图数据集，限制了冰层追踪算法的测试和比较，阻碍了技术进步。

Method: 研究基于2012年NASA OIB任务的Snow Radar数据，构建了一个包含13,717张标注和57,815张弱标注回波图的数据集，并评估了五种深度学习模型。

Result: 现有计算机视觉分割算法能识别冰层像素，但需要更先进的端到端模型以直接从回波图中提取雪深和年积累量。

Conclusion: 该数据集和基准框架为冰层追踪和雪积累估算提供了宝贵资源，有助于理解极地冰盖对气候变暖的响应。

Abstract: Tracking internal layers in radar echograms with high accuracy is essential
for understanding ice sheet dynamics and quantifying the impact of accelerated
ice discharge in Greenland and other polar regions due to contemporary global
climate warming. Deep learning algorithms have become the leading approach for
automating this task, but the absence of a standardized and well-annotated
echogram dataset has hindered the ability to test and compare algorithms
reliably, limiting the advancement of state-of-the-art methods for the radar
echogram layer tracking problem. This study introduces the first comprehensive
``deep learning ready'' radar echogram dataset derived from Snow Radar airborne
data collected during the National Aeronautics and Space Administration
Operation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled
and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation,
wet) with varying along-track resolutions. To demonstrate its utility, we
evaluated the performance of five deep learning models on the dataset. Our
results show that while current computer vision segmentation algorithms can
identify and track snow layer pixels in echogram images, advanced end-to-end
models are needed to directly extract snow depth and annual accumulation from
echograms, reducing or eliminating post-processing. The dataset and
accompanying benchmarking framework provide a valuable resource for advancing
radar echogram layer tracking and snow accumulation estimation, advancing our
understanding of polar ice sheets response to climate warming.

</details>

### [41] [SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models](https://arxiv.org/abs/2505.00788)
*Wufei Ma,Luoxin Ye,Nessa McWeeney,Celso M de Melo,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TLDR: 论文提出SpatialLLM，一种具备3D空间推理能力的大型多模态模型，通过3D数据增强和架构优化，性能超越GPT-4o 8.7%。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型缺乏3D空间推理能力，主要因数据稀缺和设计偏向2D。

Method: 开发两类3D训练数据（3D探测数据和3D对话数据），并系统整合到模型架构与训练设计中。

Result: SpatialLLM在3D推理能力上显著提升，性能超越GPT-4o 8.7%。

Conclusion: 研究为未来3D推理模型设计提供了系统性指导和宝贵见解。

Abstract: Humans naturally understand 3D spatial relationships, enabling complex
reasoning like predicting collisions of vehicles from different directions.
Current large multimodal models (LMMs), however, lack of this capability of 3D
spatial reasoning. This limitation stems from the scarcity of 3D training data
and the bias in current model designs toward 2D data. In this paper, we
systematically study the impact of 3D-informed data, architecture, and training
setups, introducing SpatialLLM, a large multi-modal model with advanced 3D
spatial reasoning abilities. To address data limitations, we develop two types
of 3D-informed training datasets: (1) 3D-informed probing data focused on
object's 3D location and orientation, and (2) 3D-informed conversation data for
complex spatial relationships. Notably, we are the first to curate VQA data
that incorporate 3D orientation relationships on real images. Furthermore, we
systematically integrate these two types of training data with the
architectural and training designs of LMMs, providing a roadmap for optimal
design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM
advances machines toward highly capable 3D-informed reasoning, surpassing
GPT-4o performance by 8.7%. Our systematic empirical design and the resulting
findings offer valuable insights for future research in this direction.

</details>

### [42] [Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging](https://arxiv.org/abs/2505.00805)
*Fadi Abdeladhim Zidi,Abdelkrim Ouafi,Fares Bougourzi,Cosimo Distante,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TLDR: 综述探讨了深度学习在高光谱成像（HSI）小麦作物分析中的应用，填补了该领域缺乏全面调查的空白。


<details>
  <summary>Details</summary>
Motivation: 小麦生产面临病虫害、气候变化等挑战，传统监测方法效率低。HSI结合深度学习有望解决这些问题，但缺乏系统性总结。

Method: 总结了基准数据集，追踪了深度学习方法进展，并分析了品种分类、病害检测和产量估计等关键应用。

Result: 综述了当前最先进的论文，并提供了未来研究方向。

Conclusion: 深度学习在HSI小麦分析中潜力巨大，但仍需克服数据高维度和样本不足等挑战。

Abstract: As one of the most widely cultivated and consumed crops, wheat is essential
to global food security. However, wheat production is increasingly challenged
by pests, diseases, climate change, and water scarcity, threatening yields.
Traditional crop monitoring methods are labor-intensive and often ineffective
for early issue detection. Hyperspectral imaging (HSI) has emerged as a
non-destructive and efficient technology for remote crop health assessment.
However, the high dimensionality of HSI data and limited availability of
labeled samples present notable challenges. In recent years, deep learning has
shown great promise in addressing these challenges due to its ability to
extract and analysis complex structures. Despite advancements in applying deep
learning methods to HSI data for wheat crop analysis, no comprehensive survey
currently exists in this field. This review addresses this gap by summarizing
benchmark datasets, tracking advancements in deep learning methods, and
analyzing key applications such as variety classification, disease detection,
and yield estimation. It also highlights the strengths, limitations, and future
opportunities in leveraging deep learning methods for HSI-based wheat crop
analysis. We have listed the current state-of-the-art papers and will continue
tracking updating them in the following
https://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.

</details>

### [43] [The Comparability of Model Fusion to Measured Data in Confuser Rejection](https://arxiv.org/abs/2505.00836)
*Conor Flynn,Christopher Ebersole,Edmund Zelnio*

Main category: cs.CV

TLDR: 为了解决合成孔径雷达（SAR）数据收集不足的问题，研究提出了一种通过合成数据训练多个模型并集成的方法，同时引入干扰物拒绝技术以提高模型对未知目标的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SAR数据收集成本高且难以覆盖所有实际场景，合成数据虽能缓解数据不足问题，但与实测数据存在差异，且无法涵盖所有潜在目标。

Method: 利用合成数据训练多个模型，并通过集成技术结合这些模型；同时引入干扰物拒绝技术，使模型能够拒绝未知目标。

Result: 通过集成技术和干扰物拒绝，提高了模型在合成数据训练下的性能和对未知目标的鲁棒性。

Conclusion: 集成技术和干扰物拒绝是解决SAR数据不足和未知目标问题的有效方法。

Abstract: Data collection has always been a major issue in the modeling and training of
large deep learning networks, as no dataset can account for every slight
deviation we might see in live usage. Collecting samples can be especially
costly for Synthetic Aperture Radar (SAR), limiting the amount of unique
targets and operating conditions we are able to observe from. To counter this
lack of data, simulators have been developed utilizing the shooting and
bouncing ray method to allow for the generation of synthetic SAR data on 3D
models. While effective, the synthetically generated data does not perfectly
correlate to the measured data leading to issues when training models solely on
synthetic data. We aim to use computational power as a substitution for this
lack of quality measured data, by ensembling many models trained on synthetic
data. Synthetic data is also not complete, as we do not know what targets might
be present in a live environment. Therefore we need to have our ensembling
techniques account for these unknown targets by applying confuser rejection in
which our models will reject unknown targets it is presented with, and only
classify those it has been trained on.

</details>

### [44] [Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?](https://arxiv.org/abs/2505.00866)
*Viktor Kocur,Charalambos Tzamos,Yaqing Ding,Zuzana Berger Haladova,Torsten Sattler,Zuzana Kukelova*

Main category: cs.CV

TLDR: 论文比较了两种简单实现方法与传统径向畸变求解器的效果，发现复杂求解器在实践中并非必要。


<details>
  <summary>Details</summary>
Motivation: 解决相机径向畸变对相对位姿估计的影响，避免复杂求解器的高实现成本。

Method: 1. 结合高效针孔求解器与采样径向畸变参数；2. 使用神经网络估计畸变参数。

Result: 实验表明，复杂径向畸变求解器在实践中不必要，简单方法效果相当。

Conclusion: 在特定条件下，采样径向畸变参数优于学习型方法，复杂求解器可被替代。

Abstract: Estimating the relative pose between two cameras is a fundamental step in
many applications such as Structure-from-Motion. The common approach to
relative pose estimation is to apply a minimal solver inside a RANSAC loop.
Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras
exhibit radial distortion. Not modeling radial distortion leads to
(significantly) worse results. However, minimal radial distortion solvers are
significantly more complex than pinhole solvers, both in terms of run-time and
implementation efforts. This paper compares radial distortion solvers with two
simple-to-implement approaches that do not use minimal radial distortion
solvers: The first approach combines an efficient pinhole solver with sampled
radial undistortion parameters, where the sampled parameters are used for
undistortion prior to applying the pinhole solver. The second approach uses a
state-of-the-art neural network to estimate the distortion parameters rather
than sampling them from a set of potential values. Extensive experiments on
multiple datasets, and different camera setups, show that complex minimal
radial distortion solvers are not necessary in practice. We discuss under which
conditions a simple sampling of radial undistortion parameters is preferable
over calibrating cameras using a learning-based prior approach. Code and newly
created benchmark for relative pose estimation under radial distortion are
available at https://github.com/kocurvik/rdnet.

</details>

### [45] [CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion](https://arxiv.org/abs/2505.00938)
*Boyuan Meng,Xiaohan Zhang,Peilin Li,Zhe Wu,Yiming Li,Wenkai Zhao,Beinan Yu,Hui-Liang Shen*

Main category: cs.CV

TLDR: CDFormer是一种针对跨域少样本目标检测中特征混淆问题的Transformer方法，通过OBD和OOD模块显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本目标检测中，特征混淆（如对象-背景混淆和对象-对象混淆）是主要挑战。

Method: 提出CDFormer，包含对象-背景区分（OBD）和对象-对象区分（OOD）模块，分别通过可学习的背景标记和增强类别区分来解决混淆问题。

Result: 在1/5/10 shot设置下，CDFormer分别提升了12.9%、11.0%和10.4%的mAP，优于现有方法。

Conclusion: CDFormer有效解决了跨域少样本目标检测中的特征混淆问题，性能显著提升。

Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects
across different domains with limited class instances. Feature confusion,
including object-background confusion and object-object confusion, presents
significant challenges in both cross-domain and few-shot settings. In this
work, we introduce CDFormer, a cross-domain few-shot object detection
transformer against feature confusion, to address these challenges. The method
specifically tackles feature confusion through two key modules:
object-background distinguishing (OBD) and object-object distinguishing (OOD).
The OBD module leverages a learnable background token to differentiate between
objects and background, while the OOD module enhances the distinction between
objects of different classes. Experimental results demonstrate that CDFormer
outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%
mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,
when fine-tuned.

</details>

### [46] [Generating Animated Layouts as Structured Text Representations](https://arxiv.org/abs/2505.00975)
*Yeonsang Shin,Jihwan Kim,Yumin Song,Kyungseung Lee,Hyunhee Chung,Taeyoung Na*

Main category: cs.CV

TLDR: 论文提出了一种名为Animated Layout Generation的新方法，通过结构化文本表示实现细粒度视频控制，并开发了VAKER系统，显著提升了视频广告生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频模型在控制文本元素和动态图形方面存在局限，尤其是在视频广告等应用中。

Method: 提出Animated Layout Generation方法，结合结构化文本表示和三阶段生成流程，开发了VAKER系统。

Result: VAKER在视频广告生成中显著优于现有方法。

Conclusion: 该方法为视频广告生成提供了高效、自动化的解决方案。

Abstract: Despite the remarkable progress in text-to-video models, achieving precise
control over text elements and animated graphics remains a significant
challenge, especially in applications such as video advertisements. To address
this limitation, we introduce Animated Layout Generation, a novel approach to
extend static graphic layouts with temporal dynamics. We propose a Structured
Text Representation for fine-grained video control through hierarchical visual
elements. To demonstrate the effectiveness of our approach, we present VAKER
(Video Ad maKER), a text-to-video advertisement generation pipeline that
combines a three-stage generation process with Unstructured Text Reasoning for
seamless integration with LLMs. VAKER fully automates video advertisement
generation by incorporating dynamic layout trajectories for objects and
graphics across specific video frames. Through extensive evaluations, we
demonstrate that VAKER significantly outperforms existing methods in generating
video advertisements. Project Page:
https://yeonsangshin.github.io/projects/Vaker

</details>

### [47] [LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment](https://arxiv.org/abs/2505.00980)
*Jiahuan Long,Xin Zhou*

Main category: cs.CV

TLDR: LMDepth是一种基于Mamba的轻量级单目深度估计网络，旨在平衡性能与计算效率，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计算法难以平衡性能与计算效率，限制了在资源受限设备上的部署。

Method: 提出改进的金字塔空间池化模块和多深度Mamba块，结合线性计算实现高效解码。

Result: 在NYUDv2和KITTI数据集上表现优异，参数和计算复杂度更低。

Conclusion: LMDepth在嵌入式平台上验证了其实际应用价值，适用于边缘计算场景。

Abstract: Monocular depth estimation provides an additional depth dimension to RGB
images, making it widely applicable in various fields such as virtual reality,
autonomous driving and robotic navigation. However, existing depth estimation
algorithms often struggle to effectively balance performance and computational
efficiency, which poses challenges for deployment on resource-constrained
devices. To address this, we propose LMDepth, a lightweight Mamba-based
monocular depth estimation network, designed to reconstruct high-precision
depth information while maintaining low computational overhead. Specifically,
we propose a modified pyramid spatial pooling module that serves as a
multi-scale feature aggregator and context extractor, ensuring global spatial
information for accurate depth estimation. Moreover, we integrate multiple
depth Mamba blocks into the decoder. Designed with linear computations, the
Mamba Blocks enable LMDepth to efficiently decode depth information from global
features, providing a lightweight alternative to Transformer-based
architectures that depend on complex attention mechanisms. Extensive
experiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of
our proposed LMDepth. Compared to previous lightweight depth estimation
methods, LMDepth achieves higher performance with fewer parameters and lower
computational complexity (measured by GFLOPs). We further deploy LMDepth on an
embedded platform with INT8 quantization, validating its practicality for
real-world edge applications.

</details>

### [48] [Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis](https://arxiv.org/abs/2505.00998)
*Yu Hua,Weiming Liu,Gui Xu,Yaqing Hou,Yew-Soon Ong,Qiang Zhang*

Main category: cs.CV

TLDR: 提出了一种确定性到随机性的多样化潜在特征映射（DSDFM）方法，用于人体运动合成，解决了基于分数生成模型（SGMs）训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 基于分数生成模型（SGMs）在人体运动合成中表现优异，但训练过程复杂且不稳定，因此需要一种更稳定且多样化的方法。

Method: DSDFM分为两个阶段：1）人体运动重建阶段学习潜在空间分布；2）多样化运动生成阶段通过确定性特征映射（DerODE）和随机多样化输出生成（DivSDE）连接高斯分布与潜在空间分布。

Result: DSDFM在训练稳定性、生成多样性和准确性上优于现有方法，实验验证了其优越性。

Conclusion: DSDFM是一种高效且多样化的人体运动合成方法，无需额外训练参数即可提升性能。

Abstract: Human motion synthesis aims to generate plausible human motion sequences,
which has raised widespread attention in computer animation. Recent score-based
generative models (SGMs) have demonstrated impressive results on this task.
However, their training process involves complex curvature trajectories,
leading to unstable training process. In this paper, we propose a
Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for
human motion synthesis. DSDFM consists of two stages. The first human motion
reconstruction stage aims to learn the latent space distribution of human
motions. The second diverse motion generation stage aims to build connections
between the Gaussian distribution and the latent space distribution of human
motions, thereby enhancing the diversity and accuracy of the generated human
motions. This stage is achieved by the designed deterministic feature mapping
procedure with DerODE and stochastic diverse output generation procedure with
DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can
enhance diversity without introducing additional training parameters.Through
qualitative and quantitative experiments, DSDFM achieves state-of-the-art
results surpassing the latest methods, validating its superiority in human
motion synthesis.

</details>

### [49] [3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer](https://arxiv.org/abs/2505.01003)
*Kamel Aouaidjia,Aofan Li,Wenhao Zhang,Chongsheng Zhang*

Main category: cs.CV

TLDR: 提出了一种结合GCN和Transformer的新方法，通过多阶图表示骨架并动态关注代表性阶数，同时改进Transformer以建模全局和局部特征依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer和GCN方法在3D人体姿态估计中分别忽略了空间邻域关系或局部时间模式，且GCN缺乏姿态特定表示。

Method: 结合GCN的多阶图表示和动态图阶注意力模块，改进Transformer以建模全局和局部特征依赖关系。

Result: 在Human3.6m、MPIINF-3DHP和HumanEva-I数据集上验证了方法的有效性。

Conclusion: 新方法通过结合GCN和Transformer的优势，显著提升了3D人体姿态估计的性能。

Abstract: Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the
prevailing techniques for 3D human pose estimation. However, Transformer-based
methods either ignore the spatial neighborhood relationships between the joints
when used for skeleton representations or disregard the local temporal patterns
of the local joint movements in skeleton sequence modeling, while GCN-based
methods often neglect the need for pose-specific representations. To address
these problems, we propose a new method that exploits the graph modeling
capability of GCN to represent each skeleton with multiple graphs of different
orders, incorporated with a newly introduced Graph Order Attention module that
dynamically emphasizes the most representative orders for each joint. The
resulting spatial features of the sequence are further processed using a
proposed temporal Body Aware Transformer that models the global body feature
dependencies in the sequence with awareness of the local inter-skeleton feature
dependencies of joints. Given that our 3D pose output aligns with the central
2D pose in the sequence, we improve the self-attention mechanism to be aware of
the central pose while diminishing its focus gradually towards the first and
the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I
datasets demonstrate the effectiveness of the proposed method. Code and models
are made available on Github.

</details>

### [50] [Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance](https://arxiv.org/abs/2505.01016)
*Vishal Gandhi,Sagar Gandhi*

Main category: cs.CV

TLDR: 研究表明，深度微调（解冻至第10层）在细粒度任务中显著提升性能（如mAP50提高10%），且对原始任务性能影响极小（<0.1% mAP差异）。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在细粒度任务中优化预训练模型的微调深度，以提升性能同时避免灾难性遗忘。

Method: 采用YOLOv8n模型，逐步解冻骨干层（22、15、10层），并在自定义水果检测数据集和COCO验证集上评估性能。

Result: 深度微调在细粒度任务中表现优异，且对原始任务性能影响可忽略。

Conclusion: 中晚期骨干层特征微调对细粒度任务高效且安全，支持更深微调策略的探索。

Abstract: The success of large pre-trained object detectors hinges on their
adaptability to diverse downstream tasks. While fine-tuning is the standard
adaptation method, specializing these models for challenging fine-grained
domains necessitates careful consideration of feature granularity. The critical
question remains: how deeply should the pre-trained backbone be fine-tuned to
optimize for the specialized task without incurring catastrophic forgetting of
the original general capabilities? Addressing this, we present a systematic
empirical study evaluating the impact of fine-tuning depth. We adapt a standard
YOLOv8n model to a custom, fine-grained fruit detection dataset by
progressively unfreezing backbone layers (freeze points at layers 22, 15, and
10) and training. Performance was rigorously evaluated on both the target fruit
dataset and, using a dual-head evaluation architecture, on the original COCO
validation set. Our results demonstrate unequivocally that deeper fine-tuning
(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\%
absolute mAP50) on the fine-grained fruit task compared to only training the
head. Strikingly, this significant adaptation and specialization resulted in
negligible performance degradation (<0.1\% absolute mAP difference) on the COCO
benchmark across all tested freeze levels. We conclude that adapting
mid-to-late backbone features is highly effective for fine-grained
specialization. Critically, our results demonstrate this adaptation can be
achieved without the commonly expected penalty of catastrophic forgetting,
presenting a compelling case for exploring deeper fine-tuning strategies,
particularly when targeting complex domains or when maximizing specialized
performance is paramount.

</details>

### [51] [Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing](https://arxiv.org/abs/2505.01032)
*Ruyu Yan,Da-Qing Zhang*

Main category: cs.CV

TLDR: 提出了一种多尺度自适应独立性测试的边缘检测与去噪方法（EDD-MAIT），通过动态调整窗口大小和结合通道注意力机制，提升了边缘检测的清晰度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法常产生过于详细的边缘图，影响清晰度；固定窗口统计测试存在尺度不匹配和计算冗余问题。

Method: 结合多尺度自适应统计测试与通道注意力机制，采用梯度驱动的自适应窗口策略动态调整窗口大小。

Result: 在BSDS500和BIPED数据集上优于传统和基于学习的方法，F-score、MSE、PSNR等指标提升，运行时间减少，对高斯噪声具有鲁棒性。

Conclusion: EDD-MAIT在边缘检测和去噪方面表现出更高的准确性、效率和鲁棒性，适用于噪声环境。

Abstract: Edge detection is crucial in image processing, but existing methods often
produce overly detailed edge maps, affecting clarity. Fixed-window statistical
testing faces issues like scale mismatch and computational redundancy. To
address these, we propose a novel Multi-scale Adaptive Independence
Testing-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive
Statistical Testing-based edge detection and denoising method that integrates a
channel attention mechanism with independence testing. A gradient-driven
adaptive window strategy adjusts window sizes dynamically, improving detail
preservation and noise suppression. EDD-MAIT achieves better robustness,
accuracy, and efficiency, outperforming traditional and learning-based methods
on BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and
reduced runtime. It also shows robustness against Gaussian noise, generating
accurate and clean edge maps in noisy environments.

</details>

### [52] [Edge Detection based on Channel Attention and Inter-region Independence Test](https://arxiv.org/abs/2505.01040)
*Ru-yu Yan,Da-Qing Zhang*

Main category: cs.CV

TLDR: CAM-EDIT结合通道注意力机制和独立性测试的边缘检测方法，显著提升性能并减少噪声干扰。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法存在噪声放大和非显著细节保留过多的问题，限制了在高精度工业场景中的应用。

Method: 提出CAM-EDIT框架，整合通道注意力机制（CAM）和基于独立性测试的边缘检测（EDIT），通过多通道融合和统计独立性分析抑制噪声。

Result: 在BSDS500和NYUDv2数据集上表现优异，F-measure分数分别为0.635和0.460，优于传统和最新学习方法，噪声鲁棒性提升2.2% PSNR。

Conclusion: CAM-EDIT在高精度工业应用中展现出潜力，生成更干净的边缘图并减少伪影。

Abstract: Existing edge detection methods often suffer from noise amplification and
excessive retention of non-salient details, limiting their applicability in
high-precision industrial scenarios. To address these challenges, we propose
CAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)
and Edge Detection via Independence Testing (EDIT). The CAM module adaptively
enhances discriminative edge features through multi-channel fusion, while the
EDIT module employs region-wise statistical independence analysis (using
Fisher's exact test and chi-square test) to suppress uncorrelated
noise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate
state-of-the-art performance. Among the nine comparison algorithms, the
F-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of
19.2\% to 26.5\% over traditional methods (Canny, CannySR), and better than the
latest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations
further reveal a 2.2\% PSNR improvement under Gaussian noise compared to
baseline methods. Qualitative results exhibit cleaner edge maps with reduced
artifacts, demonstrating its potential for high-precision industrial
applications.

</details>

### [53] [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
*Marco Salmè,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: 该研究评估了指令调优的视觉语言模型（VLMs）在低资源语言（意大利语、德语、西班牙语）中生成放射学报告的性能，发现语言和领域特定训练对提升报告质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前AI在医疗领域的应用面临低资源语言中生成准确放射学报告的挑战，缺乏同时具备医学领域和低资源语言知识的模型。

Method: 采用LLaVA架构，系统评估预训练模型在通用、领域特定和低资源语言特定数据集上的表现，并分析不同适应策略。

Result: 语言特定模型表现最佳，医学术语微调进一步提升了性能；温度参数对报告连贯性有影响。

Conclusion: 语言和领域特定训练对多语言放射学报告生成至关重要，为未来模型调优和语言适应研究提供了方向。

Abstract: The integration of artificial intelligence in healthcare has opened new
horizons for improving medical diagnostics and patient care. However,
challenges persist in developing systems capable of generating accurate and
contextually relevant radiology reports, particularly in low-resource
languages. In this study, we present a comprehensive benchmark to evaluate the
performance of instruction-tuned Vision-Language Models (VLMs) in the
specialized task of radiology report generation across three low-resource
languages: Italian, German, and Spanish. Employing the LLaVA architectural
framework, we conducted a systematic evaluation of pre-trained models utilizing
general datasets, domain-specific datasets, and low-resource language-specific
datasets. In light of the unavailability of models that possess prior knowledge
of both the medical domain and low-resource languages, we analyzed various
adaptations to determine the most effective approach for these contexts. The
results revealed that language-specific models substantially outperformed both
general and domain-specific models in generating radiology reports, emphasizing
the critical role of linguistic adaptation. Additionally, models fine-tuned
with medical terminology exhibited enhanced performance across all languages
compared to models with generic knowledge, highlighting the importance of
domain-specific training. We also explored the influence of the temperature
parameter on the coherence of report generation, providing insights for optimal
model settings. Our findings highlight the importance of tailored language and
domain-specific training for improving the quality and accuracy of radiological
reports in multilingual settings. This research not only advances our
understanding of VLMs adaptability in healthcare but also points to significant
avenues for future investigations into model tuning and language-specific
adaptations.

</details>

### [54] [Transferable Adversarial Attacks on Black-Box Vision-Language Models](https://arxiv.org/abs/2505.01050)
*Kai Hu,Weichen Yu,Li Zhang,Alexander Robey,Andy Zou,Chengming Xu,Haoqi Hu,Matt Fredrikson*

Main category: cs.CV

TLDR: 研究发现，针对视觉大语言模型（VLLMs）的对抗性攻击具有高度可迁移性，能诱导模型产生攻击者选择的错误解读，亟需加强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索对抗性攻击在VLLMs中的可迁移性和有效性，填补现有研究的空白。

Method: 通过制作针对性对抗样本和通用扰动，测试其在多款主流VLLMs（如GPT-4o、Claude、Gemini）中的效果。

Result: 实验表明，对抗样本能成功诱导模型错误解读视觉信息，且通用扰动在多模型中均有效。

Conclusion: 当前VLLMs普遍存在对抗性攻击漏洞，需开发鲁棒性解决方案以确保安全部署。

Abstract: Vision Large Language Models (VLLMs) are increasingly deployed to offer
advanced capabilities on inputs comprising both text and images. While prior
research has shown that adversarial attacks can transfer from open-source to
proprietary black-box models in text-only and vision-only contexts, the extent
and effectiveness of such vulnerabilities remain underexplored for VLLMs. We
present a comprehensive analysis demonstrating that targeted adversarial
examples are highly transferable to widely-used proprietary VLLMs such as
GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to
induce specific attacker-chosen interpretations of visual information, such as
misinterpreting hazardous content as safe, overlooking sensitive or restricted
material, or generating detailed incorrect responses aligned with the
attacker's intent. Furthermore, we discover that universal perturbations --
modifications applicable to a wide set of images -- can consistently induce
these misinterpretations across multiple proprietary VLLMs. Our experimental
results on object recognition, visual question answering, and image captioning
show that this vulnerability is common across current state-of-the-art models,
and underscore an urgent need for robust mitigations to ensure the safe and
secure deployment of VLLMs.

</details>

### [55] [GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation](https://arxiv.org/abs/2505.01057)
*Boris Kriuk,Matey Yordanov*

Main category: cs.CV

TLDR: GeloVec是一种基于CNN的注意力平滑框架，用于语义分割，通过高维几何平滑方法解决传统方法的边界不稳定性和上下文不连续性问题。


<details>
  <summary>Details</summary>
Motivation: 现有注意力支持的语义分割方法在特征映射中存在边界不稳定性和上下文不连续性，GeloVec旨在通过几何平滑方法解决这些问题。

Method: 结合改进的Chebyshev距离度量和多空间变换，通过自适应采样权重系统计算n维特征空间中的几何距离，同时利用张量投影和正交基向量增强特征表示。

Result: 在多个基准数据集上验证，mIoU分别提升2.1%、2.7%和2.4%，计算效率高且具有强泛化能力。

Conclusion: GeloVec通过Riemannian几何理论保证分割稳定性，计算高效且适用于多学科，信息无损变换。

Abstract: This paper introduces GeloVec, a new CNN-based attention smoothing framework
for semantic segmentation that addresses critical limitations in conventional
approaches. While existing attention-backed segmentation methods suffer from
boundary instability and contextual discontinuities during feature mapping, our
framework implements a higher-dimensional geometric smoothing method to
establish a robust manifold relationships between visually coherent regions.
GeloVec combines modified Chebyshev distance metrics with multispatial
transformations to enhance segmentation accuracy through stabilized feature
extraction. The core innovation lies in the adaptive sampling weights system
that calculates geometric distances in n-dimensional feature space, achieving
superior edge preservation while maintaining intra-class homogeneity. The
multispatial transformation matrix incorporates tensorial projections with
orthogonal basis vectors, creating more discriminative feature representations
without sacrificing computational efficiency. Experimental validation across
multiple benchmark datasets demonstrates significant improvements in
segmentation performance, with mean Intersection over Union (mIoU) gains of
2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets
respectively compared to state-of-the-art methods. GeloVec's mathematical
foundation in Riemannian geometry provides theoretical guarantees on
segmentation stability. Importantly, our framework maintains computational
efficiency through parallelized implementation of geodesic transformations and
exhibits strong generalization capabilities across disciplines due to the
absence of information loss during transformations.

</details>

### [56] [Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs](https://arxiv.org/abs/2505.01064)
*Hari Chandana Kuchibhotla,Sai Srinivas Kancheti,Abbavaram Gowtham Reddy,Vineeth N Balasubramanian*

Main category: cs.CV

TLDR: 论文提出了一种名为NeaR的方法，用于解决无约束输出空间的细粒度视觉识别（VF-FGVR）问题，通过利用多模态大语言模型（MLLM）生成标签并微调CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 在缺乏标注数据的领域（如医学影像），传统的细粒度视觉识别方法无法适用，而直接使用MLLM成本高且效率低。

Method: 提出NeaR方法，通过MLLM生成弱监督标签，并微调下游CLIP模型以处理标签噪声和开放性。

Result: NeaR为高效的VF-FGVR设立了新基准。

Conclusion: NeaR提供了一种高效且实用的解决方案，解决了VF-FGVR中的标签生成和模型训练问题。

Abstract: Fine-grained Visual Recognition (FGVR) involves distinguishing between
visually similar categories, which is inherently challenging due to subtle
inter-class differences and the need for large, expert-annotated datasets. In
domains like medical imaging, such curated datasets are unavailable due to
issues like privacy concerns and high annotation costs. In such scenarios
lacking labeled data, an FGVR model cannot rely on a predefined set of training
labels, and hence has an unconstrained output space for predictions. We refer
to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict
labels from an unconstrained output space without prior label information.
While recent Multimodal Large Language Models (MLLMs) show potential for
VF-FGVR, querying these models for each test input is impractical because of
high costs and prohibitive inference times. To address these limitations, we
introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel
approach that fine-tunes a downstream CLIP model using labels generated by an
MLLM. Our approach constructs a weakly supervised dataset from a small,
unlabeled training set, leveraging MLLMs for label generation. NeaR is designed
to handle the noise, stochasticity, and open-endedness inherent in labels
generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

</details>

### [57] [Improving Editability in Image Generation with Layer-wise Memory](https://arxiv.org/abs/2505.01079)
*Daneul Kim,Jaeah Lee,Jaesik Park*

Main category: cs.CV

TLDR: 该论文提出了一种支持多步图像编辑的框架，通过层记忆和背景一致性引导，解决了现有方法在连续编辑中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法主要针对单对象修改，难以处理多步编辑任务，尤其是在保持先前编辑内容的同时自然融入新对象。

Method: 提出层记忆存储潜在表示和提示嵌入，结合背景一致性引导和多查询解缠技术，实现自然的多步编辑。

Result: 通过新基准数据集验证，该方法在多步编辑任务中表现优异，仅需粗略掩码即可保持高质量结果。

Conclusion: 该框架显著提升了复杂图像编辑任务的效率和效果，支持多步编辑且用户操作简便。

Abstract: Most real-world image editing tasks require multiple sequential edits to
achieve desired results. Current editing approaches, primarily designed for
single-object modifications, struggle with sequential editing: especially with
maintaining previous edits along with adapting new objects naturally into the
existing content. These limitations significantly hinder complex editing
scenarios where multiple objects need to be modified while preserving their
contextual relationships. We address this fundamental challenge through two key
proposals: enabling rough mask inputs that preserve existing content while
naturally integrating new elements and supporting consistent editing across
multiple modifications. Our framework achieves this through layer-wise memory,
which stores latent representations and prompt embeddings from previous edits.
We propose Background Consistency Guidance that leverages memorized latents to
maintain scene coherence and Multi-Query Disentanglement in cross-attention
that ensures natural adaptation to existing content. To evaluate our method, we
present a new benchmark dataset incorporating semantic alignment metrics and
interactive editing scenarios. Through comprehensive experiments, we
demonstrate superior performance in iterative image editing tasks with minimal
user effort, requiring only rough masks while maintaining high-quality results
throughout multiple editing steps.

</details>

### [58] [Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation](https://arxiv.org/abs/2505.01091)
*Daniele Molino,Francesco di Feola,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: 提出了一种专为多模态医学数据生成设计的框架，能够生成多视角胸部X光及其临床报告，性能优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 医学数据的复杂性和临床准确性需求使得通用生成模型难以直接应用，需领域特定优化。

Method: 基于MIMIC-CXR数据集，开发了一个多模态生成框架，生成高质量X光图像和语义连贯的报告。

Result: 在FID和BLEU评分上表现优异，生成数据在下游疾病分类任务中媲美真实数据。

Conclusion: 领域特定优化能显著提升生成模型在临床中的应用价值，为未来医学数据生成研究铺路。

Abstract: Generative models have revolutionized Artificial Intelligence (AI),
particularly in multimodal applications. However, adapting these models to the
medical domain poses unique challenges due to the complexity of medical data
and the stringent need for clinical accuracy. In this work, we introduce a
framework specifically designed for multimodal medical data generation. By
enabling the generation of multi-view chest X-rays and their associated
clinical report, it bridges the gap between general-purpose vision-language
models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR
dataset, the proposed framework shows superior performance in generating
high-fidelity images and semantically coherent reports. Our quantitative
evaluation reveals significant results in terms of FID and BLEU scores,
showcasing the quality of the generated data. Notably, our framework achieves
comparable or even superior performance compared to real data on downstream
disease classification tasks, underlining its potential as a tool for medical
research and diagnostics. This study highlights the importance of
domain-specific adaptations in enhancing the relevance and utility of
generative models for clinical applications, paving the way for future
advancements in synthetic multimodal medical data generation.

</details>

### [59] [VSC: Visual Search Compositional Text-to-Image Diffusion Model](https://arxiv.org/abs/2505.01104)
*Do Huu Dat,Nam Hyeonu,Po-Yuan Mao,Tae-Hyun Oh*

Main category: cs.CV

TLDR: 提出了一种新的组合生成方法，通过成对图像嵌入改进属性-对象绑定，解决了多属性-对象对提示中的绑定问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本编码器（如CLIP）在处理复杂语言关系和修饰词时表现不佳，导致属性与对象绑定不准确。

Method: 将复杂提示分解为子提示，生成对应图像并计算视觉原型，结合文本嵌入增强表示；通过基于分割的定位训练解决交叉注意力错位问题。

Result: 在T2I CompBench基准测试中表现优于现有组合文本到图像扩散模型，图像质量更高，且在提示中绑定对数增加时更具鲁棒性。

Conclusion: 该方法显著提升了多属性-对象对提示中的绑定准确性，为复杂文本到图像生成提供了有效解决方案。

Abstract: Text-to-image diffusion models have shown impressive capabilities in
generating realistic visuals from natural-language prompts, yet they often
struggle with accurately binding attributes to corresponding objects,
especially in prompts containing multiple attribute-object pairs. This
challenge primarily arises from the limitations of commonly used text encoders,
such as CLIP, which can fail to encode complex linguistic relationships and
modifiers effectively. Existing approaches have attempted to mitigate these
issues through attention map control during inference and the use of layout
information or fine-tuning during training, yet they face performance drops
with increased prompt complexity. In this work, we introduce a novel
compositional generation method that leverages pairwise image embeddings to
improve attribute-object binding. Our approach decomposes complex prompts into
sub-prompts, generates corresponding images, and computes visual prototypes
that fuse with text embeddings to enhance representation. By applying
segmentation-based localization training, we address cross-attention
misalignment, achieving improved accuracy in binding multiple attributes to
objects. Our approaches outperform existing compositional text-to-image
diffusion models on the benchmark T2I CompBench, achieving better image
quality, evaluated by humans, and emerging robustness under scaling number of
binding pairs in the prompt.

</details>

### [60] [Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study](https://arxiv.org/abs/2505.01109)
*Ali Mammadov,Loic Le Folgoc,Julien Adam,Anne Buronfosse,Gilles Hayem,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TLDR: 研究表明，在高质量自监督学习特征提取器的支持下，简单的基于实例的多实例学习方法（MIL）性能优于复杂的基于嵌入的MIL方法，且更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 探讨在自监督学习（SSL）特征提取器质量提升的背景下，基于实例的MIL方法是否仍优于基于嵌入的方法。

Method: 通过710次实验，比较了10种MIL策略、6种自监督学习方法、4种基础模型及多种病理学适应技术，并引入了4种新的基于实例的MIL方法。

Result: 实验显示，简单的基于实例的MIL方法在性能上与复杂的基于嵌入的SOTA方法相当或更优，并在BRACS和Camelyon16数据集上取得了新SOTA结果。

Conclusion: 建议未来研究应更关注适应病理学的高质量自监督学习方法，而非复杂的基于嵌入的MIL方法，以提升可解释性和性能。

Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole
Slide Image (WSI) classification. It consists of dividing each slide into
patches, which are treated as a bag of instances labeled with a global label.
MIL includes two main approaches: instance-based and embedding-based. In the
former, each patch is classified independently, and then the patch scores are
aggregated to predict the bag label. In the latter, bag classification is
performed after aggregating patch embeddings. Even if instance-based methods
are naturally more interpretable, embedding-based MILs have usually been
preferred in the past due to their robustness to poor feature extractors.
However, recently, the quality of feature embeddings has drastically increased
using self-supervised learning (SSL). Nevertheless, many authors continue to
endorse the superiority of embedding-based MIL. To investigate this further, we
conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6
self-supervised methods with 4 backbones, 4 foundation models, and various
pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL
methods never used before in the pathology domain. Through these extensive
experiments, we show that with a good SSL feature extractor, simple
instance-based MILs, with very few parameters, obtain similar or better
performance than complex, state-of-the-art (SOTA) embedding-based MIL methods,
setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple
instance-based MIL methods are naturally more interpretable and explainable to
clinicians, our results suggest that more effort should be put into
well-adapted SSL methods for WSI rather than into complex embedding-based MIL
methods.

</details>

### [61] [FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis](https://arxiv.org/abs/2505.01172)
*Jiangtong Tan,Hu Yu,Jie Huang,Jie Xiao,Feng Zhao*

Main category: cs.CV

TLDR: 论文提出了一种基于PCA的训练无关长视频生成方法FreePCA，通过解耦全局一致性和局部质量，显著提升了生成视频的视觉和运动一致性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临分布偏移问题，现有方法难以同时兼顾全局一致性和局部质量。

Method: 利用PCA将全局和局部信息解耦为一致外观和运动强度特征，并通过余弦相似度测量和渐进整合实现高质量生成。

Result: 实验表明FreePCA无需训练即可应用于多种视频扩散模型，显著提升生成质量。

Conclusion: FreePCA为长视频生成提供了一种高效且无需训练的解决方案，具有广泛适用性。

Abstract: Long video generation involves generating extended videos using models
trained on short videos, suffering from distribution shifts due to varying
frame counts. It necessitates the use of local information from the original
short frames to enhance visual and motion quality, and global information from
the entire long frames to ensure appearance consistency. Existing training-free
methods struggle to effectively integrate the benefits of both, as appearance
and motion in videos are closely coupled, leading to motion inconsistency and
visual quality. In this paper, we reveal that global and local information can
be precisely decoupled into consistent appearance and motion intensity
information by applying Principal Component Analysis (PCA), allowing for
refined complementary integration of global consistency and local quality. With
this insight, we propose FreePCA, a training-free long video generation
paradigm based on PCA that simultaneously achieves high consistency and
quality. Concretely, we decouple consistent appearance and motion intensity
features by measuring cosine similarity in the principal component space.
Critically, we progressively integrate these features to preserve original
quality and ensure smooth transitions, while further enhancing consistency by
reusing the mean statistics of the initial noise. Experiments demonstrate that
FreePCA can be applied to various video diffusion models without requiring
training, leading to substantial improvements. Code is available at
https://github.com/JosephTiTan/FreePCA.

</details>

### [62] [TSTMotion: Training-free Scene-awarenText-to-motion Generation](https://arxiv.org/abs/2505.01182)
*Ziyan Guo,Haoxuan Qu,Hossein Rahmani,Dewen Soh,Ping Hu,Qiuhong Ke,Jun Liu*

Main category: cs.CV

TLDR: 提出了一种无需训练的、场景感知的文本到动作生成框架TSTMotion，利用预训练模型和场景信息生成动作序列。


<details>
  <summary>Details</summary>
Motivation: 现有场景感知方法依赖大规模真实动作数据，成本高昂，因此提出无需训练的方法。

Method: 结合基础模型预测场景感知动作指导，并将其融入空白背景动作生成器。

Result: 实验证明框架有效且通用。

Conclusion: TSTMotion为场景感知文本到动作生成提供了一种高效解决方案。

Abstract: Text-to-motion generation has recently garnered significant research
interest, primarily focusing on generating human motion sequences in blank
backgrounds. However, human motions commonly occur within diverse 3D scenes,
which has prompted exploration into scene-aware text-to-motion generation
methods. Yet, existing scene-aware methods often rely on large-scale
ground-truth motion sequences in diverse 3D scenes, which poses practical
challenges due to the expensive cost. To mitigate this challenge, we are the
first to propose a \textbf{T}raining-free \textbf{S}cene-aware
\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that
efficiently empowers pre-trained blank-background motion generators with the
scene-aware capability. Specifically, conditioned on the given 3D scene and
text description, we adopt foundation models together to reason, predict and
validate a scene-aware motion guidance. Then, the motion guidance is
incorporated into the blank-background motion generators with two
modifications, resulting in scene-aware text-driven motion sequences. Extensive
experiments demonstrate the efficacy and generalizability of our proposed
framework. We release our code in \href{https://tstmotion.github.io/}{Project
Page}.

</details>

### [63] [Efficient Vision-based Vehicle Speed Estimation](https://arxiv.org/abs/2505.01203)
*Andrej Macko,Lukáš Gajdošech,Viktor Kocur*

Main category: cs.CV

TLDR: 提出了一种计算高效的车辆速度估计方法，通过改进3D边界框和消失点几何技术，显著提升了实时性能。在BrnoCompSpeed数据集上验证了其检测和速度估计精度，并在多种硬件平台上展示了更高的FPS和更优的速度估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有车辆速度估计方法计算效率不足，难以满足实时需求，尤其是在边缘设备上。

Method: 基于2D检测和消失点几何的3D边界框技术，引入改进以提升实时性能，包括模型量化和小型化。

Result: 在BrnoCompSpeed数据集上，最佳模型的中位速度估计误差（0.58 km/h）、检测精度（91.02%）和召回率（91.14%）均优于现有方法，且速度快5.5倍。

Conclusion: 通过平衡计算成本和精度，量化后的小型模型最适合实际部署，显著提升了实时性能和准确性。

Abstract: This paper presents a computationally efficient method for vehicle speed
estimation from traffic camera footage. Building upon previous work that
utilizes 3D bounding boxes derived from 2D detections and vanishing point
geometry, we introduce several improvements to enhance real-time performance.
We evaluate our method in several variants on the BrnoCompSpeed dataset in
terms of vehicle detection and speed estimation accuracy. Our extensive
evaluation across various hardware platforms, including edge devices,
demonstrates significant gains in frames per second (FPS) compared to the prior
state-of-the-art, while maintaining comparable or improved speed estimation
accuracy. We analyze the trade-off between accuracy and computational cost,
showing that smaller models utilizing post-training quantization offer the best
balance for real-world deployment. Our best performing model beats previous
state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h
vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.
83.32%) while also being 5.5 times faster.

</details>

### [64] [T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph](https://arxiv.org/abs/2505.01207)
*Qingyu Xian,Weiqin Jiao,Hao Cheng,Berend Jan van der Zwaag,Yanqiu Huang*

Main category: cs.CV

TLDR: 论文提出T-Graph模块，通过构建翻译图和多层感知机，提升稀疏视图下的相机位姿估计性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图相机位姿估计在遥感应用中具有挑战性，现有方法常忽略视角间的平移信息，导致性能不佳。

Method: T-Graph通过MLP处理成对图像特征，构建全连接翻译图，并引入两种平移表示（relative-t和pair-t）。

Result: 在RelPose++和Forge方法上的实验表明，T-Graph显著提升性能，相机中心精度提高1%至6%。

Conclusion: T-Graph是一种轻量级、即插即用的模块，能有效提升稀疏视图下的相机位姿估计性能。

Abstract: Sparse-view camera pose estimation, which aims to estimate the
6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from
different viewpoints, is a fundamental yet challenging problem in remote
sensing applications. Existing methods often overlook the translation
information between each pair of viewpoints, leading to suboptimal performance
in sparse-view scenarios. To address this limitation, we introduce T-Graph, a
lightweight, plug-and-play module to enhance camera pose estimation in
sparse-view settings. T-graph takes paired image features as input and maps
them through a Multilayer Perceptron (MLP). It then constructs a fully
connected translation graph, where nodes represent cameras and edges encode
their translation relationships. It can be seamlessly integrated into existing
models as an additional branch in parallel with the original prediction,
maintaining efficiency and ease of use. Furthermore, we introduce two pairwise
translation representations, relative-t and pair-t, formulated under different
local coordinate systems. While relative-t captures intuitive spatial
relationships, pair-t offers a rotation-disentangled alternative. The two
representations contribute to enhanced adaptability across diverse application
scenarios, further improving our module's robustness. Extensive experiments on
two state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D
and IMC PhotoTourism) validate both the effectiveness and generalizability of
T-Graph. The results demonstrate consistent improvements across various
metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8
viewpoints.

</details>

### [65] [High Dynamic Range Novel View Synthesis with Single Exposure](https://arxiv.org/abs/2505.01212)
*Kaixuan Zhang,Hu Wang,Minxian Li,Mingwu Ren,Mao Ye,Xiatian Zhu*

Main category: cs.CV

TLDR: 论文提出了一种单曝光HDR-NVS方法Mono-HDR-3D，解决了多曝光方法的运动伪影和高成本问题。


<details>
  <summary>Details</summary>
Motivation: 多曝光HDR-NVS方法存在运动伪影和高成本问题，需要一种仅依赖单曝光LDR图像的方法。

Method: 提出Mono-HDR-3D，包含两个模块：LDR转HDR和HDR转LDR，实现闭环无监督学习。

Result: 实验表明Mono-HDR-3D显著优于现有方法。

Conclusion: Mono-HDR-3D为单曝光HDR-NVS提供了高效解决方案，并兼容现有NVS模型。

Abstract: High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D
scene HDR model from Low Dynamic Range (LDR) imagery. Typically,
multiple-exposure LDR images are employed to capture a wider range of
brightness levels in a scene, as a single LDR image cannot represent both the
brightest and darkest regions simultaneously. While effective, this
multiple-exposure HDR-NVS approach has significant limitations, including
susceptibility to motion artifacts (e.g., ghosting and blurring), high capture
and storage costs. To overcome these challenges, we introduce, for the first
time, the single-exposure HDR-NVS problem, where only single exposure LDR
images are available during training. We further introduce a novel approach,
Mono-HDR-3D, featuring two dedicated modules formulated by the LDR image
formation principles, one for converting LDR colors to HDR counterparts, and
the other for transforming HDR images to LDR format so that unsupervised
learning is enabled in a closed loop. Designed as a meta-algorithm, our
approach can be seamlessly integrated with existing NVS models. Extensive
experiments show that Mono-HDR-3D significantly outperforms previous methods.
Source code will be released.

</details>

### [66] [RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement](https://arxiv.org/abs/2505.01224)
*Kui Jiang,Yan Luo,Junjun Jiang,Xin Xu,Fei Ma,Fei Yu*

Main category: cs.CV

TLDR: 提出了一种基于动态排序扫描机制的Mamba改进方法（RD-UIE），用于水下图像增强，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 水下图像因波长依赖性衰减导致内容退化和颜色失真，现有Mamba模型因固定扫描路径无法适应复杂环境。

Method: 引入动态排序扫描机制和视觉自适应状态块（VSSB），结合跨特征桥（CFB）融合多尺度特征。

Result: 在多个基准测试中，RD-UIE平均性能提升0.55 dB，优于现有方法WMamba。

Conclusion: RD-UIE通过动态建模全局和局部关系，有效提升了水下图像增强效果。

Abstract: Underwater image enhancement (UIE) is a critical preprocessing step for
marine vision applications, where wavelength-dependent attenuation causes
severe content degradation and color distortion. While recent state space
models like Mamba show potential for long-range dependency modeling, their
unfolding operations and fixed scan paths on 1D sequences fail to adapt to
local object semantics and global relation modeling, limiting their efficacy in
complex underwater environments. To address this, we enhance conventional Mamba
with the sorting-based scanning mechanism that dynamically reorders scanning
sequences based on statistical distribution of spatial correlation of all
pixels. In this way, it encourages the network to prioritize the most
informative components--structural and semantic features. Upon building this
mechanism, we devise a Visually Self-adaptive State Block (VSSB) that
harmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,
enabling coherent integration of global context and local relational cues. This
exquisite design helps eliminate global focus bias, especially for widely
distributed contents, which greatly weakens the statistical frequency. For
robust feature extraction and refinement, we design a cross-feature bridge
(CFB) to adaptively fuse multi-scale representations. These efforts compose the
novel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive
experiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms
the state-of-the-art approach WMamba in both quantitative metrics and visual
fidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.
Our code is available at https://github.com/kkoucy/RD-UIE/tree/main

</details>

### [67] [Core-Set Selection for Data-efficient Land Cover Segmentation](https://arxiv.org/abs/2505.01225)
*Keiller Nogueira,Akram Zaytar,Wanli Ma,Ribana Roscher,Ronny Hänsch,Caleb Robinson,Anthony Ortiz,Simone Nsutezo,Rahul Dodhia,Juan M. Lavista Ferres,Oktay Karakuş,Paul L. Rosin*

Main category: cs.CV

TLDR: 论文提出六种核心集选择方法，用于从遥感图像分割数据集中选择重要子集，实验表明部分方法优于随机选择甚至全数据训练。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型依赖大数据集训练，但大数据的复杂性、偏差和计算资源需求被忽视，需同时考虑数据数量与质量。

Method: 提出六种基于图像、标签或两者结合的核心集选择方法，并在三个土地分类数据集（DFC2022、Vaihingen、Potsdam）上进行基准测试。

Result: 实验显示，部分子集训练方法优于随机选择基线，甚至优于全数据训练，验证了数据为中心学习在遥感领域的潜力。

Conclusion: 数据为中心的学习方法在遥感领域具有重要价值，代码已开源。

Abstract: The increasing accessibility of remotely sensed data and the potential of
such data to inform large-scale decision-making has driven the development of
deep learning models for many Earth Observation tasks. Traditionally, such
models must be trained on large datasets. However, the common assumption that
broadly larger datasets lead to better outcomes tends to overlook the
complexities of the data distribution, the potential for introducing biases and
noise, and the computational resources required for processing and storing vast
datasets. Therefore, effective solutions should consider both the quantity and
quality of data. In this paper, we propose six novel core-set selection methods
for selecting important subsets of samples from remote sensing image
segmentation datasets that rely on imagery only, labels only, and a combination
of each. We benchmark these approaches against a random-selection baseline on
three commonly used land cover classification datasets: DFC2022, Vaihingen, and
Potsdam. In each of the datasets, we demonstrate that training on a subset of
samples outperforms the random baseline, and some approaches outperform
training on all available data. This result shows the importance and potential
of data-centric learning for the remote sensing domain. The code is available
at https://github.com/keillernogueira/data-centric-rs-classification/.

</details>

### [68] [Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2505.01235)
*Youngsik Yun,Jeongmin Bae,Hyunseung Son,Seoha Kim,Hahyun Lee,Gun Bang,Youngjung Uh*

Main category: cs.CV

TLDR: 本文提出了一种在线动态场景重建方法，通过消除时间不一致性提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有在线重建方法忽视时间一致性，导致静态区域出现明显伪影，本文旨在解决这一问题。

Method: 通过从观测中学习并减去误差，恢复理想观测，从而增强时间一致性。

Result: 实验表明，该方法显著提升了时间一致性和渲染质量。

Conclusion: 该方法有效解决了在线重建中的时间不一致性问题，提升了整体质量。

Abstract: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

</details>

### [69] [Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](https://arxiv.org/abs/2505.01249)
*Christopher K. I. Williams*

Main category: cs.CV

TLDR: 论文提出了一种基于线性下采样的方法，用于融合多个注视点的高分辨率图像，并通过贝叶斯实验设计解决“下一步注视哪里”的问题。


<details>
  <summary>Details</summary>
Motivation: 解决人类和脊椎动物如何通过多个注视点的高分辨率图像融合场景的问题。

Method: 利用已知几何将视网膜变换表示为高分辨率潜在图像的线性下采样，并在因子分析（FA）和FA混合模型中进行精确推断。

Result: 在Frey面孔和MNIST数据集上的实验证明了模型的有效性。

Conclusion: 提出的方法能够有效融合多个注视点信息，并通过贝叶斯实验设计优化注视点选择。

Abstract: Humans (and many vertebrates) face the problem of fusing together multiple
fixations of a scene in order to obtain a representation of the whole, where
each fixation uses a high-resolution fovea and decreasing resolution in the
periphery. In this paper we explicitly represent the retinal transformation of
a fixation as a linear downsampling of a high-resolution latent image of the
scene, exploiting the known geometry. This linear transformation allows us to
carry out exact inference for the latent variables in factor analysis (FA) and
mixtures of FA models of the scene. Further, this allows us to formulate and
solve the choice of "where to look next" as a Bayesian experimental design
problem using the Expected Information Gain criterion. Experiments on the Frey
faces and MNIST datasets demonstrate the effectiveness of our models.

</details>

### [70] [CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking](https://arxiv.org/abs/2505.01257)
*Vladimir Somers,Baptiste Standaert,Victor Joos,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TLDR: CAMEL是一种新型的关联模块，通过数据学习关联策略，避免手工启发式方法，保持模块化设计，并在多个跟踪基准上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的跟踪方法依赖手工规则进行时间关联，难以捕捉复杂跟踪线索的交互。

Method: CAMEL采用两个基于Transformer的模块和关联中心训练方案，建模目标与关联线索的复杂交互。

Result: CAMELTrack在多个跟踪基准上达到最先进性能。

Conclusion: CAMEL通过数据驱动方法改进了关联策略，同时保持了模块化设计的优势。

Abstract: Online multi-object tracking has been recently dominated by
tracking-by-detection (TbD) methods, where recent advances rely on increasingly
sophisticated heuristics for tracklet representation, feature fusion, and
multi-stage matching. The key strength of TbD lies in its modular design,
enabling the integration of specialized off-the-shelf models like motion
predictors and re-identification. However, the extensive usage of human-crafted
rules for temporal associations makes these methods inherently limited in their
ability to capture the complex interplay between various tracking cues. In this
work, we introduce CAMEL, a novel association module for Context-Aware
Multi-Cue ExpLoitation, that learns resilient association strategies directly
from data, breaking free from hand-crafted heuristics while maintaining TbD's
valuable modularity. At its core, CAMEL employs two transformer-based modules
and relies on a novel association-centric training scheme to effectively model
the complex interactions between tracked targets and their various association
cues. Unlike end-to-end detection-by-tracking approaches, our method remains
lightweight and fast to train while being able to leverage external
off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,
achieves state-of-the-art performance on multiple tracking benchmarks. Our code
is available at https://github.com/TrackingLaboratory/CAMELTrack.

</details>

### [71] [Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain](https://arxiv.org/abs/2505.01267)
*Gaozheng Pei,Ke Ma,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TLDR: 论文提出了一种基于频率域的对抗净化方法，通过替换低频振幅谱和限制相位谱范围，有效消除对抗扰动并保留图像内容与结构。


<details>
  <summary>Details</summary>
Motivation: 现有对抗净化方法因缺乏对抗扰动的分布信息，容易破坏图像正常语义，因此需从频率域视角寻找更优解决方案。

Method: 将图像分解为振幅谱和相位谱，替换低频振幅谱并限制相位谱范围，以消除对抗扰动并保留原始图像信息。

Result: 实验证明，该方法在消除对抗扰动的同时显著优于现有防御方法。

Conclusion: 频率域视角为对抗净化提供了新思路，能更有效地平衡扰动消除与图像保留。

Abstract: The diffusion-based adversarial purification methods attempt to drown
adversarial perturbations into a part of isotropic noise through the forward
process, and then recover the clean images through the reverse process. Due to
the lack of distribution information about adversarial perturbations in the
pixel domain, it is often unavoidable to damage normal semantics. We turn to
the frequency domain perspective, decomposing the image into amplitude spectrum
and phase spectrum. We find that for both spectra, the damage caused by
adversarial perturbations tends to increase monotonically with frequency. This
means that we can extract the content and structural information of the
original clean sample from the frequency components that are less damaged.
Meanwhile, theoretical analysis indicates that existing purification methods
indiscriminately damage all frequency components, leading to excessive damage
to the image. Therefore, we propose a purification method that can eliminate
adversarial perturbations while maximizing the preservation of the content and
structure of the original image. Specifically, at each time step during the
reverse process, for the amplitude spectrum, we replace the low-frequency
components of the estimated image's amplitude spectrum with the corresponding
parts of the adversarial image. For the phase spectrum, we project the phase of
the estimated image into a designated range of the adversarial image's phase
spectrum, focusing on the low frequencies. Empirical evidence from extensive
experiments demonstrates that our method significantly outperforms most current
defense methods.

</details>

### [72] [FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors](https://arxiv.org/abs/2505.01322)
*Chenxi Li,Weijie Wang,Qiang Li,Bruno Lepri,Nicu Sebe,Weizhi Nie*

Main category: cs.CV

TLDR: FreeInsert是一种无需空间先验的3D场景文本驱动对象插入框架，利用基础模型实现灵活编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖空间先验（如2D掩码或3D边界框），难以保证插入对象的一致性，限制了灵活性和可扩展性。

Method: 结合MLLM、LGM和扩散模型，通过语义解析、空间推理和分层细化实现无监督对象插入。

Result: 实验表明，FreeInsert实现了语义一致、空间精确且视觉逼真的3D插入。

Conclusion: FreeInsert提供了一种用户友好且灵活的3D场景编辑方法，无需依赖空间先验。

Abstract: Text-driven object insertion in 3D scenes is an emerging task that enables
intuitive scene editing through natural language. However, existing 2D
editing-based methods often rely on spatial priors such as 2D masks or 3D
bounding boxes, and they struggle to ensure consistency of the inserted object.
These limitations hinder flexibility and scalability in real-world
applications. In this paper, we propose FreeInsert, a novel framework that
leverages foundation models including MLLMs, LGMs, and diffusion models to
disentangle object generation from spatial placement. This enables unsupervised
and flexible object insertion in 3D scenes without spatial priors. FreeInsert
starts with an MLLM-based parser that extracts structured semantics, including
object types, spatial relationships, and attachment regions, from user
instructions. These semantics guide both the reconstruction of the inserted
object for 3D consistency and the learning of its degrees of freedom. We
leverage the spatial reasoning capabilities of MLLMs to initialize object pose
and scale. A hierarchical, spatially aware refinement stage further integrates
spatial semantics and MLLM-inferred priors to enhance placement. Finally, the
appearance of the object is improved using the inserted-object image to enhance
visual fidelity. Experimental results demonstrate that FreeInsert achieves
semantically coherent, spatially precise, and visually realistic 3D insertions
without relying on spatial priors, offering a user-friendly and flexible
editing experience.

</details>

### [73] [Monitoring morphometric drift in lifelong learning segmentation of the spinal cord](https://arxiv.org/abs/2505.01364)
*Enamundram Naga Karthik,Sandrine Bédard,Jan Valošek,Christoph S. Aigner,Elise Bannier,Josef Bednařík,Virginie Callot,Anna Combes,Armin Curt,Gergely David,Falk Eippert,Lynn Farner,Michael G Fehlings,Patrick Freund,Tobias Granberg,Cristina Granziera,RHSCIR Network Imaging Group,Ulrike Horn,Tomáš Horák,Suzanne Humphreys,Markus Hupp,Anne Kerbrat,Nawal Kinany,Shannon Kolind,Petr Kudlička,Anna Lebret,Lisa Eunyoung Lee,Caterina Mainero,Allan R. Martin,Megan McGrath,Govind Nair,Kristin P. O'Grady,Jiwon Oh,Russell Ouellette,Nikolai Pfender,Dario Pfyffer,Pierre-François Pradat,Alexandre Prat,Emanuele Pravatà,Daniel S. Reich,Ilaria Ricchi,Naama Rotem-Kohavi,Simon Schading-Sassenhausen,Maryam Seif,Andrew Smith,Seth A Smith,Grace Sweeney,Roger Tam,Anthony Traboulsee,Constantina Andrada Treaba,Charidimos Tsagkas,Zachary Vavasour,Dimitri Van De Ville,Kenneth Arnold Weber II,Sarath Chandar,Julien Cohen-Adad*

Main category: cs.CV

TLDR: 该论文提出了一种脊髓分割模型和终身学习框架，用于监测模型更新时的形态学漂移，并应用于更新健康参与者的规范数据库。


<details>
  <summary>Details</summary>
Motivation: 评估脊髓分割模型在更新时的稳定性，特别是用于从健康参与者中获取规范值。

Method: 训练了一个多站点数据集（n=75）的脊髓分割模型，并引入终身学习框架自动监测形态学漂移。

Result: 模型在腰椎脊髓病例上表现优异（Dice得分0.95±0.03），形态学漂移监测框架提供了快速反馈，规范数据库更新所需的缩放因子几乎恒定。

Conclusion: 该模型和框架为未来分割模型的开发提供了实用工具，并已在Spinal Cord Toolbox v7.0中免费提供。

Abstract: Morphometric measures derived from spinal cord segmentations can serve as
diagnostic and prognostic biomarkers in neurological diseases and injuries
affecting the spinal cord. While robust, automatic segmentation methods to a
wide variety of contrasts and pathologies have been developed over the past few
years, whether their predictions are stable as the model is updated using new
datasets has not been assessed. This is particularly important for deriving
normative values from healthy participants. In this study, we present a spinal
cord segmentation model trained on a multisite $(n=75)$ dataset, including 9
different MRI contrasts and several spinal cord pathologies. We also introduce
a lifelong learning framework to automatically monitor the morphometric drift
as the model is updated using additional datasets. The framework is triggered
by an automatic GitHub Actions workflow every time a new model is created,
recording the morphometric values derived from the model's predictions over
time. As a real-world application of the proposed framework, we employed the
spinal cord segmentation model to update a recently-introduced normative
database of healthy participants containing commonly used measures of spinal
cord morphometry. Results showed that: (i) our model outperforms previous
versions and pathology-specific models on challenging lumbar spinal cord cases,
achieving an average Dice score of $0.95 \pm 0.03$; (ii) the automatic workflow
for monitoring morphometric drift provides a quick feedback loop for developing
future segmentation models; and (iii) the scaling factor required to update the
database of morphometric measures is nearly constant among slices across the
given vertebral levels, showing minimum drift between the current and previous
versions of the model monitored by the framework. The model is freely available
in Spinal Cord Toolbox v7.0.

</details>

### [74] [Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing](https://arxiv.org/abs/2505.01385)
*Fahong Zhang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TLDR: 本文提出了一种名为GCP的新算法，用于从遥感图像中映射多边形建筑物。GCP通过优化轮廓拟合和简化过程，显著提高了多边形生成的准确性和全局最优性。


<details>
  <summary>Details</summary>
Motivation: 解决从遥感图像中准确映射多边形建筑物的挑战，传统方法如Douglas-Peucker算法在精度和全局优化方面存在不足。

Method: GCP基于实例分割框架，通过采样轮廓多段线、使用Transformer回归模块优化拟合，再通过动态编程简化多边形，实现全局最优。

Result: GCP在公开基准测试中表现优异，其简化模块即使无先验知识也能超越传统方法。

Conclusion: GCP算法在建筑物多边形映射中具有高精度和广泛适用性，代码已开源。

Abstract: This paper addresses the challenge of mapping polygonal buildings from remote
sensing images and introduces a novel algorithm, the Global Collinearity-aware
Polygonizer (GCP). GCP, built upon an instance segmentation framework,
processes binary masks produced by any instance segmentation model. The
algorithm begins by collecting polylines sampled along the contours of the
binary masks. These polylines undergo a refinement process using a
transformer-based regression module to ensure they accurately fit the contours
of the targeted building instances. Subsequently, a collinearity-aware polygon
simplification module simplifies these refined polylines and generate the final
polygon representation. This module employs dynamic programming technique to
optimize an objective function that balances the simplicity and fidelity of the
polygons, achieving globally optimal solutions. Furthermore, the optimized
collinearity-aware objective is seamlessly integrated into network training,
enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has
been validated on two public benchmarks for polygonal building mapping. Further
experiments reveal that applying the collinearity-aware polygon simplification
module to arbitrary polylines, without prior knowledge, enhances accuracy over
traditional methods such as the Douglas-Peucker algorithm. This finding
underscores the broad applicability of GCP. The code for the proposed method
will be made available at https://github.com/zhu-xlab.

</details>

### [75] [Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer](https://arxiv.org/abs/2505.01390)
*Alice Natalina Caragliano,Claudia Tacconi,Carlo Greco,Lorenzo Nibid,Edy Ippolito,Michele Fiore,Giuseppe Perrone,Sara Ramella,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: 提出一种结合多模态深度学习和可解释AI的新方法，用于预测非小细胞肺癌患者对新辅助治疗的病理反应。


<details>
  <summary>Details</summary>
Motivation: 现有放射组学和单模态深度学习方法存在局限性，需改进数据整合和临床相关性。

Method: 采用中间融合策略整合影像和临床数据，并结合医生领域知识的多模态医生参与循环方法。

Result: 提高了预测准确性和可解释性，为临床数据整合提供了优化策略。

Conclusion: 该方法在临床应用中具有潜力，尤其在数据整合和模型解释性方面表现突出。

Abstract: This study proposes a novel approach combining Multimodal Deep Learning with
intrinsic eXplainable Artificial Intelligence techniques to predict
pathological response in non-small cell lung cancer patients undergoing
neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal
deep learning approaches, we introduce an intermediate fusion strategy that
integrates imaging and clinical data, enabling efficient interaction between
data modalities. The proposed Multimodal Doctor-in-the-Loop method further
enhances clinical relevance by embedding clinicians' domain knowledge directly
into the training process, guiding the model's focus gradually from broader
lung regions to specific lesions. Results demonstrate improved predictive
accuracy and explainability, providing insights into optimal data integration
strategies for clinical applications.

</details>

### [76] [VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models](https://arxiv.org/abs/2505.01406)
*Mohammadreza Teymoorianfard,Shiqing Ma,Amir Houmansadr*

Main category: cs.CV

TLDR: VIDSTAMP是一种水印框架，通过视频扩散模型的潜在空间嵌入水印，保持高质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法难以应对视频特定操作且影响视觉质量，需改进。

Method: 两阶段微调解码器，先在静态图像数据集上训练，再在合成视频序列上恢复时间一致性。

Result: 嵌入768比特/视频（48比特/帧），准确率95.0%，视频质量接近未加水印。

Conclusion: VIDSTAMP在容量与质量平衡上优于现有方法，无额外推理成本。

Abstract: The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TLDR: 论文提出解释性视图假说，认为机制可解释性研究能提取和理解神经网络中的隐含解释，并定义了机制可解释性的标准。


<details>
  <summary>Details</summary>
Motivation: 探讨机制可解释性研究的理论基础，明确其与其他可解释性范式的区别及其固有局限性。

Method: 提出机制可解释性的定义（模型级、本体性、因果机制性、可证伪性），并引入解释忠实性和解释乐观主义原则。

Result: 验证了解释忠实性的合理性，并明确了机制可解释性的适用范围和限制。

Conclusion: 机制可解释性是一种有效的研究方法，但其成功依赖于解释乐观主义原则的成立。

Abstract: Mechanistic Interpretability aims to understand neural networks through
causal explanations. We argue for the Explanatory View Hypothesis: that
Mechanistic Interpretability research is a principled approach to understanding
models because neural networks contain implicit explanations which can be
extracted and understood. We hence show that Explanatory Faithfulness, an
assessment of how well an explanation fits a model, is well-defined. We propose
a definition of Mechanistic Interpretability (MI) as the practice of producing
Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural
networks, allowing us to distinguish MI from other interpretability paradigms
and detail MI's inherent limits. We formulate the Principle of Explanatory
Optimism, a conjecture which we argue is a necessary precondition for the
success of Mechanistic Interpretability.

</details>

### [78] [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
*Daria Gitman,Igor Gitman,Evelina Bakhturina*

Main category: cs.LG

TLDR: NeMo-Inspector是一个开源工具，用于简化合成数据集的分析和清理，显著提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 当真实数据稀缺或难以获取时，合成数据是替代方案，但确保其质量需要大量人工检查，耗时且复杂。

Method: 开发了NeMo-Inspector工具，集成推理能力，用于分析和清理合成数据集。

Result: 使用该工具后，GSM-Plus数据集的低质量样本从46.99%降至19.51%，OpenMath模型的准确性也有所提升。

Conclusion: NeMo-Inspector能高效提升合成数据集质量，支持LLM的优化。

Abstract: Adapting Large Language Models (LLMs) to novel tasks and enhancing their
overall capabilities often requires large, high-quality training datasets.
Synthetic data, generated at scale, serves a valuable alternative when
real-world data is scarce or difficult to obtain. However, ensuring the quality
of synthetic datasets is challenging, as developers must manually inspect and
refine numerous samples to identify errors and areas for improvement. This
process is time-consuming and requires specialized tools. We introduce
NeMo-Inspector, an open-source tool designed to simplify the analysis of
synthetic datasets with integrated inference capabilities. We demonstrate its
effectiveness through two real-world cases. Analysis and cleaning of the
synthetically generated GSM-Plus dataset with NeMo-Inspector led to a
significant decrease in low-quality samples from 46.99% to 19.51%. The tool
also helped identify and correct generation errors in OpenMath models,
improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K
dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from
Nemotron-4-340B.

</details>

### [79] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
*Ruiquan Huang,Yingbin Liang,Jing Yang*

Main category: cs.LG

TLDR: 论文研究了单层Transformer在解决正则语言识别任务（如‘even pairs’和‘parity check’）中的学习动态，发现其训练过程分为两个阶段，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在正则语言识别任务中的学习机制，特别是单层Transformer如何通过梯度下降学习解决这些任务。

Method: 理论分析单层Transformer（注意力层加线性层）在梯度下降下的训练动态，并结合Chain-of-Thought（CoT）解决‘parity check’任务。

Result: 训练过程分为两个阶段：注意力层快速映射数据为可分离向量，随后线性层对数增长并接近最大间隔超平面，损失以$O(1/t)$速率下降。

Conclusion: 单层Transformer能有效解决‘even pairs’任务，而‘parity check’需结合CoT，实验验证了理论分析的正确性。

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>

### [80] [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
*Ling Tang,Yuefeng Chen,Hui Xue,Quanshi Zhang*

Main category: cs.LG

TLDR: 提出了一种新的水印方法，将所有权信息嵌入深度神经网络（DNN），且对微调具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 保护DNN模型的知识产权，防止未经授权的微调。

Method: 通过修正的傅里叶变换提取卷积滤波器的特定频率成分，设计水印模块将信息编码到这些成分中。

Result: 实验证明该方法在微调过程中能保持水印的稳定性。

Conclusion: 该方法为DNN模型的所有权保护提供了有效解决方案。

Abstract: This paper proves a new watermarking method to embed the ownership
information into a deep neural network (DNN), which is robust to fine-tuning.
Specifically, we prove that when the input feature of a convolutional layer
only contains low-frequency components, specific frequency components of the
convolutional filter will not be changed by gradient descent during the
fine-tuning process, where we propose a revised Fourier transform to extract
frequency components from the convolutional filter. Additionally, we also prove
that these frequency components are equivariant to weight scaling and weight
permutations. In this way, we design a watermark module to encode the watermark
information to specific frequency components in a convolutional filter.
Preliminary experiments demonstrate the effectiveness of our method.

</details>

### [81] [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TLDR: 本文通过引入多元解释美德框架，评估和改进机制可解释性（MI）中的解释方法，提出紧凑证明是一种有前景的方法，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性（MI）缺乏统一的解释评估方法，限制了其发展。本文旨在回答“什么是好的解释？”这一核心问题。

Method: 引入基于哲学科学的多元解释美德框架（贝叶斯、库恩、德意志和诺莫逻辑视角），系统评估和改进MI中的解释。

Result: 紧凑证明因其综合多种解释美德而被认为是一种有前景的方法。未来研究方向包括明确定义解释简洁性、关注统一解释及推导神经网络的普适原则。

Conclusion: 改进的MI方法将提升我们监控、预测和引导AI系统的能力。

Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.

</details>

### [82] [On-demand Test-time Adaptation for Edge Devices](https://arxiv.org/abs/2505.00986)
*Xiao Ma,Young D. Kwon,Dong Ma*

Main category: cs.LG

TLDR: 论文提出了一种新的按需TTA范式（OD-TTA），通过轻量级域偏移检测、源域选择模块和解耦BN更新方案，显著降低了计算和内存开销，提升了边缘设备上的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法在边缘设备上因内存和能耗问题实用性差，需要一种更高效的TTA方法。

Method: 1) 轻量级域偏移检测；2) 源域选择模块；3) 解耦BN更新方案。

Result: OD-TTA在性能相当甚至更优的情况下，显著降低了能耗和计算开销。

Conclusion: OD-TTA为TTA在边缘设备上的实际应用提供了可行方案。

Abstract: Continual Test-time adaptation (CTTA) continuously adapts the deployed model
on every incoming batch of data. While achieving optimal accuracy, existing
CTTA approaches present poor real-world applicability on resource-constrained
edge devices, due to the substantial memory overhead and energy consumption. In
this work, we first introduce a novel paradigm -- on-demand TTA -- which
triggers adaptation only when a significant domain shift is detected. Then, we
present OD-TTA, an on-demand TTA framework for accurate and efficient
adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a
lightweight domain shift detection mechanism to activate TTA only when it is
needed, drastically reducing the overall computation overhead, 2) a source
domain selection module that chooses an appropriate source model for
adaptation, ensuring high and robust accuracy, 3) a decoupled Batch
Normalization (BN) update scheme to enable memory-efficient adaptation with
small batch sizes. Extensive experiments show that OD-TTA achieves comparable
and even better performance while reducing the energy and computation overhead
remarkably, making TTA a practical reality.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [83] [A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture](https://arxiv.org/abs/2505.01313)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.NE

TLDR: 提出了一种基于ResNet框架的神经架构搜索空间，优化目标包括卷积、池化、全连接层参数及残差网络连接性，并使用验证集损失值作为次要优化目标。


<details>
  <summary>Details</summary>
Motivation: 通过设计高效的搜索空间和优化目标，自动发现高性能的神经网络架构，提升模型在多个数据集上的表现。

Method: 以ResNet为框架，构建搜索空间，优化卷积、池化、全连接层参数及残差网络连接性，同时结合验证集损失值进行多目标优化。

Result: 在MNIST、Fashion-MNIST和CIFAR100数据集上找到了具有竞争力的网络架构。

Conclusion: 提出的搜索空间和优化方法能有效发现高性能网络架构，验证了其可行性和有效性。

Abstract: This paper proposes a neural architecture search space using ResNet as a
framework, with search objectives including parameters for convolution,
pooling, fully connected layers, and connectivity of the residual network. In
addition to recognition accuracy, this paper uses the loss value on the
validation set as a secondary objective for optimization. The experimental
results demonstrate that the search space of this paper together with the
optimisation approach can find competitive network architectures on the MNIST,
Fashion-MNIST and CIFAR100 datasets.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [84] [GENMO: A GENeralist Model for Human MOtion](https://arxiv.org/abs/2505.01425)
*Jiefeng Li,Jinkun Cao,Haotian Zhang,Davis Rempe,Jan Kautz,Umar Iqbal,Ye Yuan*

Main category: cs.GR

TLDR: GENMO是一个统一的人类运动通用模型，将运动生成和估计结合在一个框架中，通过约束生成和扩散协同实现高精度和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将运动生成和估计分离，限制了知识共享和模型维护效率。GENMO旨在通过统一框架解决这一问题。

Method: 将运动估计重新定义为约束运动生成，结合回归和扩散方法，利用2D注释和文本描述的野生视频数据增强生成多样性。

Result: GENMO在多种人类运动任务中表现优异，生成和估计能力均得到提升。

Conclusion: GENMO展示了统一框架的潜力，能够同时处理多种运动任务，并实现生成与估计的协同优化。

Abstract: Human motion modeling traditionally separates motion generation and
estimation into distinct tasks with specialized models. Motion generation
models focus on creating diverse, realistic motions from inputs like text,
audio, or keyframes, while motion estimation models aim to reconstruct accurate
motion trajectories from observations like videos. Despite sharing underlying
representations of temporal dynamics and kinematics, this separation limits
knowledge transfer between tasks and requires maintaining separate models. We
present GENMO, a unified Generalist Model for Human Motion that bridges motion
estimation and generation in a single framework. Our key insight is to
reformulate motion estimation as constrained motion generation, where the
output motion must precisely satisfy observed conditioning signals. Leveraging
the synergy between regression and diffusion, GENMO achieves accurate global
motion estimation while enabling diverse motion generation. We also introduce
an estimation-guided training objective that exploits in-the-wild videos with
2D annotations and text descriptions to enhance generative diversity.
Furthermore, our novel architecture handles variable-length motions and mixed
multimodal conditions (text, audio, video) at different time intervals,
offering flexible control. This unified approach creates synergistic benefits:
generative priors improve estimated motions under challenging conditions like
occlusions, while diverse video data enhances generation capabilities.
Extensive experiments demonstrate GENMO's effectiveness as a generalist
framework that successfully handles multiple human motion tasks within a single
model.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [85] [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
*Quang P. M. Pham,Khoi T. N. Nguyen,Nhi H. Doan,Cuong A. Pham,Kentaro Inui,Dezhen Song*

Main category: cs.RO

TLDR: SmallPlan框架利用大型语言模型（LLMs）作为教师模型，训练轻量级小型语言模型（SLMs）进行高效路径规划，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 解决大规模动态环境中机器人路径规划的高计算成本和实时性问题。

Method: 结合LLM引导的监督微调（SFT）和强化学习（RL）训练SLMs，生成最优动作序列。

Result: SLMs在路径规划任务中表现优异，与GPT-4o相当，且避免了幻觉和过拟合。

Conclusion: SmallPlan资源高效，适合边缘设备部署，推动实际自主机器人应用。

Abstract: Efficient path planning in robotics, particularly within large-scale, dynamic
environments, remains a significant hurdle. While Large Language Models (LLMs)
offer strong reasoning capabilities, their high computational cost and limited
adaptability in dynamic scenarios hinder real-time deployment on edge devices.
We present SmallPlan -- a novel framework leveraging LLMs as teacher models to
train lightweight Small Language Models (SLMs) for high-level path planning
tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate
across scene graphs that compactly represent full-scaled 3D scenes. The SLMs
are trained in a simulation-powered, interleaved manner with LLM-guided
supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not
only enables SLMs to successfully complete navigation tasks but also makes them
aware of important factors like travel distance and number of trials. Through
experiments, we demonstrate that the fine-tuned SLMs perform competitively with
larger models like GPT-4o on sequential path planning, without suffering from
hallucination and overfitting. SmallPlan is resource-efficient, making it
well-suited for edge-device deployment and advancing practical autonomous
robotics.

</details>

### [86] [Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning](https://arxiv.org/abs/2505.00935)
*Roberto Bigazzi*

Main category: cs.RO

TLDR: 论文探讨了计算能力提升与深度学习革命如何推动具身人工智能的发展，重点关注智能自主机器人在室内环境中的训练与部署。


<details>
  <summary>Details</summary>
Motivation: 旨在促进具身AI和自主智能体的研究，推动未来在该领域的发展。

Method: 利用3D模型进行仿真训练，学习智能体与环境的连续交互，包括信息收集、编码和任务执行。

Result: 提出了完整的具身智能体创建流程，包括概念、实现和部署，并进行了详细的实验研究。

Conclusion: 论文为具身AI领域的研究提供了重要贡献，为未来工作奠定了基础。

Abstract: The increase in available computing power and the Deep Learning revolution
have allowed the exploration of new topics and frontiers in Artificial
Intelligence research. A new field called Embodied Artificial Intelligence,
which places at the intersection of Computer Vision, Robotics, and Decision
Making, has been gaining importance during the last few years, as it aims to
foster the development of smart autonomous robots and their deployment in
society. The recent availability of large collections of 3D models for
photorealistic robotic simulation has allowed faster and safe training of
learning-based agents for millions of frames and a careful evaluation of their
behavior before deploying the models on real robotic platforms. These
intelligent agents are intended to perform a certain task in a possibly unknown
environment. To this end, during the training in simulation, the agents learn
to perform continuous interactions with the surroundings, such as gathering
information from the environment, encoding and extracting useful cues for the
task, and performing actions towards the final goal; where every action of the
agent influences the interactions. This dissertation follows the complete
creation process of embodied agents for indoor environments, from their concept
to their implementation and deployment. We aim to contribute to research in
Embodied AI and autonomous agents, in order to foster future work in this
field. We present a detailed analysis of the procedure behind implementing an
intelligent embodied agent, comprehending a thorough description of the current
state-of-the-art in literature, technical explanations of the proposed methods,
and accurate experimental studies on relevant robotic tasks.

</details>

### [87] [Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse](https://arxiv.org/abs/2505.00995)
*Taewook Park,Jinwoo Lee,Hyondong Oh,Won-Jae Yun,Kyu-Wha Lee*

Main category: cs.RO

TLDR: 论文提出了一种轻量级无人机（UAV）系统，用于温室中的番茄产量估计，解决了地面机器人（UGV）在温室中的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着农业劳动力减少和成本上升，机器人产量估计变得重要。UGV在温室中部署受限，需更高效解决方案。

Method: 开发配备RGB-D相机、3D LiDAR和IMU传感器的UAV，采用LiDAR-惯性里程计算法导航，并使用3D多目标跟踪算法估计番茄数量和重量。

Result: 在收获行数据集中，计数准确率94.4%，重量估计准确率87.5%；在生长行数据集中定性分析了遮挡问题。

Conclusion: UAV在温室产量估计中具有潜力，未来需进一步研究遮挡环境下的感知改进。

Abstract: As the agricultural workforce declines and labor costs rise, robotic yield
estimation has become increasingly important. While unmanned ground vehicles
(UGVs) are commonly used for indoor farm monitoring, their deployment in
greenhouses is often constrained by infrastructure limitations, sensor
placement challenges, and operational inefficiencies. To address these issues,
we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D
camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial
odometry algorithm for precise navigation in GNSS-denied environments and
utilizes a 3D multi-object tracking algorithm to estimate the count and weight
of cherry tomatoes. We evaluate the system using two dataset: one from a
harvesting row and another from a growing row. In the harvesting-row dataset,
the proposed system achieves 94.4\% counting accuracy and 87.5\% weight
estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For
the growing-row dataset, which consists of occluded unripened fruits, we
qualitatively analyze tracking performance and highlight future research
directions for improving perception in greenhouse with strong occlusions. Our
findings demonstrate the potential of UAVs for efficient robotic yield
estimation in commercial greenhouses.

</details>

### [88] [NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization](https://arxiv.org/abs/2505.01113)
*Xun Li,Jian Yang,Fenli Jia,Muyu Wang,Qi Wu,Jun Wu,Jinpeng Mi,Jilin Hu,Peidong Liang,Xuan Tang,Ke Li,Xiong You,Xian Wei*

Main category: cs.RO

TLDR: NeuroLoc是一种受生物大脑导航机制启发的相机定位方法，通过Hebbian学习模块、方向学习嵌入和3D网格中心预测，解决了场景模糊和动态干扰问题，提升了复杂环境中的定位鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中相机定位面临的场景模糊、环境干扰和动态物体变化问题。

Method: 结合Hebbian学习模块保存历史信息，利用方向学习嵌入恢复真实方向，并通过3D网格中心预测减少错误预测。

Result: 在常用室内外数据集上验证，NeuroLoc提升了复杂环境中的鲁棒性，仅用单张图像即可改善姿态回归性能。

Conclusion: NeuroLoc通过生物启发方法有效解决了相机定位中的关键问题，具有实际应用潜力。

Abstract: Recently, camera localization has been widely adopted in autonomous robotic
navigation due to its efficiency and convenience. However, autonomous
navigation in unknown environments often suffers from scene ambiguity,
environmental disturbances, and dynamic object transformation in camera
localization. To address this problem, inspired by the biological brain
navigation mechanism (such as grid cells, place cells, and head direction
cells), we propose a novel neurobiological camera location method, namely
NeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells
to save and replay historical information, aiming to restore the details of
historical representations and solve the issue of scene fuzziness. Secondly, we
utilized the head direction cell-inspired internal direction learning as
multi-head attention embedding to help restore the true orientation in similar
scenes. Finally, we added a 3D grid center prediction in the pose regression
module to reduce the final wrong prediction. We evaluate the proposed NeuroLoc
on commonly used benchmark indoor and outdoor datasets. The experimental
results show that our NeuroLoc can enhance the robustness in complex
environments and improve the performance of pose regression by using only a
single image.

</details>

<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [89] [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
*Zhiyu Liao,Kang Chen,Yuanguo Lin,Kangkang Li,Yunxuan Liu,Hefeng Chen,Xingwang Huang,Yuanhui Yu*

Main category: cs.CR

TLDR: 本文系统调查了大型语言模型（LLMs）的攻击与防御技术，分类攻击类型并分析防御策略，强调未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理中广泛应用，但其安全漏洞和伦理挑战亟需解决。

Method: 分类攻击类型（如对抗提示攻击、模型窃取等），分析防御策略（预防和检测方法）。

Result: 总结了攻击机制与防御进展，指出动态威胁适应、资源限制等挑战。

Conclusion: 需开发自适应防御、可解释安全技术及标准化评估框架，强调跨学科合作与伦理考量。

Abstract: Large Language Models (LLMs) have become central to numerous natural language
processing tasks, but their vulnerabilities present significant security and
ethical challenges. This systematic survey explores the evolving landscape of
attack and defense techniques in LLMs. We classify attacks into adversarial
prompt attack, optimized attacks, model theft, as well as attacks on
application of LLMs, detailing their mechanisms and implications. Consequently,
we analyze defense strategies, including prevention-based and detection-based
defense methods. Although advances have been made, challenges remain to adapt
to the dynamic threat landscape, balance usability with robustness, and address
resource constraints in defense implementation. We highlight open problems,
including the need for adaptive scalable defenses, explainable security
techniques, and standardized evaluation frameworks. This survey provides
actionable insights and directions for developing secure and resilient LLMs,
emphasizing the importance of interdisciplinary collaboration and ethical
considerations to mitigate risks in real-world applications.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [90] [CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment](https://arxiv.org/abs/2505.01237)
*Edson Araujo,Andrew Rouditchenko,Yuan Gong,Saurabhchand Bhati,Samuel Thomas,Brian Kingsbury,Leonid Karlinsky,Rogerio Feris,James R. Glass*

Main category: cs.MM

TLDR: CAV-MAE Sync通过改进音频-视觉学习中的时间对齐和优化目标冲突问题，提出了一种简单有效的自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频-视觉学习中存在时间粒度不匹配和优化目标冲突的问题。

Method: 将音频视为与视频帧对齐的时间序列，分离对比和重建目标，并引入可学习的注册令牌。

Result: 在AudioSet、VGG Sound和ADE20K Sound数据集上实现了最先进的性能。

Conclusion: CAV-MAE Sync通过简单改进解决了关键挑战，性能优于复杂架构。

Abstract: Recent advances in audio-visual learning have shown promising results in
learning representations across modalities. However, most approaches rely on
global audio representations that fail to capture fine-grained temporal
correspondences with visual frames. Additionally, existing methods often
struggle with conflicting optimization objectives when trying to jointly learn
reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync
as a simple yet effective extension of the original CAV-MAE framework for
self-supervised audio-visual learning. We address three key challenges: First,
we tackle the granularity mismatch between modalities by treating audio as a
temporal sequence aligned with video frames, rather than using global
representations. Second, we resolve conflicting optimization goals by
separating contrastive and reconstruction objectives through dedicated global
tokens. Third, we improve spatial localization by introducing learnable
register tokens that reduce semantic load on patch tokens. We evaluate the
proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on
zero-shot retrieval, classification and localization tasks demonstrating
state-of-the-art performance and outperforming more complex architectures.

</details>

### [91] [FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing](https://arxiv.org/abs/2505.01263)
*Gaoxiang Cong,Liang Li,Jiadong Pan,Zhedong Zhang,Amin Beheshti,Anton van den Hengel,Yuankai Qi,Qingming Huang*

Main category: cs.MM

TLDR: FlowDubber是一种基于大型语言模型（LLM）的电影配音方法，通过语音增强流匹配和双重对比对齐，实现了高质量的音频-视觉同步和发音。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注降低单词错误率，忽视了唇同步和音质的重要性，FlowDubber旨在解决这些问题。

Method: 采用Qwen2.5作为LLM骨干，结合语义感知学习和双重对比对齐（DCA），并通过基于流的语音增强（FVE）提升音质。

Result: 在多个基准测试中优于现有方法，实现了更好的音频-视觉同步和音质。

Conclusion: FlowDubber通过LLM和流匹配技术，显著提升了电影配音的质量和同步性。

Abstract: Movie Dubbing aims to convert scripts into speeches that align with the given
movie clip in both temporal and emotional aspects while preserving the vocal
timbre of a given brief reference audio. Existing methods focus primarily on
reducing the word error rate while ignoring the importance of lip-sync and
acoustic quality. To address these issues, we propose a large language model
(LLM) based flow matching architecture for dubbing, named FlowDubber, which
achieves high-quality audio-visual sync and pronunciation by incorporating a
large speech language model and dual contrastive aligning while achieving
better acoustic quality via the proposed voice-enhanced flow matching than
previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the
in-context sequence from movie scripts and reference audio. Then, the proposed
semantic-aware learning focuses on capturing LLM semantic knowledge at the
phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment
with lip movement, reducing ambiguities where similar phonemes might be
confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves
acoustic quality in two aspects, which introduces an LLM-based acoustics flow
matching guidance to strengthen clarity and uses affine style prior to enhance
identity when recovering noise into mel-spectrograms via gradient vector field
prediction. Extensive experiments demonstrate that our method outperforms
several state-of-the-art methods on two primary benchmarks. The demos are
available at
{\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.

</details>

<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [92] [Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey](https://arxiv.org/abs/2505.00747)
*Zhiying Song,Tenghui Xie,Fuxi Wen,Jun Li*

Main category: cs.OH

TLDR: 本文综述了基于V2X通信的多智能体协同感知技术，重点探讨了信息表示、信息融合和大规模部署三个维度，并提出了将V2X视为动态信息传感器的新视角。


<details>
  <summary>Details</summary>
Motivation: 通过V2X通信扩展自动驾驶车辆的感知能力，解决传统车载传感器的局限性，如通信限制、异构性、移动性和可扩展性。

Method: 从信息表示（数据级、特征级、对象级）、信息融合（理想与非理想条件）和大规模部署三个方面进行分类和总结。

Result: 提出了减少数据量和压缩消息的方法，探讨了异构性、定位误差、延迟和数据包丢失等问题的解决方案，并总结了支持密集交通场景的系统级方法。

Conclusion: 本文通过将V2X视为信息传感器，为实际智能交通系统中协同感知的部署提供了新的视角和挑战分析。

Abstract: Cooperative perception extends the perception capabilities of autonomous
vehicles by enabling multi-agent information sharing via Vehicle-to-Everything
(V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic
"information sensor" characterized by limited communication, heterogeneity,
mobility, and scalability. This survey provides a comprehensive review of
recent advancements from the perspective of information-centric cooperative
perception, focusing on three key dimensions: information representation,
information fusion, and large-scale deployment. We categorize information
representation into data-level, feature-level, and object-level schemes, and
highlight emerging methods for reducing data volume and compressing messages
under communication constraints. In information fusion, we explore techniques
under both ideal and non-ideal conditions, including those addressing
heterogeneity, localization errors, latency, and packet loss. Finally, we
summarize system-level approaches to support scalability in dense traffic
scenarios. Compared with existing surveys, this paper introduces a new
perspective by treating V2X communication as an information sensor and
emphasizing the challenges of deploying cooperative perception in real-world
intelligent transportation systems.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [93] [Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting](https://arxiv.org/abs/2505.00735)
*Jin Hyun Park,Harine Choi,Praewa Pitiphat*

Main category: eess.IV

TLDR: 该论文提出了一种结合RGB和深度图像的双编码器架构，通过注意力机制融合特征，显著提升了图像修复的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图像修复方法仅依赖RGB图像，忽略了深度信息对空间和结构理解的重要性。结合深度信息可提升修复的准确性和上下文感知能力。

Method: 采用双编码器架构，分别处理RGB和深度图像，通过注意力机制在解码器中融合特征。使用线和方形掩码测试模型鲁棒性，并通过Grad-CAM可视化分析关注区域。

Result: 实验表明，结合深度信息显著提升了修复质量，注意力机制进一步优化了性能，定量和定性评估均优于基线模型。

Conclusion: 深度信息的引入和注意力机制的结合有效提升了图像修复的准确性和上下文感知能力，为相关领域提供了新思路。

Abstract: Existing deep learning-based image inpainting methods typically rely on
convolutional networks with RGB images to reconstruct images. However, relying
exclusively on RGB images may neglect important depth information, which plays
a critical role in understanding the spatial and structural context of a scene.
Just as human vision leverages stereo cues to perceive depth, incorporating
depth maps into the inpainting process can enhance the model's ability to
reconstruct images with greater accuracy and contextual awareness. In this
paper, we propose a novel approach that incorporates both RGB and depth images
for enhanced image inpainting. Our models employ a dual encoder architecture,
where one encoder processes the RGB image and the other handles the depth
image. The encoded features from both encoders are then fused in the decoder
using an attention mechanism, effectively integrating the RGB and depth
representations. We use two different masking strategies, line and square, to
test the robustness of the model under different types of occlusions. To
further analyze the effectiveness of our approach, we use Gradient-weighted
Class Activation Mapping (Grad-CAM) visualizations to examine the regions of
interest the model focuses on during inpainting. We show that incorporating
depth information alongside the RGB image significantly improves the
reconstruction quality. Through both qualitative and quantitative comparisons,
we demonstrate that the depth-integrated model outperforms the baseline, with
attention mechanisms further enhancing inpainting performance, as evidenced by
multiple evaluation metrics and visualization.

</details>

### [94] [A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond](https://arxiv.org/abs/2505.00737)
*Jiajia Li,Xinda Qi,Seyed Hamidreza Nabaei,Meiqi Liu,Dong Chen,Xin Zhang,Xunyuan Yin,Zhaojian Li*

Main category: eess.IV

TLDR: 综述了植物表型分析中的3D重建技术，包括经典方法、NeRF和3DGS，探讨了它们的优缺点及未来前景。


<details>
  <summary>Details</summary>
Motivation: 植物表型分析对精准农业和作物改良至关重要，3D重建技术为其提供了自动化与高精度的潜力。

Method: 回顾了经典3D重建、NeRF和3DGS的方法、应用及性能。

Result: 经典方法简单灵活但面临数据密度和噪声问题；NeRF高质量但计算成本高；3DGS在效率和扩展性上具潜力。

Conclusion: 不同3D重建技术各有优劣，未来需结合其优势推动植物表型分析的自动化与高通量化。

Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and
their interactions with the environment, making it crucial for advancing
precision agriculture and crop improvement. 3D reconstruction technologies have
emerged as powerful tools for capturing detailed plant morphology and
structure, offering significant potential for accurate and automated
phenotyping. This paper provides a comprehensive review of the 3D
reconstruction techniques for plant phenotyping, covering classical
reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel
3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on
high-resolution sensors, are widely adopted due to their simplicity and
flexibility in representing plant structures. However, they face challenges
such as data density, noise, and scalability. NeRF, a recent advancement,
enables high-quality, photorealistic 3D reconstructions from sparse viewpoints,
but its computational cost and applicability in outdoor environments remain
areas of active research. The emerging 3DGS technique introduces a new paradigm
in reconstructing plant structures by representing geometry through Gaussian
primitives, offering potential benefits in both efficiency and scalability. We
review the methodologies, applications, and performance of these approaches in
plant phenotyping and discuss their respective strengths, limitations, and
future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).
Through this review, we aim to provide insights into how these diverse 3D
reconstruction techniques can be effectively leveraged for automated and
high-throughput plant phenotyping, contributing to the next generation of
agricultural technology.

</details>

### [95] [Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging](https://arxiv.org/abs/2505.01239)
*Elena Mulero Ayllón,Massimiliano Mantegna,Linlin Shen,Paolo Soda,Valerio Guarrasi,Matteo Tortora*

Main category: eess.IV

TLDR: 本文对深度学习模型在肺肿瘤分割中的表现进行了基准测试，发现基础模型（如MedSAM~2）在准确性和计算效率上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 肺肿瘤分割的准确性对诊断和治疗计划至关重要，但肿瘤形态、大小和位置的复杂性为自动化分割带来了挑战。

Method: 研究比较了传统架构（如U-Net、DeepLabV3）、自配置模型（如nnUNet）和基础模型（如MedSAM、MedSAM~2）在两种肺肿瘤分割数据集上的表现，评估了少样本学习和微调等学习范式。

Result: 基础模型（尤其是MedSAM~2）在准确性和计算效率上均优于传统模型。

Conclusion: 基础模型在肺肿瘤分割中展现出潜力，有望改善临床工作流程和患者预后。

Abstract: Accurate lung tumor segmentation is crucial for improving diagnosis,
treatment planning, and patient outcomes in oncology. However, the complexity
of tumor morphology, size, and location poses significant challenges for
automated segmentation. This study presents a comprehensive benchmarking
analysis of deep learning-based segmentation models, comparing traditional
architectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,
and foundation models like MedSAM, and MedSAM~2. Evaluating performance across
two lung tumor segmentation datasets, we assess segmentation accuracy and
computational efficiency under various learning paradigms, including few-shot
learning and fine-tuning. The results reveal that while traditional models
struggle with tumor delineation, foundation models, particularly MedSAM~2,
outperform them in both accuracy and computational efficiency. These findings
underscore the potential of foundation models for lung tumor segmentation,
highlighting their applicability in improving clinical workflows and patient
outcomes.

</details>
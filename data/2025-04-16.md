<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 117]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.IR](#cs.IR) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.SI](#cs.SI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LayerFlow: Layer-wise Exploration of LLM Embeddings using Uncertainty-aware Interlinked Projections](https://arxiv.org/abs/2504.10504)
*Rita Sevastjanova,Robin Gerling,Thilo Spinner,Mennatallah El-Assady*

Main category: cs.CL

TLDR: LayerFlow是一个可视化分析工具，用于展示嵌入表示中的不确定性，通过多种视觉组件帮助用户理解数据转换和解释中的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）中嵌入表示的语言属性（如语义和语法）对研究者和任务应用至关重要，但降维技术引入的不确定性可能影响用户对数据的解读。

Method: LayerFlow通过一个互联的投影设计展示嵌入表示，并利用凸包、数据点距离、聚类摘要和投影质量指标等视觉组件传达不确定性。

Result: 通过案例研究验证了LayerFlow的实用性，表明多视觉组件和数据视角能有效传达不确定性。

Conclusion: LayerFlow通过可视化手段解决了嵌入表示中的不确定性问题，为研究者和用户提供了更可靠的数据解读工具。

Abstract: Large language models (LLMs) represent words through contextual word
embeddings encoding different language properties like semantics and syntax.
Understanding these properties is crucial, especially for researchers
investigating language model capabilities, employing embeddings for tasks
related to text similarity, or evaluating the reasons behind token importance
as measured through attribution methods. Applications for embedding exploration
frequently involve dimensionality reduction techniques, which reduce
high-dimensional vectors to two dimensions used as coordinates in a
scatterplot. This data transformation step introduces uncertainty that can be
propagated to the visual representation and influence users' interpretation of
the data. To communicate such uncertainties, we present LayerFlow - a visual
analytics workspace that displays embeddings in an interlinked projection
design and communicates the transformation, representation, and interpretation
uncertainty. In particular, to hint at potential data distortions and
uncertainties, the workspace includes several visual components, such as convex
hulls showing 2D and HD clusters, data point pairwise distances, cluster
summaries, and projection quality metrics. We show the usability of the
presented workspace through replication and expert case studies that highlight
the need to communicate uncertainty through multiple visual components and
different data perspectives.

</details>

### [2] [Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models](https://arxiv.org/abs/2504.10615)
*Thilo Hagendorff,Sarah Fabi*

Main category: cs.CL

TLDR: 该研究提出了一个量化大语言模型内部推理能力的基准测试，通过要求模型用非英语的初始响应语言选择正确答案，评估了18个模型的性能，发现GPT-4.5表现最佳。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型内部推理能力（即模型在单个词预测之间的推断“跳跃”）是研究的关键。

Method: 设计了一个包含4000项的基准测试，要求模型通过非英语的初始响应语言选择正确答案，以评估其内部推理能力。

Result: GPT-4.5以74.7%的准确率表现最佳，显著优于其他模型（如Grok-2和Llama 3.1 405B）。

Conclusion: 研究表明大语言模型能够通过潜在空间计算进行推理，但未来需进一步研究其内部策略及潜在安全问题。

Abstract: Large language models (LLMs) can perform reasoning computations both
internally within their latent space and externally by generating explicit
token sequences like chains of thought. Significant progress in enhancing
reasoning abilities has been made by scaling test-time compute. However,
understanding and quantifying model-internal reasoning abilities - the
inferential "leaps" models make between individual token predictions - remains
crucial. This study introduces a benchmark (n = 4,000 items) designed to
quantify model-internal reasoning in different domains. We achieve this by
having LLMs indicate the correct solution to reasoning problems not through
descriptive text, but by selecting a specific language of their initial
response token that is different from English, the benchmark language. This not
only requires models to reason beyond their context window, but also to
overrise their default tendency to respond in the same language as the prompt,
thereby posing an additional cognitive strain. We evaluate a set of 18 LLMs,
showing significant performance variations, with GPT-4.5 achieving the highest
accuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B
(65.6%). Control experiments and difficulty scaling analyses suggest that while
LLMs engage in internal reasoning, we cannot rule out heuristic exploitations
under certain conditions, marking an area for future investigation. Our
experiments demonstrate that LLMs can "think" via latent-space computations,
revealing model-internal inference strategies that need further understanding,
especially regarding safety-related concerns such as covert planning,
goal-seeking, or deception emerging without explicit token traces.

</details>

### [3] [Better Estimation of the KL Divergence Between Language Models](https://arxiv.org/abs/2504.10637)
*Afra Amini,Tim Vieira,Ryan Cotterell*

Main category: cs.CL

TLDR: 提出了一种Rao-Blackwellized估计器，用于降低KL散度估计的方差，比传统蒙特卡洛估计器更稳定。


<details>
  <summary>Details</summary>
Motivation: KL散度在语言模型中有广泛应用，但传统蒙特卡洛估计器方差高且可能为负值。

Method: 引入Rao-Blackwellized估计器，理论证明其方差更低，并推导了其梯度估计器。

Result: 实验表明新估计器显著降低方差，训练更稳定，模型性能更优。

Conclusion: Rao-Blackwellized估计器在KL散度估计中优于传统方法，适用于多种应用场景。

Abstract: Estimating the Kullback--Leibler (KL) divergence between language models has
many applications, e.g., reinforcement learning from human feedback (RLHF),
interpretability, and knowledge distillation. However, computing the exact KL
divergence between two arbitrary language models is intractable. Thus,
practitioners often resort to the use of sampling-based estimators. While it is
easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased
estimate of the KL divergence between language models, this estimator
notoriously suffers from high variance, and can even result in a negative
estimate of the KL divergence, a non-negative quantity. In this paper, we
introduce a Rao--Blackwellized estimator that is also unbiased and provably has
variance less than or equal to that of the standard Monte Carlo estimator. In
an empirical study on sentiment-controlled fine-tuning, we show that our
estimator provides more stable KL estimates and reduces variance substantially
in practice. Additionally, we derive an analogous Rao--Blackwellized estimator
of the gradient of the KL divergence, which leads to more stable training and
produces models that more frequently appear on the Pareto frontier of reward
vs. KL compared to the ones trained with the MC estimator of the gradient.

</details>

### [4] [Weight-of-Thought Reasoning: Exploring Neural Network Weights for Enhanced LLM Reasoning](https://arxiv.org/abs/2504.10646)
*Saif Punjwani,Larry Heck*

Main category: cs.CL

TLDR: 论文提出了一种名为Weight-of-Thought（WoT）的新方法，通过分析神经网络权重来改进大语言模型的推理能力，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Chain-of-Thought）仅关注输出层面的推理，忽略了内部权重动态，限制了推理能力和可解释性。

Method: WoT通过图结构消息传递、多步推理和注意力机制，在推理前分析权重空间，构建推理节点图。

Result: 在多种推理任务（如逻辑、数学、几何等）中，WoT表现优于传统方法，尤其在复杂问题上。

Conclusion: WoT不仅提升了推理性能，还增强了可解释性，为改进大语言模型的推理能力提供了新方向。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities when prompted with strategies such as Chain-of-Thought (CoT).
However, these approaches focus on token-level output without considering
internal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a
novel approach that examines neural network weights before inference to
identify reasoning pathways. Unlike existing methods, WoT explores the weight
space through graph-based message passing, multi-step reasoning processes, and
attention mechanisms. Our implementation creates an interconnected graph of
reasoning nodes. Experiments on diverse reasoning tasks (syllogistic,
mathematical, algebraic, combinatorial, and geometric) demonstrate that WoT
achieves superior performance compared to traditional methods, particularly for
complex problems. This approach leads to both improved performance and greater
interpretability of the reasoning process, offering a promising direction for
enhancing LLM reasoning capabilities.

</details>

### [5] [Improving In-Context Learning with Reasoning Distillation](https://arxiv.org/abs/2504.10647)
*Nafis Sadeq,Xin Xu,Zhouhang Xie,Julian McAuley,Byungkyu Kang,Prarit Lamba,Xiang Gao*

Main category: cs.CL

TLDR: 论文提出ReDis方法，通过数据增强、过滤、监督微调和对齐提升语言模型的归纳推理能力，在多个任务上显著超越基线甚至GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 语言模型依赖语义先验进行上下文学习，但在归纳推理任务上表现不佳，现有方法难以提升模型对输入输出间规则的理解。

Method: 提出ReDis技术，结合数据增强、过滤、监督微调和对齐，优化语言模型的归纳推理能力。

Result: 在1D-ARC、ACRE和MiniSCAN任务上，ReDis分别相对GPT-4o提升23.2%、2.8%和66.6%。

Conclusion: ReDis显著提升了语言模型的归纳推理能力，并在多个任务上超越基线及GPT-4o。

Abstract: Language models rely on semantic priors to perform in-context learning, which
leads to poor performance on tasks involving inductive reasoning.
Instruction-tuning methods based on imitation learning can superficially
enhance the in-context learning performance of language models, but they often
fail to improve the model's understanding of the underlying rules that connect
inputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning
distillation technique designed to improve the inductive reasoning capabilities
of language models. Through a careful combination of data augmentation,
filtering, supervised fine-tuning, and alignment, ReDis achieves significant
performance improvements across a diverse range of tasks, including 1D-ARC,
List Function, ACRE, and MiniSCAN. Experiments on three language model
backbones show that ReDis outperforms equivalent few-shot prompting baselines
across all tasks and even surpasses the teacher model, GPT-4o, in some cases.
ReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%,
2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within
a similar hypothesis search space. The code, dataset, and model checkpoints
will be made available at
https://github.com/NafisSadeq/reasoning-distillation.git.

</details>

### [6] [LITERA: An LLM Based Approach to Latin-to-English Translation](https://arxiv.org/abs/2504.10660)
*Paul Rosu*

Main category: cs.CL

TLDR: 本文介绍了一个基于LLM的拉丁语到英语翻译平台LITERA，通过多层次的翻译流程和优化的GPT-4o模型，显著提高了翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 解决拉丁语文本翻译的挑战，特别是古典拉丁语的翻译需求。

Method: 使用优化的GPT-4o-mini和GPT-4o模型，结合高质量平行数据集和多层次翻译流程。

Result: 显著提升了BLEU和BLEURT分数，特别是在古典拉丁语翻译中。

Conclusion: LITERA展示了在拉丁语翻译中的高效性和准确性，为研究提供了有力工具。

Abstract: This paper introduces an LLM-based Latin-to-English translation platform
designed to address the challenges of translating Latin texts. We named the
model LITERA, which stands for Latin Interpretation and Translations into
English for Research Assistance. Through a multi-layered translation process
utilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an
unprecedented level of accuracy, showcased by greatly improved BLEU scores,
particularly in classical Latin, along with improved BLEURT scores. The
development of LITERA involved close collaboration with Duke University's
Classical Studies Department, which was instrumental in creating a small,
high-quality parallel Latin-English dataset. This paper details the
architecture, fine-tuning methodology, and prompting strategies used in LITERA,
emphasizing its ability to produce literal translations.

</details>

### [7] [Characterizing Knowledge Manipulation in a Russian Wikipedia Fork](https://arxiv.org/abs/2504.10663)
*Mykola Trokhymovych,Oleksandr Kosovan,Nathan Forrester,Pablo Aragón,Diego Saez-Trumper,Ricardo Baeza-Yates*

Main category: cs.CL

TLDR: 本文分析了俄罗斯维基百科的分支Ruwiki，提出了一种方法来识别和分类知识操纵的主要变化，并提供了适用于其他类似项目的分析方法。


<details>
  <summary>Details</summary>
Motivation: 研究Ruwiki的目的是识别与知识操纵相关的实践和叙述，以揭示其对原始内容的修改。

Method: 通过比较俄罗斯维基百科和Ruwiki的190万篇文章，利用元信息、地理、时间、类别和文本特征进行分析。

Result: 研究揭示了Ruwiki编辑的主要知识操纵主题，并提供了其范围的数值估计。

Conclusion: 该研究不仅揭示了Ruwiki的显著变化，还提供了一种可应用于其他维基分支和协作项目的方法论。

Abstract: Wikipedia is powered by MediaWiki, a free and open-source software that is
also the infrastructure for many other wiki-based online encyclopedias. These
include the recently launched website Ruwiki, which has copied and modified the
original Russian Wikipedia content to conform to Russian law. To identify
practices and narratives that could be associated with different forms of
knowledge manipulation, this article presents an in-depth analysis of this
Russian Wikipedia fork. We propose a methodology to characterize the main
changes with respect to the original version. The foundation of this study is a
comprehensive comparative analysis of more than 1.9M articles from Russian
Wikipedia and its fork. Using meta-information and geographical, temporal,
categorical, and textual features, we explore the changes made by Ruwiki
editors. Furthermore, we present a classification of the main topics of
knowledge manipulation in this fork, including a numerical estimation of their
scope. This research not only sheds light on significant changes within Ruwiki,
but also provides a methodology that could be applied to analyze other
Wikipedia forks and similar collaborative projects.

</details>

### [8] [Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content](https://arxiv.org/abs/2504.10679)
*F. A. Rizvi,T. Navojith,A. M. N. H. Adhikari,W. P. U. Senevirathna,Dharshana Kasthurirathna,Lakmini Abeywardhana*

Main category: cs.CL

TLDR: 论文提出了一种混合NLP方法，用于改进银行领域多语言和代码混合内容的关键词提取、内容过滤和基于方面的分类，验证了微调Transformer模型在金融文本分析中的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统NLP模型在处理代码混合文本（如僧伽罗语-英语）时表现不佳，无法捕捉领域知识，因此需要更有效的方法来维护银行品牌声誉。

Method: 采用混合方法，包括微调的SpaCy NER模型、FinBERT-based KeyBERT嵌入、YAKE和EmbedRank进行英文关键词提取；使用微调的XLM-RoBERTa模型和僧伽罗语金融词汇提取代码混合和僧伽罗语关键词。内容过滤和方面分类分别使用BERT-base-uncased和XLM-RoBERTa模型。

Result: 英文关键词提取准确率91.2%，僧伽罗语关键词提取准确率87.4%。内容过滤和方面分类的准确率分别为85.2%/88.1%和87.4%/85.9%，均优于传统方法。

Conclusion: 微调Transformer模型在多语言金融文本分析中优于传统方法，为银行品牌声誉监控提供了准确且可扩展的解决方案。

Abstract: Brand reputation in the banking sector is maintained through insightful
analysis of customer opinion on code-mixed and multilingual content.
Conventional NLP models misclassify or ignore code-mixed text, when mix with
low resource languages such as Sinhala-English and fail to capture
domain-specific knowledge. This study introduces a hybrid NLP method to improve
keyword extraction, content filtering, and aspect-based classification of
banking content. Keyword extraction in English is performed with a hybrid
approach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT
embeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%.
Code-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa
model integrated with a domain-specific Sinhala financial vocabulary, and it
results in an accuracy of 87.4%. To ensure data quality, irrelevant comment
filtering was performed using several models, with the BERT-base-uncased model
achieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better
than GPT-4o, SVM, and keyword-based filtering. Aspect classification followed
the same pattern, with the BERT-base-uncased model achieving 87.4% for English
and XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based
approaches. These findings confirm that fine-tuned transformer models
outperform traditional methods in multilingual financial text analysis. The
present framework offers an accurate and scalable solution for brand reputation
monitoring in code-mixed and low-resource banking environments.

</details>

### [9] [EMAFusion: A Self-Optimizing System for Seamless LLM Selection and Integration](https://arxiv.org/abs/2504.10681)
*Soham Shah,Kumar Shridhar,Surojit Chatterjee,Souvik Sen*

Main category: cs.CL

TLDR: EMAFusion是一个自优化的LLM选择和执行框架，通过结合分类路由器和学习路由器，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM部署的高成本和低效率问题，避免依赖标签数据或任务特定启发式方法。

Method: 结合分类路由器、学习路由器和级联方法，逐步从低成本模型升级到高成本模型。

Result: 性能提升2.6个百分点，成本降低4倍，比GPT-4成本低20倍以上。

Conclusion: EMAFusion提供灵活的成本-准确性权衡，是一种高效的LLM选择和执行策略。

Abstract: While recent advances in large language models (LLMs) have significantly
enhanced performance across diverse natural language tasks, the high
computational and financial costs associated with their deployment remain
substantial barriers. Existing routing strategies partially alleviate this
challenge by assigning queries to cheaper or specialized models, but they
frequently rely on extensive labeled data or fragile task-specific heuristics.
Conversely, fusion techniques aggregate multiple LLM outputs to boost accuracy
and robustness, yet they often exacerbate cost and may reinforce shared biases.
  We introduce EMAFusion, a new framework that self-optimizes for seamless LLM
selection and reliable execution for a given query. Specifically, EMAFusion
integrates a taxonomy-based router for familiar query types, a learned router
for ambiguous inputs, and a cascading approach that progressively escalates
from cheaper to more expensive models based on multi-judge confidence
evaluations. Through extensive evaluations, we find EMAFusion outperforms the
best individual models by over 2.6 percentage points (94.3% vs. 91.7%), while
being 4X cheaper than the average cost. EMAFusion further achieves a remarkable
17.1 percentage point improvement over models like GPT-4 at less than 1/20th
the cost. Our combined routing approach delivers 94.3% accuracy compared to
taxonomy-based (88.1%) and learned model predictor-based (91.7%) methods alone,
demonstrating the effectiveness of our unified strategy. Finally, EMAFusion
supports flexible cost-accuracy trade-offs, allowing users to balance their
budgetary constraints and performance needs.

</details>

### [10] [HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving](https://arxiv.org/abs/2504.10724)
*Avinash Kumar,Shashank Nag,Jason Clemons,Lizy John,Poulami Das*

Main category: cs.CL

TLDR: HELIOS通过动态选择模型和贪婪加载层数，优化LLM的吞吐量、能效和响应时间。


<details>
  <summary>Details</summary>
Motivation: 解决EE-LLMs因保守加载整个模型而限制资源节省和吞吐量的问题，以及静态模型选择无法适应输入查询变化的问题。

Method: 1. 候选LLM短列表并实时评估；2. 贪婪加载选定模型的有限层数；3. 动态监控和切换模型。

Result: HELIOS在吞吐量、能效、响应时间和推理批次大小上分别提升1.48倍、1.10倍、1.39倍和3.7倍。

Conclusion: HELIOS有效优化了LLM部署中的性能指标，适应动态需求。

Abstract: Deploying large language models (LLMs) presents critical challenges due to
the inherent trade-offs associated with key performance metrics, such as
latency, accuracy, and throughput. Typically, gains in one metric is
accompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently
navigate this trade-off space by skipping some of the later model layers when
it confidently finds an output token early, thus reducing latency without
impacting accuracy. However, as the early exits taken depend on the task and
are unknown apriori to request processing, EE-LLMs conservatively load the
entire model, limiting resource savings and throughput. Also, current
frameworks statically select a model for a user task, limiting our ability to
adapt to changing nature of the input queries.
  We propose HELIOS to address these challenges. First, HELIOS shortlists a set
of candidate LLMs, evaluates them using a subset of prompts, gathering
telemetry data in real-time. Second, HELIOS uses the early exit data from these
evaluations to greedily load the selected model only up to a limited number of
layers. This approach yields memory savings which enables us to process more
requests at the same time, thereby improving throughput. Third, HELIOS monitors
and periodically reassesses the performance of the candidate LLMs and if
needed, switches to another model that can service incoming queries more
efficiently (such as using fewer layers without lowering accuracy). Our
evaluations show that HELIOS achieves 1.48$\times$ throughput, 1.10$\times$
energy-efficiency, 1.39$\times$ lower response time, and 3.7$\times$
improvements in inference batch sizes compared to the baseline, when optimizing
for the respective service level objectives.

</details>

### [11] [The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks](https://arxiv.org/abs/2504.10768)
*Ralf Schmälzle,Sue Lim,Yuetong Du,Gary Bente*

Main category: cs.CL

TLDR: 研究表明，通过极短的演讲片段（薄切片）可以准确预测整体演讲质量，且基于大语言模型（LLM）的评估与人类评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 探索薄切片方法在科学演讲中的应用，验证LLM在评估演讲质量中的有效性和效率。

Method: 使用真实科学演讲数据集，通过LLM评估完整演讲及其薄切片，并与人类评分进行相关性分析。

Result: 极短片段（少于10%的演讲内容）即可强预测整体评价，且LLM评估与人类评分高度一致。

Conclusion: 薄切片方法在演讲评估中有效，LLM可作为高效工具用于沟通反馈。

Abstract: This paper examines the thin-slicing approach - the ability to make accurate
judgments based on minimal information - in the context of scientific
presentations. Drawing on research from nonverbal communication and personality
psychology, we show that brief excerpts (thin slices) reliably predict overall
presentation quality. Using a novel corpus of over one hundred real-life
science talks, we employ Large Language Models (LLMs) to evaluate transcripts
of full presentations and their thin slices. By correlating LLM-based
evaluations of short excerpts with full-talk assessments, we determine how much
information is needed for accurate predictions. Our results demonstrate that
LLM-based evaluations align closely with human ratings, proving their validity,
reliability, and efficiency. Critically, even very short excerpts (less than 10
percent of a talk) strongly predict overall evaluations. This suggests that the
first moments of a presentation convey relevant information that is used in
quality evaluations and can shape lasting impressions. The findings are robust
across different LLMs and prompting strategies. This work extends thin-slicing
research to public speaking and connects theories of impression formation to
LLMs and current research on AI communication. We discuss implications for
communication and social cognition research on message reception. Lastly, we
suggest an LLM-based thin-slicing framework as a scalable feedback tool to
enhance human communication.

</details>

### [12] [GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction](https://arxiv.org/abs/2504.10792)
*Jessica Lin,Amir Zeldes*

Main category: cs.CL

TLDR: 提出了一种结合主观判断和摘要方法的新方法，用于分级实体显著性，表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在一致性或输出限制上的不足，满足用户对长文档中重要实体的需求。

Method: 基于多摘要的实体显著性评分方法，利用12种语体的英文数据集，计算实体在多个摘要中的出现频率。

Result: 新方法与人类摘要评分相关性更强，性能优于现有技术（包括LLMs）。

Conclusion: 该方法为分级实体显著性提取提供了有效解决方案，并公开了数据和代码以支持后续研究。

Abstract: Determining and ranking the most salient entities in a text is critical for
user-facing systems, especially as users increasingly rely on models to
interpret long documents they only partially read. Graded entity salience
addresses this need by assigning entities scores that reflect their relative
importance in a text. Existing approaches fall into two main categories:
subjective judgments of salience, which allow for gradient scoring but lack
consistency, and summarization-based methods, which define salience as
mention-worthiness in a summary, promoting explainability but limiting outputs
to binary labels (entities are either summary-worthy or not). In this paper, we
introduce a novel approach for graded entity salience that combines the
strengths of both approaches. Using an English dataset spanning 12 spoken and
written genres, we collect 5 summaries per document and calculate each entity's
salience score based on its presence across these summaries. Our approach shows
stronger correlation with scores based on human summaries and alignments, and
outperforms existing techniques, including LLMs. We release our data and code
at https://github.com/jl908069/gum_sum_salience to support further research on
graded salient entity extraction.

</details>

### [13] [Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies](https://arxiv.org/abs/2504.10797)
*Annabella Sakunkoo,Jonathan Sakunkoo*

Main category: cs.CL

TLDR: 研究探讨了LLMs如何基于姓名（尤其是姓氏与名字的结合）对不同种族和性别的人进行不公平的排序，揭示了AI在反映和强化社会地位偏见方面的复杂性。


<details>
  <summary>Details</summary>
Motivation: 姓名承载了深刻的个人和文化意义，并可能影响社会对个体能力和价值的判断。随着LLMs的广泛应用，评估其是否基于姓名产生偏见至关重要。

Method: 通过大规模分析5个种族的姓名变体，研究AI如何展现姓名偏见，重点关注性别和种族的交互作用。

Result: 研究发现LLMs基于姓名反映并强化了社会地位偏见，东亚和南亚姓名在某些情况下排名更高，性别也调节了偏见。

Conclusion: 研究强调了在评估LLMs时需考虑种族、性别和混合身份的交叉性，以更全面地理解偏见。

Abstract: Across cultures, names tell a lot about their bearers as they carry deep
personal and cultural significance. Names also serve as powerful signals of
gender, race, and status in the social hierarchy - a pecking order in which
individual positions shape others' expectations on their perceived competence
and worth. With the widespread adoption of LLMs and as names are often an input
for LLMs, it is crucial to evaluate whether LLMs may sort people into status
positions based on first and last names and, if so, whether it is in an unfair,
biased fashion. While prior work has primarily investigated biases in first
names, little attention has been paid to last names and even less to the
combined effects of first and last names. In this study, we conduct a
large-scale analysis of name variations across 5 ethnicities to examine how AI
exhibits name biases. Our study investigates three key characteristics of
inequality and finds that LLMs reflect and reinforce status hierarchies based
on names that signal gender and ethnicity as they encode differential
expectations of competence, leadership, and economic potential. Contrary to the
common assumption that AI tends to favor Whites, we show that East and, in some
contexts, South Asian names receive higher rankings. We also disaggregate
Asians, a population projected to be the largest immigrant group in the U.S. by
2055. Our results challenge the monolithic Asian model minority assumption,
illustrating a more complex and stratified model of bias. Gender moderates
biases, with girls facing unfair disadvantages in certain racial groups.
Additionally, spanning cultural categories by adopting Western first names
improves AI-perceived status for East and Southeast Asian students,
particularly for girls. Our findings underscore the importance of
intersectional and more nuanced understandings of race, gender, and mixed
identities in the evaluation of LLMs.

</details>

### [14] [CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives](https://arxiv.org/abs/2504.10823)
*Ayoung Lee,Ryan Sungmo Kwon,Peter Railton,Lu Wang*

Main category: cs.CL

TLDR: 论文介绍了CLASH数据集，用于评估大型语言模型（LLM）在高风险困境中的推理能力，发现模型在模糊决策、价值转移理解和价值偏好方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究在评估LLM处理高风险、多价值冲突困境能力的空白。

Method: 构建CLASH数据集（345个高风险困境和3,795个多元价值观视角），并测试10种前沿模型的性能。

Result: 模型在模糊决策场景中准确率低于50%，对价值转移理解不足，但能预测心理不适；发现模型价值偏好与可引导性相关。

Conclusion: LLM需提升复杂价值推理能力，且第三方视角更利于价值引导。

Abstract: Navigating high-stakes dilemmas involving conflicting values is challenging
even for humans, let alone for AI. Yet prior work in evaluating the reasoning
capabilities of large language models (LLMs) in such situations has been
limited to everyday scenarios. To close this gap, this work first introduces
CLASH (Character perspective-based LLM Assessments in Situations with
High-stakes), a meticulously curated dataset consisting of 345 high-impact
dilemmas along with 3,795 individual perspectives of diverse values. In
particular, we design CLASH in a way to support the study of critical aspects
of value-based decision-making processes which are missing from prior work,
including understanding decision ambivalence and psychological discomfort as
well as capturing the temporal shifts of values in characters' perspectives. By
benchmarking 10 open and closed frontier models, we uncover several key
findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,
achieve less than 50% accuracy in identifying situations where the decision
should be ambivalent, while they perform significantly better in clear-cut
scenarios. (2) While LLMs reasonably predict psychological discomfort as marked
by human, they inadequately comprehend perspectives involving value shifts,
indicating a need for LLMs to reason over complex values. (3) Our experiments
also reveal a significant correlation between LLMs' value preferences and their
steerability towards a given value. (4) Finally, LLMs exhibit greater
steerability when engaged in value reasoning from a third-party perspective,
compared to a first-person setup, though certain value pairs benefit uniquely
from the first-person framing.

</details>

### [15] [Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive Language Generators](https://arxiv.org/abs/2504.10845)
*Phill Kyu Rhee*

Main category: cs.CL

TLDR: 论文提出了一种将大语言模型（LLMs）解释为概率左上下文敏感语言（CSLs）生成器的新框架，分解Transformer为三个基本组件，为理解其生成能力提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型（LLMs）的底层机制，尤其是其如何通过简单的token预测实现类人智能输出。

Method: 将Transformer分解为上下文窗口、注意力机制和自回归生成框架，提出LLMs是概率左CSLs的动态近似。

Result: 证明了Transformer可以随机近似CSLs，为生成式AI的理论和应用提供了新基础。

Conclusion: 该框架为理解LLMs的生成能力和未来潜力提供了新视角，连接了形式语言理论与Transformer的生成能力。

Abstract: Large Language Models (LLMs), powered by Transformers, have demonstrated
human-like intelligence capabilities, yet their underlying mechanisms remain
poorly understood. This paper presents a novel framework for interpreting LLMs
as probabilistic left context-sensitive languages (CSLs) generators. We
hypothesize that Transformers can be effectively decomposed into three
fundamental components: context windows, attention mechanisms, and
autoregressive generation frameworks. This decomposition allows for the
development of more flexible and interpretable computational models, moving
beyond the traditional view of attention and autoregression as inseparable
processes. We argue that next-token predictions can be understood as
probabilistic, dynamic approximations of left CSL production rules, providing
an intuitive explanation for how simple token predictions can yield human-like
intelligence outputs. Given that all CSLs are left context-sensitive
(Penttonen, 1974), we conclude that Transformers stochastically approximate
CSLs, which are widely recognized as models of human-like intelligence. This
interpretation bridges the gap between Formal Language Theory and the observed
generative power of Transformers, laying a foundation for future advancements
in generative AI theory and applications. Our novel perspective on Transformer
architectures will foster a deeper understanding of LLMs and their future
potentials.

</details>

### [16] [Ai2 Scholar QA: Organized Literature Synthesis with Attribution](https://arxiv.org/abs/2504.10861)
*Amanpreet Singh,Joseph Chee Chang,Chloe Anastasiades,Dany Haddad,Aakanksha Naik,Amber Tanaka,Angele Zamarron,Cecile Nguyen,Jena D. Hwang,Jason Dunkleberger,Matt Latzke,Smita Rao,Jaron Lochner,Rob Evans,Rodney Kinney,Daniel S. Weld,Doug Downey,Sergey Feldman*

Main category: cs.CL

TLDR: Ai2 Scholar QA是一个免费的开源科学问答系统，性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有科学问答系统多为昂贵且闭源，限制了研究和应用。

Method: 提供可定制的开源Python包、交互式网页应用、公开API和可下载数据集。

Result: 在科学QA基准测试中表现优于竞争对手。

Conclusion: Ai2 Scholar QA为科学问答提供了高效、免费且开放的解决方案。

Abstract: Retrieval-augmented generation is increasingly effective in answering
scientific questions from literature, but many state-of-the-art systems are
expensive and closed-source. We introduce Ai2 Scholar QA, a free online
scientific question answering application. To facilitate research, we make our
entire pipeline public: as a customizable open-source Python package and
interactive web app, along with paper indexes accessible through public APIs
and downloadable datasets. We describe our system in detail and present
experiments analyzing its key design decisions. In an evaluation on a recent
scientific QA benchmark, we find that Ai2 Scholar QA outperforms competing
systems.

</details>

### [17] [Efficient Reasoning Models: A Survey](https://arxiv.org/abs/2504.10903)
*Sicheng Feng,Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CL

TLDR: 该论文综述了高效推理的最新进展，将其分为三个方向：缩短推理链、开发小型语言模型和设计高效解码策略。


<details>
  <summary>Details</summary>
Motivation: 解决因生成长推理链（CoTs）带来的计算开销问题，推动高效推理技术的发展。

Method: 分类综述现有工作，包括压缩推理链、模型压缩和高效解码策略。

Result: 总结了三个关键方向的研究进展，并提供了相关论文的GitHub资源。

Conclusion: 高效推理技术是解决计算开销问题的关键，未来需进一步优化这三个方向。

Abstract: Reasoning models have demonstrated remarkable progress in solving complex and
logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to
arriving at a final answer. Yet, the emergence of this "slow-thinking"
paradigm, with numerous tokens generated in sequence, inevitably introduces
substantial computational overhead. To this end, it highlights an urgent need
for effective acceleration. This survey aims to provide a comprehensive
overview of recent advances in efficient reasoning. It categorizes existing
works into three key directions: (1) shorter - compressing lengthy CoTs into
concise yet effective reasoning chains; (2) smaller - developing compact
language models with strong reasoning capabilities through techniques such as
knowledge distillation, other model compression techniques, and reinforcement
learning; and (3) faster - designing efficient decoding strategies to
accelerate inference. A curated collection of papers discussed in this survey
is available in our GitHub repository.

</details>

### [18] [Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From](https://arxiv.org/abs/2504.10906)
*Changjiang Gao,Hankun Lin,Shujian Huang,Xin Huang,Xue Han,Junlan Feng,Chao Deng,Jiajun Chen*

Main category: cs.CL

TLDR: 该论文研究了大型语言模型（LLMs）的跨语言上下文检索能力，评估了40多个模型在12种语言中的表现，发现小型后训练开源模型表现优异，且后训练显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 跨语言上下文检索是LLMs跨语言对齐的关键能力，但现有研究不足，需深入探究其来源和提升方法。

Method: 通过跨语言机器阅读理解（xMRC）任务评估40多个LLMs在12种语言中的表现，分析后训练对性能的影响。

Result: 小型后训练开源模型表现优异，性能接近GPT-4o；后训练显著提升性能，且跨语言检索分为问题编码和答案检索两阶段。

Conclusion: 大规模预训练无法提升xMRC性能，需通过多语言后训练释放LLMs的跨语言检索潜力。

Abstract: The ability of cross-lingual context retrieval is a fundamental aspect of
cross-lingual alignment of large language models (LLMs), where the model
extracts context information in one language based on requests in another
language. Despite its importance in real-life applications, this ability has
not been adequately investigated for state-of-the-art models. In this paper, we
evaluate the cross-lingual context retrieval ability of over 40 LLMs across 12
languages to understand the source of this ability, using cross-lingual machine
reading comprehension (xMRC) as a representative scenario. Our results show
that several small, post-trained open LLMs show strong cross-lingual context
retrieval ability, comparable to closed-source LLMs such as GPT-4o, and their
estimated oracle performances greatly improve after post-training. Our
interpretability analysis shows that the cross-lingual context retrieval
process can be divided into two main phases: question encoding and answer
retrieval, which are formed in pre-training and post-training, respectively.
The phasing stability correlates with xMRC performance, and the xMRC bottleneck
lies at the last model layers in the second phase, where the effect of
post-training can be evidently observed. Our results also indicate that
larger-scale pretraining cannot improve the xMRC performance. Instead, larger
LLMs need further multilingual post-training to fully unlock their
cross-lingual context retrieval potential. Our code and is available at
https://github.com/NJUNLP/Cross-Lingual-Context-Retrieval

</details>

### [19] [Exploring the Role of KG-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)
*Yingjian Chen,Feiyang Li,Xingyu Song,Tianxiao Li,Issey Sudeka,Irene Li*

Main category: cs.CL

TLDR: 研究探索了基于知识图谱的RAG框架在日本医学QA中的应用，发现其对小型开源LLMs效果有限，且效果依赖于检索内容的质量和相关性。


<details>
  <summary>Details</summary>
Motivation: 由于隐私限制，商业LLMs（如GPT-4）在日本临床环境中无法使用，因此研究转向开源LLMs的指令调优，但RAG的潜力尚未充分探索。

Method: 首次提出基于知识图谱的RAG框架，用于日本医学QA的小型开源LLMs。

Result: 实验表明，KG-based RAG对小型开源LLMs在日本医学QA中的效果有限，且效果受检索内容质量和相关性影响显著。

Conclusion: 研究揭示了RAG在日本医学QA中的挑战与潜力，并为其他低资源语言提供了参考。

Abstract: Large language models (LLMs) perform well in medical QA, but their
effectiveness in Japanese contexts is limited due to privacy constraints that
prevent the use of commercial models like GPT-4 in clinical settings. As a
result, recent efforts focus on instruction-tuning open-source LLMs, though the
potential of combining them with retrieval-augmented generation (RAG) remains
underexplored. To bridge this gap, we are the first to explore a knowledge
graph-based (KG) RAG framework for Japanese medical QA small-scale open-source
LLMs. Experimental results show that KG-based RAG has only a limited impact on
Japanese medical QA using small-scale open-source LLMs. Further case studies
reveal that the effectiveness of the RAG is sensitive to the quality and
relevance of the external retrieved content. These findings offer valuable
insights into the challenges and potential of applying RAG in Japanese medical
QA, while also serving as a reference for other low-resource languages.

</details>

### [20] [ReZero: Enhancing LLM search ability by trying one-more-time](https://arxiv.org/abs/2504.11001)
*Alan Dao,Thinh Le*

Main category: cs.CL

TLDR: ReZero是一个新的强化学习框架，通过奖励重试失败的搜索查询来提升LLM在知识密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前方法通常关注查询生成或结果推理，但未明确鼓励在搜索失败后继续尝试。

Method: ReZero通过直接奖励重试行为，激励LLM探索替代查询而非过早停止。

Result: ReZero显著提升了性能，准确率达到46.88%，而基线为25%。

Conclusion: 通过奖励持久性，ReZero增强了LLM在复杂信息搜索场景中的鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)
performance on knowledge-intensive tasks but depends heavily on initial search
query quality. Current methods, often using Reinforcement Learning (RL),
typically focus on query formulation or reasoning over results, without
explicitly encouraging persistence after a failed search. We introduce ReZero
(Retry-Zero), a novel RL framework that directly rewards the act of retrying a
search query following an initial unsuccessful attempt. This incentivizes the
LLM to explore alternative queries rather than prematurely halting. ReZero
demonstrates significant improvement, achieving 46.88% accuracy compared to a
25% baseline. By rewarding persistence, ReZero enhances LLM robustness in
complex information-seeking scenarios where initial queries may prove
insufficient.

</details>

### [21] [Dynamic Compressing Prompts for Efficient Inference of Large Language Models](https://arxiv.org/abs/2504.11004)
*Jinwu Hu,Wei Zhang,Yufeng Wang,Yu Hu,Bin Xiao,Mingkui Tan,Qing Du*

Main category: cs.CL

TLDR: 论文提出了一种名为LLM-DCP的任务无关方法，通过动态压缩提示（Prompt）减少令牌数量，同时尽量保持性能。该方法将提示压缩建模为马尔可夫决策过程（MDP），并采用分层提示压缩（HPC）训练策略，实验表明其在高压缩率下优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的提示技术通常需要冗长的提示，增加了计算成本并可能因上下文窗口限制而影响性能。现有提示压缩方法在保留关键信息、适应上下文变化和跨任务有效性方面存在挑战。

Method: 提出动态压缩提示（LLM-DCP）方法，将提示压缩建模为MDP，通过DCP-Agent逐步移除冗余令牌。设计了平衡压缩率、输出质量和关键信息保留的奖励函数，并采用分层提示压缩（HPC）训练策略逐步增加压缩难度。

Result: 实验表明，该方法在高压缩率下优于现有技术，且无需依赖外部黑盒LLM。

Conclusion: LLM-DCP是一种高效的任务无关提示压缩方法，能显著减少令牌数量并保持性能，适用于多种任务。

Abstract: Large Language Models (LLMs) have shown outstanding performance across a
variety of tasks, partly due to advanced prompting techniques. However, these
techniques often require lengthy prompts, which increase computational costs
and can hinder performance because of the limited context windows of LLMs.
While prompt compression is a straightforward solution, existing methods
confront the challenges of retaining essential information, adapting to context
changes, and remaining effective across different tasks. To tackle these
issues, we propose a task-agnostic method called Dynamic Compressing Prompts
(LLM-DCP). Our method reduces the number of prompt tokens while aiming to
preserve the performance as much as possible. We model prompt compression as a
Markov Decision Process (MDP), enabling the DCP-Agent to sequentially remove
redundant tokens by adapting to dynamic contexts and retaining crucial content.
We develop a reward function for training the DCP-Agent that balances the
compression rate, the quality of the LLM output, and the retention of key
information. This allows for prompt token reduction without needing an external
black-box LLM. Inspired by the progressive difficulty adjustment in curriculum
learning, we introduce a Hierarchical Prompt Compression (HPC) training
strategy that gradually increases the compression difficulty, enabling the
DCP-Agent to learn an effective compression method that maintains information
integrity. Experiments demonstrate that our method outperforms state-of-the-art
techniques, especially at higher compression rates. The code for our approach
will be available at https://github.com/Fhujinwu/DCP.

</details>

### [22] [LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews](https://arxiv.org/abs/2504.11042)
*Sukannya Purkayastha,Zhuang Li,Anne Lauscher,Lizhen Qu,Iryna Gurevych*

Main category: cs.CL

TLDR: 论文介绍了LazyReview数据集，用于检测同行评审中的懒惰思维，并通过实验证明微调LLMs能显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 同行评审中的懒惰思维问题影响评审质量，但缺乏相关研究和数据集。

Method: 构建LazyReview数据集，标注懒惰思维类别，并实验微调LLMs。

Result: 微调后LLMs性能提升10-20点，反馈改进的评审更全面。

Conclusion: 高质量数据和反馈能提升评审质量，数据集和指南将公开。

Abstract: Peer review is a cornerstone of quality control in scientific publishing.
With the increasing workload, the unintended use of `quick' heuristics,
referred to as lazy thinking, has emerged as a recurring issue compromising
review quality. Automated methods to detect such heuristics can help improve
the peer-reviewing process. However, there is limited NLP research on this
issue, and no real-world dataset exists to support the development of detection
tools. This work introduces LazyReview, a dataset of peer-review sentences
annotated with fine-grained lazy thinking categories. Our analysis reveals that
Large Language Models (LLMs) struggle to detect these instances in a zero-shot
setting. However, instruction-based fine-tuning on our dataset significantly
boosts performance by 10-20 performance points, highlighting the importance of
high-quality training data. Furthermore, a controlled experiment demonstrates
that reviews revised with lazy thinking feedback are more comprehensive and
actionable than those written without such feedback. We will release our
dataset and the enhanced guidelines that can be used to train junior reviewers
in the community. (Code available here:
https://github.com/UKPLab/arxiv2025-lazy-review)

</details>

### [23] [DeepMLF: Multimodal language model with learnable tokens for deep fusion in sentiment analysis](https://arxiv.org/abs/2504.11082)
*Efthymios Georgiou,Vassilis Katsouros,Yannis Avrithis,Alexandros Potamianos*

Main category: cs.CL

TLDR: 论文提出了一种名为DeepMLF的新型多模态语言模型，通过可学习令牌实现深度融合，在多模态情感分析任务中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态融合的深度和容量分配在多模态情感分析中尚未充分研究，论文旨在探索这些因素对融合效果的影响。

Method: 引入DeepMLF模型，结合视听编码器和预训练解码器，通过可学习令牌实现模态交互和独立信息流的平衡。

Result: 在三个MSA基准测试中达到最优性能，发现5-7层为最佳融合深度，且少量令牌（约20个）效果最佳。

Conclusion: 深度融合和多模态容量分配对性能至关重要，DeepMLF的设计和训练策略为多模态融合提供了新思路。

Abstract: While multimodal fusion has been extensively studied in Multimodal Sentiment
Analysis (MSA), the role of fusion depth and multimodal capacity allocation
remains underexplored. In this work, we position fusion depth, scalability, and
dedicated multimodal capacity as primary factors for effective fusion. We
introduce DeepMLF, a novel multimodal language model (LM) with learnable tokens
tailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a
pretrained decoder LM augmented with multimodal information across its layers.
We append learnable tokens to the LM that: 1) capture modality interactions in
a controlled fashion and 2) preserve independent information flow for each
modality. These fusion tokens gather linguistic information via causal
self-attention in LM Blocks and integrate with audiovisual information through
cross-attention MM Blocks. Serving as dedicated multimodal capacity, this
design enables progressive fusion across multiple layers, providing depth in
the fusion process. Our training recipe combines modality-specific losses and
language modelling loss, with the decoder LM tasked to predict ground truth
polarity. Across three MSA benchmarks with varying dataset characteristics,
DeepMLF achieves state-of-the-art performance. Our results confirm that deeper
fusion leads to better performance, with optimal fusion depths (5-7) exceeding
those of existing approaches. Additionally, our analysis on the number of
fusion tokens reveals that small token sets ($\sim$20) achieve optimal
performance. We examine the importance of representation learning order (fusion
curriculum) through audiovisual encoder initialization experiments. Our
ablation studies demonstrate the superiority of the proposed fusion design and
gating while providing a holistic examination of DeepMLF's scalability to LLMs,
and the impact of each training objective and embedding regularization.

</details>

### [24] [Using LLMs as prompt modifier to avoid biases in AI image generators](https://arxiv.org/abs/2504.11104)
*René Peinl*

Main category: cs.CL

TLDR: 研究探讨了如何通过LLM修改用户提示来减少文本到图像生成系统中的偏见，实验表明该方法能显著增加图像多样性并减少偏见。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成系统中因中性提示导致的偏见问题，无需修改图像生成器本身。

Method: 使用LLM修改用户提示，并在Stable Diffusion XL、3.5和Flux上进行实验。

Result: LLM修改的提示显著增加了图像多样性并减少了偏见，但对某些上下文（如残疾表现）仍有局限。

Conclusion: 该方法为减少偏见提供了有效途径，尤其适用于较简单的图像生成器，但需进一步优化复杂提示的处理。

Abstract: This study examines how Large Language Models (LLMs) can reduce biases in
text-to-image generation systems by modifying user prompts. We define bias as a
model's unfair deviation from population statistics given neutral prompts. Our
experiments with Stable Diffusion XL, 3.5 and Flux demonstrate that
LLM-modified prompts significantly increase image diversity and reduce bias
without the need to change the image generators themselves. While occasionally
producing results that diverge from original user intent for elaborate prompts,
this approach generally provides more varied interpretations of underspecified
requests rather than superficial variations. The method works particularly well
for less advanced image generators, though limitations persist for certain
contexts like disability representation. All prompts and generated images are
available at https://iisys-hof.github.io/llm-prompt-img-gen/

</details>

### [25] [Benchmarking Vision Language Models on German Factual Data](https://arxiv.org/abs/2504.11108)
*René Peinl,Vincent Tischler*

Main category: cs.CL

TLDR: 本文分析了开放权重视觉语言模型（VLMs）在德语和英语中的事实知识表现，发现模型在德语图像内容识别上存在不足，尤其是在名人和景点方面。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索VLMs在非英语语言（如德语）中的表现，填补现有研究中对高资源语言支持的不足。

Method: 通过分离图像和文本因素，使用陪审团评估方法，分析德语和国际背景下的图像及提示语言的准确性。

Result: 结果显示，VLMs在德语图像内容（如名人和景点）的视觉认知上表现较差，但对动植物（科学名称或英文通用名）和部分商品（如汽车和超市产品）的识别较好。

Conclusion: 结论指出VLMs在德语支持上仍有不足，尤其是在视觉认知和语言识别方面，需要进一步改进。

Abstract: Similar to LLMs, the development of vision language models is mainly driven
by English datasets and models trained in English and Chinese language, whereas
support for other languages, even those considered high-resource languages such
as German, remains significantly weaker. In this work we present an analysis of
open-weight VLMs on factual knowledge in the German and English language. We
disentangle the image-related aspects from the textual ones by analyzing
accu-racy with jury-as-a-judge in both prompt languages and images from German
and international contexts. We found that for celebrities and sights, VLMs
struggle because they are lacking visual cognition of German image contents.
For animals and plants, the tested models can often correctly identify the
image contents ac-cording to the scientific name or English common name but
fail in German lan-guage. Cars and supermarket products were identified equally
well in English and German images across both prompt languages.

</details>

### [26] [MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos](https://arxiv.org/abs/2504.11169)
*Laura De Grazia,Pol Pastells,Mauro Vázquez Chas,Desmond Elliott,Danae Sánchez Villegas,Mireia Farrús,Mariona Taulé*

Main category: cs.CL

TLDR: 论文提出MuSeD数据集，用于多模态性别歧视检测，并评估了LLMs和多模态LLMs在此任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中性别歧视通过多模态内容传播，需多模态分析方法。

Method: 引入MuSeD数据集，提出标注框架，评估LLMs和多模态LLMs。

Result: 视觉信息对性别歧视标注至关重要，模型能检测显性歧视，但对隐性歧视效果差。

Conclusion: 隐性性别歧视检测依赖社会文化背景，任务具有挑战性。

Abstract: Sexism is generally defined as prejudice and discrimination based on sex or
gender, affecting every sector of society, from social institutions to
relationships and individual behavior. Social media platforms amplify the
impact of sexism by conveying discriminatory content not only through text but
also across multiple modalities, highlighting the critical need for a
multimodal approach to the analysis of sexism online. With the rise of social
media platforms where users share short videos, sexism is increasingly
spreading through video content. Automatically detecting sexism in videos is a
challenging task, as it requires analyzing the combination of verbal, audio,
and visual elements to identify sexist content. In this study, (1) we introduce
MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of
$\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose
an innovative annotation framework for analyzing the contribution of textual
and multimodal labels in the classification of sexist and non-sexist content;
and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs
on the task of sexism detection. We find that visual information plays a key
role in labeling sexist content for both humans and models. Models effectively
detect explicit sexism; however, they struggle with implicit cases, such as
stereotypes, instances where annotators also show low agreement. This
highlights the inherent difficulty of the task, as identifying implicit sexism
depends on the social and cultural context.

</details>

### [27] [Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting](https://arxiv.org/abs/2504.11183)
*Ej Zhou,Weiming Lu*

Main category: cs.CL

TLDR: 研究利用高资源语言语料评估低资源语言中的偏见，并实验去偏方法，证明高资源语言的去偏方法可有效迁移至低资源语言。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的社会偏见可能加剧社会不平等，但现有研究多集中于英语数据，低资源语言因数据不足表现更差。

Method: 评估五种语言（英、中、俄、印尼、泰）的多语言模型，分析性别、宗教、国籍和种族-肤色四个偏见维度，构建多语言偏见评估数据集，并实验三种去偏方法（CDA、Dropout、SenDeb）。

Result: 高资源语言的去偏方法可有效迁移至低资源语言，为多语言NLP公平性研究提供实用见解。

Conclusion: 研究为低资源语言的偏见评估和去偏提供了可行方案，推动了多语言NLP的公平性研究。

Abstract: Social bias in language models can potentially exacerbate social
inequalities. Despite it having garnered wide attention, most research focuses
on English data. In a low-resource scenario, the models often perform worse due
to insufficient training data. This study aims to leverage high-resource
language corpora to evaluate bias and experiment with debiasing methods in
low-resource languages. We evaluated the performance of recent multilingual
models in five languages: English (\textsc{eng}), Chinese (\textsc{zho}),
Russian (\textsc{rus}), Indonesian (\textsc{ind}) and Thai (\textsc{tha}), and
analyzed four bias dimensions: \textit{gender}, \textit{religion},
\textit{nationality}, and \textit{race-color}. By constructing multilingual
bias evaluation datasets, this study allows fair comparisons between models
across languages. We have further investigated three debiasing
methods-\texttt{CDA}, \texttt{Dropout}, \texttt{SenDeb}-and demonstrated that
debiasing methods from high-resource languages can be effectively transferred
to low-resource ones, providing actionable insights for fairness research in
multilingual NLP.

</details>

### [28] [Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items](https://arxiv.org/abs/2504.11186)
*Minjie Zou,Sahana Srinivasan,Thaddaeus Wai Soon Lo,Ke Zou,Gabriel Dawei Yang,Xuguang Ai,Hyunjae Kim,Maxwell Singer,Fares Antaki,Kelvin Li,Robert Chang,Marcus Tan,David Ziyou Chen,Dianbo Liu,Qingyu Chen,Yih Chung Tham*

Main category: cs.CL

TLDR: 该研究评估了四种推理型大语言模型在眼科领域的表现，发现OpenAI o1和DeepSeek-R1在准确性和推理能力上表现最佳，而不同模型在文本生成指标上各有优劣。


<details>
  <summary>Details</summary>
Motivation: 探索推理型大语言模型在眼科等专业领域的表现，填补现有研究的空白。

Method: 使用5,888道眼科考试题目（MedMCQA数据集）在零样本设置下评估四种模型，结合定量指标（准确率、Macro-F1、文本生成指标）和定性评估（专家评审）。

Result: OpenAI o1和DeepSeek-R1在准确性和Macro-F1上表现最佳，其他模型在不同文本生成指标上各有优势；推理时间差异显著。

Conclusion: 推理型大语言模型在眼科领域具有潜力，但不同模型在性能和推理风格上存在差异，需根据具体需求选择。

Abstract: Recent advances in reasoning-focused large language models (LLMs) mark a
shift from general LLMs toward models designed for complex decision-making, a
crucial aspect in medicine. However, their performance in specialized domains
like ophthalmology remains underexplored. This study comprehensively evaluated
and compared the accuracy and reasoning capabilities of four newly developed
reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0
Flash-Thinking. Each model was assessed using 5,888 multiple-choice
ophthalmology exam questions from the MedMCQA dataset in zero-shot setting.
Quantitative evaluation included accuracy, Macro-F1, and five text-generation
metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed
against ground-truth reasonings. Average inference time was recorded for a
subset of 100 randomly selected questions. Additionally, two board-certified
ophthalmologists qualitatively assessed clarity, completeness, and reasoning
structure of responses to differential diagnosis questions.O1 (0.902) and
DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in
Macro-F1 (0.900). The performance of models across the text-generation metrics
varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1
and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0
Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and
o1 (0.176) led AlignScore. Inference time across the models varied, with
DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest
(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0
Flash-Thinking tended to provide detailed and comprehensive intermediate
reasoning, whereas o1 and o3-mini displayed concise and summarized
justifications.

</details>

### [29] [From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs](https://arxiv.org/abs/2504.11277)
*Guocong Li,Weize Liu,Yihang Wu,Ping Wang,Shuaihan Huang,Hongxia Xu,Jian Wu*

Main category: cs.CL

TLDR: 本文提出了一种三阶段微调方法，提升大语言模型（LLMs）检测和纠正输入中误导信息的能力，从而提高回答准确性并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注输出修正，而忽略了提升LLMs检测和纠正输入中误导信息的潜力。

Method: 三阶段微调方法：1) 训练LLMs识别误导信息；2) 训练LLMs利用内置或外部知识纠正误导信息；3) 训练LLMs基于修正后的查询生成准确回答。

Result: 实验表明，该方法显著提升了LLM回答的准确性和事实性，增强了幻觉检测能力，并减少了幻觉生成。

Conclusion: 该方法有效解决了输入中误导信息的问题，提升了LLMs的实用性和可靠性。

Abstract: Large language models (LLMs) exhibit excellent performance in natural
language processing (NLP), but remain highly sensitive to the quality of input
queries, especially when these queries contain misleading or inaccurate
information. Existing methods focus on correcting the output, but they often
overlook the potential of improving the ability of LLMs to detect and correct
misleading content in the input itself. In this paper, we propose a novel
three-stage fine-tuning method that enhances the ability of LLMs to detect and
correct misleading information in the input, further improving response
accuracy and reducing hallucinations. Specifically, the three stages include
(1) training LLMs to identify misleading information, (2) training LLMs to
correct the misleading information using built-in or external knowledge, and
(3) training LLMs to generate accurate answers based on the corrected queries.
To evaluate our method, we conducted experiments on three datasets for the
hallucination detection task and the question answering (QA) task, as well as
two datasets containing misleading information that we constructed. The
experimental results demonstrate that our method significantly improves the
accuracy and factuality of LLM responses, while also enhancing the ability to
detect hallucinations and reducing the generation of hallucinations in the
output, particularly when the query contains misleading information. We will
publicly release our code upon acceptance.

</details>

### [30] [Automated Python Translation](https://arxiv.org/abs/2504.11290)
*Joshua Otten,Antonios Anastasopoulos,Kevin Moran*

Main category: cs.CL

TLDR: 论文提出了一种自动化翻译Python关键词和术语的方法，以解决非英语使用者在理解Python代码时的障碍。


<details>
  <summary>Details</summary>
Motivation: Python的英语特性对非英语使用者造成理解障碍，因此需要一种方法将其翻译为其他语言。

Method: 使用机器翻译和大语言模型构建自动化翻译管道，对五个常用Python库的术语进行七种语言的翻译，并在法语、希腊语和孟加拉语中进行质量测试。

Result: 成功获取了多语言翻译，并进行了质量测试，为未来实现通用Python提供了基础。

Conclusion: 该方法为非英语使用者提供了更清晰的Python学习路径，有望实现跨语言的Python普及。

Abstract: Python is one of the most commonly used programming languages in industry and
education. Its English keywords and built-in functions/modules allow it to come
close to pseudo-code in terms of its readability and ease of writing. However,
those who do not speak English may not experience these advantages. In fact,
they may even be hindered in their ability to understand Python code, as the
English nature of its terms creates an additional layer of overhead. To that
end, we introduce the task of automatically translating Python's natural
modality (keywords, error types, identifiers, etc.) into other human languages.
This presents a unique challenge, considering the abbreviated nature of these
forms, as well as potential untranslatability of advanced
mathematical/programming concepts across languages. We therefore create an
automated pipeline to translate Python into other human languages, comparing
strategies using machine translation and large language models. We then use
this pipeline to acquire translations from five common Python libraries
(pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a
quality test on a subset of these terms in French, Greek, and Bengali. We hope
this will provide a clearer path forward towards creating a universal Python,
accessible to anyone regardless of nationality or language background.

</details>

### [31] [Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.11331)
*Hao Liu,Lijun He,Jiaxi Liang,Zhihan Ren,Fan Li*

Main category: cs.CL

TLDR: DASCO框架通过依赖解析树增强多模态情感分析，解决了SCP、MIM和SNE三大挑战，并在实验中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态情感分析中难以同时解决SCP、MIM和SNE三大核心挑战。

Method: 提出DASCO框架，结合多任务预训练策略和依赖树，增强情感推理和噪声过滤。

Result: 在JMASA任务中，Twitter2015数据集上F1提升3.1%，精度提升5.4%。

Conclusion: DASCO在多模态情感分析中表现优异，解决了现有方法的局限性。

Abstract: Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract
fine-grained information from image-text pairs to identify aspect terms and
determine their sentiment polarity. However, existing approaches often fall
short in simultaneously addressing three core challenges: Sentiment Cue
Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise
Elimination (SNE). To overcome these limitations, we propose DASCO
(\textbf{D}ependency Structure \textbf{A}ugmented \textbf{Sco}ping Framework),
a fine-grained scope-oriented framework that enhances aspect-level sentiment
reasoning by leveraging dependency parsing trees. First, we designed a
multi-task pretraining strategy for MABSA on our base model, combining
aspect-oriented enhancement, image-text matching, and aspect-level
sentiment-sensitive cognition. This improved the model's perception of aspect
terms and sentiment cues while achieving effective image-text alignment,
addressing key challenges like SCP and MIM. Furthermore, we incorporate
dependency trees as syntactic branch combining with semantic branch, guiding
the model to selectively attend to critical contextual elements within a
target-specific scope while effectively filtering out irrelevant noise for
addressing SNE problem. Extensive experiments on two benchmark datasets across
three subtasks demonstrate that DASCO achieves state-of-the-art performance in
MABSA, with notable gains in JMASA (+3.1\% F1 and +5.4\% precision on
Twitter2015).

</details>

### [32] [REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective](https://arxiv.org/abs/2504.11337)
*Zhihao Xu,Yongqi Tong,Xin Zhang,Jun Zhou,Xiting Wang*

Main category: cs.CL

TLDR: 论文提出了一种数据驱动的方法，通过奖励一致性（RC）概念和奖励一致性采样框架，有效缓解多目标偏好对齐中的冲突。


<details>
  <summary>Details</summary>
Motivation: 多目标偏好对齐中，优化一个目标（如帮助性）常会损害其他目标（如无害性），现有研究多关注算法解决方案，本文探索数据驱动方法。

Method: 提出奖励一致性（RC）概念，识别符合多目标偏好的样本；开发奖励一致性采样框架，自动构建缓解冲突的偏好数据集。

Result: 生成的数据在无害率和帮助性胜率上平均提升13.37%，并能持续解决多目标场景中的冲突。

Conclusion: 数据驱动方法能有效缓解多目标偏好对齐中的冲突，提升模型性能。

Abstract: Multi-objective preference alignment in language models often encounters a
challenging trade-off: optimizing for one human preference (e.g., helpfulness)
frequently compromises others (e.g., harmlessness) due to the inherent
conflicts between competing objectives. While prior work mainly focuses on
algorithmic solutions, we explore a novel data-driven approach to uncover the
types of data that can effectively mitigate these conflicts. Specifically, we
propose the concept of Reward Consistency (RC), which identifies samples that
align with multiple preference objectives, thereby reducing conflicts during
training. Through gradient-based analysis, we demonstrate that RC-compliant
samples inherently constrain performance degradation during multi-objective
optimization. Building on these insights, we further develop Reward Consistency
Sampling, a framework that automatically constructs preference datasets that
effectively mitigate conflicts during multi-objective alignment. Our generated
data achieves an average improvement of 13.37% in both the harmless rate and
helpfulness win rate when optimizing harmlessness and helpfulness, and can
consistently resolve conflicts in varying multi-objective scenarios.

</details>

### [33] [OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution](https://arxiv.org/abs/2504.11369)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TLDR: OpenTuringBench是一个基于OLLMs的新基准，用于训练和评估机器生成文本检测器，涵盖多种挑战性任务，并提供了OTBDetector框架。


<details>
  <summary>Details</summary>
Motivation: 随着OLLMs在生成式AI中的应用增加，检测其输出成为新挑战，需要新的基准和方法。

Method: 提出OpenTuringBench基准和OTBDetector框架，基于对比学习检测和归属OLLM生成的文本。

Result: OpenTuringBench任务具有不同难度，OTBDetector在多项任务中表现优异，超越现有检测器。

Conclusion: OpenTuringBench和OTBDetector为检测OLLM生成文本提供了有效工具，资源已公开。

Abstract: Open Large Language Models (OLLMs) are increasingly leveraged in generative
AI applications, posing new challenges for detecting their outputs. We propose
OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate
machine-generated text detectors on the Turing Test and Authorship Attribution
problems. OpenTuringBench focuses on a representative set of OLLMs, and
features a number of challenging evaluation tasks, including
human/machine-manipulated texts, out-of-domain texts, and texts from previously
unseen models. We also provide OTBDetector, a contrastive learning framework to
detect and attribute OLLM-based machine-generated texts. Results highlight the
relevance and varying degrees of difficulty of the OpenTuringBench tasks, with
our detector achieving remarkable capabilities across the various tasks and
outperforming most existing detectors. Resources are available on the
OpenTuringBench Hugging Face repository at
https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench

</details>

### [34] [Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False Presuppositions](https://arxiv.org/abs/2504.11373)
*Wang Bill Zhu,Tianqi Chen,Ching Ying Lin,Jade Law,Mazen Jizzini,Jorge J. Nieva,Ruishan Liu,Robin Jia*

Main category: cs.CL

TLDR: 论文评估了大型语言模型（LLM）在回答癌症患者真实问题时的表现，发现模型虽通常准确，但常忽略问题中的错误前提，存在安全隐患。为此，作者提出了Cancer-Myth数据集，显示前沿LLM在纠正错误前提方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在复杂、个性化医疗问题中的表现，尤其是对错误前提的识别能力，以揭示其在临床决策中的潜在风险。

Method: 通过真实患者问题评估LLM，并引入Cancer-Myth数据集，测试模型对错误前提的纠正能力。

Result: LLM（如GPT-4-Turbo）在准确性上表现良好（4.13/5），但对错误前提的纠正率低于30%。

Conclusion: LLM在临床可靠性上存在重大缺陷，需更强大的医疗AI系统保障措施。

Abstract: Cancer patients are increasingly turning to large language models (LLMs) as a
new form of internet search for medical information, making it critical to
assess how well these models handle complex, personalized questions. However,
current medical benchmarks focus on medical exams or consumer-searched
questions and do not evaluate LLMs on real patient questions with detailed
clinical contexts. In this paper, we first evaluate LLMs on cancer-related
questions drawn from real patients, reviewed by three hematology oncology
physicians. While responses are generally accurate, with GPT-4-Turbo scoring
4.13 out of 5, the models frequently fail to recognize or address false
presuppositions in the questions-posing risks to safe medical decision-making.
To study this limitation systematically, we introduce Cancer-Myth, an
expert-verified adversarial dataset of 585 cancer-related questions with false
presuppositions. On this benchmark, no frontier LLM -- including GPT-4o,
Gemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions
more than 30% of the time. Even advanced medical agentic methods do not prevent
LLMs from ignoring false presuppositions. These findings expose a critical gap
in the clinical reliability of LLMs and underscore the need for more robust
safeguards in medical AI systems.

</details>

### [35] [RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models](https://arxiv.org/abs/2504.11381)
*Juan Diego Rodriguez,Wenxuan Ding,Katrin Erk,Greg Durrett*

Main category: cs.CL

TLDR: 论文探讨了大型语言模型（LLMs）在生成答案与自我验证之间的不一致性（generator-validator gap），并提出RankAlign方法显著缩小了该差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多任务中表现更强大和准确，但其行为仍存在不可靠性，尤其是对相同信息在不同提示下的不一致性。

Method: 定义了一种更严格的generator-validator gap衡量方式，并提出RankAlign，一种基于排名的训练方法。

Result: RankAlign平均缩小了31.8%的gap，优于所有基线方法，并在域外任务和词汇项上表现良好。

Conclusion: RankAlign有效解决了LLMs的生成与验证不一致问题，具有广泛适用性。

Abstract: Although large language models (LLMs) have become generally more capable and
accurate across many tasks, some fundamental sources of unreliability remain in
their behavior. One key limitation is their inconsistency at reporting the the
same information when prompts are changed. In this paper, we consider the
discrepancy between a model's generated answer and their own verification of
that answer, the generator-validator gap. We define this gap in a more
stringent way than prior work: we expect correlation of scores from a generator
and a validator over the entire set of candidate answers. We show that
according to this measure, a large gap exists in various settings, including
question answering, lexical semantics tasks, and next-word prediction. We then
propose RankAlign, a ranking-based training method, and show that it
significantly closes the gap by 31.8% on average, surpassing all baseline
methods. Moreover, this approach generalizes well to out-of-domain tasks and
lexical items.

</details>

### [36] [Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning](https://arxiv.org/abs/2504.11409)
*Ali Taghibakhshi,Sharath Turuvekere Sreenivas,Saurav Muralidharan,Marcin Chochowski,Yashaswi Karnati,Raviraj Joshi,Ameya Sunil Mahabaleshwarkar,Zijia Chen,Yoshi Suhara,Oluwatobi Olabiyi,Daniel Korzekwa,Mostofa Patwary,Mohammad Shoeybi,Jan Kautz,Bryan Catanzaro,Ashwath Aithal,Nima Tajbakhsh,Pavlo Molchanov*

Main category: cs.CL

TLDR: 该论文提出了一种针对混合LLM架构（结合注意力与状态空间模型）的新型压缩方法，通过分组感知剪枝策略和知识蒸馏，显著提升了模型的准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 探索混合架构压缩的有效性，以在减少训练成本的同时保持或提升模型性能。

Method: 引入分组感知剪枝策略保护SSM块的结构完整性，结合SSM、FFN、嵌入维度和层剪枝，并进行知识蒸馏再训练。

Result: 将Nemotron-H 8B模型压缩至4B参数，训练token减少40倍，准确率超过同类模型，推理速度提升2倍。

Conclusion: 该方法显著推进了Pareto前沿，展示了混合架构压缩的潜力。

Abstract: Hybrid LLM architectures that combine Attention and State Space Models (SSMs)
achieve state-of-the-art accuracy and runtime performance. Recent work has
demonstrated that applying compression and distillation to Attention-only
models yields smaller, more accurate models at a fraction of the training cost.
In this work, we explore the effectiveness of compressing Hybrid architectures.
We introduce a novel group-aware pruning strategy that preserves the structural
integrity of SSM blocks and their sequence modeling capabilities. Furthermore,
we demonstrate the necessity of such SSM pruning to achieve improved accuracy
and inference speed compared to traditional approaches. Our compression recipe
combines SSM, FFN, embedding dimension, and layer pruning, followed by
knowledge distillation-based retraining, similar to the MINITRON technique.
Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B
parameters with up to 40x fewer training tokens. The resulting model surpasses
the accuracy of similarly-sized models while achieving 2x faster inference,
significantly advancing the Pareto frontier.

</details>

### [37] [Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts](https://arxiv.org/abs/2504.11420)
*Quanyu Long,Jianda Chen,Zhengyuan Liu,Nancy F. Chen,Wenya Wang,Sinno Jialin Pan*

Main category: cs.CL

TLDR: 提出了一种基于MDP的三编码器顺序检索器，通过分阶段训练显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统检索增强框架在组合检索任务中的局限性，需多源协调。

Method: 三编码器顺序检索器，将检索过程建模为MDP，分两阶段训练。

Result: 实验结果显示方法显著优于基线，验证了显式建模依赖关系的重要性。

Conclusion: 组合检索在需要多证据的任务中具有潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
numerous tasks, yet they often rely on external context to handle complex
tasks. While retrieval-augmented frameworks traditionally focus on selecting
top-ranked documents in a single pass, many real-world scenarios demand
compositional retrieval, where multiple sources must be combined in a
coordinated manner. In this work, we propose a tri-encoder sequential retriever
that models this process as a Markov Decision Process (MDP), decomposing the
probability of retrieving a set of elements into a sequence of conditional
probabilities and allowing each retrieval step to be conditioned on previously
selected examples. We train the retriever in two stages: first, we efficiently
construct supervised sequential data for initial policy training; we then
refine the policy to align with the LLM's preferences using a reward grounded
in the structural correspondence of generated programs. Experimental results
show that our method consistently and significantly outperforms baselines,
underscoring the importance of explicitly modeling inter-example dependencies.
These findings highlight the potential of compositional retrieval for tasks
requiring multiple pieces of evidence or examples.

</details>

### [38] [A Dual-Space Framework for General Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2504.11426)
*Xue Zhang,Songming Zhang,Yunlong Liang,Fandong Meng,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TLDR: 论文提出了一种双空间知识蒸馏（DSKD）框架，解决了当前白盒知识蒸馏（KD）在输出空间和词汇表差异上的限制，通过统一预测头和精确令牌对齐（ETA）算法，实现了更高效的知识转移。


<details>
  <summary>Details</summary>
Motivation: 当前白盒KD框架存在输出空间和词汇表差异的限制，导致教师模型和学生模型之间的相似性受限，且无法应用于不同词汇表的LLMs。

Method: 提出DSKD框架，引入两个投影器统一预测头，开发ETA算法对齐令牌，支持任意两个LLMs的知识蒸馏。

Result: 在指令跟随、数学推理和代码生成任务上，DSKD显著优于现有白盒KD方法和跨令牌器KD方法。

Conclusion: DSKD是一种通用的KD框架，能够有效解决输出空间和词汇表差异问题，提升知识蒸馏效果。

Abstract: Knowledge distillation (KD) is a promising solution to compress large
language models (LLMs) by transferring their knowledge to smaller models.
During this process, white-box KD methods usually minimize the distance between
the output distributions of the teacher model and the student model to transfer
more information. However, we reveal that the current white-box KD framework
exhibits two limitations: a) bridging probability distributions from different
output spaces will limit the similarity between the teacher model and the
student model; b) this framework cannot be applied to LLMs with different
vocabularies. One of the root causes for these limitations is that the
distributions from the teacher and the student for KD are output by different
prediction heads, which yield distributions in different output spaces and
dimensions. Therefore, in this paper, we propose a dual-space knowledge
distillation (DSKD) framework that unifies the prediction heads of the teacher
and the student models for KD. Specifically, we first introduce two projectors
with ideal initialization to project the teacher/student hidden states into the
student/teacher representation spaces. After this, the hidden states from
different models can share the same head and unify the output spaces of the
distributions. Furthermore, we develop an exact token alignment (ETA) algorithm
to align the same tokens in two differently-tokenized sequences. Based on the
above, our DSKD framework is a general KD framework that supports both
off-policy and on-policy KD, and KD between any two LLMs regardless of their
vocabularies. Extensive experiments on instruction-following, mathematical
reasoning, and code generation benchmarks show that DSKD significantly
outperforms existing methods based on the current white-box KD framework and
surpasses other cross-tokenizer KD methods for LLMs with different
vocabularies.

</details>

### [39] [Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models](https://arxiv.org/abs/2504.11431)
*Maria Teleki,Xiangjue Dong,Haoran Liu,James Caverlee*

Main category: cs.CL

TLDR: 论文研究了话语中的男性默认现象，提出了一种框架（GDCF和D-WEAT）来发现和分析性别化话语词，并在LLMs中测量其性别偏见。研究发现男性话语词在嵌入模型中表现更稳定，可能导致男性在下游任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 男性默认是一种广泛存在但研究不足的性别偏见，本研究旨在揭示话语中的男性默认现象及其影响。

Method: 使用GDCF框架大规模发现性别化话语词，并通过D-WEAT测量LLMs中的性别偏见。分析了15,117个播客内容，结合LDA和BERTopic技术。

Result: 在商业、技术/政治和视频游戏领域存在性别化话语的男性默认现象。男性话语词在LLM嵌入模型中表现更稳定，可能导致男性在下游任务中优势。

Conclusion: 男性话语词在嵌入模型中的优势是一种表征性伤害，揭示了男性默认现象的存在及其潜在影响。

Abstract: Masculine defaults are widely recognized as a significant type of gender
bias, but they are often unseen as they are under-researched. Masculine
defaults involve three key parts: (i) the cultural context, (ii) the masculine
characteristics or behaviors, and (iii) the reward for, or simply acceptance
of, those masculine characteristics or behaviors. In this work, we study
discourse-based masculine defaults, and propose a twofold framework for (i) the
large-scale discovery and analysis of gendered discourse words in spoken
content via our Gendered Discourse Correlation Framework (GDCF); and (ii) the
measurement of the gender bias associated with these gendered discourse words
in LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus
our study on podcasts, a popular and growing form of social media, analyzing
15,117 podcast episodes. We analyze correlations between gender and discourse
words -- discovered via LDA and BERTopic -- to automatically form gendered
discourse word lists. We then study the prevalence of these gendered discourse
words in domain-specific contexts, and find that gendered discourse-based
masculine defaults exist in the domains of business, technology/politics, and
video games. Next, we study the representation of these gendered discourse
words from a state-of-the-art LLM embedding model from OpenAI, and find that
the masculine discourse words have a more stable and robust representation than
the feminine discourse words, which may result in better system performance on
downstream tasks for men. Hence, men are rewarded for their discourse patterns
with better system performance by one of the state-of-the-art language models
-- and this embedding disparity is a representational harm and a masculine
default.

</details>

### [40] [TextArena](https://arxiv.org/abs/2504.11442)
*Leon Guertler,Bobby Cheng,Simon Yu,Bo Liu,Leshem Choshen,Cheston Tan*

Main category: cs.CL

TLDR: TextArena是一个开源的文本游戏集合，用于训练和评估大型语言模型（LLMs）的代理行为，填补了传统基准测试在动态社交技能评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试很少评估动态社交技能（如谈判、心智理论和欺骗），TextArena旨在填补这一空白。

Method: 提供57+种独特环境（包括单人和多人游戏），支持在线对战系统（人类与其他模型），并实时计算TrueSkill分数。

Result: TextArena为研究、社区和扩展性设计，支持轻松添加新游戏、测试模型和训练模型。

Conclusion: TextArena是一个功能强大且易于扩展的工具，适用于评估和提升LLMs的社交能力。

Abstract: TextArena is an open-source collection of competitive text-based games for
training and evaluation of agentic behavior in Large Language Models (LLMs). It
spans 57+ unique environments (including single-player, two-player, and
multi-player setups) and allows for easy evaluation of model capabilities via
an online-play system (against humans and other submitted models) with
real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social
skills such as negotiation, theory of mind, and deception, creating a gap that
TextArena addresses. Designed with research, community and extensibility in
mind, TextArena emphasizes ease of adding new games, adapting the framework,
testing models, playing against the models, and training models. Detailed
documentation of environments, games, leaderboard, and examples are available
on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.

</details>

### [41] [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456)
*Zhiwei He,Tian Liang,Jiahao Xu,Qiuzhi Liu,Xingyu Chen,Yue Wang,Linfeng Song,Dian Yu,Zhenwen Liang,Wenxuan Wang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.CL

TLDR: DeepMath-103K是一个新的大规模数学问题数据集，专为通过强化学习训练高级推理模型设计，解决了现有数据不足的问题，并在难度和多样性上超越现有资源。


<details>
  <summary>Details</summary>
Motivation: 解决现有数学推理训练数据不足、难度低且易受评测基准污染的问题，推动AI在复杂数学推理上的进步。

Method: 通过严格的源分析、去污染处理和难度筛选（Levels 5-9）构建DeepMath-103K数据集，包含103K数学问题，每个问题附带可验证答案和多种解决方案。

Result: 在DeepMath-103K上训练的模型在数学评测基准上表现显著提升，验证了数据集的有效性。

Conclusion: DeepMath-103K为AI推理系统的开发提供了高质量数据支持，公开数据集以促进社区进步。

Abstract: The capacity for complex mathematical reasoning is a key benchmark for
artificial intelligence. While reinforcement learning (RL) applied to LLMs
shows promise, progress is significantly hindered by the lack of large-scale
training data that is sufficiently challenging, possesses verifiable answer
formats suitable for RL, and is free from contamination with evaluation
benchmarks. To address these limitations, we introduce DeepMath-103K, a new,
large-scale dataset comprising approximately 103K mathematical problems,
specifically designed to train advanced reasoning models via RL. DeepMath-103K
is curated through a rigorous pipeline involving source analysis, stringent
decontamination against numerous benchmarks, and filtering for high difficulty
(primarily Levels 5-9), significantly exceeding existing open resources in
challenge. Each problem includes a verifiable final answer, enabling rule-based
RL, and three distinct R1-generated solutions suitable for diverse training
paradigms like supervised fine-tuning or distillation. Spanning a wide range of
mathematical topics, DeepMath-103K promotes the development of generalizable
reasoning. We demonstrate that models trained on DeepMath-103K achieve
significant improvements on challenging mathematical benchmarks, validating its
effectiveness. We release DeepMath-103K publicly to facilitate community
progress in building more capable AI reasoning systems:
https://github.com/zwhe99/DeepMath.

</details>

### [42] [Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective](https://arxiv.org/abs/2410.10291)
*Xiangru Zhu,Penglei Sun,Yaoxian Song,Yanghua Xiao,Zhixu Li,Chengyu Wang,Jun Huang,Bei Yang,Xiaoxiao Xu*

Main category: cs.CL

TLDR: 论文提出了一种新指标SemVarEffect和基准SemVarBench，用于评估文本到图像（T2I）合成中语义变化对输出的因果影响。实验表明，CogView-3-Plus和Ideogram 2表现最佳，但对象关系的语义变化理解较弱。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型难以捕捉词序变化带来的语义差异，且现有评估方法依赖间接指标，无法可靠评估这些挑战。

Method: 提出SemVarEffect指标和SemVarBench基准，通过两种语言排列实现语义变化，避免可预测的字面变化。

Result: CogView-3-Plus和Ideogram 2得分最高（0.2/1），对象关系的语义变化得分较低（0.07/1）。跨模态对齐在UNet或Transformers中起关键作用。

Conclusion: 研究为T2I合成领域提供了有效的评估框架，推动了人类指令理解的探索。

Abstract: Accurate interpretation and visualization of human instructions are crucial
for text-to-image (T2I) synthesis. However, current models struggle to capture
semantic variations from word order changes, and existing evaluations, relying
on indirect metrics like text-image similarity, fail to reliably assess these
challenges. This often obscures poor performance on complex or uncommon
linguistic patterns by the focus on frequent word combinations. To address
these deficiencies, we propose a novel metric called SemVarEffect and a
benchmark named SemVarBench, designed to evaluate the causality between
semantic variations in inputs and outputs in T2I synthesis. Semantic variations
are achieved through two types of linguistic permutations, while avoiding
easily predictable literal variations. Experiments reveal that the
CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.
Semantic variations in object relations are less understood than attributes,
scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in
UNet or Transformers plays a crucial role in handling semantic variations, a
factor previously overlooked by a focus on textual encoders. Our work
establishes an effective evaluation framework that advances the T2I synthesis
community's exploration of human instruction understanding. Our benchmark and
code are available at https://github.com/zhuxiangru/SemVarBench .

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514)
*Yijun Liang,Ming Li,Chenrui Fan,Ziyue Li,Dang Nguyen,Kwesi Cobbina,Shweta Bhardwaj,Jiuhai Chen,Fuxiao Liu,Tianyi Zhou*

Main category: cs.CV

TLDR: ColorBench是一个评估视觉语言模型（VLMs）颜色理解能力的基准测试，揭示了现有模型在颜色感知、推理和鲁棒性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs是否能够像人类一样感知、理解和利用颜色，填补现有研究中对颜色理解的空白。

Method: 通过设计多样化的测试场景（ColorBench），评估32种不同VLMs在颜色感知、推理和鲁棒性方面的表现。

Result: 发现语言模型比视觉编码器更重要，模型间性能差距较小，CoT推理能提升颜色理解能力，颜色线索可能误导模型。

Conclusion: 当前VLMs在颜色理解方面存在明显不足，ColorBench为未来研究提供了基础工具。

Abstract: Color plays an important role in human perception and usually provides
critical clues in visual reasoning. However, it is unclear whether and how
vision-language models (VLMs) can perceive, understand, and leverage color as
humans. This paper introduces ColorBench, an innovative benchmark meticulously
crafted to assess the capabilities of VLMs in color understanding, including
color perception, reasoning, and robustness. By curating a suite of diverse
test scenarios, with grounding in real applications, ColorBench evaluates how
these models perceive colors, infer meanings from color-based cues, and
maintain consistent performance under varying color transformations. Through an
extensive evaluation of 32 VLMs with varying language models and vision
encoders, our paper reveals some undiscovered findings: (i) The scaling law
(larger models are better) still holds on ColorBench, while the language model
plays a more important role than the vision encoder. (ii) However, the
performance gaps across models are relatively small, indicating that color
understanding has been largely neglected by existing VLMs. (iii) CoT reasoning
improves color understanding accuracies and robustness, though they are
vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on
ColorBench but they can also mislead models in some tasks. These findings
highlight the critical limitations of current VLMs and underscore the need to
enhance color comprehension. Our ColorBenchcan serve as a foundational tool for
advancing the study of human-level color understanding of multimodal AI.

</details>

### [44] [Enhancing Image Restoration through Learning Context-Rich and Detail-Accurate Features](https://arxiv.org/abs/2504.10558)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TLDR: 本文提出了一种多尺度设计（LCDNet），通过结合空间和频域知识，选择性恢复图像信息，并引入跳连注意力机制（SCAM）优化信息传播。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像恢复中过于关注空间细节，而忽略了频域变化的理解，导致恢复效果不佳。

Method: 开发了混合尺度频率选择块（HSFSBlock）和跳连注意力机制（SCAM），结合空间和频域信息，选择性恢复图像。

Result: 在多种图像恢复任务中，LCDNet性能优于或与现有最优算法相当。

Conclusion: 多尺度设计和SCAM机制有效提升了图像恢复的质量和选择性。

Abstract: Image restoration involves recovering high-quality images from their
corrupted versions, requiring a nuanced balance between spatial details and
contextual information. While certain methods address this balance, they
predominantly emphasize spatial aspects, neglecting frequency variation
comprehension. In this paper, we present a multi-scale design that optimally
balances these competing objectives, seamlessly integrating spatial and
frequency domain knowledge to selectively recover the most informative
information. Specifically, we develop a hybrid scale frequency selection block
(HSFSBlock), which not only captures multi-scale information from the spatial
domain, but also selects the most informative components for image restoration
in the frequency domain. Furthermore, to mitigate the inherent noise introduced
by skip connections employing only addition or concatenation, we introduce a
skip connection attention mechanism (SCAM) to selectively determines the
information that should propagate through skip connections. The resulting
tightly interlinked architecture, named as LCDNet. Extensive experiments
conducted across diverse image restoration tasks showcase that our model
attains performance levels that are either superior or comparable to those of
state-of-the-art algorithms.

</details>

### [45] [Data Augmentation Through Random Style Replacement](https://arxiv.org/abs/2504.10563)
*Qikai Yang,Cheng Ji,Huaiying Luo,Panfeng Li,Zhicheng Ding*

Main category: cs.CV

TLDR: 提出一种结合风格增强和随机擦除的数据增强技术，通过选择性替换图像子区域提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有风格增强方法可能无法充分利用风格变换的优势，且容易过拟合，需要更鲁棒的增强策略。

Method: 先对训练图像随机风格变换，再随机替换部分区域为风格变换后的子区域。

Result: 实验表明，相比现有方法，该技术性能更优且收敛更快。

Conclusion: 该方法能无缝兼容多种风格变换算法，并有效提升训练鲁棒性。

Abstract: In this paper, we introduce a novel data augmentation technique that combines
the advantages of style augmentation and random erasing by selectively
replacing image subregions with style-transferred patches. Our approach first
applies a random style transfer to training images, then randomly substitutes
selected areas of these images with patches derived from the style-transferred
versions. This method is able to seamlessly accommodate a wide range of
existing style transfer algorithms and can be readily integrated into diverse
data augmentation pipelines. By incorporating our strategy, the training
process becomes more robust and less prone to overfitting. Comparative
experiments demonstrate that, relative to previous style augmentation methods,
our technique achieves superior performance and faster convergence.

</details>

### [46] [H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models](https://arxiv.org/abs/2504.10567)
*Yushu Wu,Yanyu Li,Ivan Skorokhodov,Anil Kag,Willi Menapace,Sharath Girish,Aliaksandr Siarohin,Yanzhi Wang,Sergey Tulyakov*

Main category: cs.CV

TLDR: 本文系统研究了自动编码器（AE）的设计，优化了计算分布，提出了一种高效高压缩的视频AE，支持移动设备实时解码，并提出了一种新的潜在一致性损失函数。


<details>
  <summary>Details</summary>
Motivation: 自动编码器在潜在扩散模型中的潜力未被充分挖掘，尤其是在网络设计、压缩比和训练策略方面。

Method: 研究了AE的架构设计选择，优化计算分布，统一了普通AE和图像条件I2V VAE的设计，并提出了一种新的潜在一致性损失。

Result: 提出的AE实现了超高压缩比和移动设备实时解码，重建质量显著优于现有技术。

Conclusion: 通过训练DiT验证了AE的潜力，展示了快速、高质量的文本到视频生成能力。

Abstract: Autoencoder (AE) is the key to the success of latent diffusion models for
image and video generation, reducing the denoising resolution and improving
efficiency. However, the power of AE has long been underexplored in terms of
network design, compression ratio, and training strategy. In this work, we
systematically examine the architecture design choices and optimize the
computation distribution to obtain a series of efficient and high-compression
video AEs that can decode in real time on mobile devices. We also unify the
design of plain Autoencoder and image-conditioned I2V VAE, achieving
multifunctionality in a single network. In addition, we find that the widely
adopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no
significant improvements when training AEs at scale. We propose a novel latent
consistency loss that does not require complicated discriminator design or
hyperparameter tuning, but provides stable improvements in reconstruction
quality. Our AE achieves an ultra-high compression ratio and real-time decoding
speed on mobile while outperforming prior art in terms of reconstruction
metrics by a large margin. We finally validate our AE by training a DiT on its
latent space and demonstrate fast, high-quality text-to-video generation
capability.

</details>

### [47] [AgMMU: A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark](https://arxiv.org/abs/2504.10568)
*Aruna Gauba,Irene Pi,Yunze Man,Ziqi Pang,Vikram S. Adve,Yu-Xiong Wang*

Main category: cs.CV

TLDR: AgMMU是一个专注于农业领域的视觉-语言模型数据集，用于评估和开发知识密集型专家领域的模型。它包含来自真实用户与专家对话的多选题和开放式问题，揭示现有模型在感知和知识结合上的挑战，并通过微调实验提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 农业领域需要结合视觉观察和精确知识，但现有视觉-语言模型在此类任务中表现不足。AgMMU旨在填补这一空白，推动知识密集型模型的发展。

Method: 通过三步数据整理流程（GPT-4o、LLaMA模型和人工验证），从116,231条真实对话中提取问题和答案，构建包含5,460个评估问题和205,399条开发数据的多模态数据集。

Result: 现有模型在需要详细感知和事实知识的任务中表现不佳，开源模型与专有模型存在显著差距。微调实验使LLaVA-1.5的评估准确率提升3.1%。

Conclusion: AgMMU可作为农业领域的评估基准和开发工具，推动知识密集型视觉-语言模型的进步。

Abstract: We curate a dataset AgMMU for evaluating and developing vision-language
models (VLMs) to produce factually accurate answers for knowledge-intensive
expert domains. Our AgMMU concentrates on one of the most socially beneficial
domains, agriculture, which requires connecting detailed visual observation
with precise knowledge to diagnose, e.g., pest identification, management
instructions, etc. As a core uniqueness of our dataset, all facts, questions,
and answers are extracted from 116,231 conversations between real-world users
and authorized agricultural experts. After a three-step dataset curation
pipeline with GPT-4o, LLaMA models, and human verification, AgMMU features an
evaluation set of 5,460 multiple-choice questions (MCQs) and open-ended
questions (OEQs). We also provide a development set that contains 205,399
pieces of agricultural knowledge information, including disease identification,
symptoms descriptions, management instructions, insect and pest identification,
and species identification. As a multimodal factual dataset, it reveals that
existing VLMs face significant challenges with questions requiring both
detailed perception and factual knowledge. Moreover, open-source VLMs still
demonstrate a substantial performance gap compared to proprietary ones. To
advance knowledge-intensive VLMs, we conduct fine-tuning experiments using our
development set, which improves LLaVA-1.5 evaluation accuracy by up to 3.1%. We
hope that AgMMU can serve both as an evaluation benchmark dedicated to
agriculture and a development suite for incorporating knowledge-intensive
expertise into general-purpose VLMs.

</details>

### [48] [Skeleton-Based Intake Gesture Detection With Spatial-Temporal Graph Convolutional Networks](https://arxiv.org/abs/2504.10635)
*Chunzhuo Wang,Zhewen Xue,T. Sunil Kumar,Guido Camps,Hans Hallez,Bart Vanrumste*

Main category: cs.CV

TLDR: 该研究提出了一种基于骨架的ST-GCN-BiLSTM模型，用于检测饮食手势，验证了其在实验室和智能手机数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 肥胖和不健康饮食习惯是普遍问题，自动检测饮食手势有助于改善日常饮食监控。

Method: 结合了扩张时空图卷积网络（ST-GCN）和双向长短时记忆（BiLSTM）的骨架方法。

Result: 在实验室数据集上，检测进食和饮水手势的F1分数分别为86.18%和74.84%；在智能手机数据集上分别为85.40%和67.80%。

Conclusion: 骨架数据可用于饮食手势检测，且该方法在跨数据集验证中表现出鲁棒性。

Abstract: Overweight and obesity have emerged as widespread societal challenges,
frequently linked to unhealthy eating patterns. A promising approach to enhance
dietary monitoring in everyday life involves automated detection of food intake
gestures. This study introduces a skeleton based approach using a model that
combines a dilated spatial-temporal graph convolutional network (ST-GCN) with a
bidirectional long-short-term memory (BiLSTM) framework, as called
ST-GCN-BiLSTM, to detect intake gestures. The skeleton-based method provides
key benefits, including environmental robustness, reduced data dependency, and
enhanced privacy preservation. Two datasets were employed for model validation.
The OREBA dataset, which consists of laboratory-recorded videos, achieved
segmental F1-scores of 86.18% and 74.84% for identifying eating and drinking
gestures. Additionally, a self-collected dataset using smartphone recordings in
more adaptable experimental conditions was evaluated with the model trained on
OREBA, yielding F1-scores of 85.40% and 67.80% for detecting eating and
drinking gestures. The results not only confirm the feasibility of utilizing
skeleton data for intake gesture detection but also highlight the robustness of
the proposed approach in cross-dataset validation.

</details>

### [49] [SilVar-Med: A Speech-Driven Visual Language Model for Explainable Abnormality Detection in Medical Imaging](https://arxiv.org/abs/2504.10642)
*Tan-Hanh Pham,Chris Ngo,Trong-Duong Bui,Minh Luu Quang,Tan-Huong Pham,Truong-Son Hy*

Main category: cs.CV

TLDR: 论文提出了一种基于语音交互的多模态医学视觉语言模型SilVar-Med，解决了现有模型依赖文本指令和缺乏推理解释的问题，提升了临床实用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型依赖文本指令，在临床环境中实用性受限，且缺乏预测背后的推理解释，影响临床决策的可靠性。

Method: 提出SilVar-Med，一种端到端的语音驱动医学VLM，结合语音交互与视觉语言模型，并引入推理数据集解释预测。

Result: 通过实验验证了推理驱动的医学图像解释与语音交互的可行性。

Conclusion: SilVar-Med推动了医学AI领域的发展，提供了更透明、交互性更强且临床可行的诊断支持系统。

Abstract: Medical Visual Language Models have shown great potential in various
healthcare applications, including medical image captioning and diagnostic
assistance. However, most existing models rely on text-based instructions,
limiting their usability in real-world clinical environments especially in
scenarios such as surgery, text-based interaction is often impractical for
physicians. In addition, current medical image analysis models typically lack
comprehensive reasoning behind their predictions, which reduces their
reliability for clinical decision-making. Given that medical diagnosis errors
can have life-changing consequences, there is a critical need for interpretable
and rational medical assistance. To address these challenges, we introduce an
end-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image
assistant that integrates speech interaction with VLMs, pioneering the task of
voice-based communication for medical image analysis. In addition, we focus on
the interpretation of the reasoning behind each prediction of medical
abnormalities with a proposed reasoning dataset. Through extensive experiments,
we demonstrate a proof-of-concept study for reasoning-driven medical image
interpretation with end-to-end speech interaction. We believe this work will
advance the field of medical AI by fostering more transparent, interactive, and
clinically viable diagnostic support systems. Our code and dataset are publicly
available at SiVar-Med.

</details>

### [50] [Relation-Rich Visual Document Generator for Visual Information Extraction](https://arxiv.org/abs/2504.10659)
*Zi-Han Jiang,Chien-Wei Lin,Wei-Hua Li,Hsuan-Tung Liu,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.CV

TLDR: 论文提出了RIDGE方法，通过两阶段生成内容与布局，解决视觉文档理解中数据稀缺和布局多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉信息提取（VIE）中因布局多样性和数据稀缺受限，且合成文档生成器缺乏内容与布局的复杂关联。

Method: RIDGE采用两阶段方法：1）利用LLMs生成内容；2）基于OCR结果生成多样布局，无需人工标注。

Result: 实验表明，RIDGE显著提升了文档理解模型在VIE基准上的性能。

Conclusion: RIDGE通过内容与布局的协同生成，有效解决了VIE中的挑战，代码和模型将开源。

Abstract: Despite advances in Large Language Models (LLMs) and Multimodal LLMs (MLLMs)
for visual document understanding (VDU), visual information extraction (VIE)
from relation-rich documents remains challenging due to the layout diversity
and limited training data. While existing synthetic document generators attempt
to address data scarcity, they either rely on manually designed layouts and
templates, or adopt rule-based approaches that limit layout diversity. Besides,
current layout generation methods focus solely on topological patterns without
considering textual content, making them impractical for generating documents
with complex associations between the contents and layouts. In this paper, we
propose a Relation-rIch visual Document GEnerator (RIDGE) that addresses these
limitations through a two-stage approach: (1) Content Generation, which
leverages LLMs to generate document content using a carefully designed
Hierarchical Structure Text format which captures entity categories and
relationships, and (2) Content-driven Layout Generation, which learns to create
diverse, plausible document layouts solely from easily available Optical
Character Recognition (OCR) results, requiring no human labeling or annotations
efforts. Experimental results have demonstrated that our method significantly
enhances the performance of document understanding models on various VIE
benchmarks. The code and model will be available at
https://github.com/AI-Application-and-Integration-Lab/RIDGE .

</details>

### [51] [Perturbed State Space Feature Encoders for Optical Flow with Event Cameras](https://arxiv.org/abs/2504.10669)
*Gokul Raju Govinda Raju,Nikola Zubić,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TLDR: 提出了一种名为P-SSE的新方法，用于事件相机多帧光流估计，通过扰动状态动力学矩阵提升性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机在光流估计中优于传统相机，但现有神经网络在时空推理上仍有局限。

Method: 采用P-SSE方法，结合大感受野的时空特征处理和线性计算复杂度，并通过扰动技术优化状态动力学矩阵。

Result: 在DSEC-Flow和MVSEC数据集上，EPE性能分别提升8.48%和11.86%。

Conclusion: P-SSE通过扰动技术和双向流框架，显著提升了事件相机光流估计的性能。

Abstract: With their motion-responsive nature, event-based cameras offer significant
advantages over traditional cameras for optical flow estimation. While deep
learning has improved upon traditional methods, current neural networks adopted
for event-based optical flow still face temporal and spatial reasoning
limitations. We propose Perturbed State Space Feature Encoders (P-SSE) for
multi-frame optical flow with event cameras to address these challenges. P-SSE
adaptively processes spatiotemporal features with a large receptive field akin
to Transformer-based methods, while maintaining the linear computational
complexity characteristic of SSMs. However, the key innovation that enables the
state-of-the-art performance of our model lies in our perturbation technique
applied to the state dynamics matrix governing the SSM system. This approach
significantly improves the stability and performance of our model. We integrate
P-SSE into a framework that leverages bi-directional flows and recurrent
connections, expanding the temporal context of flow prediction. Evaluations on
DSEC-Flow and MVSEC datasets showcase P-SSE's superiority, with 8.48% and
11.86% improvements in EPE performance, respectively.

</details>

### [52] [H-MoRe: Learning Human-centric Motion Representation for Action Analysis](https://arxiv.org/abs/2504.10676)
*Zhanbo Huang,Xiaoming Liu,Yu Kong*

Main category: cs.CV

TLDR: H-MoRe是一种自监督学习的人类运动表示方法，通过动态保留相关运动并过滤背景，结合人体姿态和形状信息，显著提升了下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖合成数据的全监督学习，而H-MoRe旨在直接从真实场景中自监督学习，以更精确地表示人类运动。

Method: H-MoRe利用运动学原理，以矩阵形式表示绝对和相对运动（世界-局部流），结合人体姿态和形状信息。

Result: 实验显示H-MoRe在步态识别（+16.01%）、动作识别（+8.92%）和视频生成（FVD降低67.07%）等任务中表现优异，且推理效率高（34 fps）。

Conclusion: H-MoRe为人类运动表示提供了高效且精确的解决方案，适用于实时应用，代码和模型将公开。

Abstract: In this paper, we propose H-MoRe, a novel pipeline for learning precise
human-centric motion representation. Our approach dynamically preserves
relevant human motion while filtering out background movement. Notably, unlike
previous methods relying on fully supervised learning from synthetic data,
H-MoRe learns directly from real-world scenarios in a self-supervised manner,
incorporating both human pose and body shape information. Inspired by
kinematics, H-MoRe represents absolute and relative movements of each body
point in a matrix format that captures nuanced motion details, termed
world-local flows. H-MoRe offers refined insights into human motion, which can
be integrated seamlessly into various action-related applications. Experimental
results demonstrate that H-MoRe brings substantial improvements across various
downstream tasks, including gait recognition(CL@R1: +16.01%), action
recognition(Acc@1: +8.92%), and video generation(FVD: -67.07%). Additionally,
H-MoRe exhibits high inference efficiency (34 fps), making it suitable for most
real-time scenarios. Models and code will be released upon publication.

</details>

### [53] [NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results](https://arxiv.org/abs/2504.10685)
*Yuqian Fu,Xingyu Qiu,Bin Ren,Yanwei Fu,Radu Timofte,Nicu Sebe,Ming-Hsuan Yang,Luc Van Gool,Kaijin Zhang,Qingpeng Nong,Xiugang Dong,Hong Gao,Xiangsheng Zhou,Jiancheng Pan,Yanxing Liu,Xiao He,Jiahao Li,Yuze Sun,Xiaomeng Huang,Zhenyu Zhang,Ran Ma,Yuhan Liu,Zijian Zhuang,Shuai Yi,Yixiong Zou,Lingyi Hong,Mingxi Chen,Runze Li,Xingdong Sheng,Wenqiang Zhang,Weisen Chen,Yongxin Yan,Xinguo Chen,Yuanjie Shao,Zhengrong Zuo,Nong Sang,Hao Wu,Haoran Sun,Shuming Hu,Yan Zhang,Zhiguang Shi,Yu Zhang,Chao Chen,Tao Wang,Da Feng,Linhai Zhuo,Ziming Lin,Yali Huang,Jie Me,Yiming Yang,Mi Guo,Mingyuan Jiu,Mingliang Xu,Maomao Xiong,Qunshu Zhang,Xinyu Cao,Yuqing Yang,Dianmo Sheng,Xuanpu Zhao,Zhiyu Li,Xuyang Ding,Wenqian Li*

Main category: cs.CV

TLDR: 本文介绍了2025年NTIRE首届跨域少样本目标检测（CD-FSOD）挑战赛，旨在提升目标检测器在新领域中的性能，吸引了42个团队参与，最终13个团队提交了有效方案。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本目标检测（CD-FSOD）对现有检测模型提出了挑战，本次挑战赛旨在推动该领域的技术进步。

Method: 挑战赛吸引了多团队参与，提出了多种创新模型，分别在开源和闭源设置下取得了新的SOTA结果。

Result: 共有42个团队提交方案，13个团队完成最终有效提交，部分模型表现优异。

Conclusion: 挑战赛成功推动了CD-FSOD领域的研究，展示了多种创新解决方案。

Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) poses significant challenges
to existing object detection and few-shot detection models when applied across
domains. In conjunction with NTIRE 2025, we organized the 1st CD-FSOD
Challenge, aiming to advance the performance of current object detectors on
entirely novel target domains with only limited labeled data. The challenge
attracted 152 registered participants, received submissions from 42 teams, and
concluded with 13 teams making valid final submissions. Participants approached
the task from diverse perspectives, proposing novel models that achieved new
state-of-the-art (SOTA) results under both open-source and closed-source
settings. In this report, we present an overview of the 1st NTIRE 2025 CD-FSOD
Challenge, highlighting the proposed solutions and summarizing the results
submitted by the participants.

</details>

### [54] [The Tenth NTIRE 2025 Efficient Super-Resolution Challenge Report](https://arxiv.org/abs/2504.10686)
*Bin Ren,Hang Guo,Lei Sun,Zongwei Wu,Radu Timofte,Yawei Li,Yao Zhang,Xinning Chai,Zhengxue Cheng,Yingsheng Qin,Yucai Yang,Li Song,Hongyuan Yu,Pufan Xu,Cheng Wan,Zhijuan Huang,Peng Guo,Shuyuan Cui,Chenjun Li,Xuehai Hu,Pan Pan,Xin Zhang,Heng Zhang,Qing Luo,Linyan Jiang,Haibo Lei,Qifang Gao,Yaqing Li,Weihua Luo,Tsing Li,Qing Wang,Yi Liu,Yang Wang,Hongyu An,Liou Zhang,Shijie Zhao,Lianhong Song,Long Sun,Jinshan Pan,Jiangxin Dong,Jinhui Tang,Jing Wei,Mengyang Wang,Ruilong Guo,Qian Wang,Qingliang Liu,Yang Cheng,Davinci,Enxuan Gu,Pinxin Liu,Yongsheng Yu,Hang Hua,Yunlong Tang,Shihao Wang,Yukun Yang,Zhiyu Zhang,Yukun Yang,Jiyu Wu,Jiancheng Huang,Yifan Liu,Yi Huang,Shifeng Chen,Rui Chen,Yi Feng,Mingxi Li,Cailu Wan,Xiangji Wu,Zibin Liu,Jinyang Zhong,Kihwan Yoon,Ganzorig Gankhuyag,Shengyun Zhong,Mingyang Wu,Renjie Li,Yushen Zuo,Zhengzhong Tu,Zongang Gao,Guannan Chen,Yuan Tian,Wenhui Chen,Weijun Yuan,Zhan Li,Yihang Chen,Yifan Deng,Ruting Deng,Yilin Zhang,Huan Zheng,Yanyan Wei,Wenxuan Zhao,Suiyi Zhao,Fei Wang,Kun Li,Yinggan Tang,Mengjie Su,Jae-hyeon Lee,Dong-Hyeop Son,Ui-Jin Choi,Tiancheng Shao,Yuqing Zhang,Mengcheng Ma,Donggeun Ko,Youngsang Kwak,Jiun Lee,Jaehwa Kwak,Yuxuan Jiang,Qiang Zhu,Siyue Teng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull,Jing Hu,Hui Deng,Xuan Zhang,Lin Zhu,Qinrui Fan,Weijian Deng,Junnan Wu,Wenqin Deng,Yuquan Liu,Zhaohong Xu,Jameer Babu Pinjari,Kuldeep Purohit,Zeyu Xiao,Zhuoyuan Li,Surya Vashisth,Akshay Dudhane,Praful Hambarde,Sachin Chaudhary,Satya Naryan Tazi,Prashant Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Wei-Chen Shen,I-Hsiang Chen,Yunzhe Xu,Chen Zhao,Zhizhou Chen,Akram Khatami-Rizi,Ahmad Mahmoudi-Aznaveh,Alejandro Merino,Bruno Longarela,Javier Abad,Marcos V. Conde,Simone Bianco,Luca Cogo,Gianmarco Corti*

Main category: cs.CV

TLDR: 本文综述了NTIRE 2025高效单图像超分辨率挑战赛，分析了参赛方法及其结果，强调了该领域的前沿进展。


<details>
  <summary>Details</summary>
Motivation: 推动深度模型在计算效率（如运行时间、参数量和FLOPs）和性能（PSNR）上的优化，为单图像超分辨率技术设立新基准。

Method: 挑战赛吸引了244名注册者和43支有效参赛团队，通过分析其提交的方法和结果，评估了高效超分辨率技术的创新性。

Result: 参赛模型在DIV2K_LSDIR_valid和DIV2K_LSDIR_test数据集上分别实现了至少26.90 dB和26.99 dB的PSNR。

Conclusion: 挑战赛展示了单图像高效超分辨率技术的显著进展，为未来研究提供了重要参考和基准。

Abstract: This paper presents a comprehensive review of the NTIRE 2025 Challenge on
Single-Image Efficient Super-Resolution (ESR). The challenge aimed to advance
the development of deep models that optimize key computational metrics, i.e.,
runtime, parameters, and FLOPs, while achieving a PSNR of at least 26.90 dB on
the $\operatorname{DIV2K\_LSDIR\_valid}$ dataset and 26.99 dB on the
$\operatorname{DIV2K\_LSDIR\_test}$ dataset. A robust participation saw
\textbf{244} registered entrants, with \textbf{43} teams submitting valid
entries. This report meticulously analyzes these methods and results,
emphasizing groundbreaking advancements in state-of-the-art single-image ESR
techniques. The analysis highlights innovative approaches and establishes
benchmarks for future research in the field.

</details>

### [55] [SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion Models](https://arxiv.org/abs/2504.10716)
*Stathis Galanakis,Alexandros Lattas,Stylianos Moschoglou,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TLDR: SpinMeRound是一种基于扩散模型的方法，旨在从新视角生成一致且准确的头部肖像。


<details>
  <summary>Details</summary>
Motivation: 当前方法在生成头部肖像时受限于有限的视角范围，且现有的大规模扩散模型在面部数据上表现不佳。

Method: 利用多个输入视图和身份嵌入，合成多样视角的肖像，同时保持身份特征。

Result: 实验表明，该方法在360度头部合成中表现优异，超越了当前最先进的多视角扩散模型。

Conclusion: SpinMeRound在生成新视角头部肖像方面具有显著优势。

Abstract: Despite recent progress in diffusion models, generating realistic head
portraits from novel viewpoints remains a significant challenge. Most current
approaches are constrained to limited angular ranges, predominantly focusing on
frontal or near-frontal views. Moreover, although the recent emerging
large-scale diffusion models have been proven robust in handling 3D scenes,
they underperform on facial data, given their complex structure and the uncanny
valley pitfalls. In this paper, we propose SpinMeRound, a diffusion-based
approach designed to generate consistent and accurate head portraits from novel
viewpoints. By leveraging a number of input views alongside an identity
embedding, our method effectively synthesizes diverse viewpoints of a subject
whilst robustly maintaining its unique identity features. Through
experimentation, we showcase our model's generation capabilities in 360 head
synthesis, while beating current state-of-the-art multiview diffusion models.

</details>

### [56] [Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization](https://arxiv.org/abs/2504.10727)
*Darryl Hannan,John Cooper,Dylan White,Timothy Doster,Henry Kvinge,Yijing Watkins*

Main category: cs.CV

TLDR: 本文分析了最新多模态大语言模型（MLLMs）在地球观测（EO）目标定位任务中的表现，发现其在某些零样本场景下表现良好，并探讨了提示选择、地面采样距离优化及失败案例。


<details>
  <summary>Details</summary>
Motivation: MLLMs在计算机视觉领域表现优异，但在EO等分布外领域的细粒度空间推理任务（如目标定位）中表现不佳。本文旨在评估最新MLLMs在此类任务中的潜力。

Method: 通过分析经过细粒度空间推理训练的MLLMs，在EO目标定位任务中进行基准测试，并研究提示选择、GSD优化及失败案例。

Result: 这些模型在特定零样本场景下表现良好，适用于某些EO定位任务。

Conclusion: 本文为评估MLLMs在EO任务中的适用性及优化提供了有价值的参考。

Abstract: Multimodal large language models (MLLMs) have altered the landscape of
computer vision, obtaining impressive results across a wide range of tasks,
especially in zero-shot settings. Unfortunately, their strong performance does
not always transfer to out-of-distribution domains, such as earth observation
(EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks,
such as image captioning and scene understanding, while failing at tasks that
require more fine-grained spatial reasoning, such as object localization.
However, MLLMs are advancing rapidly and insights quickly become out-dated. In
this work, we analyze more recent MLLMs that have been explicitly trained to
include fine-grained spatial reasoning capabilities, benchmarking them on EO
object localization tasks. We demonstrate that these models are performant in
certain settings, making them well suited for zero-shot scenarios.
Additionally, we provide a detailed discussion focused on prompt selection,
ground sample distance (GSD) optimization, and analyzing failure cases. We hope
that this work will prove valuable as others evaluate whether an MLLM is well
suited for a given EO localization task and how to optimize it.

</details>

### [57] [CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates](https://arxiv.org/abs/2504.10738)
*Ankit Kumar Shaw,Kun Jiang,Tuopu Wen,Chandan Kumar Sah,Yining Shi,Mengmeng Yang,Diange Yang,Xiaoli Lian*

Main category: cs.CV

TLDR: CleanMAP是一个基于多模态大语言模型（MLLM）的框架，用于过滤和优化众包数据，以实现高精度HD地图更新。


<details>
  <summary>Details</summary>
Motivation: 智能网联车辆（ICV）和车路云一体化系统的快速发展对实时高清地图更新的准确性提出了更高要求，但众包数据的不一致性（如运动模糊、光照变化等）导致地图可靠性难以保障。

Method: CleanMAP采用MLLM驱动的车道可见性评分模型，动态分段置信度评分函数，以及置信度驱动的局部地图融合策略，以优化数据质量。

Result: 在真实自动驾驶数据集上的实验表明，融合前三个局部地图可实现最低平均更新误差（0.28m），优于基线（0.37m），且与人类评估的吻合度达84.88%。

Conclusion: CleanMAP为众包HD地图更新提供了一种可扩展且可靠的解决方案，提升了自动驾驶导航的精确性和可靠性。

Abstract: The rapid growth of intelligent connected vehicles (ICVs) and integrated
vehicle-road-cloud systems has increased the demand for accurate, real-time HD
map updates. However, ensuring map reliability remains challenging due to
inconsistencies in crowdsourced data, which suffer from motion blur, lighting
variations, adverse weather, and lane marking degradation. This paper
introduces CleanMAP, a Multimodal Large Language Model (MLLM)-based
distillation framework designed to filter and refine crowdsourced data for
high-confidence HD map updates. CleanMAP leverages an MLLM-driven lane
visibility scoring model that systematically quantifies key visual parameters,
assigning confidence scores (0-10) based on their impact on lane detection. A
novel dynamic piecewise confidence-scoring function adapts scores based on lane
visibility, ensuring strong alignment with human evaluations while effectively
filtering unreliable data. To further optimize map accuracy, a
confidence-driven local map fusion strategy ranks and selects the top-k
highest-scoring local maps within an optimal confidence range (best score minus
10%), striking a balance between data quality and quantity. Experimental
evaluations on a real-world autonomous vehicle dataset validate CleanMAP's
effectiveness, demonstrating that fusing the top three local maps achieves the
lowest mean map update error of 0.28m, outperforming the baseline (0.37m) and
meeting stringent accuracy thresholds (<= 0.32m). Further validation with
real-vehicle data confirms 84.88% alignment with human evaluators, reinforcing
the model's robustness and reliability. This work establishes CleanMAP as a
scalable and deployable solution for crowdsourced HD map updates, ensuring more
precise and reliable autonomous navigation. The code will be available at
https://Ankit-Zefan.github.io/CleanMap/

</details>

### [58] [Hearing Anywhere in Any Environment](https://arxiv.org/abs/2504.10746)
*Xiulong Liu,Anurag Kumar,Paul Calamia,Sebastia V. Amengual,Calvin Murdock,Ishwarya Ananthabhotla,Philip Robinson,Eli Shlizerman,Vamsi Krishna Ithapu,Ruohan Gao*

Main category: cs.CV

TLDR: xRIR框架通过结合几何特征提取器和RIR编码器，实现了跨房间的RIR预测，显著优于现有方法，并在真实环境中验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 混合现实中真实的声学体验对沉浸感至关重要，但现有神经方法局限于训练环境，无法泛化到新房间。

Method: 结合全景深度图像的几何特征提取器和少量参考RIR样本的声学特征编码器，构建跨房间RIR预测模型。

Result: 在包含260个房间的ACOUSTICROOMS数据集上表现优异，并在四个真实环境中验证了泛化能力。

Conclusion: xRIR框架为跨环境声学体验重建提供了有效解决方案，数据集和方法的真实性得到验证。

Abstract: In mixed reality applications, a realistic acoustic experience in spatial
environments is as crucial as the visual experience for achieving true
immersion. Despite recent advances in neural approaches for Room Impulse
Response (RIR) estimation, most existing methods are limited to the single
environment on which they are trained, lacking the ability to generalize to new
rooms with different geometries and surface materials. We aim to develop a
unified model capable of reconstructing the spatial acoustic experience of any
environment with minimum additional measurements. To this end, we present xRIR,
a framework for cross-room RIR prediction. The core of our generalizable
approach lies in combining a geometric feature extractor, which captures
spatial context from panorama depth images, with a RIR encoder that extracts
detailed acoustic features from only a few reference RIR samples. To evaluate
our method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity
simulation of over 300,000 RIRs from 260 rooms. Experiments show that our
method strongly outperforms a series of baselines. Furthermore, we successfully
perform sim-to-real transfer by evaluating our model on four real-world
environments, demonstrating the generalizability of our approach and the
realism of our dataset.

</details>

### [59] [Real-time Seafloor Segmentation and Mapping](https://arxiv.org/abs/2504.10750)
*Michele Grimaldi,Nouf Alkaabi,Francesco Ruscio,Sebastian Realpe Rua,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TLDR: 本文提出了一种结合机器学习和计算机视觉的框架，用于自主水下车辆（AUV）监测Posidonia oceanica草甸边界，通过改进的Mask R-CNN模型和新增岩石类别，提升了水下环境中的分割性能。


<details>
  <summary>Details</summary>
Motivation: Posidonia oceanica草甸的全球性衰退需要高效监测工具，但水下环境的复杂性和数据稀缺性限制了现有技术的应用。

Method: 结合Mask R-CNN模型和边界跟踪策略，新增岩石类别以优化分割，并在真实水下图像和仿真环境中验证框架。

Result: 框架使AUV能自主完成水下检查和岩石分割任务，为草甸保护提供有效工具。

Conclusion: 该框架对海洋环境保护具有重要潜力，支持Posidonia oceanica草甸的针对性保护。

Abstract: Posidonia oceanica meadows are a species of seagrass highly dependent on
rocks for their survival and conservation. In recent years, there has been a
concerning global decline in this species, emphasizing the critical need for
efficient monitoring and assessment tools. While deep learning-based semantic
segmentation and visual automated monitoring systems have shown promise in a
variety of applications, their performance in underwater environments remains
challenging due to complex water conditions and limited datasets. This paper
introduces a framework that combines machine learning and computer vision
techniques to enable an autonomous underwater vehicle (AUV) to inspect the
boundaries of Posidonia oceanica meadows autonomously. The framework
incorporates an image segmentation module using an existing Mask R-CNN model
and a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a
new class dedicated to rocks is introduced to enhance the existing model,
aiming to contribute to a comprehensive monitoring approach and provide a
deeper understanding of the intricate interactions between the meadow and its
surrounding environment. The image segmentation model is validated using real
underwater images, while the overall inspection framework is evaluated in a
realistic simulation environment, replicating actual monitoring scenarios with
real underwater images. The results demonstrate that the proposed framework
enables the AUV to autonomously accomplish the main tasks of underwater
inspection and segmentation of rocks. Consequently, this work holds significant
potential for the conservation and protection of marine environments, providing
valuable insights into the status of Posidonia oceanica meadows and supporting
targeted preservation efforts

</details>

### [60] [ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models](https://arxiv.org/abs/2504.10757)
*Amirhosein Chahe,Lifeng Zhou*

Main category: cs.CV

TLDR: 研究探讨了在自动驾驶任务中，通过显式建模推理能力提升视觉语言模型（VLM）性能的方法。结果表明，基于推理的微调显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域需要透明的推理能力以确保安全，但现有视觉语言模型缺乏这一能力。

Method: 使用GPT-4o生成结构化推理链，并在DriveLM基准上对多个小型VLM家族进行基于推理的微调。

Result: 基于推理的微调模型（如Llama3.2-11B-reason）在准确性和文本生成质量上表现最佳。

Conclusion: 显式推理能增强模型内部表示，为开发更可解释的自动驾驶系统提供了方向。

Abstract: Vision-language models (VLMs) show promise for autonomous driving but often
lack transparent reasoning capabilities that are critical for safety. We
investigate whether explicitly modeling reasoning during fine-tuning enhances
VLM performance on driving decision tasks. Using GPT-4o, we generate structured
reasoning chains for driving scenarios from the DriveLM benchmark with
category-specific prompting strategies. We compare reasoning-based fine-tuning,
answer-only fine-tuning, and baseline instruction-tuned models across multiple
small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results
demonstrate that reasoning-based fine-tuning consistently outperforms
alternatives, with Llama3.2-11B-reason achieving the highest performance.
Models fine-tuned with reasoning show substantial improvements in accuracy and
text generation quality, suggesting explicit reasoning enhances internal
representations for driving decisions. These findings highlight the importance
of transparent decision processes in safety-critical domains and offer a
promising direction for developing more interpretable autonomous driving
systems.

</details>

### [61] [SeeTree -- A modular, open-source system for tree detection and orchard localization](https://arxiv.org/abs/2504.10764)
*Jostan Brown,Cindy Grimm,Joseph R. Davidson*

Main category: cs.CV

TLDR: SeeTree是一个开源嵌入式系统，用于果园树木检测和定位，支持多种传感器集成，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 果园精准管理需要高精度定位，但现有商业解决方案有限。

Method: 基于粒子滤波的视觉定位系统，支持视觉、GNSS和轮式里程计集成，扩展了全果园定位能力。

Result: 在商业果园实验中，系统99%的情况下能准确定位，99%的转弯跟踪成功。

Conclusion: 系统开源，支持社区进一步研究和开发。

Abstract: Accurate localization is an important functional requirement for precision
orchard management. However, there are few off-the-shelf commercial solutions
available to growers. In this paper, we present SeeTree, a modular, open source
embedded system for tree trunk detection and orchard localization that is
deployable on any vehicle. Building on our prior work on vision-based in-row
localization using particle filters, SeeTree includes several new capabilities.
First, it provides capacity for full orchard localization including out-of-row
headland turning. Second, it includes the flexibility to integrate either
visual, GNSS, or wheel odometry in the motion model. During field experiments
in a commercial orchard, the system converged to the correct location 99% of
the time over 800 trials, even when starting with large uncertainty in the
initial particle locations. When turning out of row, the system correctly
tracked 99% of the turns (860 trials representing 43 unique row changes). To
help support adoption and future research and development, we make our dataset,
design files, and source code freely available to the community.

</details>

### [62] [Minimal Sensing for Orienting a Solar Panel](https://arxiv.org/abs/2504.10765)
*Jeremy Klotz,Shree K. Nayar*

Main category: cs.CV

TLDR: 论文提出了一种利用四个光电探测器最小化感知的方法，通过优化倾斜角度最大化太阳能板的辐照度，解决了多局部最大值环境下的方向优化问题。


<details>
  <summary>Details</summary>
Motivation: 太阳能板在任意方向和环境下需要找到最大辐照度的方向以最大化能量收集，但多局部最大值环境使传统梯度上升法失效。

Method: 使用四个光电探测器测量，通过优化倾斜角度模糊辐照度函数，消除局部最大值，使其变为单峰函数，再用梯度上升法找到最大值。

Result: 在多种真实环境（阳光直射、阴天、城市遮挡、复杂室内光）中，该方法显著提高了能量收集效率。

Conclusion: 优化倾斜角度的方法在多局部最大值环境中有效，显著优于传统太阳能板方向控制方法。

Abstract: A solar panel harvests the most energy when pointing in the direction that
maximizes the total illumination (irradiance) falling on it. Given an arbitrary
orientation of a panel and an arbitrary environmental illumination, we address
the problem of finding the direction of maximum total irradiance. We develop a
minimal sensing approach where measurements from just four photodetectors are
used to iteratively vary the tilt of the panel to maximize the irradiance. Many
environments produce irradiance functions with multiple local maxima. As a
result, simply measuring the gradient of the irradiance function and applying
gradient ascent will not work. We show that a larger, optimized tilt between
the detectors and the panel is equivalent to blurring the irradiance function.
This has the effect of eliminating local maxima and turning the irradiance
function into a unimodal one, whose maximum can be found using gradient ascent.
We show that there is a close relationship between our approach and scale space
theory. We have collected a large dataset of high-dynamic range lighting
environments in New York City, called \textit{UrbanSky}. We used this dataset
to conduct simulations to verify the robustness of our approach. Finally, we
have built a portable solar panel with four compact detectors and an actuator
to conduct experiments in various real-world settings: direct sunlight, cloudy
sky, urban settings with occlusions and shadows, and complex indoor lighting.
In all cases, we show significant improvements in harvested energy compared to
standard approaches for controlling the orientation of a solar panel.

</details>

### [63] [Rainy: Unlocking Satellite Calibration for Deep Learning in Precipitation](https://arxiv.org/abs/2504.10776)
*Zhenyu Yu,Hanqing Chen,Mohd Yamani Idna Idris,Pei Wang*

Main category: cs.CV

TLDR: 论文提出Rainy数据集和Taper Loss方法，解决降水估计中多源数据整合和模型应用的挑战，支持五项任务，并展示了QRS与计算机视觉的结合。


<details>
  <summary>Details</summary>
Motivation: 降水对水文循环、生态系统和水资源管理至关重要，但传统方法受限于数据获取和特征关系复杂性。缺乏标准化多源卫星数据也阻碍了AI模型的应用。

Method: 提出Rainy数据集（整合卫星与站点数据）和Taper Loss方法，支持五项任务（如卫星校准、降水预测等），并选择基准模型和评估指标。

Result: Rainy数据集和Taper Loss展示了QRS与计算机视觉的无缝协作，为AI在QRS领域的应用提供了数据支持和跨学科合作参考。

Conclusion: Rainy数据集和Taper Loss为降水估计提供了新工具，促进了QRS与计算机视觉的融合，推动了跨学科研究。

Abstract: Precipitation plays a critical role in the Earth's hydrological cycle,
directly affecting ecosystems, agriculture, and water resource management.
Accurate precipitation estimation and prediction are crucial for understanding
climate dynamics, disaster preparedness, and environmental monitoring. In
recent years, artificial intelligence (AI) has gained increasing attention in
quantitative remote sensing (QRS), enabling more advanced data analysis and
improving precipitation estimation accuracy. Although traditional methods have
been widely used for precipitation estimation, they face limitations due to the
difficulty of data acquisition and the challenge of capturing complex feature
relationships. Furthermore, the lack of standardized multi-source satellite
datasets, and in most cases, the exclusive reliance on station data,
significantly hinders the effective application of advanced AI models. To
address these challenges, we propose the Rainy dataset, a multi-source
spatio-temporal dataset that integrates pure satellite data with station data,
and propose Taper Loss, designed to fill the gap in tasks where only in-situ
data is available without area-wide support. The Rainy dataset supports five
main tasks: (1) satellite calibration, (2) precipitation event prediction, (3)
precipitation level prediction, (4) spatiotemporal prediction, and (5)
precipitation downscaling. For each task, we selected benchmark models and
evaluation metrics to provide valuable references for researchers. Using
precipitation as an example, the Rainy dataset and Taper Loss demonstrate the
seamless collaboration between QRS and computer vision, offering data support
for AI for Science in the field of QRS and providing valuable insights for
interdisciplinary collaboration and integration.

</details>

### [64] [Visual Language Models show widespread visual deficits on neuropsychological tests](https://arxiv.org/abs/2504.10786)
*Gene Tangtartharakul,Katherine R. Storrs*

Main category: cs.CV

TLDR: VLMs在复杂视觉任务中表现优异，但在基础视觉概念（如方向、位置）上存在显著缺陷，与人类视觉能力形成对比。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在基础视觉能力上的表现，揭示其与人类视觉的差距。

Method: 使用51项来自临床和实验测试的工具，系统评估三种先进VLMs的视觉能力。

Result: VLMs在对象识别任务中表现优异，但在低中阶视觉能力上存在显著缺陷。

Conclusion: VLMs可能无需基础视觉概念即可实现复杂任务，这与人类视觉发展不同。

Abstract: Visual Language Models (VLMs) show remarkable performance in visual reasoning
tasks, successfully tackling college-level challenges that require high-level
understanding of images. However, some recent reports of VLMs struggling to
reason about elemental visual concepts like orientation, position, continuity,
and occlusion suggest a potential gulf between human and VLM vision. Here we
use the toolkit of neuropsychology to systematically assess the capabilities of
three state-of-the-art VLMs across visual domains. Using 51 tests drawn from
six clinical and experimental batteries, we characterise the visual abilities
of leading VLMs relative to normative performance in healthy adults. While the
models excel in straightforward object recognition tasks, we find widespread
deficits in low- and mid-level visual abilities that would be considered
clinically significant in humans. These selective deficits, profiled through
validated test batteries, suggest that an artificial system can achieve complex
object recognition without developing foundational visual concepts that in
humans require no explicit training.

</details>

### [65] [3D Wavelet Convolutions with Extended Receptive Fields for Hyperspectral Image Classification](https://arxiv.org/abs/2504.10795)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TLDR: 论文提出WCNet，一种结合小波变换改进的3D-DenseNet模型，用于解决高光谱图像分类中的高维数据、稀疏分布和光谱冗余问题，提升分类性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维数据、稀疏分布和光谱冗余等挑战，导致过拟合和泛化能力受限。

Method: 引入小波变换扩展卷积感受野，通过级联引导CNN更好地响应低频信息，称为小波卷积，动态关注不同频率带。

Result: 在IN、UP和KSC数据集上表现优异，优于主流高光谱图像分类方法。

Conclusion: WCNet通过小波变换动态扩展感受野，显著提升分类性能，同时避免过多参数增加。

Abstract: Deep neural networks face numerous challenges in hyperspectral image
classification, including high-dimensional data, sparse ground object
distributions, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To better adapt to ground
object distributions while expanding receptive fields without introducing
excessive parameters and skipping redundant information, this paper proposes
WCNet, an improved 3D-DenseNet model integrated with wavelet transforms. We
introduce wavelet transforms to effectively extend convolutional receptive
fields and guide CNNs to better respond to low frequencies through cascading,
termed wavelet convolution. Each convolution focuses on different frequency
bands of the input signal with gradually increasing effective ranges. This
process enables greater emphasis on low-frequency components while adding only
a small number of trainable parameters. This dynamic approach allows the model
to flexibly focus on critical spatial structures when processing different
regions, rather than relying on fixed receptive fields of single static
kernels. The Wavelet Conv module enhances model representation capability by
expanding receptive fields through 3D wavelet transforms without increasing
network depth or width. Experimental results demonstrate superior performance
on the IN, UP, and KSC datasets, outperforming mainstream hyperspectral image
classification methods.

</details>

### [66] [The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability](https://arxiv.org/abs/2504.10804)
*Jiani Liu,Zhiyuan Wang,Zeliang Zhang,Chao Huang,Susan Liang,Yunlong Tang,Chenliang Xu*

Main category: cs.CV

TLDR: 本文研究了Vision Transformers（ViTs）在对抗性鲁棒性中的计算冗余问题，并提出利用冗余提升对抗样本的转移性和质量。


<details>
  <summary>Details</summary>
Motivation: ViTs在安全关键任务中表现优异，但其独特的架构特性在对抗性鲁棒性方面提出了新挑战。研究发现ViTs生成的对抗样本转移性更高，表明其结构特性有利于转移攻击。

Method: 通过分析ViTs的数据级和模型级冗余，设计了一系列技术，包括注意力稀疏操作、注意力头置换、干净令牌正则化、幽灵MoE多样化和测试时对抗训练。

Result: 在ImageNet-1k数据集上的实验表明，所提方法在转移性和通用性上显著优于现有基线。

Conclusion: ViTs的计算冗余可被有效利用以提升对抗攻击的效果，为对抗性鲁棒性研究提供了新视角。

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
range of applications, including many safety-critical tasks. However, their
unique architectural properties raise new challenges and opportunities in
adversarial robustness. In particular, we observe that adversarial examples
crafted on ViTs exhibit higher transferability compared to those crafted on
CNNs, suggesting that ViTs contain structural characteristics favorable for
transferable attacks. In this work, we investigate the role of computational
redundancy in ViTs and its impact on adversarial transferability. Unlike prior
studies that aim to reduce computation for efficiency, we propose to exploit
this redundancy to improve the quality and transferability of adversarial
examples. Through a detailed analysis, we identify two forms of redundancy,
including the data-level and model-level, that can be harnessed to amplify
attack effectiveness. Building on this insight, we design a suite of
techniques, including attention sparsity manipulation, attention head
permutation, clean token regularization, ghost MoE diversification, and
test-time adversarial training. Extensive experiments on the ImageNet-1k
dataset validate the effectiveness of our approach, showing that our methods
significantly outperform existing baselines in both transferability and
generality across diverse model architectures.

</details>

### [67] [Tabular foundation model to detect empathy from visual cues](https://arxiv.org/abs/2504.10808)
*Md Rakibul Hasan,Shafin Rahman,Md Zakir Hossain,Aneesh Krishna,Tom Gedeon*

Main category: cs.CV

TLDR: 该论文探讨了使用表格基础模型（如TabPFN v2和TabICL）通过上下文学习和微调来检测视频交互中的共情能力，显著提升了跨主体共情检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和伦理问题，视频数据集通常以提取的特征（表格数据）形式发布。受文本基础模型成功的启发，研究探索了表格基础模型在共情检测中的应用。

Method: 实验使用了两种表格基础模型（TabPFN v2和TabICL），通过上下文学习和微调设置，在公共人机交互基准数据集上进行测试。

Result: 实验结果显示，共情检测的准确性和AUC显著提升（准确率：0.590→0.730；AUC：0.564→0.669）。

Conclusion: 研究不仅提升了性能，还提供了新的见解和评估设置，确保在未见过的受试者上的泛化能力。由于隐私限制，视频特征以表格形式发布的趋势将持续，因此该研究对未来共情检测数据集具有广泛适用性。

Abstract: Detecting empathy from video interactions is an emerging area of research.
Video datasets, however, are often released as extracted features (i.e.,
tabular data) rather than raw footage due to privacy and ethical concerns.
Prior research on such tabular datasets established tree-based classical
machine learning approaches as the best-performing models. Motivated by the
recent success of textual foundation models (i.e., large language models), we
explore the use of tabular foundation models in empathy detection from tabular
visual features. We experiment with two recent tabular foundation models $-$
TabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups.
Our experiments on a public human-robot interaction benchmark demonstrate a
significant boost in cross-subject empathy detection accuracy over several
strong baselines (accuracy: $0.590 \rightarrow 0.730$; AUC: $0.564 \rightarrow
0.669$). In addition to performance improvement, we contribute novel insights
and an evaluation setup to ensure generalisation on unseen subjects in this
public benchmark. As the practice of releasing video features as tabular
datasets is likely to persist due to privacy constraints, our findings will be
widely applicable to future empathy detection video datasets as well.

</details>

### [68] [GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR](https://arxiv.org/abs/2504.10809)
*Christophe Bolduc,Yannick Hold-Geoffroy,Zhixin Shu,Jean-François Lalonde*

Main category: cs.CV

TLDR: GaSLight是一种从普通图像生成空间变化光照的方法，首次将HDR高斯泼溅作为光源表示，并利用扩散模型增强动态范围，实现3D渲染中的光照效果。


<details>
  <summary>Details</summary>
Motivation: 解决普通图像作为3D渲染光源的挑战，提供更真实的光照效果。

Method: 两阶段方法：1) 利用扩散模型增强图像动态范围；2) 使用高斯泼溅建模3D光照。

Result: 在HDR估计和虚拟场景照明中取得最先进效果，并引入新数据集进行评估。

Conclusion: GaSLight为图像作为光源提供了高效且准确的解决方案，推动了3D渲染技术的发展。

Abstract: We present GaSLight, a method that generates spatially-varying lighting from
regular images. Our method proposes using HDR Gaussian Splats as light source
representation, marking the first time regular images can serve as light
sources in a 3D renderer. Our two-stage process first enhances the dynamic
range of images plausibly and accurately by leveraging the priors embedded in
diffusion models. Next, we employ Gaussian Splats to model 3D lighting,
achieving spatially variant lighting. Our approach yields state-of-the-art
results on HDR estimations and their applications in illuminating virtual
objects and scenes. To facilitate the benchmarking of images as light sources,
we introduce a novel dataset of calibrated and unsaturated HDR to evaluate
images as light sources. We assess our method using a combination of this novel
dataset and an existing dataset from the literature. The code to reproduce our
method will be available upon acceptance.

</details>

### [69] [PatrolVision: Automated License Plate Recognition in the wild](https://arxiv.org/abs/2504.10810)
*Anmol Singhal Navya Singhal*

Main category: cs.CV

TLDR: 论文提出了一种基于低功耗GPU的巡逻系统原型，用于城市环境中的自动车牌识别（ALPR），解决了车牌检测和字符识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 公共服务的AI技术采用率低，尤其在交通监控领域。现有ALPR系统缺乏端到端解决方案，且难以处理现实中的车牌扭曲问题。

Method: 使用RFB-Net检测车牌并校正扭曲，然后通过YOLO网络进行字符识别。系统在新构建的16,000张图像数据集上测试。

Result: 车牌检测精度86%，字符识别准确率67%，部分匹配准确率89%。系统延迟为64FPS（Tesla P4 GPU）。

Conclusion: 原型系统在复杂环境下表现良好，为城市巡逻提供了可行的ALPR解决方案。

Abstract: Adoption of AI driven techniques in public services remains low due to
challenges related to accuracy and speed of information at population scale.
Computer vision techniques for traffic monitoring have not gained much
popularity despite their relative strength in areas such as autonomous driving.
Despite large number of academic methods for Automatic License Plate
Recognition (ALPR) systems, very few provide an end to end solution for
patrolling in the city. This paper presents a novel prototype for a low power
GPU based patrolling system to be deployed in an urban environment on
surveillance vehicles for automated vehicle detection, recognition and
tracking. In this work, we propose a complete ALPR system for Singapore license
plates having both single and double line creating our own YOLO based network.
We focus on unconstrained capture scenarios as would be the case in real world
application, where the license plate (LP) might be considerably distorted due
to oblique views. In this work, we first detect the license plate from the full
image using RFB-Net and rectify multiple distorted license plates in a single
image. After that, the detected license plate image is fed to our network for
character recognition. We evaluate the performance of our proposed system on a
newly built dataset covering more than 16,000 images. The system was able to
correctly detect license plates with 86\% precision and recognize characters of
a license plate in 67\% of the test set, and 89\% accuracy with one incorrect
character (partial match). We also test latency of our system and achieve 64FPS
on Tesla P4 GPU

</details>

### [70] [IlluSign: Illustrating Sign Language Videos by Leveraging the Attention Mechanism](https://arxiv.org/abs/2504.10822)
*Janna Bruner,Amit Moryossef,Lior Wolf*

Main category: cs.CV

TLDR: 将手语视频转换为静态插图，作为教育资源的补充，利用生成模型实现低成本生成。


<details>
  <summary>Details</summary>
Motivation: 手语的动态特性使其难以详细研究，尤其是对新手和教育者，现有插图制作成本高。

Method: 利用扩散模型在去噪过程中注入风格信息，结合几何和边缘信息，生成手语插图。

Result: 提出了一种低成本生成手语插图的方法，解决了教育资源不足的问题。

Conclusion: 该方法为手语教育提供了经济高效的插图生成方案。

Abstract: Sign languages are dynamic visual languages that involve hand gestures, in
combination with non manual elements such as facial expressions. While video
recordings of sign language are commonly used for education and documentation,
the dynamic nature of signs can make it challenging to study them in detail,
especially for new learners and educators. This work aims to convert sign
language video footage into static illustrations, which serve as an additional
educational resource to complement video content. This process is usually done
by an artist, and is therefore quite costly. We propose a method that
illustrates sign language videos by leveraging generative models' ability to
understand both the semantic and geometric aspects of images. Our approach
focuses on transferring a sketch like illustration style to video footage of
sign language, combining the start and end frames of a sign into a single
illustration, and using arrows to highlight the hand's direction and motion.
While many style transfer methods address domain adaptation at varying levels
of abstraction, applying a sketch like style to sign languages, especially for
hand gestures and facial expressions, poses a significant challenge. To tackle
this, we intervene in the denoising process of a diffusion model, injecting
style as keys and values into high resolution attention layers, and fusing
geometric information from the image and edges as queries. For the final
illustration, we use the attention mechanism to combine the attention weights
from both the start and end illustrations, resulting in a soft combination. Our
method offers a cost effective solution for generating sign language
illustrations at inference time, addressing the lack of such resources in
educational materials.

</details>

### [71] [OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding](https://arxiv.org/abs/2504.10825)
*Dianbing Xi,Jiepeng Wang,Yuanzhi Liang,Xi Qiu,Yuchi Huo,Rui Wang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TLDR: OmniVDiff是一个可控视频扩散框架，支持多模态视频内容的生成和理解，通过动态调整视觉模态角色实现灵活控制。


<details>
  <summary>Details</summary>
Motivation: 旨在通过单一扩散模型实现多模态视频内容的合成与理解，提升可控视频扩散的灵活性和可扩展性。

Method: 在色彩空间中学习联合分布，采用自适应控制策略动态调整视觉模态的角色（生成或条件）。

Result: 支持文本条件视频生成、视频理解和X条件视频生成，实验验证了其有效性。

Conclusion: OmniVDiff为可控视频扩散提供了统一框架，适用于多种下游应用。

Abstract: In this paper, we propose a novel framework for controllable video diffusion,
OmniVDiff, aiming to synthesize and comprehend multiple video visual content in
a single diffusion model. To achieve this, OmniVDiff treats all video visual
modalities in the color space to learn a joint distribution, while employing an
adaptive control strategy that dynamically adjusts the role of each visual
modality during the diffusion process, either as a generation modality or a
conditioning modality. This allows flexible manipulation of each modality's
role, enabling support for a wide range of tasks. Consequently, our model
supports three key functionalities: (1) Text-conditioned video generation:
multi-modal visual video sequences (i.e., rgb, depth, canny, segmentaion) are
generated based on the text conditions in one diffusion process; (2) Video
understanding: OmniVDiff can estimate the depth, canny map, and semantic
segmentation across the input rgb frames while ensuring coherence with the rgb
input; and (3) X-conditioned video generation: OmniVDiff generates videos
conditioned on fine-grained attributes (e.g., depth maps or segmentation maps).
By integrating these diverse tasks into a unified video diffusion framework,
OmniVDiff enhances the flexibility and scalability for controllable video
diffusion, making it an effective tool for a variety of downstream
applications, such as video-to-video translation. Extensive experiments
demonstrate the effectiveness of our approach, highlighting its potential for
various video-related applications.

</details>

### [72] [LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation](https://arxiv.org/abs/2504.10829)
*Hengyu Shi,Junhao Su,Huansheng Ning,Xiaoming Wei,Jialin Gao*

Main category: cs.CV

TLDR: LayoutCoT利用RAG和CoT技术提升LLM在布局生成中的推理能力，无需训练即可实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型需大量数据或微调，而训练免费方法推理能力有限，无法生成高质量布局。

Method: 将布局序列化，通过RAG生成初步布局，再经CoT模块迭代优化。

Result: 在五个数据集上表现优于现有方法，包括专为深度推理设计的模型。

Conclusion: LayoutCoT展示了LLM在布局生成任务中的深度推理潜力。

Abstract: Conditional layout generation aims to automatically generate visually
appealing and semantically coherent layouts from user-defined constraints.
While recent methods based on generative models have shown promising results,
they typically require substantial amounts of training data or extensive
fine-tuning, limiting their versatility and practical applicability.
Alternatively, some training-free approaches leveraging in-context learning
with Large Language Models (LLMs) have emerged, but they often suffer from
limited reasoning capabilities and overly simplistic ranking mechanisms, which
restrict their ability to generate consistently high-quality layouts. To this
end, we propose LayoutCoT, a novel approach that leverages the reasoning
capabilities of LLMs through a combination of Retrieval-Augmented Generation
(RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms
layout representations into a standardized serialized format suitable for
processing by LLMs. A Layout-aware RAG is used to facilitate effective
retrieval and generate a coarse layout by LLMs. This preliminary layout,
together with the selected exemplars, is then fed into a specially designed CoT
reasoning module for iterative refinement, significantly enhancing both
semantic coherence and visual quality. We conduct extensive experiments on five
public datasets spanning three conditional layout generation tasks.
Experimental results demonstrate that LayoutCoT achieves state-of-the-art
performance without requiring training or fine-tuning. Notably, our CoT
reasoning module enables standard LLMs, even those without explicit deep
reasoning abilities, to outperform specialized deep-reasoning models such as
deepseek-R1, highlighting the potential of our approach in unleashing the deep
reasoning capabilities of LLMs for layout generation tasks.

</details>

### [73] [LightFormer: A lightweight and efficient decoder for remote sensing image segmentation](https://arxiv.org/abs/2504.10834)
*Sihang Chen,Lijun Yun,Ze Liu,JianFeng Zhu,Jie Chen,Hui Wang,Yueping Nie*

Main category: cs.CV

TLDR: LightFormer是一种轻量级解码器，用于遥感图像的实时语义分割，显著降低模型复杂度，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在遥感图像语义分割中解码器复杂度高、难以实时部署的问题。

Method: 采用特征融合与细化模块及空间信息选择模块（SISM），结合通道处理和可学习门控机制，高效聚合多尺度信息。

Result: 在多个基准测试中表现优异，如ISPRS Vaihingen上达到83.9% mIoU，仅需14.7% FLOPs和15.9%参数。

Conclusion: LightFormer是计算经济性与高精度分割并重的实用解决方案，适用于灾害评估等实时任务。

Abstract: Deep learning techniques have achieved remarkable success in the semantic
segmentation of remote sensing images and in land-use change detection.
Nevertheless, their real-time deployment on edge platforms remains constrained
by decoder complexity. Herein, we introduce LightFormer, a lightweight decoder
for time-critical tasks that involve unstructured targets, such as disaster
assessment, unmanned aerial vehicle search-and-rescue, and cultural heritage
monitoring. LightFormer employs a feature-fusion and refinement module built on
channel processing and a learnable gating mechanism to aggregate multi-scale,
multi-range information efficiently, which drastically curtails model
complexity. Furthermore, we propose a spatial information selection module
(SISM) that integrates long-range attention with a detail preservation branch
to capture spatial dependencies across multiple scales, thereby substantially
improving the recognition of unstructured targets in complex scenes. On the
ISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9%
vs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters,
thus achieving an excellent accuracy-efficiency trade-off. Consistent results
on LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its
robustness and superior perception of unstructured objects. These findings
highlight LightFormer as a practical solution for remote sensing applications
where both computational economy and high-precision segmentation are
imperative.

</details>

### [74] [A comprehensive review of remote sensing in wetland classification and mapping](https://arxiv.org/abs/2504.10842)
*Shuai Yuan,Xiangan Liang,Tianwu Lin,Shuang Chen,Rui Liu,Jie Wang,Hongsheng Zhang,Peng Gong*

Main category: cs.CV

TLDR: 本文综述了湿地分类与制图的研究进展，通过分析1200多篇论文，总结了湿地类型、方法、传感器类型和研究地点的趋势，探讨了湿地变化的驱动因素，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 湿地是支持生物多样性和人类福祉的关键生态系统，但自20世纪以来显著减少。尽管已有综述总结了该领域的发展，但仍缺乏对湿地分类与制图的全面深入理解。

Method: 通过元分析1200多篇论文，综述湿地特征、现有数据和方法，总结典型湿地制图产品，并探讨湿地变化的驱动因素。

Result: 揭示了湿地分类与制图的趋势、驱动因素和局限性，提出了应对全球环境变化和技术创新的未来方向。

Conclusion: 本文巩固了对湿地遥感的理解，为湿地科学的变革性进展提供了科学建议。

Abstract: Wetlands constitute critical ecosystems that support both biodiversity and
human well-being; however, they have experienced a significant decline since
the 20th century. Back in the 1970s, researchers began to employ remote sensing
technologies for wetland classification and mapping to elucidate the extent and
variations of wetlands. Although some review articles summarized the
development of this field, there is a lack of a thorough and in-depth
understanding of wetland classification and mapping: (1) the scientific
importance of wetlands, (2) major data, methods used in wetland classification
and mapping, (3) driving factors of wetland changes, (4) current research
paradigm and limitations, (5) challenges and opportunities in wetland
classification and mapping under the context of technological innovation and
global environmental change. In this review, we aim to provide a comprehensive
perspective and new insights into wetland classification and mapping for
readers to answer these questions. First, we conduct a meta-analysis of over
1,200 papers, encompassing wetland types, methods, sensor types, and study
sites, examining prevailing trends in wetland classification and mapping. Next,
we review and synthesize the wetland features and existing data and methods in
wetland classification and mapping. We also summarize typical wetland mapping
products and explore the intrinsic driving factors of wetland changes across
multiple spatial and temporal scales. Finally, we discuss current limitations
and propose future directions in response to global environmental change and
technological innovation. This review consolidates our understanding of wetland
remote sensing and offers scientific recommendations that foster transformative
progress in wetland science.

</details>

### [75] [Enhancing Features in Long-tailed Data Using Large Vision Mode](https://arxiv.org/abs/2504.10852)
*Pengxiao Han,Changkun Ye,Jinguang Tong,Cuicui Jiang,Jie Hong,Li Fang,Xuesong Li*

Main category: cs.CV

TLDR: 研究探索了如何利用大型视觉模型（LVMs）增强长尾数据特征，无需语言信息，并通过原型损失进一步优化特征。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的基础模型在长尾识别中需要语言数据，但并非所有任务都适用，因此研究转向纯视觉模型。

Method: 从LVM提取特征并与基线网络的特征融合，设计原型损失以优化潜在空间中的特征。

Result: 在ImageNet-LT和iNaturalist2018数据集上验证了方法的有效性。

Conclusion: 纯视觉模型可以有效增强长尾数据特征，无需依赖语言信息。

Abstract: Language-based foundation models, such as large language models (LLMs) or
large vision-language models (LVLMs), have been widely studied in long-tailed
recognition. However, the need for linguistic data is not applicable to all
practical tasks. In this study, we aim to explore using large vision models
(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features
without any language information. Specifically, we extract features from the
LVM and fuse them with features in the baseline network's map and latent space
to obtain the augmented features. Moreover, we design several prototype-based
losses in the latent space to further exploit the potential of the augmented
features. In the experimental section, we validate our approach on two
benchmark datasets: ImageNet-LT and iNaturalist2018.

</details>

### [76] [LVLM_CSP: Accelerating Large Vision Language Models via Clustering, Scattering, and Pruning for Reasoning Segmentation](https://arxiv.org/abs/2504.10854)
*Hanning Chen,Yang Ni,Wenjun Huang,Hyunwoo Oh,Yezi Liu,Tamoghno Das,Mohsen Imani*

Main category: cs.CV

TLDR: LVLM_CSP是一种无需训练的视觉令牌修剪方法，显著降低计算开销，同时保持高分割精度。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在推理分割任务中计算开销大，现有修剪方法难以平衡计算开销与精度。

Method: LVLM_CSP通过聚类、散射和修剪三阶段，逐步减少图像令牌数量。

Result: 实验显示，LVLM_CSP在7B LVLM上减少65%计算开销且精度无损，减少70%时仅损失1%精度。

Conclusion: LVLM_CSP为LVLM推理分割任务提供高效且精确的令牌修剪解决方案。

Abstract: Large Vision Language Models (LVLMs) have been widely adopted to guide vision
foundation models in performing reasoning segmentation tasks, achieving
impressive performance. However, the substantial computational overhead
associated with LVLMs presents a new challenge. The primary source of this
computational cost arises from processing hundreds of image tokens. Therefore,
an effective strategy to mitigate such overhead is to reduce the number of
image tokens, a process known as image token pruning. Previous studies on image
token pruning for LVLMs have primarily focused on high level visual
understanding tasks, such as visual question answering and image captioning. In
contrast, guiding vision foundation models to generate accurate visual masks
based on textual queries demands precise semantic and spatial reasoning
capabilities. Consequently, pruning methods must carefully control individual
image tokens throughout the LVLM reasoning process. Our empirical analysis
reveals that existing methods struggle to adequately balance reductions in
computational overhead with the necessity to maintain high segmentation
accuracy. In this work, we propose LVLM_CSP, a novel training free visual token
pruning method specifically designed for LVLM based reasoning segmentation
tasks. LVLM_CSP consists of three stages: clustering, scattering, and pruning.
Initially, the LVLM performs coarse-grained visual reasoning using a subset of
selected image tokens. Next, fine grained reasoning is conducted, and finally,
most visual tokens are pruned in the last stage. Extensive experiments
demonstrate that LVLM_CSP achieves a 65% reduction in image token inference
FLOPs with virtually no accuracy degradation, and a 70% reduction with only a
minor 1% drop in accuracy on the 7B LVLM.

</details>

### [77] [DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and Visible Images Fusion](https://arxiv.org/abs/2504.10871)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui,Yuxin Jing,Yuhan Lyu*

Main category: cs.CV

TLDR: 本文提出了一种退化感知的自适应图像融合方法（DAAF），通过结合自适应退化优化和图像融合，解决了现有红外与可见光图像融合算法忽视图像退化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有红外与可见光图像融合算法通常关注高质量图像，而忽略了低光和噪声等退化问题，限制了实际应用潜力。

Method: DAAF包含自适应退化优化网络（ADON）和特征交互局部-全局融合网络（FILGF）。ADON通过频域特征分解和Retinex分解分别处理红外和可见光图像的退化问题；FILGF通过多尺度局部-全局特征交互融合实现图像融合。

Result: 实验表明，DAAF在正常和复杂退化场景下均优于现有算法。

Conclusion: DAAF通过统一建模退化优化和图像融合，显著提升了红外与可见光图像融合的实用性和性能。

Abstract: Existing infrared and visible image fusion(IVIF) algorithms often prioritize
high-quality images, neglecting image degradation such as low light and noise,
which limits the practical potential. This paper propose Degradation-Aware
Adaptive image Fusion (DAAF), which achieves unified modeling of adaptive
degradation optimization and image fusion. Specifically, DAAF comprises an
auxiliary Adaptive Degradation Optimization Network (ADON) and a Feature
Interactive Local-Global Fusion (FILGF) Network. Firstly, ADON includes
infrared and visible-light branches. Within the infrared branch,
frequency-domain feature decomposition and extraction are employed to isolate
Gaussian and stripe noise. In the visible-light branch, Retinex decomposition
is applied to extract illumination and reflectance components, enabling
complementary enhancement of detail and illumination distribution.
Subsequently, FILGF performs interactive multi-scale local-global feature
fusion. Local feature fusion consists of intra-inter model feature complement,
while global feature fusion is achieved through a interactive cross-model
attention. Extensive experiments have shown that DAAF outperforms current IVIF
algorithms in normal and complex degradation scenarios.

</details>

### [78] [Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2504.10873)
*Tonko E. W. Bossen,Andreas Møgelmose,Ross Greer*

Main category: cs.CV

TLDR: 研究评估了视觉语言模型（VLMs）在零样本下理解交通手势的能力，发现当前模型表现不佳，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶中交通手势的正确理解对安全至关重要，但现有模型能力不足。

Method: 创建两个数据集（ATG和ITGI），通过三种方法（描述相似性、分类和姿态重建）评估VLMs。

Result: 模型表现较差：描述相似性低于0.59，分类F1分数仅0.14-0.39，远低于专家基线0.70。

Conclusion: 当前VLMs在交通手势理解上不够准确和鲁棒，需更多研究改进。

Abstract: In autonomous driving, it is crucial to correctly interpret traffic gestures
(TGs), such as those of an authority figure providing orders or instructions,
or a pedestrian signaling the driver, to ensure a safe and pleasant traffic
environment for all road users. This study investigates the capabilities of
state-of-the-art vision-language models (VLMs) in zero-shot interpretation,
focusing on their ability to caption and classify human gestures in traffic
contexts. We create and publicly share two custom datasets with varying formal
and informal TGs, such as 'Stop', 'Reverse', 'Hail', etc. The datasets are
"Acted TG (ATG)" and "Instructive TG In-The-Wild (ITGI)". They are annotated
with natural language, describing the pedestrian's body position and gesture.
We evaluate models using three methods utilizing expert-generated captions as
baseline and control: (1) caption similarity, (2) gesture classification, and
(3) pose sequence reconstruction similarity. Results show that current VLMs
struggle with gesture understanding: sentence similarity averages below 0.59,
and classification F1 scores reach only 0.14-0.39, well below the expert
baseline of 0.70. While pose reconstruction shows potential, it requires more
data and refined metrics to be reliable. Our findings reveal that although some
SOTA VLMs can interpret zero-shot human traffic gestures, none are accurate and
robust enough to be trustworthy, emphasizing the need for further research in
this domain.

</details>

### [79] [Weather-Aware Object Detection Transformer for Domain Adaptation](https://arxiv.org/abs/2504.10877)
*Soheil Gharatappeh,Salimeh Sekeh,Vikas Dhiman*

Main category: cs.CV

TLDR: 论文研究了三种新方法以增强RT-DETR在雾天环境中的鲁棒性，但均未显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: RT-DETR在雾天条件下性能下降，需提升其在恶劣天气中的表现。

Method: 1. 基于感知损失的域适应；2. 天气自适应注意力；3. 天气融合编码器。

Result: 提出的方法均未持续优于基线RT-DETR。

Conclusion: 分析了方法的局限性，为未来天气感知目标检测研究提供了方向。

Abstract: RT-DETRs have shown strong performance across various computer vision tasks
but are known to degrade under challenging weather conditions such as fog. In
this work, we investigate three novel approaches to enhance RT-DETR robustness
in foggy environments: (1) Domain Adaptation via Perceptual Loss, which
distills domain-invariant features from a teacher network to a student using
perceptual supervision; (2) Weather Adaptive Attention, which augments the
attention mechanism with fog-sensitive scaling by introducing an auxiliary
foggy image stream; and (3) Weather Fusion Encoder, which integrates a
dual-stream encoder architecture that fuses clear and foggy image features via
multi-head self and cross-attention. Despite the architectural innovations,
none of the proposed methods consistently outperform the baseline RT-DETR. We
analyze the limitations and potential causes, offering insights for future
research in weather-aware object detection.

</details>

### [80] [Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content](https://arxiv.org/abs/2504.10878)
*Yilang Peng,Sijia Qian,Yingdan Lu,Cuihua Shen*

Main category: cs.CV

TLDR: 提出了一种基于大型语言模型（LLM）的框架，用于预测社交媒体视觉内容的可信度，并解释其判断依据。该方法通过多模态LLM提取可解释特征，提升机器学习模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 在视觉主导的社交媒体环境中，预测视觉内容的可信度并理解人类判断的依据对打击虚假信息至关重要。但由于视觉特征的多样性和丰富性，这些任务具有挑战性。

Method: 利用多模态LLM（如GPT-4o）提取和量化可解释特征，通过针对性提示将其整合到机器学习模型中，以改进可信度预测。

Result: 在4,191个视觉社交媒体帖子上测试，该方法在R2上比零样本GPT预测高出13%，并揭示了信息具体性和图像格式等关键特征。

Conclusion: 该方法为虚假信息缓解、视觉可信度研究以及LLM在社会科学中的作用提供了重要启示。

Abstract: In today's visually dominated social media landscape, predicting the
perceived credibility of visual content and understanding what drives human
judgment are crucial for countering misinformation. However, these tasks are
challenging due to the diversity and richness of visual features. We introduce
a Large Language Model (LLM)-informed feature discovery framework that
leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and
explain its reasoning. We extract and quantify interpretable features using
targeted prompts and integrate them into machine learning models to improve
credibility predictions. We tested this approach on 4,191 visual social media
posts across eight topics in science, health, and politics, using credibility
ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot
GPT-based predictions by 13 percent in R2, and revealed key features like
information concreteness and image format. We discuss the implications for
misinformation mitigation, visual credibility, and the role of LLMs in social
science.

</details>

### [81] [Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task](https://arxiv.org/abs/2504.10880)
*Aviral Chharia,Tianyu Ren,Tomotake Furuhata,Kenji Shimada*

Main category: cs.CV

TLDR: Safe-Construct框架通过3D多视角任务和合成数据生成，显著提升了建筑工地安全违规识别的性能，比现有方法提高了7.6%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D目标检测，无法应对真实场景中的复杂违规行为，且缺乏标准化基准和多样化数据。

Method: 提出Safe-Construct框架，将违规识别重新定义为3D多视角任务，并结合场景级上下文和3D空间理解；开发SICSG合成数据生成器。

Result: 在近真实场景中测试，涵盖多种违规类型和挑战条件，性能提升7.6%。

Conclusion: Safe-Construct为高风险行业的安全监控提供了可扩展且鲁棒的新基准。

Abstract: Recognizing safety violations in construction environments is critical yet
remains underexplored in computer vision. Existing models predominantly rely on
2D object detection, which fails to capture the complexities of real-world
violations due to: (i) an oversimplified task formulation treating violation
recognition merely as object detection, (ii) inadequate validation under
realistic conditions, (iii) absence of standardized baselines, and (iv) limited
scalability from the unavailability of synthetic dataset generators for diverse
construction scenarios. To address these challenges, we introduce
Safe-Construct, the first framework that reformulates violation recognition as
a 3D multi-view engagement task, leveraging scene-level worker-object context
and 3D spatial understanding. We also propose the Synthetic Indoor Construction
Site Generator (SICSG) to create diverse, scalable training data, overcoming
data limitations. Safe-Construct achieves a 7.6% improvement over
state-of-the-art methods across four violation types. We rigorously evaluate
our approach in near-realistic settings, incorporating four violations, four
workers, 14 objects, and challenging conditions like occlusions (worker-object,
worker-worker) and variable illumination (back-lighting, overexposure,
sunlight). By integrating 3D multi-view spatial understanding and synthetic
data generation, Safe-Construct sets a new benchmark for scalable and robust
safety monitoring in high-risk industries. Project Website:
https://Safe-Construct.github.io/Safe-Construct

</details>

### [82] [Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models](https://arxiv.org/abs/2504.10883)
*Karan Jain,Mohammad Nayeem Teli*

Main category: cs.CV

TLDR: 提出了一种基于可逆UNet架构的单GPU内存高效扩散模型，用于高维医学图像生成，显著降低内存和能耗。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但计算资源需求高，尤其是3D医学数据集（如CT、MRI）的处理。

Method: 采用可逆UNet架构和可逆注意力模块，使内存使用与数据集维度无关。

Result: 在BraTS2020数据集上，峰值内存消耗降低15%，图像质量与SOTA相当。

Conclusion: 该模型适用于多种图像生成任务，特别适合资源受限的医学图像处理。

Abstract: Diffusion models have recently gained state of the art performance on many
image generation tasks. However, most models require significant computational
resources to achieve this. This becomes apparent in the application of medical
image synthesis due to the 3D nature of medical datasets like CT-scans, MRIs,
electron microscope, etc. In this paper we propose a novel architecture for a
single GPU memory-efficient training for diffusion models for high dimensional
medical datasets. The proposed model is built by using an invertible UNet
architecture with invertible attention modules. This leads to the following two
contributions: 1. denoising diffusion models and thus enabling memory usage to
be independent of the dimensionality of the dataset, and 2. reducing the energy
usage during training. While this new model can be applied to a multitude of
image generation tasks, we showcase its memory-efficiency on the 3D BraTS2020
dataset leading to up to 15\% decrease in peak memory consumption during
training with comparable results to SOTA while maintaining the image quality.

</details>

### [83] [PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving](https://arxiv.org/abs/2504.10885)
*Zeyu Zhang,Zijian Chen,Zicheng Zhang,Yuze Sun,Yuan Tian,Ziheng Jia,Chunyi Li,Xiaohong Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TLDR: 论文提出了一种动态多模态评估框架OVPG，用于自动生成多样且可验证的评估数据，解决了现有静态基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多为静态且与预训练数据重叠，导致复杂性固定和数据污染问题；人工标注数据集则存在耗时、偏见和一致性问题。

Method: OVPG框架包含原始材料采样、视觉内容生成和谜题规则设计模块，确保评估实例原始、随机且唯一可解。

Result: 基于OVPG构建了PuzzleBench，包含11,840个VQA样本，针对LMM的视觉识别、逻辑推理和上下文理解能力。

Conclusion: PuzzleBench通过动态生成和开放设计，能够持续适应LMM能力的演进，优于静态基准测试。

Abstract: Large Multimodal Models (LMMs) have demonstrated impressive capabilities
across a wide range of multimodal tasks, achieving ever-increasing performance
on various evaluation benchmarks. However, existing benchmarks are typically
static and often overlap with pre-training datasets, leading to fixed
complexity constraints and substantial data contamination issues. Meanwhile,
manually annotated datasets are labor-intensive, time-consuming, and subject to
human bias and inconsistency, leading to reliability and reproducibility
issues. To address these problems, we propose a fully dynamic multimodal
evaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which
aims to generate fresh, diverse, and verifiable evaluation data automatically
in puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw
material sampling module, a visual content generation module, and a puzzle rule
design module, which ensures that each evaluation instance is primitive, highly
randomized, and uniquely solvable, enabling continual adaptation to the
evolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a
dynamic and scalable benchmark comprising 11,840 VQA samples. It features six
carefully designed puzzle tasks targeting three core LMM competencies, visual
recognition, logical reasoning, and context understanding. PuzzleBench differs
from static benchmarks that quickly become outdated. It enables ongoing dataset
refreshing through OVPG and a rich set of open-ended puzzle designs, allowing
seamless adaptation to the evolving capabilities of LMMs.

</details>

### [84] [CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors](https://arxiv.org/abs/2504.10888)
*Jiahuan Long,Wen Yao,Tingsong Jiang,Chao Ma*

Main category: cs.CV

TLDR: CDUPatch是一种针对可见-红外双模态目标检测器的通用跨模态对抗补丁攻击方法，通过优化颜色分布和红外纹理，提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有双模态对抗补丁攻击在不同物理场景中效果有限，需改进跨模态攻击能力。

Method: 提出RGB-to-infrared适配器，统一优化跨模态补丁；引入多尺度裁剪策略和新数据集MSDrone。

Result: 在四个基准数据集上表现优于现有方法，物理测试显示强跨尺度、视角和场景的迁移性。

Conclusion: CDUPatch显著提升了对抗补丁的通用性和实际攻击效果。

Abstract: Adversarial patches are widely used to evaluate the robustness of object
detection systems in real-world scenarios. These patches were initially
designed to deceive single-modal detectors (e.g., visible or infrared) and have
recently been extended to target visible-infrared dual-modal detectors.
However, existing dual-modal adversarial patch attacks have limited attack
effectiveness across diverse physical scenarios. To address this, we propose
CDUPatch, a universal cross-modal patch attack against visible-infrared object
detectors across scales, views, and scenarios. Specifically, we observe that
color variations lead to different levels of thermal absorption, resulting in
temperature differences in infrared imaging. Leveraging this property, we
propose an RGB-to-infrared adapter that maps RGB patches to infrared patches,
enabling unified optimization of cross-modal patches. By learning an optimal
color distribution on the adversarial patch, we can manipulate its thermal
response and generate an adversarial infrared texture. Additionally, we
introduce a multi-scale clipping strategy and construct a new visible-infrared
dataset, MSDrone, which contains aerial vehicle images in varying scales and
perspectives. These data augmentation strategies enhance the robustness of our
patch in real-world conditions. Experiments on four benchmark datasets (e.g.,
DroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms
existing patch attacks in the digital domain. Extensive physical tests further
confirm strong transferability across scales, views, and scenarios.

</details>

### [85] [Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A Detailed Annotation Framework and Multi-Label Classification Model](https://arxiv.org/abs/2504.10889)
*Shripad Pate,Aiman Farooq,Suvrankar Dutta,Musadiq Aadil Sheikh,Atin Kumar,Deepak Mishra*

Main category: cs.CV

TLDR: 提出了一种新的肋骨骨折标注协议，并利用跨模态嵌入（结合影像和临床描述）提升分类性能，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏细粒度标注，特别是骨折特征、类型和解剖位置的精确标注，影响了治疗计划的制定。

Method: 引入新的骨折标注协议，并使用双曲嵌入将影像特征和临床描述映射到共享的非欧几里得流形，以捕捉骨折的层次结构。

Result: 在AirRib和RibFrac数据集上，平均召回率分别提升6%和17.5%。

Conclusion: 该方法通过跨模态嵌入和层次化建模，显著提升了肋骨骨折分类的准确性。

Abstract: Accurate rib fracture identification and classification are essential for
treatment planning. However, existing datasets often lack fine-grained
annotations, particularly regarding rib fracture characterization, type, and
precise anatomical location on individual ribs. To address this, we introduce a
novel rib fracture annotation protocol tailored for fracture classification.
Further, we enhance fracture classification by leveraging cross-modal
embeddings that bridge radiological images and clinical descriptions. Our
approach employs hyperbolic embeddings to capture the hierarchical nature of
fracture, mapping visual features and textual descriptions into a shared
non-Euclidean manifold. This framework enables more nuanced similarity
computations between imaging characteristics and clinical descriptions,
accounting for the inherent hierarchical relationships in fracture taxonomy.
Experimental results demonstrate that our approach outperforms existing methods
across multiple classification tasks, with average recall improvements of 6% on
the AirRib dataset and 17.5% on the public RibFrac dataset.

</details>

### [86] [InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation](https://arxiv.org/abs/2504.10905)
*Yukang Lin,Yan Hong,Zunnan Xu,Xindi Li,Chao Xu,Chuanbiao Song,Ronghui Li,Haoxing Chen,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang,Xiu Li*

Main category: cs.CV

TLDR: 该论文提出了一种新方法InterAnimate，用于生成逼真的手脸交互动画，并发布了大规模数据集InterHF。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成研究多关注孤立动作，而忽略了交互动作（如手脸交互），这对生物识别认证系统至关重要。

Method: 通过同时学习时空接触动力学和生物力学变形效果，提出区域感知扩散模型InterAnimate，结合可学习的时空潜在变量。

Result: InterAnimate生成了高度逼真的动画，并建立了新的基准。

Conclusion: 该研究首次系统研究了手脸交互，为相关领域提供了数据集和模型支持。

Abstract: Recent video generation research has focused heavily on isolated actions,
leaving interactive motions-such as hand-face interactions-largely unexamined.
These interactions are essential for emerging biometric authentication systems,
which rely on interactive motion-based anti-spoofing approaches. From a
security perspective, there is a growing need for large-scale, high-quality
interactive videos to train and strengthen authentication models. In this work,
we introduce a novel paradigm for animating realistic hand-face interactions.
Our approach simultaneously learns spatio-temporal contact dynamics and
biomechanically plausible deformation effects, enabling natural interactions
where hand movements induce anatomically accurate facial deformations while
maintaining collision-free contact. To facilitate this research, we present
InterHF, a large-scale hand-face interaction dataset featuring 18 interaction
patterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a
region-aware diffusion model designed specifically for interaction animation.
InterAnimate leverages learnable spatial and temporal latents to effectively
capture dynamic interaction priors and integrates a region-aware interaction
mechanism that injects these priors into the denoising process. To the best of
our knowledge, this work represents the first large-scale effort to
systematically study human hand-face interactions. Qualitative and quantitative
results show InterAnimate produces highly realistic animations, setting a new
benchmark. Code and data will be made public to advance research.

</details>

### [87] [Towards Efficient Partially Relevant Video Retrieval with Active Moment Discovering](https://arxiv.org/abs/2504.10920)
*Peipei Song,Long Zhang,Long Lan,Weidong Chen,Dan Guo,Xun Yang,Meng Wang*

Main category: cs.CV

TLDR: 本文提出了一种名为AMDNet的方法，用于解决部分相关视频检索（PRVR）任务中的内容独立性和信息冗余问题，通过主动发现语义一致的视频片段，并结合多样性和相关性损失优化检索性能。


<details>
  <summary>Details</summary>
Motivation: PRVR任务中，未修剪视频包含大量背景内容，现有方法多尺度建模存在内容独立性和信息冗余问题，影响检索效果。

Method: 提出AMDNet，使用可学习的片段锚点捕捉关键片段，结合掩码多片段注意力机制和多样性、相关性损失优化表示。

Result: 在TVR和ActivityNet Captions数据集上表现优越，AMDNet参数量减少15.5倍，SumR指标提升6.0分。

Conclusion: AMDNet通过主动发现关键片段和优化损失函数，显著提升了PRVR任务的效率和性能。

Abstract: Partially relevant video retrieval (PRVR) is a practical yet challenging task
in text-to-video retrieval, where videos are untrimmed and contain much
background content. The pursuit here is of both effective and efficient
solutions to capture the partial correspondence between text queries and
untrimmed videos. Existing PRVR methods, which typically focus on modeling
multi-scale clip representations, however, suffer from content independence and
information redundancy, impairing retrieval performance. To overcome these
limitations, we propose a simple yet effective approach with active moment
discovering (AMDNet). We are committed to discovering video moments that are
semantically consistent with their queries. By using learnable span anchors to
capture distinct moments and applying masked multi-moment attention to
emphasize salient moments while suppressing redundant backgrounds, we achieve
more compact and informative video representations. To further enhance moment
modeling, we introduce a moment diversity loss to encourage different moments
of distinct regions and a moment relevance loss to promote semantically
query-relevant moments, which cooperate with a partially relevant retrieval
loss for end-to-end optimization. Extensive experiments on two large-scale
video datasets (\ie, TVR and ActivityNet Captions) demonstrate the superiority
and efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller
(\#parameters) while 6.0 points higher (SumR) than the up-to-date method
GMMFormer on TVR.

</details>

### [88] [Cross-Frequency Implicit Neural Representation with Self-Evolving Parameters](https://arxiv.org/abs/2504.10929)
*Chang Yu,Yisi Luo,Kai Ye,Xile Zhao,Deyu Meng*

Main category: cs.CV

TLDR: 提出了一种基于Haar小波变换的自进化跨频率隐式神经表示（CF-INR），通过分解数据为四个频率分量并自动优化参数，显著提升了视觉数据表示的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统INR方法在原始空间中混合不同频率分量，且需手动配置参数（如频率参数ω或秩R），限制了其效率和精度。

Method: 使用Haar小波变换将数据分解为四个频率分量，提出跨频率张量分解范式，通过自进化优化自动更新参数。

Result: 在图像回归、修复、去噪和云去除等任务中，CF-INR表现优于现有方法。

Conclusion: CF-INR通过自进化参数优化和跨频率分解，显著提升了视觉数据表示的精度和效率。

Abstract: Implicit neural representation (INR) has emerged as a powerful paradigm for
visual data representation. However, classical INR methods represent data in
the original space mixed with different frequency components, and several
feature encoding parameters (e.g., the frequency parameter $\omega$ or the rank
$R$) need manual configurations. In this work, we propose a self-evolving
cross-frequency INR using the Haar wavelet transform (termed CF-INR), which
decouples data into four frequency components and employs INRs in the wavelet
space. CF-INR allows the characterization of different frequency components
separately, thus enabling higher accuracy for data representation. To more
precisely characterize cross-frequency components, we propose a cross-frequency
tensor decomposition paradigm for CF-INR with self-evolving parameters, which
automatically updates the rank parameter $R$ and the frequency parameter
$\omega$ for each frequency component through self-evolving optimization. This
self-evolution paradigm eliminates the laborious manual tuning of these
parameters, and learns a customized cross-frequency feature encoding
configuration for each dataset. We evaluate CF-INR on a variety of visual data
representation and recovery tasks, including image regression, inpainting,
denoising, and cloud removal. Extensive experiments demonstrate that CF-INR
outperforms state-of-the-art methods in each case.

</details>

### [89] [Recognition of Geometrical Shapes by Dictionary Learning](https://arxiv.org/abs/2504.10958)
*Alexander Köhler,Michael Breuß*

Main category: cs.CV

TLDR: 本文探讨了字典学习在几何形状识别中的应用，并展示了优化方法对识别质量的重要影响。


<details>
  <summary>Details</summary>
Motivation: 字典学习在图像重建等任务中表现出强大的表示能力，但尚未广泛应用于形状识别领域。本文旨在探索其在几何形状识别中的潜力。

Method: 采用字典学习方法，通过优化算法生成过完备的原子集来表示输入形状。

Result: 实验结果表明，字典学习在形状识别任务中具有潜力，且优化方法的选择对识别质量有显著影响。

Conclusion: 字典学习是一种有前景的形状识别方法，未来可进一步研究其优化和应用。

Abstract: Dictionary learning is a versatile method to produce an overcomplete set of
vectors, called atoms, to represent a given input with only a few atoms. In the
literature, it has been used primarily for tasks that explore its powerful
representation capabilities, such as for image reconstruction. In this work, we
present a first approach to make dictionary learning work for shape
recognition, considering specifically geometrical shapes. As we demonstrate,
the choice of the underlying optimization method has a significant impact on
recognition quality. Experimental results confirm that dictionary learning may
be an interesting method for shape recognition tasks.

</details>

### [90] [An Efficient and Mixed Heterogeneous Model for Image Restoration](https://arxiv.org/abs/2504.10967)
*Yubin Gu,Yuan Meng,Kaihang Zheng,Xiaoshuai Sun,Jiayi Ji,Weijian Ruan,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TLDR: RestorMixer是一种基于混合架构融合的高效通用图像恢复模型，结合了CNN、Mamba和注意力机制的优势，在多任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前图像恢复模型多为单一架构，难以同时处理多样化的退化类型，因此需要一种能整合不同架构优势的通用模型。

Method: RestorMixer采用三阶段编码器-解码器结构，分别利用CNN提取局部特征、Mamba建模全局上下文、注意力机制动态优化特征。

Result: 实验表明，RestorMixer在多个图像恢复任务中性能领先且推理效率高。

Conclusion: RestorMixer通过混合架构设计有效整合了不同模型的优势，为通用图像恢复提供了高效解决方案。

Abstract: Image restoration~(IR), as a fundamental multimedia data processing task, has
a significant impact on downstream visual applications. In recent years,
researchers have focused on developing general-purpose IR models capable of
handling diverse degradation types, thereby reducing the cost and complexity of
model development. Current mainstream approaches are based on three
architectural paradigms: CNNs, Transformers, and Mambas. CNNs excel in
efficient inference, whereas Transformers and Mamba excel at capturing
long-range dependencies and modeling global contexts. While each architecture
has demonstrated success in specialized, single-task settings, limited efforts
have been made to effectively integrate heterogeneous architectures to jointly
address diverse IR challenges. To bridge this gap, we propose RestorMixer, an
efficient and general-purpose IR model based on mixed-architecture fusion.
RestorMixer adopts a three-stage encoder-decoder structure, where each stage is
tailored to the resolution and feature characteristics of the input. In the
initial high-resolution stage, CNN-based blocks are employed to rapidly extract
shallow local features. In the subsequent stages, we integrate a refined
multi-directional scanning Mamba module with a multi-scale window-based
self-attention mechanism. This hierarchical and adaptive design enables the
model to leverage the strengths of CNNs in local feature extraction, Mamba in
global context modeling, and attention mechanisms in dynamic feature
refinement. Extensive experimental results demonstrate that RestorMixer
achieves leading performance across multiple IR tasks while maintaining high
inference efficiency. The official code can be accessed at
https://github.com/ClimBin/RestorMixer.

</details>

### [91] [AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images](https://arxiv.org/abs/2504.10972)
*Yihang Liu,Lianghua He,Ying Wen,Longzhen Yang,Hongzhou Chen*

Main category: cs.CV

TLDR: AFiRe提出了一种基于解剖学一致性的自监督框架，通过Token-wise对比学习和像素级异常修复，提升放射影像分析的细粒度表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督方法（如对比学习）忽视了解剖学细节，而这对放射影像分析至关重要。AFiRe旨在通过解剖学一致性增强细粒度表征。

Method: AFiRe结合两种自监督方案：(1) Token-wise解剖学引导对比学习，增强空间-解剖学区分；(2) 像素级异常修复，优化几何细节。还提出合成病变掩膜以增强解剖多样性。

Result: AFiRe在解剖区分、泛化能力和细粒度信息整合方面优于现有方法，尤其在有限标注的多标签分类任务中表现突出。

Conclusion: AFiRe通过解剖学驱动的自监督学习，显著提升了放射影像分析的细粒度表征和泛化能力。

Abstract: Current self-supervised methods, such as contrastive learning, predominantly
focus on global discrimination, neglecting the critical fine-grained anatomical
details required for accurate radiographic analysis. To address this challenge,
we propose an Anatomy-driven self-supervised framework for enhancing
Fine-grained Representation in radiographic image analysis (AFiRe). The core
idea of AFiRe is to align the anatomical consistency with the unique
token-processing characteristics of Vision Transformer. Specifically, AFiRe
synergistically performs two self-supervised schemes: (i) Token-wise
anatomy-guided contrastive learning, which aligns image tokens based on
structural and categorical consistency, thereby enhancing fine-grained
spatial-anatomical discrimination; (ii) Pixel-level anomaly-removal
restoration, which particularly focuses on local anomalies, thereby refining
the learned discrimination with detailed geometrical information. Additionally,
we propose Synthetic Lesion Mask to enhance anatomical diversity while
preserving intra-consistency, which is typically corrupted by traditional data
augmentations, such as Cropping and Affine transformations. Experimental
results show that AFiRe: (i) provides robust anatomical discrimination,
achieving more cohesive feature clusters compared to state-of-the-art
contrastive learning methods; (ii) demonstrates superior generalization,
surpassing 7 radiography-specific self-supervised methods in multi-label
classification tasks with limited labeling; and (iii) integrates fine-grained
information, enabling precise anomaly detection using only image-level
annotations.

</details>

### [92] [Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion](https://arxiv.org/abs/2504.10974)
*Zhisheng Zhang,Peng Zhang,Fengxiang Wang,Liangli Ma,Fuchun Sun*

Main category: cs.CV

TLDR: 提出一种特征空间变换方法，结合自监督多帧融合策略，显著提升前视声纳图像质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖模拟数据，真实数据稀缺且跨模态退化问题严重，限制了实际应用。

Method: 通过特征空间变换将声纳图像映射到鲁棒特征域，并采用自监督多帧融合策略去除噪声和增强亮度。

Result: 在三个真实数据集上显著优于现有方法，有效抑制噪声、保留细节并提升亮度。

Conclusion: 该方法在提升水下目标检测性能方面具有强大潜力。

Abstract: Enhancing forward-looking sonar images is critical for accurate underwater
target detection. Current deep learning methods mainly rely on supervised
training with simulated data, but the difficulty in obtaining high-quality
real-world paired data limits their practical use and generalization. Although
self-supervised approaches from remote sensing partially alleviate data
shortages, they neglect the cross-modal degradation gap between sonar and
remote sensing images. Directly transferring pretrained weights often leads to
overly smooth sonar images, detail loss, and insufficient brightness. To
address this, we propose a feature-space transformation that maps sonar images
from the pixel domain to a robust feature domain, effectively bridging the
degradation gap. Additionally, our self-supervised multi-frame fusion strategy
leverages complementary inter-frame information to naturally remove speckle
noise and enhance target-region brightness. Experiments on three self-collected
real-world forward-looking sonar datasets show that our method significantly
outperforms existing approaches, effectively suppressing noise, preserving
detailed edges, and substantially improving brightness, demonstrating strong
potential for underwater target detection applications.

</details>

### [93] [Adaptive Decision Boundary for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2504.10976)
*Linhao Li,Yongzhang Tan,Siyuan Yang,Hao Cheng,Yongfeng Dong,Liang Yang*

Main category: cs.CV

TLDR: 论文提出了一种自适应决策边界策略（ADBS），用于解决Few-Shot Class-Incremental Learning（FSCIL）中决策空间优化的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统FSCIL方法仅关注防止灾难性遗忘，忽略了每个类的具体决策空间。ADBS旨在通过动态调整决策边界和优化类间区分度来解决这一问题。

Method: ADBS为每个类分配特定决策边界，并在训练中动态调整。同时，引入类间约束损失以增强类间区分度。

Result: 在CIFAR100、miniImageNet和CUB200三个基准测试中，ADBS结合现有FSCIL方法取得了最优性能。

Conclusion: ADBS是一种即插即用的策略，显著提升了FSCIL的性能，为类增量学习提供了新的优化方向。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new
classes from a limited set of training samples without forgetting knowledge of
previously learned classes. Conventional FSCIL methods typically build a robust
feature extractor during the base training session with abundant training
samples and subsequently freeze this extractor, only fine-tuning the classifier
in subsequent incremental phases. However, current strategies primarily focus
on preventing catastrophic forgetting, considering only the relationship
between novel and base classes, without paying attention to the specific
decision spaces of each class. To address this challenge, we propose a
plug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible
with most FSCIL methods. Specifically, we assign a specific decision boundary
to each class and adaptively adjust these boundaries during training to
optimally refine the decision spaces for the classes in each session.
Furthermore, to amplify the distinctiveness between classes, we employ a novel
inter-class constraint loss that optimizes the decision boundaries and
prototypes for each class. Extensive experiments on three benchmarks, namely
CIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS
method with existing FSCIL techniques significantly improves performance,
achieving overall state-of-the-art results.

</details>

### [94] [Deep Learning in Concealed Dense Prediction](https://arxiv.org/abs/2504.10979)
*Pancheng Zhao,Deng-Ping Fan,Shupeng Cheng,Salman Khan,Fahad Shahbaz Khan,David Clifton,Peng Xu,Jufeng Yang*

Main category: cs.CV

TLDR: 论文介绍了隐蔽密集预测（CDP）任务，分析了其特点、挑战及与通用视觉任务的区别，总结了深度学习在CDP中的努力，并探讨了其在大模型时代的应用和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，需要关注更复杂的视觉任务，CDP因其隐蔽目标的特性具有重要价值。

Method: 通过实验对三种CDP任务进行分析，比较了25种先进方法在12个隐蔽数据集上的表现，并提出了分类法。

Result: 总结了CDP的挑战和特点，构建了CvpINST数据集和CvpAgent，提出了6个潜在研究方向。

Conclusion: CDP在未来有广阔的应用前景，需进一步研究其在大模型时代的发展方向。

Abstract: Deep learning is developing rapidly and handling common computer vision tasks
well. It is time to pay attention to more complex vision tasks, as model size,
knowledge, and reasoning capabilities continue to improve. In this paper, we
introduce and review a family of complex tasks, termed Concealed Dense
Prediction (CDP), which has great value in agriculture, industry, etc. CDP's
intrinsic trait is that the targets are concealed in their surroundings, thus
fully perceiving them requires fine-grained representations, prior knowledge,
auxiliary reasoning, etc. The contributions of this review are three-fold: (i)
We introduce the scope, characteristics, and challenges specific to CDP tasks
and emphasize their essential differences from generic vision tasks. (ii) We
develop a taxonomy based on concealment counteracting to summarize deep
learning efforts in CDP through experiments on three tasks. We compare 25
state-of-the-art methods across 12 widely used concealed datasets. (iii) We
discuss the potential applications of CDP in the large model era and summarize
6 potential research directions. We offer perspectives for the future
development of CDP by constructing a large-scale multimodal instruction
fine-tuning dataset, CvpINST, and a concealed visual perception agent,
CvpAgent.

</details>

### [95] [Seeing like a Cephalopod: Colour Vision with a Monochrome Event Camera](https://arxiv.org/abs/2504.10984)
*Sami Arja,Nimrod Kruger,Alexandre Marcireau,Nicholas Owen Ralph,Saeed Afshar,Gregory Cohen*

Main category: cs.CV

TLDR: 受头足类动物视觉机制启发，设计了一种结合球透镜和事件相机的光谱成像系统，实现了无需传统滤色片的波长依赖性聚焦。


<details>
  <summary>Details</summary>
Motivation: 头足类动物仅有一种光感受器却能辨别颜色，其机制为通过瞳孔形状和光学色差感知光谱信息。这一生物机制为设计新型光谱成像系统提供了灵感。

Method: 采用球透镜与事件相机结合，通过电机系统调整焦距，模拟头足类动物的自适应透镜运动，实现可见光和近红外光谱的波长依赖性聚焦。

Result: 系统成功实现了光谱分辨，验证了生物启发的光谱辨别在仿真和实际设置中的有效性，且无需传统滤色片或计算去马赛克。

Conclusion: 该研究为基于自然进化解决方案的新型光谱传感系统开辟了新途径。

Abstract: Cephalopods exhibit unique colour discrimination capabilities despite having
one type of photoreceptor, relying instead on chromatic aberration induced by
their ocular optics and pupil shapes to perceive spectral information. We took
inspiration from this biological mechanism to design a spectral imaging system
that combines a ball lens with an event-based camera. Our approach relies on a
motorised system that shifts the focal position, mirroring the adaptive lens
motion in cephalopods. This approach has enabled us to achieve
wavelength-dependent focusing across the visible light and near-infrared
spectrum, making the event a spectral sensor. We characterise chromatic
aberration effects, using both event-based and conventional frame-based
sensors, validating the effectiveness of bio-inspired spectral discrimination
both in simulation and in a real setup as well as assessing the spectral
discrimination performance. Our proposed approach provides a robust spectral
sensing capability without conventional colour filters or computational
demosaicing. This approach opens new pathways toward new spectral sensing
systems inspired by nature's evolutionary solutions. Code and analysis are
available at: https://samiarja.github.io/neuromorphic_octopus_eye/

</details>

### [96] [DMPT: Decoupled Modality-aware Prompt Tuning for Multi-modal Object Re-identification](https://arxiv.org/abs/2504.10985)
*Minghui Lin,Shu Wang,Xiang Wang,Jianhua Tang,Longbin Fu,Zhengrong Zuo,Nong Sang*

Main category: cs.CV

TLDR: 提出了一种高效的提示调优框架DMPT，用于多模态对象重识别，通过冻结主干网络并优化少量新参数，显著减少计算和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模预训练主干网络的多模态对象重识别方法需要优化大量参数，导致计算和存储成本高。

Method: DMPT框架通过解耦模态特定提示和模态无关语义提示，并设计Prompt Inverse Bind策略，促进多模态信息互补。

Result: 在多个基准测试中，DMPT仅需优化6.5%的主干参数即可达到与现有最先进方法竞争的结果。

Conclusion: DMPT是一种高效的多模态对象重识别方法，显著降低了计算和存储需求，同时保持了高性能。

Abstract: Current multi-modal object re-identification approaches based on large-scale
pre-trained backbones (i.e., ViT) have displayed remarkable progress and
achieved excellent performance. However, these methods usually adopt the
standard full fine-tuning paradigm, which requires the optimization of
considerable backbone parameters, causing extensive computational and storage
requirements. In this work, we propose an efficient prompt-tuning framework
tailored for multi-modal object re-identification, dubbed DMPT, which freezes
the main backbone and only optimizes several newly added decoupled
modality-aware parameters. Specifically, we explicitly decouple the visual
prompts into modality-specific prompts which leverage prior modality knowledge
from a powerful text encoder and modality-independent semantic prompts which
extract semantic information from multi-modal inputs, such as visible,
near-infrared, and thermal-infrared. Built upon the extracted features, we
further design a Prompt Inverse Bind (PromptIBind) strategy that employs bind
prompts as a medium to connect the semantic prompt tokens of different
modalities and facilitates the exchange of complementary multi-modal
information, boosting final re-identification results. Experimental results on
multiple common benchmarks demonstrate that our DMPT can achieve competitive
results to existing state-of-the-art methods while requiring only 6.5%
fine-tuning of the backbone parameters.

</details>

### [97] [PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation](https://arxiv.org/abs/2504.10986)
*Bo-Cheng Hu,Ge-Peng Ji,Dian Shao,Deng-Ping Fan*

Main category: cs.CV

TLDR: PraNet-V2通过引入Dual-Supervised Reverse Attention (DSRA)模块，解决了PraNet-V1在多类分割任务中的不足，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: PraNet-V1在多类分割任务中表现不佳，需要改进以支持更广泛的分割任务。

Method: 提出PraNet-V2，核心是DSRA模块，结合了显式背景监督、独立背景建模和语义增强的注意力融合。

Result: 在四个息肉分割数据集上表现优异，并在三种先进语义分割模型中实现了1.36%的Dice分数提升。

Conclusion: PraNet-V2在多类分割任务中表现优越，具有广泛的应用潜力。

Abstract: Accurate medical image segmentation is essential for effective diagnosis and
treatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by
introducing a reverse attention (RA) module that utilizes background
information. However, PraNet-V1 struggles with multi-class segmentation tasks.
To address this limitation, we propose PraNet-V2, which, compared to PraNet-V1,
effectively performs a broader range of tasks including multi-class
segmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention
(DSRA) module, which incorporates explicit background supervision, independent
background modeling, and semantically enriched attention fusion. Our PraNet-V2
framework demonstrates strong performance on four polyp segmentation datasets.
Additionally, by integrating DSRA to iteratively enhance foreground
segmentation results in three state-of-the-art semantic segmentation models, we
achieve up to a 1.36% improvement in mean Dice score. Code is available at:
https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor.

</details>

### [98] [TMCIR: Token Merge Benefits Composed Image Retrieval](https://arxiv.org/abs/2504.10995)
*Chaoyang Wang,Zeyu Zhang,Long Teng,Zijun Li,Shichao Kan*

Main category: cs.CV

TLDR: TMCIR提出了一种新框架，通过意图感知跨模态对齐和自适应令牌融合，解决了组合图像检索中视觉和文本信息融合不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 当前组合图像检索方法在视觉和文本信息融合上存在偏差，无法准确捕捉用户搜索意图。

Method: 1) 意图感知跨模态对齐：通过扩散模型生成伪目标图像，对比微调CLIP编码器；2) 自适应令牌融合：动态平衡视觉和文本表示。

Result: 在Fashion-IQ和CIRR数据集上，TMCIR显著优于现有方法。

Conclusion: TMCIR通过改进意图捕捉和动态融合，提升了组合图像检索的准确性。

Abstract: Composed Image Retrieval (CIR) retrieves target images using a multi-modal
query that combines a reference image with text describing desired
modifications. The primary challenge is effectively fusing this visual and
textual information. Current cross-modal feature fusion approaches for CIR
exhibit an inherent bias in intention interpretation. These methods tend to
disproportionately emphasize either the reference image features
(visual-dominant fusion) or the textual modification intent (text-dominant
fusion through image-to-text conversion). Such an imbalanced representation
often fails to accurately capture and reflect the actual search intent of the
user in the retrieval results. To address this challenge, we propose TMCIR, a
novel framework that advances composed image retrieval through two key
innovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP
encoders contrastively using intent-reflecting pseudo-target images,
synthesized from reference images and textual descriptions via a diffusion
model. This step enhances the encoder ability of text to capture nuanced
intents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune
all encoders contrastively by comparing adaptive token-fusion features with the
target image. This mechanism dynamically balances visual and textual
representations within the contrastive learning pipeline, optimizing the
composed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR
datasets demonstrate that TMCIR significantly outperforms state-of-the-art
methods, particularly in capturing nuanced user intent.

</details>

### [99] [MediSee: Reasoning-based Pixel-level Perception in Medical Images](https://arxiv.org/abs/2504.11008)
*Qinyue Tong,Ziqian Lu,Jun Liu,Yangming Zheng,Zheming Lu*

Main category: cs.CV

TLDR: 论文提出了一种新的医学视觉任务MedSD，旨在通过逻辑推理理解医学图像的隐式查询，并生成相应的分割掩码和边界框。作者还提出了一个数据集MLMR-SD和基线模型MediSee，实验表明该方法优于传统医学参考分割方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像感知方法依赖特定任务或精确输入提示（如边界框或文本标签），限制了通用性。普通用户更倾向于使用需要逻辑推理的口头查询，因此需要一种能理解隐式查询的方法。

Method: 提出了MedSD任务，并构建了MLMR-SD数据集，包含大量医学实体目标及其推理。设计了基线模型MediSee，用于医学推理分割和检测。

Result: 实验结果表明，MediSee能有效处理隐式口头查询的MedSD任务，并优于传统医学参考分割方法。

Conclusion: MedSD任务和MediSee模型为医学图像理解提供了更通用的解决方案，尤其适合普通用户的使用场景。

Abstract: Despite remarkable advancements in pixel-level medical image perception,
existing methods are either limited to specific tasks or heavily rely on
accurate bounding boxes or text labels as input prompts. However, the medical
knowledge required for input is a huge obstacle for general public, which
greatly reduces the universality of these methods. Compared with these
domain-specialized auxiliary information, general users tend to rely on oral
queries that require logical reasoning. In this paper, we introduce a novel
medical vision task: Medical Reasoning Segmentation and Detection (MedSD),
which aims to comprehend implicit queries about medical images and generate the
corresponding segmentation mask and bounding box for the target object. To
accomplish this task, we first introduce a Multi-perspective, Logic-driven
Medical Reasoning Segmentation and Detection (MLMR-SD) dataset, which
encompasses a substantial collection of medical entity targets along with their
corresponding reasoning. Furthermore, we propose MediSee, an effective baseline
model designed for medical reasoning segmentation and detection. The
experimental results indicate that the proposed method can effectively address
MedSD with implicit colloquial queries and outperform traditional medical
referring segmentation methods.

</details>

### [100] [GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*](https://arxiv.org/abs/2504.11014)
*Eunsoo Im,Jung Kwon Lee,Changhyun Jee*

Main category: cs.CV

TLDR: GATE3D是一个新颖的弱监督框架，用于通用单目3D目标检测，通过2D和3D预测的一致性损失解决多领域训练中的挑战，并在KITTI和室内数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决单目3D目标检测在多领域训练中因标注数据稀缺和数据集偏差导致的泛化能力不足问题。

Method: 提出GATE3D框架，利用伪标签和一致性损失（2D与3D预测之间）进行弱监督训练。

Result: 在KITTI和自建室内数据集上表现优异，显著加速了有限标注数据的学习。

Conclusion: GATE3D展示了在机器人、增强现实和虚拟现实等领域的广泛应用潜力。

Abstract: The emerging trend in computer vision emphasizes developing universal models
capable of simultaneously addressing multiple diverse tasks. Such universality
typically requires joint training across multi-domain datasets to ensure
effective generalization. However, monocular 3D object detection presents
unique challenges in multi-domain training due to the scarcity of datasets
annotated with accurate 3D ground-truth labels, especially beyond typical
road-based autonomous driving contexts. To address this challenge, we introduce
a novel weakly supervised framework leveraging pseudo-labels. Current
pretrained models often struggle to accurately detect pedestrians in non-road
environments due to inherent dataset biases. Unlike generalized image-based 2D
object detection models, achieving similar generalization in monocular 3D
detection remains largely unexplored. In this paper, we propose GATE3D, a novel
framework designed specifically for generalized monocular 3D object detection
via weak supervision. GATE3D effectively bridges domain gaps by employing
consistency losses between 2D and 3D predictions. Remarkably, our model
achieves competitive performance on the KITTI benchmark as well as on an
indoor-office dataset collected by us to evaluate the generalization
capabilities of our framework. Our results demonstrate that GATE3D
significantly accelerates learning from limited annotated data through
effective pre-training strategies, highlighting substantial potential for
broader impacts in robotics, augmented reality, and virtual reality
applications. Project page: https://ies0411.github.io/GATE3D/

</details>

### [101] [AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era](https://arxiv.org/abs/2504.11015)
*Chenyang Zhu,Xing Zhang,Yuyang Sun,Ching-Chun Chang,Isao Echizen*

Main category: cs.CV

TLDR: 论文提出AnimeDL-2M，首个针对动漫图像的大规模伪造检测与定位基准，并开发了AniXplore模型以解决动漫与自然图像的领域差异。


<details>
  <summary>Details</summary>
Motivation: 动漫领域因AI生成伪造图像而面临版权和内容篡改威胁，但现有研究多集中于自然图像，动漫领域研究不足。

Method: 构建包含200万张真实、部分伪造和全AI生成图像的AnimeDL-2M数据集，并提出针对动漫视觉特性的AniXplore模型。

Result: 实验显示现有自然图像检测模型在动漫领域表现不佳，而AniXplore显著优于现有方法。

Conclusion: AnimeDL-2M和AniXplore填补了动漫伪造检测的空白，为社区和行业提供了有效工具。

Abstract: Recent advances in image generation, particularly diffusion models, have
significantly lowered the barrier for creating sophisticated forgeries, making
image manipulation detection and localization (IMDL) increasingly challenging.
While prior work in IMDL has focused largely on natural images, the anime
domain remains underexplored-despite its growing vulnerability to AI-generated
forgeries. Misrepresentations of AI-generated images as hand-drawn artwork,
copyright violations, and inappropriate content modifications pose serious
threats to the anime community and industry. To address this gap, we propose
AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive
annotations. It comprises over two million images including real, partially
manipulated, and fully AI-generated samples. Experiments indicate that models
trained on existing IMDL datasets of natural images perform poorly when applied
to anime images, highlighting a clear domain gap between anime and natural
images. To better handle IMDL tasks in anime domain, we further propose
AniXplore, a novel model tailored to the visual characteristics of anime
imagery. Extensive evaluations demonstrate that AniXplore achieves superior
performance compared to existing methods. Dataset and code can be found in
https://flytweety.github.io/AnimeDL2M/.

</details>

### [102] [DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen](https://arxiv.org/abs/2504.11019)
*Hyejin Lee,Seokjun Hong,Jeonghoon Song,Haechan Cho,Zhixiong Jin,Byeonghun Kim,Joobin Jin,Jaegyun Im,Byeongjoon Noh,Hwasoo Yeo*

Main category: cs.CV

TLDR: DRIFT数据集通过无人机视频同步采集，提供高分辨率车辆轨迹数据，支持多尺度交通分析。


<details>
  <summary>Details</summary>
Motivation: 可靠的交通数据对城市交通管理和研究至关重要，但现有数据集难以满足多尺度分析需求。

Method: 利用无人机在约250米高度同步拍摄视频，通过视频同步和正射地图对齐处理，生成81,699条车辆轨迹。

Result: DRIFT数据集包含高分辨率车辆轨迹，支持从微观到宏观的交通分析，并提供开源工具。

Conclusion: DRIFT数据集为学术研究和实际应用提供了重要资源，公开可访问。

Abstract: Reliable traffic data are essential for understanding urban mobility and
developing effective traffic management strategies. This study introduces the
DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale
urban traffic dataset collected systematically from synchronized drone videos
at approximately 250 meters altitude, covering nine interconnected
intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle
trajectories that include directional information, processed through video
synchronization and orthomap alignment, resulting in a comprehensive dataset of
81,699 vehicle trajectories. Through our DRIFT dataset, researchers can
simultaneously analyze traffic at multiple scales - from individual vehicle
maneuvers like lane-changes and safety metrics such as time-to-collision to
aggregate network flow dynamics across interconnected urban intersections. The
DRIFT dataset is structured to enable immediate use without additional
preprocessing, complemented by open-source models for object detection and
trajectory extraction, as well as associated analytical tools. DRIFT is
expected to significantly contribute to academic research and practical
applications, such as traffic flow analysis and simulation studies. The dataset
and related resources are publicly accessible at
https://github.com/AIxMobility/The-DRIFT.

</details>

### [103] [Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation](https://arxiv.org/abs/2504.11024)
*Andrea Simonelli,Norman Müller,Peter Kontschieder*

Main category: cs.CV

TLDR: 本文提出了一种高效的3D交互式分割方法，通过结合体素稀疏编码器和轻量级Transformer解码器，显著提升了在已知和未知数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 随着数字3D环境的普及，对高效、精确且适应性强的3D交互分割方法的需求日益增长。

Method: 采用体素稀疏编码器和轻量级Transformer解码器，实现隐式点击融合，优化性能与效率。

Result: 在ScanNet、ScanNet++、S3DIS和KITTI-360等数据集上表现优异，且在未见过的几何分布（如高斯泼溅）中也有显著改进。

Conclusion: 该方法在3D交互分割任务中超越了现有技术，适用于多样化的环境和对象。

Abstract: The increasing availability of digital 3D environments, whether through
image-based 3D reconstruction, generation, or scans obtained by robots, is
driving innovation across various applications. These come with a significant
demand for 3D interaction, such as 3D Interactive Segmentation, which is useful
for tasks like object selection and manipulation. Additionally, there is a
persistent need for solutions that are efficient, precise, and performing well
across diverse settings, particularly in unseen environments and with
unfamiliar objects. In this work, we introduce a 3D interactive segmentation
method that consistently surpasses previous state-of-the-art techniques on both
in-domain and out-of-domain datasets. Our simple approach integrates a
voxel-based sparse encoder with a lightweight transformer-based decoder that
implements implicit click fusion, achieving superior performance and maximizing
efficiency. Our method demonstrates substantial improvements on benchmark
datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on
unseen geometric distributions such as the ones obtained by Gaussian Splatting.
The project web-page is available at https://simonelli-andrea.github.io/easy3d.

</details>

### [104] [TADACap: Time-series Adaptive Domain-Aware Captioning](https://arxiv.org/abs/2504.11441)
*Elizabeth Fons,Rachneet Kaur,Zhen Zeng,Soham Palande,Tucker Balch,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CV

TLDR: TADACap是一个基于检索的框架，用于为时间序列图像生成领域感知的标题，无需重新训练即可适应新领域。其改进版TADACap-diverse通过检索多样化的图像-标题对，显著减少了标注需求，同时保持了语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列标题生成方法通常提供通用描述，难以适应新领域且需要大量重新训练。

Method: 提出TADACap框架，利用检索策略生成领域感知标题；进一步提出TADACap-diverse，从目标领域数据库中检索多样化图像-标题对。

Result: TADACap-diverse在语义准确性上与现有方法相当，但标注需求显著减少。

Conclusion: TADACap-diverse为时间序列图像标题生成提供了一种高效且适应性强的解决方案。

Abstract: While image captioning has gained significant attention, the potential of
captioning time-series images, prevalent in areas like finance and healthcare,
remains largely untapped. Existing time-series captioning methods typically
offer generic, domain-agnostic descriptions of time-series shapes and struggle
to adapt to new domains without substantial retraining. To address these
limitations, we introduce TADACap, a retrieval-based framework to generate
domain-aware captions for time-series images, capable of adapting to new
domains without retraining. Building on TADACap, we propose a novel retrieval
strategy that retrieves diverse image-caption pairs from a target domain
database, namely TADACap-diverse. We benchmarked TADACap-diverse against
state-of-the-art methods and ablation variants. TADACap-diverse demonstrates
comparable semantic accuracy while requiring significantly less annotation
effort.

</details>

### [105] [Defending Against Frequency-Based Attacks with Diffusion Models](https://arxiv.org/abs/2504.11034)
*Fatemeh Amerehi,Patrick Healy*

Main category: cs.CV

TLDR: 论文探讨了对抗性净化的有效性，特别是扩散模型在应对光谱和空间对抗攻击时的表现。


<details>
  <summary>Details</summary>
Motivation: 对抗训练通常局限于特定攻击类型，而对抗性净化通过独立训练的生成模型，能够更好地应对未见过的攻击场景。

Method: 研究利用扩散模型进行对抗性净化，测试其在光谱和空间对抗攻击中的表现。

Result: 研究发现净化方法能有效处理从低频到高频区域的多种失真模式。

Conclusion: 对抗性净化，尤其是扩散模型，展现出在多样化对抗攻击中的广泛适用性。

Abstract: Adversarial training is a common strategy for enhancing model robustness
against adversarial attacks. However, it is typically tailored to the specific
attack types it is trained on, limiting its ability to generalize to unseen
threat models. Adversarial purification offers an alternative by leveraging a
generative model to remove perturbations before classification. Since the
purifier is trained independently of both the classifier and the threat models,
it is better equipped to handle previously unseen attack scenarios. Diffusion
models have proven highly effective for noise purification, not only in
countering pixel-wise adversarial perturbations but also in addressing
non-adversarial data shifts. In this study, we broaden the focus beyond
pixel-wise robustness to explore the extent to which purification can mitigate
both spectral and spatial adversarial attacks. Our findings highlight its
effectiveness in handling diverse distortion patterns across low- to
high-frequency regions.

</details>

### [106] [QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](https://arxiv.org/abs/2504.11038)
*Yudong Zhang,Ruobing Xie,Jiansheng Chen,Xingwu Sun,Zhanhui Kang,Yu Wang*

Main category: cs.CV

TLDR: 论文提出了一种查询无关的视觉攻击方法（QAVA），旨在生成对未知问题也能导致错误回答的对抗样本，相比传统攻击方法更高效。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击针对特定图像和问题，但在多问题场景下效果有限。QAVA旨在解决这一问题，提升攻击的鲁棒性。

Method: 提出QAVA方法，生成对抗样本以影响未知问题的回答，无需依赖特定问题。

Result: QAVA在未知问题场景下攻击效果显著，性能接近已知目标问题的攻击。

Conclusion: QAVA扩展了视觉对抗攻击的应用范围，揭示了LVLMs在视觉对抗威胁中的新漏洞。

Abstract: In typical multimodal tasks, such as Visual Question Answering (VQA),
adversarial attacks targeting a specific image and question can lead large
vision-language models (LVLMs) to provide incorrect answers. However, it is
common for a single image to be associated with multiple questions, and LVLMs
may still answer other questions correctly even for an adversarial image
attacked by a specific question. To address this, we introduce the
query-agnostic visual attack (QAVA), which aims to create robust adversarial
examples that generate incorrect responses to unspecified and unknown
questions. Compared to traditional adversarial attacks focused on specific
images and questions, QAVA significantly enhances the effectiveness and
efficiency of attacks on images when the question is unknown, achieving
performance comparable to attacks on known target questions. Our research
broadens the scope of visual adversarial attacks on LVLMs in practical
settings, uncovering previously overlooked vulnerabilities, particularly in the
context of visual adversarial threats. The code is available at
https://github.com/btzyd/qava.

</details>

### [107] [Leveraging LLMs and attention-mechanism for automatic annotation of historical maps](https://arxiv.org/abs/2504.11050)
*Yunshuang Yuan,Monika Sester*

Main category: cs.CV

TLDR: 提出了一种利用大语言模型和注意力机制自动标注历史地图的新方法，实现了高召回率和精确度。


<details>
  <summary>Details</summary>
Motivation: 历史地图是研究过去地理景观的重要资源，但传统方法依赖人工解读，难以扩展。

Method: 结合大语言模型生成粗分类标签，并通过注意力机制细化到高分辨率。

Result: 召回率超过90%，IoU和精确度表现良好（如Wood的IoU为84.2%，精确度为87.1%）。

Conclusion: 该方法无需精细人工标注即可高效分析历史地图，具有扩展潜力。

Abstract: Historical maps are essential resources that provide insights into the
geographical landscapes of the past. They serve as valuable tools for
researchers across disciplines such as history, geography, and urban studies,
facilitating the reconstruction of historical environments and the analysis of
spatial transformations over time. However, when constrained to analogue or
scanned formats, their interpretation is limited to humans and therefore not
scalable. Recent advancements in machine learning, particularly in computer
vision and large language models (LLMs), have opened new avenues for automating
the recognition and classification of features and objects in historical maps.
In this paper, we propose a novel distillation method that leverages LLMs and
attention mechanisms for the automatic annotation of historical maps. LLMs are
employed to generate coarse classification labels for low-resolution historical
image patches, while attention mechanisms are utilized to refine these labels
to higher resolutions. Experimental results demonstrate that the refined labels
achieve a high recall of more than 90%. Additionally, the intersection over
union (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with
precision scores of 87.1% and 79.5%, respectively, indicate that most labels
are well-aligned with ground-truth annotations. Notably, these results were
achieved without the use of fine-grained manual labels during training,
underscoring the potential of our approach for efficient and scalable
historical map analysis.

</details>

### [108] [Crane: Context-Guided Prompt Learning and Attention Refinement for Zero-Shot Anomaly Detections](https://arxiv.org/abs/2504.11055)
*Alireza Salehi,Mohammadreza Salehi,Reshad Hosseini,Cees G. M. Snoek,Makoto Yamada,Mohammad Sabokrou*

Main category: cs.CV

TLDR: 提出一种基于CLIP的新方法，通过调整文本编码器的提示和修改视觉编码器，提升图像级和像素级异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法依赖正常训练数据且跨领域泛化能力差，现有CLIP方法在图像级和像素级检测间存在性能差距。

Method: 基于图像上下文调整文本编码器提示，修改CLIP视觉编码器以提取更丰富的密集特征。

Result: 在14个数据集上性能提升2%至29%，达到最先进水平。

Conclusion: 该方法有效解决了图像级和像素级异常检测的性能差距，具有广泛适用性。

Abstract: Anomaly Detection (AD) involves identifying deviations from normal data
distributions and is critical in fields such as medical diagnostics and
industrial defect detection. Traditional AD methods typically require the
availability of normal training samples; however, this assumption is not always
feasible, as collecting such data can be impractical. Additionally, these
methods often struggle to generalize across different domains. Recent
advancements, such as AnomalyCLIP and AdaCLIP, utilize the zero-shot
generalization capabilities of CLIP but still face a performance gap between
image-level and pixel-level anomaly detection. To address this gap, we propose
a novel approach that conditions the prompts of the text encoder based on image
context extracted from the vision encoder. Also, to capture fine-grained
variations more effectively, we have modified the CLIP vision encoder and
altered the extraction of dense features. These changes ensure that the
features retain richer spatial and structural information for both normal and
anomalous prompts. Our method achieves state-of-the-art performance, improving
performance by 2% to 29% across different metrics on 14 datasets. This
demonstrates its effectiveness in both image-level and pixel-level anomaly
detection.

</details>

### [109] [UKDM: Underwater keypoint detection and matching using underwater image enhancement techniques](https://arxiv.org/abs/2504.11063)
*Pedro Diaz-Garcia,Felix Escalona,Miguel Cazorla*

Main category: cs.CV

TLDR: 论文探讨了水下图像增强技术对关键点检测和匹配的改进，通过深度学习方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过水下图像增强技术提高关键点检测和匹配的准确性，解决传统方法在水下环境中的不足。

Method: 采用生成对抗网络和卷积神经网络等深度学习模型，对不同水下数据集进行测试。

Result: 实验结果显示，这些方法在关键点检测和匹配的准确性和鲁棒性上显著优于传统方法。

Conclusion: 深度学习技术在水下图像增强和关键点处理中具有显著优势，为相关应用提供了新思路。

Abstract: The purpose of this paper is to explore the use of underwater image
enhancement techniques to improve keypoint detection and matching. By applying
advanced deep learning models, including generative adversarial networks and
convolutional neural networks, we aim to find the best method which improves
the accuracy of keypoint detection and the robustness of matching algorithms.
We evaluate the performance of these techniques on various underwater datasets,
demonstrating significant improvements over traditional methods.

</details>

### [110] [Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage](https://arxiv.org/abs/2504.11066)
*Marco Micheletto,Giulia Orrù,Luca Ghiani,Gian Luca Marcialis*

Main category: cs.CV

TLDR: 论文提出了一种名为Closeness Binary Code（CC）的附加模块，用于增强指纹验证系统中的Presentation Attack Detection（PAD）功能。该模块利用真实指纹在特征空间中的聚类特性，无需依赖特定用户样本即可设计。


<details>
  <summary>Details</summary>
Motivation: 现有PAD系统通常独立于指纹验证系统设计，未能充分利用用户模板的潜在安全优势。本文旨在通过一种创新的附加模块，在不增加复杂性和成本的情况下提升安全性。

Method: 提出CC模块，利用真实指纹在欧几里得特征空间中的聚类特性（同一手指样本相近，其他手指样本次之，其他用户样本最远）。该模块无需特定用户样本即可设计，并在验证阶段利用样本的“接近性”属性。

Result: 在基准数据集和先进PAD方法上的实验证实了CC模块的有效性，可轻松与指纹验证系统中的主PAD模块集成。

Conclusion: CC模块为指纹验证系统提供了一种高效且低成本的PAD增强方案，无需依赖特定用户样本，同时显著提升了安全性。

Abstract: Presentation Attack Detection (PAD) systems are usually designed
independently of the fingerprint verification system. While this can be
acceptable for use cases where specific user templates are not predetermined,
it represents a missed opportunity to enhance security in scenarios where
integrating PAD with the fingerprint verification system could significantly
leverage users' templates, which are the real target of a potential
presentation attack. This does not mean that a PAD should be specifically
designed for such users; that would imply the availability of many enrolled
users' PAI and, consequently, complexity, time, and cost increase. On the
contrary, we propose to equip a basic PAD, designed according to the state of
the art, with an innovative add-on module called the Closeness Binary Code (CC)
module. The term "closeness" refers to a peculiar property of the bona
fide-related features: in an Euclidean feature space, genuine fingerprints tend
to cluster in a specific pattern. First, samples from the same finger are close
to each other, then samples from other fingers of the same user and finally,
samples from fingers of other users. This property is statistically verified in
our previous publication, and further confirmed in this paper. It is
independent of the user population and the feature set class, which can be
handcrafted or deep network-based (embeddings). Therefore, the add-on can be
designed without the need for the targeted user samples; moreover, it exploits
her/his samples' "closeness" property during the verification stage. Extensive
experiments on benchmark datasets and state-of-the-art PAD methods confirm the
benefits of the proposed add-on, which can be easily coupled with the main PAD
module integrated into the fingerprint verification system.

</details>

### [111] [Change State Space Models for Remote Sensing Change Detection](https://arxiv.org/abs/2504.11080)
*Elman Ghazaei,Erchan Aptoula*

Main category: cs.CV

TLDR: 提出了一种基于状态空间模型的Change State Space Model（CSSM），专注于双时相图像中的相关变化，显著提升计算效率并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决ConvNets和ViT在变化检测中的局限性，如ConvNets难以建模长距离依赖，ViT计算效率低。

Method: 设计CSSM，专注于双时相图像中的相关变化，过滤无关信息，减少网络参数。

Result: 在三个基准数据集上，CSSM在计算复杂度显著降低的情况下，性能优于ConvNets、ViT和Mamba-based模型。

Conclusion: CSSM是一种高效且高性能的变化检测模型，适用于大规模数据集。

Abstract: Despite their frequent use for change detection, both ConvNets and Vision
transformers (ViT) exhibit well-known limitations, namely the former struggle
to model long-range dependencies while the latter are computationally
inefficient, rendering them challenging to train on large-scale datasets.
Vision Mamba, an architecture based on State Space Models has emerged as an
alternative addressing the aforementioned deficiencies and has been already
applied to remote sensing change detection, though mostly as a feature
extracting backbone. In this article the Change State Space Model is
introduced, that has been specifically designed for change detection by
focusing on the relevant changes between bi-temporal images, effectively
filtering out irrelevant information. By concentrating solely on the changed
features, the number of network parameters is reduced, enhancing significantly
computational efficiency while maintaining high detection performance and
robustness against input degradation. The proposed model has been evaluated via
three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based
counterparts at a fraction of their computational complexity. The
implementation will be made available at https://github.com/Elman295/CSSM upon
acceptance.

</details>

### [112] [Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting](https://arxiv.org/abs/2504.11092)
*Jiaxin Huang,Sheng Miao,BangBnag Yang,Yuewen Ma,Yiyi Liao*

Main category: cs.CV

TLDR: Vivid4D通过结合几何先验和生成先验，从单目视频中合成多视角视频，提升4D动态场景重建效果。


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建4D动态场景具有挑战性，因为每个时间戳仅从单一视角观察。

Method: 将视角增强重新定义为视频修复任务，利用单目深度先验将观察视角变换到新视角，并通过视频修复模型完成缺失区域。

Result: 实验表明，该方法有效提升了单目4D场景的重建和补全效果。

Conclusion: Vivid4D通过整合几何和生成先验，实现了更高质量的4D动态场景重建。

Abstract: Reconstructing 4D dynamic scenes from casually captured monocular videos is
valuable but highly challenging, as each timestamp is observed from a single
viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular
video synthesis by augmenting observation views - synthesizing multi-view
videos from a monocular input. Unlike existing methods that either solely
leverage geometric priors for supervision or use generative priors while
overlooking geometry, we integrate both. This reformulates view augmentation as
a video inpainting task, where observed views are warped into new viewpoints
based on monocular depth priors. To achieve this, we train a video inpainting
model on unposed web videos with synthetically generated masks that mimic
warping occlusions, ensuring spatially and temporally consistent completion of
missing regions. To further mitigate inaccuracies in monocular depth priors, we
introduce an iterative view augmentation strategy and a robust reconstruction
loss. Experiments demonstrate that our method effectively improves monocular 4D
scene reconstruction and completion.

</details>

### [113] [Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR](https://arxiv.org/abs/2504.11101)
*Yulong Zhang,Tianyi Liang,Xinyue Huang,Erfei Cui,Xu Guo,Pei Chu,Chenhui Li,Ru Zhang,Wenhai Wang,Gongshen Liu*

Main category: cs.CV

TLDR: 论文提出了一种名为Consensus Entropy (CE)的无训练后处理方法，通过聚合多个VLMs的输出量化OCR不确定性，显著提升了OCR任务的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在OCR任务中虽平均准确率提升，但仍存在样本级质量下降问题，且缺乏可靠的自动检测低质量输出的方法。

Method: 利用多个VLMs输出的收敛性（正确预测）和发散性（错误预测），开发了轻量级多模型框架，识别问题样本并优化输出。

Result: CE在多个OCR基准测试中表现优异，F1分数比VLM-as-judge方法高15.2%，数学计算任务准确率提升6.0%，且仅需重述7.3%的输入。

Conclusion: CE无需训练或监督，即插即用，显著提升了OCR任务的性能，成为当前最佳解决方案。

Abstract: The Optical Character Recognition (OCR) task is important for evaluating
Vision-Language Models (VLMs) and providing high-quality data sources for LLM
training data. While state-of-the-art VLMs show improved average OCR accuracy,
they still struggle with sample-level quality degradation and lack reliable
automatic detection of low-quality outputs. We introduce Consensus Entropy
(CE), a training-free post-inference method that quantifies OCR uncertainty by
aggregating outputs from multiple VLMs. Our approach exploits a key insight:
correct VLM OCR predictions converge in output space while errors diverge. We
develop a lightweight multi-model framework that effectively identifies
problematic samples, selects the best outputs and combines model strengths.
Experiments across multiple OCR benchmarks and VLMs demonstrate that CE
outperforms VLM-as-judge approaches and single-model baselines at the same cost
and achieves state-of-the-art results across multiple metrics. For instance,
our solution demonstrates: achieving 15.2\% higher F1 scores than VLM-as-judge
methods in quality verification, delivering 6.0\% accuracy gains on
mathematical calculation tasks, and requiring rephrasing only 7.3\% of inputs
while maintaining overall performance. Notably, the entire process requires
neither training nor supervision while maintaining plug-and-play functionality
throughout.

</details>

### [114] [Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models](https://arxiv.org/abs/2504.11106)
*Jiangtao Liu,Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.CV

TLDR: TCBS-Attack是一种新型的黑盒越狱攻击方法，通过优化接近决策边界的令牌生成语义连贯的对抗提示，成功绕过T2I模型的多层防御。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制（如提示检查器和图像检查器）易受复杂对抗攻击，需要更强大的攻击方法来验证其脆弱性。

Method: 提出TCBS-Attack，通过迭代优化接近文本和图像检查器决策边界的令牌，生成对抗提示。

Result: TCBS-Attack在多种T2I模型上表现优异，ASR-4达45%，ASR-1达21%，显著优于基线方法。

Conclusion: TCBS-Attack展示了现有防御机制的脆弱性，为未来防御设计提供了重要参考。

Abstract: Recent advancements in Text-to-Image (T2I) generation have significantly
enhanced the realism and creativity of generated images. However, such powerful
generative capabilities pose risks related to the production of inappropriate
or harmful content. Existing defense mechanisms, including prompt checkers and
post-hoc image checkers, are vulnerable to sophisticated adversarial attacks.
In this work, we propose TCBS-Attack, a novel query-based black-box jailbreak
attack that searches for tokens located near the decision boundaries defined by
text and image checkers. By iteratively optimizing tokens near these
boundaries, TCBS-Attack generates semantically coherent adversarial prompts
capable of bypassing multiple defensive layers in T2I models. Extensive
experiments demonstrate that our method consistently outperforms
state-of-the-art jailbreak attacks across various T2I models, including
securely trained open-source models and commercial online services like DALL-E
3. TCBS-Attack achieves an ASR-4 of 45\% and an ASR-1 of 21\% on jailbreaking
full-chain T2I models, significantly surpassing baseline methods.

</details>

### [115] [S$^2$Teacher: Step-by-step Teacher for Sparsely Annotated Oriented Object Detection](https://arxiv.org/abs/2504.11111)
*Yu Lin,Jianghang Lin,Kai Ye,You Shen,Yan Zhang,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TLDR: 论文提出了一种稀疏标注的定向目标检测（SAOOD）方法，通过S$^2$Teacher逐步挖掘伪标签，提升检测性能并减少标注负担。


<details>
  <summary>Details</summary>
Motivation: 解决密集标注在复杂遥感场景中的困难，减少人工标注成本。

Method: 提出S$^2$Teacher方法，逐步挖掘伪标签并对未标注对象进行损失重加权。

Result: 在DOTA数据集上，仅用10%标注实例即接近全监督性能。

Conclusion: SAOOD和S$^2$Teacher有效平衡了检测精度与标注效率。

Abstract: Although fully-supervised oriented object detection has made significant
progress in multimodal remote sensing image understanding, it comes at the cost
of labor-intensive annotation. Recent studies have explored weakly and
semi-supervised learning to alleviate this burden. However, these methods
overlook the difficulties posed by dense annotations in complex remote sensing
scenes. In this paper, we introduce a novel setting called sparsely annotated
oriented object detection (SAOOD), which only labels partial instances, and
propose a solution to address its challenges. Specifically, we focus on two key
issues in the setting: (1) sparse labeling leading to overfitting on limited
foreground representations, and (2) unlabeled objects (false negatives)
confusing feature learning. To this end, we propose the S$^2$Teacher, a novel
method that progressively mines pseudo-labels for unlabeled objects, from easy
to hard, to enhance foreground representations. Additionally, it reweights the
loss of unlabeled objects to mitigate their impact during training. Extensive
experiments demonstrate that S$^2$Teacher not only significantly improves
detector performance across different sparse annotation levels but also
achieves near-fully-supervised performance on the DOTA dataset with only 10%
annotation instances, effectively balancing detection accuracy with annotation
efficiency. The code will be public.

</details>

### [116] [Flyweight FLIM Networks for Salient Object Detection in Biomedical Images](https://arxiv.org/abs/2504.11112)
*Leonardo M. Joao,Jancarlo F. Gomes,Silvio J. F. Guimaraes,Ewa Kijak,Alexandre X. Falcao*

Main category: cs.CV

TLDR: 该论文提出了一种名为FLIM的轻量级显著目标检测方法，通过从少量标记图像中学习卷积核，避免了大数据集和反向传播的需求，并结合自适应解码器实现了高效模型。


<details>
  <summary>Details</summary>
Motivation: 解决显著目标检测（SOD）在资源受限应用中需要大量计算资源和标注数据的问题。

Method: 提出FLIM方法，学习图像块中的卷积核，并结合多扩张层和网络简化技术，无需反向传播。

Result: 实验表明FLIM模型在效率和效果上优于轻量级模型，且与重量级模型竞争。

Conclusion: FLIM网络在数据有限和资源受限的应用中具有潜力。

Abstract: Salient Object Detection (SOD) with deep learning often requires substantial
computational resources and large annotated datasets, making it impractical for
resource-constrained applications. Lightweight models address computational
demands but typically strive in complex and scarce labeled-data scenarios.
Feature Learning from Image Markers (FLIM) learns an encoder's convolutional
kernels among image patches extracted from discriminative regions marked on a
few representative images, dismissing large annotated datasets, pretraining,
and backpropagation. Such a methodology exploits information redundancy
commonly found in biomedical image applications. This study presents methods to
learn dilated-separable convolutional kernels and multi-dilation layers without
backpropagation for FLIM networks. It also proposes a novel network
simplification method to reduce kernel redundancy and encoder size. By
combining a FLIM encoder with an adaptive decoder, a concept recently
introduced to estimate a pointwise convolution per image, this study presents
very efficient (named flyweight) SOD models for biomedical images. Experimental
results in challenging datasets demonstrate superior efficiency and
effectiveness to lightweight models. By requiring significantly fewer
parameters and floating-point operations, the results show competitive
effectiveness to heavyweight models. These advances highlight the potential of
FLIM networks for data-limited and resource-constrained applications with
information redundancy.

</details>

### [117] [K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery](https://arxiv.org/abs/2504.11128)
*P. Tomkiewicz,J. Jaworski,P. Zielonka,A. Wilinski*

Main category: cs.CV

TLDR: 本文提出了一种基于多模态卫星影像的密度梯度分析方法，用于评估城市指标，并应用于公共交通等城市系统。


<details>
  <summary>Details</summary>
Motivation: 通过结合光学和SAR数据，开发一种方法以分割城市区域、识别城市中心并量化密度梯度，为城市规划和公共交通分析提供工具。

Method: 结合光学和SAR数据，计算密度梯度系数（α）和最小有效距离（LD），并利用K-means聚类识别密度梯度图中的均匀和高变异性区域。

Result: 通过对比两种不同城市形态（单中心与多中心）的分析，发现密度梯度特征与公共交通网络拓扑结构相关。

Conclusion: 该方法为城市提供了一种低成本、全球适用的公共交通初步评估工具，基于开源卫星数据。

Abstract: This paper presents a novel computational approach for evaluating urban
metrics through density gradient analysis using multi-modal satellite imagery,
with applications including public transport and other urban systems. By
combining optical and Synthetic Aperture Radar (SAR) data, we develop a method
to segment urban areas, identify urban centers, and quantify density gradients.
Our approach calculates two key metrics: the density gradient coefficient
($\alpha$) and the minimum effective distance (LD) at which density reaches a
target threshold. We further employ machine learning techniques, specifically
K-means clustering, to objectively identify uniform and high-variability
regions within density gradient plots. We demonstrate that these metrics
provide an effective screening tool for public transport analyses by revealing
the underlying urban structure. Through comparative analysis of two
representative cities with contrasting urban morphologies (monocentric vs
polycentric), we establish relationships between density gradient
characteristics and public transport network topologies. Cities with clear
density peaks in their gradient plots indicate distinct urban centers requiring
different transport strategies than those with more uniform density
distributions. This methodology offers urban planners a cost-effective,
globally applicable approach to preliminary public transport assessment using
freely available satellite data. The complete implementation, with additional
examples and documentation, is available in an open-source repository under the
MIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.

</details>

### [118] [Visual Re-Ranking with Non-Visual Side Information](https://arxiv.org/abs/2504.11134)
*Gustav Hanning,Gabrielle Flood,Viktor Larsson*

Main category: cs.CV

TLDR: 论文提出了一种基于图神经网络的重新排序方法GCSA，利用多模态信息提升视觉位置识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有重新排序方法仅依赖初始检索的图像描述符，信号有限，无法充分利用其他可用信息。

Method: 提出GCSA方法，通过图神经网络结合视觉描述符和其他传感器数据（如WiFi信号、相机位姿）进行重新排序。

Result: 在两个大规模数据集上实验表明，GCSA显著提升了图像检索和视觉定位任务的性能。

Conclusion: GCSA通过多模态信息融合，有效提升了视觉位置识别的准确性和实用性。

Abstract: The standard approach for visual place recognition is to use global image
descriptors to retrieve the most similar database images for a given query
image. The results can then be further improved with re-ranking methods that
re-order the top scoring images. However, existing methods focus on re-ranking
based on the same image descriptors that were used for the initial retrieval,
which we argue provides limited additional signal.
  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),
which is a graph neural network-based re-ranking method that, in addition to
the visual descriptors, can leverage other types of available side information.
This can for example be other sensor data (such as signal strength of nearby
WiFi or BlueTooth endpoints) or geometric properties such as camera poses for
database images. In many applications this information is already present or
can be acquired with low effort. Our architecture leverages the concept of
affinity vectors to allow for a shared encoding of the heterogeneous
multi-modal input. Two large-scale datasets, covering both outdoor and indoor
localization scenarios, are utilized for training and evaluation. In
experiments we show significant improvement not only on image retrieval
metrics, but also for the downstream visual localization task.

</details>

### [119] [Taming Consistency Distillation for Accelerated Human Image Animation](https://arxiv.org/abs/2504.11143)
*Xiang Wang,Shiwei Zhang,Hangjie Yuan,Yujie Wei,Yingya Zhang,Changxin Gao,Yuehuan Wang,Nong Sang*

Main category: cs.CV

TLDR: DanceLCM是一种基于一致性模型的人类图像动画加速方法，通过分段一致性蒸馏和运动聚焦损失，显著减少推理步骤（2-4步）而不降低视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型依赖多次迭代去噪步骤，导致高推理成本和慢速。直接应用一致性模型会导致质量下降，如视觉模糊和运动退化。

Method: 提出DanceLCM方法，包括分段一致性蒸馏（辅助轻量头部）和运动聚焦损失，同时注入面部保真特征。

Result: 实验表明，DanceLCM在2-4步推理内达到与先进视频扩散模型相当的效果，显著降低推理负担。

Conclusion: DanceLCM通过改进的一致性蒸馏和运动优化，实现了高效且高质量的人类图像动画。

Abstract: Recent advancements in human image animation have been propelled by video
diffusion models, yet their reliance on numerous iterative denoising steps
results in high inference costs and slow speeds. An intuitive solution involves
adopting consistency models, which serve as an effective acceleration paradigm
through consistency distillation. However, simply employing this strategy in
human image animation often leads to quality decline, including visual
blurring, motion degradation, and facial distortion, particularly in dynamic
regions. In this paper, we propose the DanceLCM approach complemented by
several enhancements to improve visual quality and motion continuity at
low-step regime: (1) segmented consistency distillation with an auxiliary
light-weight head to incorporate supervision from real video latents,
mitigating cumulative errors resulting from single full-trajectory generation;
(2) a motion-focused loss to centre on motion regions, and explicit injection
of facial fidelity features to improve face authenticity. Extensive qualitative
and quantitative experiments demonstrate that DanceLCM achieves results
comparable to state-of-the-art video diffusion models with a mere 2-4 inference
steps, significantly reducing the inference burden without compromising video
quality. The code and models will be made publicly available.

</details>

### [120] [GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention](https://arxiv.org/abs/2504.11150)
*Mahir Gulzar,Yar Muhammad,Naveed Muhammad*

Main category: cs.CV

TLDR: 提出了一种基于车道图的运动预测模型，通过交叉注意力融合多上下文信息，实现了对未来车辆轨迹的鲁棒预测。


<details>
  <summary>Details</summary>
Motivation: 预测周围车辆的未来轨迹依赖于上下文信息，包括静态（如车道）和动态（如交通参与者）元素。现有方法需要更高效地融合这些信息。

Method: 采用编码器-交互器-解码器架构：编码器用轻量级GRU编码场景上下文，交互器通过交叉注意力融合场景特征与图目标提议，解码器用Laplacian混合密度网络回归多模态轨迹。

Result: 在nuScenes数据集上实现了最先进的性能。

Conclusion: 通过交叉注意力关注未来目标相关场景元素，模型能够生成更鲁棒的轨迹预测。

Abstract: Predicting future trajectories of surrounding vehicles heavily relies on what
contextual information is given to a motion prediction model. The context
itself can be static (lanes, regulatory elements, etc) or dynamic (traffic
participants). This paper presents a lane graph-based motion prediction model
that first predicts graph-based goal proposals and later fuses them with cross
attention over multiple contextual elements. We follow the famous
encoder-interactor-decoder architecture where the encoder encodes scene context
using lightweight Gated Recurrent Units, the interactor applies cross-context
attention over encoded scene features and graph goal proposals, and the decoder
regresses multimodal trajectories via Laplacian Mixture Density Network from
the aggregated encodings. Using cross-attention over graph-based goal proposals
gives robust trajectory estimates since the model learns to attend to future
goal-relevant scene elements for the intended agent. We evaluate our work on
nuScenes motion prediction dataset, achieving state-of-the-art results.

</details>

### [121] [SAR-to-RGB Translation with Latent Diffusion for Earth Observation](https://arxiv.org/abs/2504.11154)
*Kaan Aydin,Joelle Hanna,Damian Borth*

Main category: cs.CV

TLDR: 提出了一种基于扩散模型（DM）的SAR-to-RGB转换方法，生成合成光学图像以弥补Sentinel-2（S2）数据缺失问题。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2（S2）图像常因云层或数据缺失不可用，而Sentinel-1（SAR）数据可补充，但需转换为光学图像。

Method: 探索了三种扩散模型（DM）设置：两种标准扩散（带/不带类别条件）和一种冷扩散，用于SAR-to-RGB转换。

Result: 生成图像虽不完美，但下游任务（如土地分类和云去除）表现良好，类别条件提升分类精度。

Conclusion: 扩散模型在SAR-to-RGB转换中潜力显著，传统评价指标可能低估其实际应用价值。

Abstract: Earth observation satellites like Sentinel-1 (S1) and Sentinel-2 (S2) provide
complementary remote sensing (RS) data, but S2 images are often unavailable due
to cloud cover or data gaps. To address this, we propose a diffusion model
(DM)-based approach for SAR-to-RGB translation, generating synthetic optical
images from SAR inputs. We explore three different setups: two using Standard
Diffusion, which reconstruct S2 images by adding and removing noise (one
without and one with class conditioning), and one using Cold Diffusion, which
blends S2 with S1 before removing the SAR signal. We evaluate the generated
images in downstream tasks, including land cover classification and cloud
removal. While generated images may not perfectly replicate real S2 data, they
still provide valuable information. Our results show that class conditioning
improves classification accuracy, while cloud removal performance remains
competitive despite our approach not being optimized for it. Interestingly,
despite exhibiting lower perceptual quality, the Cold Diffusion setup performs
well in land cover classification, suggesting that traditional quantitative
evaluation metrics may not fully reflect the practical utility of generated
images. Our findings highlight the potential of DMs for SAR-to-RGB translation
in RS applications where RGB images are missing.

</details>

### [122] [DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention](https://arxiv.org/abs/2504.11160)
*Haohan Chen,Hongjia Liu,Shiyong Lan,Wenwu Wang,Yixin Qiao,Yao Li,Guonan Deng*

Main category: cs.CV

TLDR: DMAGaze提出了一种新的凝视估计框架，通过分离凝视相关特征、局部眼部特征和头部姿态特征，结合多尺度注意力模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 凝视估计常受复杂无关信息干扰，需更精确的特征提取方法。

Method: 设计了连续掩码分离器（Disentangler）和多尺度全局局部注意力模块（MS-GLAM），结合头部姿态和局部眼部特征。

Result: 在两个主流数据集上验证，性能达到最优。

Conclusion: DMAGaze通过多特征融合和注意力机制，显著提升了凝视估计的精度。

Abstract: Gaze estimation, which predicts gaze direction, commonly faces the challenge
of interference from complex gaze-irrelevant information in face images. In
this work, we propose DMAGaze, a novel gaze estimation framework that exploits
information from facial images in three aspects: gaze-relevant global features
(disentangled from facial image), local eye features (extracted from cropped
eye patch), and head pose estimation features, to improve overall performance.
Firstly, we design a new continuous mask-based Disentangler to accurately
disentangle gaze-relevant and gaze-irrelevant information in facial images by
achieving the dual-branch disentanglement goal through separately
reconstructing the eye and non-eye regions. Furthermore, we introduce a new
cascaded attention module named Multi-Scale Global Local Attention Module
(MS-GLAM). Through a customized cascaded attention structure, it effectively
focuses on global and local information at multiple scales, further enhancing
the information from the Disentangler. Finally, the global gaze-relevant
features disentangled by the upper face branch, combined with head pose and
local eye features, are passed through the detection head for high-precision
gaze estimation. Our proposed DMAGaze has been extensively validated on two
mainstream public datasets, achieving state-of-the-art performance.

</details>

### [123] [TSAL: Few-shot Text Segmentation Based on Attribute Learning](https://arxiv.org/abs/2504.11164)
*Chenming Li,Chengxu Liu,Yuanting Fan,Xiao Jin,Xingsong Hou,Xueming Qian*

Main category: cs.CV

TLDR: 论文提出了一种基于少样本学习的场景文本分割方法TSAL，利用CLIP的先验知识学习文本属性，并通过视觉引导分支和自适应提示引导分支提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 当前场景文本分割受限于高质量数据集的缺乏和像素标注的高成本，少样本学习方法为解决这一问题提供了可能。

Method: TSAL结合CLIP的先验知识，设计了视觉引导分支和自适应提示引导分支，并引入自适应特征对齐模块（AFA）来捕获文本属性。

Result: 实验表明，TSAL在少样本设置下在多个文本分割数据集上达到了SOTA性能。

Conclusion: TSAL在少样本场景下能有效捕获文本独特属性，实现精确分割，并在文本相关领域展现出潜力。

Abstract: Recently supervised learning rapidly develops in scene text segmentation.
However, the lack of high-quality datasets and the high cost of pixel
annotation greatly limit the development of them. Considering the
well-performed few-shot learning methods for downstream tasks, we investigate
the application of the few-shot learning method to scene text segmentation. We
propose TSAL, which leverages CLIP's prior knowledge to learn text attributes
for segmentation. To fully utilize the semantic and texture information in the
image, a visual-guided branch is proposed to separately extract text and
background features. To reduce data dependency and improve text detection
accuracy, the adaptive prompt-guided branch employs effective adaptive prompt
templates to capture various text attributes. To enable adaptive prompts
capture distinctive text features and complex background distribution, we
propose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of
different attributes with visual features and prompt prototypes, AFA enables
adaptive prompts to capture both general and distinctive attribute information.
TSAL can capture the unique attributes of text and achieve precise segmentation
using only few images. Experiments demonstrate that our method achieves SOTA
performance on multiple text segmentation datasets under few-shot settings and
show great potential in text-related domains.

</details>

### [124] [YOLO-RS: Remote Sensing Enhanced Crop Detection Methods](https://arxiv.org/abs/2504.11165)
*Linlin Xiao,Zhang Tiancong,Yutong Jia,Xinyu Nie,Mengyao Wang,Xiaohang Shao*

Main category: cs.CV

TLDR: 提出了一种新型目标检测模型YOLO-RS，通过引入CAA机制和多尺度特征融合网络，显著提升了遥感图像中小目标的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测方法在复杂背景和小目标检测中表现不佳，难以满足实际应用需求。

Method: 基于Yolov11，引入CAA机制和多尺度特征融合网络，采用双向特征融合策略和ACmix模块解决类别不平衡问题。

Result: 在PDT和CWC数据集上，YOLO-RS的召回率和mAP提升了2-3%，F1分数显著提高，计算复杂度仅增加5.2 GFLOPs。

Conclusion: YOLO-RS在遥感图像小目标检测中表现出高效性和应用潜力。

Abstract: With the rapid development of remote sensing technology, crop classification
and health detection based on deep learning have gradually become a research
hotspot. However, the existing target detection methods show poor performance
when dealing with small targets in remote sensing images, especially in the
case of complex background and image mixing, which is difficult to meet the
practical application requirementsite. To address this problem, a novel target
detection model YOLO-RS is proposed in this paper. The model is based on the
latest Yolov11 which significantly enhances the detection of small targets by
introducing the Context Anchor Attention (CAA) mechanism and an efficient
multi-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional
feature fusion strategy in the feature fusion process, which effectively
enhances the model's performance in the detection of small targets. Small
target detection. Meanwhile, the ACmix module at the end of the model backbone
network solves the category imbalance problem by adaptively adjusting the
contrast and sample mixing, thus enhancing the detection accuracy in complex
scenes. In the experiments on the PDT remote sensing crop health detection
dataset and the CWC crop classification dataset, YOLO-RS improves both the
recall and the mean average precision (mAP) by about 2-3\% or so compared with
the existing state-of-the-art methods, while the F1-score is also significantly
improved. Moreover, the computational complexity of the model only increases by
about 5.2 GFLOPs, indicating its significant advantages in both performance and
efficiency. The experimental results validate the effectiveness and application
potential of YOLO-RS in the task of detecting small targets in remote sensing
images.

</details>

### [125] [TerraMind: Large-Scale Generative Multimodality for Earth Observation](https://arxiv.org/abs/2504.11171)
*Johannes Jakubik,Felix Yang,Benedikt Blumenstiel,Erik Scheurer,Rocco Sedona,Stefano Maurogiovanni,Jente Bosmans,Nikolaos Dionelis,Valerio Marsocci,Niklas Kopp,Rahul Ramachandran,Paolo Fraccaro,Thomas Brunschwiler,Gabriele Cavallaro,Juan Bernabe-Moreno,Nicolas Longépé*

Main category: cs.CV

TLDR: TerraMind是一种多模态基础模型，首次实现地球观测（EO）的任意到任意生成，通过双尺度表示（标记级和像素级）预训练，支持零样本和少样本应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态模型在地球观测中无法同时处理高层次上下文和细粒度空间信息的问题。

Method: 采用双尺度早期融合方法，结合标记级和像素级数据预训练，并引入“Thinking-in-Modalities”（TiM）能力。

Result: 在PANGAEA等标准基准测试中超越现有技术，支持零样本和少样本应用。

Conclusion: TerraMind通过双尺度表示和TiM能力，为地球观测提供了强大的多模态生成模型，并开源了数据和代码。

Abstract: We present TerraMind, the first any-to-any generative, multimodal foundation
model for Earth observation (EO). Unlike other multimodal models, TerraMind is
pretrained on dual-scale representations combining both token-level and
pixel-level data across modalities. On a token level, TerraMind encodes
high-level contextual information to learn cross-modal relationships, while on
a pixel level, TerraMind leverages fine-grained representations to capture
critical spatial nuances. We pretrained TerraMind on nine geospatial modalities
of a global, large-scale dataset. In this paper, we demonstrate that (i)
TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and
few-shot applications for Earth observation, (ii) TerraMind introduces
"Thinking-in-Modalities" (TiM) -- the capability of generating additional
artificial data during finetuning and inference to improve the model output --
and (iii) TerraMind achieves beyond state-of-the-art performance in
community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the
model weights, and our code is open-sourced under a permissive license.

</details>

### [126] [TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data](https://arxiv.org/abs/2504.11172)
*Benedikt Blumenstiel,Paolo Fraccaro,Valerio Marsocci,Johannes Jakubik,Stefano Maurogiovanni,Mikolaj Czerkawski,Rocco Sedona,Gabriele Cavallaro,Thomas Brunschwiler,Juan Bernabe-Moreno,Nicolas Longépé*

Main category: cs.CV

TLDR: TerraMesh是一个全球多样化的多模态数据集，结合光学、合成孔径雷达、高程和土地覆盖数据，用于大规模预训练和跨模态学习。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集在规模、地理覆盖或传感器多样性上有限，阻碍了地球观测领域的大规模基础模型发展。

Method: 引入TerraMesh数据集，包含900多万个样本，提供详细数据处理步骤和统计数据。

Result: 实验证明在TerraMesh上预训练的模型性能有所提升。

Conclusion: TerraMesh将公开提供，支持地球观测领域的研究和应用。

Abstract: Large-scale foundation models in Earth Observation can learn versatile,
label-efficient representations by leveraging massive amounts of unlabeled
data. However, existing public datasets are often limited in scale, geographic
coverage, or sensor variety. We introduce TerraMesh, a new globally diverse,
multimodal dataset combining optical, synthetic aperture radar, elevation, and
land-cover modalities in an Analysis-Ready Data format. TerraMesh includes over
9 million samples with eight spatiotemporal aligned modalities, enabling
large-scale pre-training and fostering robust cross-modal correlation learning.
We provide detailed data processing steps, comprehensive statistics, and
empirical evidence demonstrating improved model performance when pre-trained on
TerraMesh. The dataset will be made publicly available with a permissive
license.

</details>

### [127] [Video Summarization with Large Language Models](https://arxiv.org/abs/2504.11199)
*Min Jung Lee,Dayoung Gong,Minsu Cho*

Main category: cs.CV

TLDR: 提出了一种基于大语言模型（LLM）的视频摘要框架（LLMVS），通过多模态大语言模型（M-LLM）将视频帧转化为字幕序列，并利用LLM评估帧的重要性，解决了现有方法在语义捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 视频内容的爆炸式增长对高效导航、搜索和检索提出了挑战，现有视频摘要方法因依赖视觉特征和时序动态而难以捕捉语义，导致摘要不完整或不连贯。

Method: LLMVS将视频帧转化为字幕序列，利用LLM评估局部上下文中的帧重要性，并通过全局注意力机制优化，确保摘要细节与整体叙事一致。

Result: 实验结果表明，LLMVS在标准基准测试中优于现有方法，展示了LLM在多模态内容处理中的潜力。

Conclusion: LLMVS通过结合LLM的语义理解能力，显著提升了视频摘要的质量，为多媒体内容处理提供了新思路。

Abstract: The exponential increase in video content poses significant challenges in
terms of efficient navigation, search, and retrieval, thus requiring advanced
video summarization techniques. Existing video summarization methods, which
heavily rely on visual features and temporal dynamics, often fail to capture
the semantics of video content, resulting in incomplete or incoherent
summaries. To tackle the challenge, we propose a new video summarization
framework that leverages the capabilities of recent Large Language Models
(LLMs), expecting that the knowledge learned from massive data enables LLMs to
evaluate video frames in a manner that better aligns with diverse semantics and
human judgments, effectively addressing the inherent subjectivity in defining
keyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates
video frames into a sequence of captions using a Muti-modal Large Language
Model (M-LLM) and then assesses the importance of each frame using an LLM,
based on the captions in its local context. These local importance scores are
refined through a global attention mechanism in the entire context of video
captions, ensuring that our summaries effectively reflect both the details and
the overarching narrative. Our experimental results demonstrate the superiority
of the proposed method over existing ones in standard benchmarks, highlighting
the potential of LLMs in the processing of multimedia content.

</details>

### [128] [Focal Split: Untethered Snapshot Depth from Differential Defocus](https://arxiv.org/abs/2504.11202)
*Junjie Luo,John Mamish,Alan Fu,Thomas Concannon,Josiah Hester,Emma Alexander,Qi Guo*

Main category: cs.CV

TLDR: Focal Split是一种基于差分离焦深度计算的手持式深度相机，具有完全自主的电源和计算能力，被动式设计避免了光源的功耗。


<details>
  <summary>Details</summary>
Motivation: 开发一种低功耗、便携且易于DIY的深度相机，适用于实时深度测量。

Method: 使用差分离焦深度理论（DfDD），通过两个光传感器同时捕捉场景的两个不同离焦图像，并通过高效算法计算每个像素的深度和置信度。

Result: 原型系统功耗4.9W，可测量0.4m至1.2m范围内的物体，输出480×360稀疏深度图，帧率为2.1FPS。

Conclusion: Focal Split是一种高效、低功耗且易于构建的深度相机解决方案，适合DIY爱好者和实时应用。

Abstract: We introduce Focal Split, a handheld, snapshot depth camera with fully
onboard power and computing based on depth-from-differential-defocus (DfDD).
Focal Split is passive, avoiding power consumption of light sources. Its
achromatic optical system simultaneously forms two differentially defocused
images of the scene, which can be independently captured using two photosensors
in a snapshot. The data processing is based on the DfDD theory, which
efficiently computes a depth and a confidence value for each pixel with only
500 floating point operations (FLOPs) per pixel from the camera measurements.
We demonstrate a Focal Split prototype, which comprises a handheld custom
camera system connected to a Raspberry Pi 5 for real-time data processing. The
system consumes 4.9 W and is powered on a 5 V, 10,000 mAh battery. The
prototype can measure objects with distances from 0.4 m to 1.2 m, outputting
480$\times$360 sparse depth maps at 2.1 frames per second (FPS) using
unoptimized Python scripts. Focal Split is DIY friendly. A comprehensive guide
to building your own Focal Split depth camera, code, and additional data can be
found at https://focal-split.qiguo.org.

</details>

### [129] [3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians](https://arxiv.org/abs/2504.11218)
*Zeming wei,Junyi Lin,Yang Liu,Weixing Chen,Jingzhou Luo,Guanbin Li,Liang Lin*

Main category: cs.CV

TLDR: 论文提出了3DAffordSplat数据集和AffordSplatNet模型，用于基于3D高斯泼溅的3D功能推理，解决了现有方法在泛化性和鲁棒性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于稀疏3D点云的方法在功能推理中存在泛化性和鲁棒性不足的问题，而3D高斯泼溅（3DGS）虽能提供高保真渲染，但缺乏专门的数据集。

Method: 提出3DAffordSplat数据集和AffordSplatNet模型，利用跨模态结构对齐模块结合3D点云和3DGS表示。

Result: 实验表明，3DAffordSplat显著提升了3DGS领域的功能学习，AffordSplatNet在多种场景下优于现有方法。

Conclusion: 3DAffordSplat和AffordSplatNet为3D功能推理提供了高效且泛化性强的解决方案。

Abstract: 3D affordance reasoning is essential in associating human instructions with
the functional regions of 3D objects, facilitating precise, task-oriented
manipulations in embodied AI. However, current methods, which predominantly
depend on sparse 3D point clouds, exhibit limited generalizability and
robustness due to their sensitivity to coordinate variations and the inherent
sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers
high-fidelity, real-time rendering with minimal computational overhead by
representing scenes as dense, continuous distributions. This positions 3DGS as
a highly effective approach for capturing fine-grained affordance details and
improving recognition accuracy. Nevertheless, its full potential remains
largely untapped due to the absence of large-scale, 3DGS-specific affordance
datasets. To overcome these limitations, we present 3DAffordSplat, the first
large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.
This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,
and 6,631 manually annotated affordance labels, encompassing 21 object
categories and 18 affordance types. Building upon this dataset, we introduce
AffordSplatNet, a novel model specifically designed for affordance reasoning
using 3DGS representations. AffordSplatNet features an innovative cross-modal
structure alignment module that exploits structural consistency priors to align
3D point cloud and 3DGS representations, resulting in enhanced affordance
recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat
dataset significantly advances affordance learning within the 3DGS domain,
while AffordSplatNet consistently outperforms existing methods across both seen
and unseen settings, highlighting its robust generalization capabilities.

</details>

### [130] [CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image](https://arxiv.org/abs/2504.11230)
*Jingshun Huang,Haitao Lin,Tianyu Wang,Yanwei Fu,Xiangyang Xue,Yi Zhu*

Main category: cs.CV

TLDR: 本文提出了一种单阶段网络CAP-Net，用于估计类别级铰接物体的6D姿态和尺寸，结合RGB-D特征实现端到端预测，并引入新数据集RGBD-Art。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖几何线索和多阶段流程，忽略了RGB图像的密集语义信息，导致对小部件物体的姿态估计精度不足。

Method: CAP-Net通过统一网络预测点级类别标签、质心偏移和NPCS映射，结合聚类算法分离部件并恢复姿态和尺寸。

Result: 在RGBD-Art数据集上，CAP-Net显著优于现有方法，并在实际机器人任务中表现出鲁棒性和优异的仿真到现实迁移能力。

Conclusion: CAP-Net和RGBD-Art数据集为类别级铰接物体姿态估计提供了高效且实用的解决方案。

Abstract: This paper tackles category-level pose estimation of articulated objects in
robotic manipulation tasks and introduces a new benchmark dataset. While recent
methods estimate part poses and sizes at the category level, they often rely on
geometric cues and complex multi-stage pipelines that first segment parts from
the point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation
for 6D poses. These approaches overlook dense semantic cues from RGB images,
leading to suboptimal accuracy, particularly for objects with small parts. To
address these limitations, we propose a single-stage Network, CAP-Net, for
estimating the 6D poses and sizes of Categorical Articulated Parts. This method
combines RGB-D features to generate instance segmentation and NPCS
representations for each part in an end-to-end manner. CAP-Net uses a unified
network to simultaneously predict point-wise class labels, centroid offsets,
and NPCS maps. A clustering algorithm then groups points of the same predicted
class based on their estimated centroid distances to isolate each part.
Finally, the NPCS region of each part is aligned with the point cloud to
recover its final pose and size. To bridge the sim-to-real domain gap, we
introduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,
featuring photorealistic RGB images and depth noise simulated from real
sensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our
method significantly outperforms the state-of-the-art approach. Real-world
deployments of our model in robotic tasks underscore its robustness and
exceptional sim-to-real transfer capabilities, confirming its substantial
practical utility. Our dataset, code and pre-trained models are available on
the project page.

</details>

### [131] [Leveraging multimodal explanatory annotations for video interpretation with Modality Specific Dataset](https://arxiv.org/abs/2504.11232)
*Elisa Ancarani,Julie Tores,Lucile Sassatelli,Rémy Sun,Hui-Yin Wu,Frédéric Precioso*

Main category: cs.CV

TLDR: 研究探讨了基于概念监督的多模态视频解释模型效果，使用MOByGaze数据集和提出的CMSDs方法，显示其优于传统训练方法，并缩小了早期与晚期融合模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过模态特定的概念标注提升多模态视频模型的解释性和性能。

Method: 提出Concept Modality Specific Datasets (CMSDs)，按概念模态（视觉、文本或音频）分类数据子集，并比较早期和晚期融合模型的性能。

Result: CMSDs训练模型在早期和晚期融合中均优于传统方法，晚期融合模型性能接近早期融合。

Conclusion: 模态特定标注对开发鲁棒、可解释的视频模型至关重要，推动了复杂视频分析中可解释多模态学习的进展。

Abstract: We examine the impact of concept-informed supervision on multimodal video
interpretation models using MOByGaze, a dataset containing human-annotated
explanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs),
which consist of data subsets categorized by the modality (visual, textual, or
audio) of annotated concepts. Models trained on CMSDs outperform those using
traditional legacy training in both early and late fusion approaches. Notably,
this approach enables late fusion models to achieve performance close to that
of early fusion models. These findings underscore the importance of
modality-specific annotations in developing robust, self-explainable video
models and contribute to advancing interpretable multimodal learning in complex
video analysis.

</details>

### [132] [Enhanced Small Target Detection via Multi-Modal Fusion and Attention Mechanisms: A YOLOv5 Approach](https://arxiv.org/abs/2504.11262)
*Xiaoxiao Ma,Junxiong Tong*

Main category: cs.CV

TLDR: 提出了一种基于多模态图像融合和注意力机制的小目标检测方法，结合YOLOv5和卷积注意力模块，显著提升了复杂环境中的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现代战争对情报的依赖增加，小目标检测在复杂环境中面临干扰和实时性挑战。

Method: 利用YOLOv5框架，融合红外和可见光数据，并引入卷积注意力模块，通过特征点匹配实现多模态数据集配准。

Result: 在反无人机和Visdrone数据集上验证了方法的有效性，对小目标和弱光目标检测表现优异。

Conclusion: 该方法通过多模态融合和注意力机制，显著提升了小目标检测的准确性和鲁棒性。

Abstract: With the rapid development of information technology, modern warfare
increasingly relies on intelligence, making small target detection critical in
military applications. The growing demand for efficient, real-time detection
has created challenges in identifying small targets in complex environments due
to interference. To address this, we propose a small target detection method
based on multi-modal image fusion and attention mechanisms. This method
leverages YOLOv5, integrating infrared and visible light data along with a
convolutional attention module to enhance detection performance. The process
begins with multi-modal dataset registration using feature point matching,
ensuring accurate network training. By combining infrared and visible light
features with attention mechanisms, the model improves detection accuracy and
robustness. Experimental results on anti-UAV and Visdrone datasets demonstrate
the effectiveness and practicality of our approach, achieving superior
detection results for small and dim targets.

</details>

### [133] [Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning](https://arxiv.org/abs/2504.11268)
*Juan Garcia Giraldo,Nikolaos Dimitriadis,Ke Wang,Pascal Frossard*

Main category: cs.CV

TLDR: 论文提出了一种针对单输入多输出（SIMO）多任务场景的模型合并方法，解决了现有方法因特征表示不对齐导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法仅适用于单输入单输出（SISO）场景，忽略了多任务可能共享同一输入的情况（如场景理解），导致性能下降。

Method: 提出了两种简单高效的修复方法，重新对齐合并后的编码器与任务特定解码器的特征表示。

Result: 实验表明，任务算术足以实现多任务能力，但需重新对齐特征表示；所提方法性能与传统多任务学习相当，但样本和训练步骤更少。

Conclusion: 该方法计算高效且灵活，为离线识别任务关系提供了新思路。

Abstract: Model merging is a flexible and computationally tractable approach to merge
single-task checkpoints into a multi-task model. Prior work has solely focused
on constrained multi-task settings where there is a one-to-one mapping between
a sample and a task, overlooking the paradigm where multiple tasks may operate
on the same sample, e.g., scene understanding. In this paper, we focus on the
multi-task setting with single-input-multiple-outputs (SIMO) and show that it
qualitatively differs from the single-input-single-output model merging
settings studied in the literature due to the existence of task-specific
decoders and diverse loss objectives. We identify that existing model merging
methods lead to significant performance degradation, primarily due to
representation misalignment between the merged encoder and task-specific
decoders. We propose two simple and efficient fixes for the SIMO setting to
re-align the feature representation after merging. Compared to joint
fine-tuning, our approach is computationally effective and flexible, and sheds
light into identifying task relationships in an offline manner. Experiments on
NYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task
arithmetic suffices to enable multi-task capabilities; however, the
representations generated by the merged encoder has to be re-aligned with the
task-specific heads; (2) the proposed architecture rivals traditional
multi-task learning in performance but requires fewer samples and training
steps by leveraging the existence of task-specific models.

</details>

### [134] [Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution](https://arxiv.org/abs/2504.11271)
*Xinning Chai,Yao Zhang,Yuxuan Zhang,Zhengxue Cheng,Yingsheng Qin,Yucai Yang,Li Song*

Main category: cs.CV

TLDR: 论文提出DSCLoRA方法，通过低秩适应和知识蒸馏提升轻量级超分辨率模型的性能，不增加复杂度或推理成本。


<details>
  <summary>Details</summary>
Motivation: CNN在图像超分辨率中表现优异，但性能提升通常需要更深的网络和更大的特征图，导致复杂度和成本增加。

Method: 结合ConvLoRA和知识蒸馏，替换SPAN中的SPAB模块为SConvLB模块，并在像素洗牌块及其前卷积层中集成ConvLoRA。

Result: 在基准数据集上，DSCLoRA在PSNR和SSIM上优于SPAN，同时保持高效性，并在NTIRE 2025挑战赛中排名第一。

Conclusion: DSCLoRA通过低秩适应和知识蒸馏，有效提升了轻量级模型的性能，且不增加复杂度。

Abstract: Convolutional neural networks (CNNs) have been widely used in efficient image
super-resolution. However, for CNN-based methods, performance gains often
require deeper networks and larger feature maps, which increase complexity and
inference costs. Inspired by LoRA's success in fine-tuning large language
models, we explore its application to lightweight models and propose
Distillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which
improves model performance without increasing architectural complexity or
inference costs. Specifically, we integrate ConvLoRA into the efficient SR
network SPAN by replacing the SPAB module with the proposed SConvLB module and
incorporating ConvLoRA layers into both the pixel shuffle block and its
preceding convolutional layer. DSCLoRA leverages low-rank decomposition for
parameter updates and employs a spatial feature affinity-based knowledge
distillation strategy to transfer second-order statistical information from
teacher models (pre-trained SPAN) to student models (ours). This method
preserves the core knowledge of lightweight models and facilitates optimal
solution discovery under certain conditions. Experiments on benchmark datasets
show that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its
efficiency and competitive image quality. Notably, DSCLoRA ranked first in the
Overall Performance Track of the NTIRE 2025 Efficient Super-Resolution
Challenge. Our code and models are made publicly available at
https://github.com/Yaozzz666/DSCF-SR.

</details>

### [135] [UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer](https://arxiv.org/abs/2504.11289)
*Xiang Wang,Shiwei Zhang,Longxiang Tang,Yingya Zhang,Changxin Gao,Yuehuan Wang,Nong Sang*

Main category: cs.CV

TLDR: UniAnimate-DiT利用Wan2.1模型和LoRA技术，通过轻量级姿态编码器和简单拼接操作，实现了高保真且时间一致的人像动画生成。


<details>
  <summary>Details</summary>
Motivation: 为了在保持Wan2.1模型强大生成能力的同时减少训练内存开销，并实现高质量的人像动画。

Method: 采用LoRA技术微调少量参数，设计轻量级姿态编码器，并通过拼接操作整合参考外观和姿态信息。

Result: 实验表明，该方法能生成视觉上逼真且时间一致的高保真动画，并支持从480P无缝扩展到720P。

Conclusion: UniAnimate-DiT在高效训练和高质量动画生成方面表现出色，代码已开源。

Abstract: This report presents UniAnimate-DiT, an advanced project that leverages the
cutting-edge and powerful capabilities of the open-source Wan2.1 model for
consistent human image animation. Specifically, to preserve the robust
generative capabilities of the original Wan2.1 model, we implement Low-Rank
Adaptation (LoRA) technique to fine-tune a minimal set of parameters,
significantly reducing training memory overhead. A lightweight pose encoder
consisting of multiple stacked 3D convolutional layers is designed to encode
motion information of driving poses. Furthermore, we adopt a simple
concatenation operation to integrate the reference appearance into the model
and incorporate the pose information of the reference image for enhanced pose
alignment. Experimental results show that our approach achieves visually
appearing and temporally consistent high-fidelity animations. Trained on 480p
(832x480) videos, UniAnimate-DiT demonstrates strong generalization
capabilities to seamlessly upscale to 720P (1280x720) during inference. The
training and inference code is publicly available at
https://github.com/ali-vilab/UniAnimate-DiT.

</details>

### [136] [Autoregressive Distillation of Diffusion Transformers](https://arxiv.org/abs/2504.11295)
*Yeongmin Kim,Sotiris Anagnostidis,Yuming Du,Edgar Schönfeld,Jonas Kohler,Markos Georgopoulos,Albert Pumarola,Ali Thabet,Artsiom Sanakoyeu*

Main category: cs.CV

TLDR: 论文提出了一种名为AutoRegressive Distillation（ARD）的新方法，通过利用ODE的历史轨迹预测未来步骤，减少资源消耗和曝光偏差，并在图像生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成高保真图像时资源消耗大，且依赖最近去噪样本作为输入，容易受到曝光偏差的影响。

Method: ARD通过引入历史轨迹预测未来步骤，修改教师Transformer架构，添加时间嵌入和块级因果注意力掩码，并在低层Transformer中利用历史输入。

Result: 在ImageNet和T2I合成任务中，ARD实现了FID的显著降低（5倍），仅需1.1%额外FLOPs，并在4步内达到1.84的FID。

Conclusion: ARD有效减少了资源消耗和曝光偏差，提升了图像生成性能，适用于高分辨率任务。

Abstract: Diffusion models with transformer architectures have demonstrated promising
capabilities in generating high-fidelity images and scalability for high
resolution. However, iterative sampling process required for synthesis is very
resource-intensive. A line of work has focused on distilling solutions to
probability flow ODEs into few-step student models. Nevertheless, existing
methods have been limited by their reliance on the most recent denoised samples
as input, rendering them susceptible to exposure bias. To address this
limitation, we propose AutoRegressive Distillation (ARD), a novel approach that
leverages the historical trajectory of the ODE to predict future steps. ARD
offers two key benefits: 1) it mitigates exposure bias by utilizing a predicted
historical trajectory that is less susceptible to accumulated errors, and 2) it
leverages the previous history of the ODE trajectory as a more effective source
of coarse-grained information. ARD modifies the teacher transformer
architecture by adding token-wise time embedding to mark each input from the
trajectory history and employs a block-wise causal attention mask for training.
Furthermore, incorporating historical inputs only in lower transformer layers
enhances performance and efficiency. We validate the effectiveness of ARD in a
class-conditioned generation on ImageNet and T2I synthesis. Our model achieves
a $5\times$ reduction in FID degradation compared to the baseline methods while
requiring only 1.1\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of
1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available
1024p text-to-image distilled models in prompt adherence score with a minimal
drop in FID compared to the teacher. Project page:
https://github.com/alsdudrla10/ARD.

</details>

### [137] [CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection](https://arxiv.org/abs/2504.11305)
*Jincheng Kang,Yi Cen,Yigang Cen,Ke Wang,Yuhan Liu*

Main category: cs.CV

TLDR: CFIS-YOLO是一种轻量级目标检测模型，针对边缘设备优化，显著提升了木材缺陷检测的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统木材缺陷检测方法成本高且主观性强，而主流深度学习模型在边缘部署时难以平衡准确性和效率。

Method: 模型采用增强的C2f结构、动态特征重组模块和新型损失函数，结合辅助边界框和角度约束。

Result: 在公开数据集上，mAP@0.5达到77.5%，比YOLOv10s高4个百分点；边缘设备上实现135 FPS，功耗降低至17.3%。

Conclusion: CFIS-YOLO是资源受限环境下木材缺陷检测的实用有效解决方案。

Abstract: Wood defect detection is critical for ensuring quality control in the wood
processing industry. However, current industrial applications face two major
challenges: traditional methods are costly, subjective, and labor-intensive,
while mainstream deep learning models often struggle to balance detection
accuracy and computational efficiency for edge deployment. To address these
issues, this study proposes CFIS-YOLO, a lightweight object detection model
optimized for edge devices. The model introduces an enhanced C2f structure, a
dynamic feature recombination module, and a novel loss function that
incorporates auxiliary bounding boxes and angular constraints. These
innovations improve multi-scale feature fusion and small object localization
while significantly reducing computational overhead. Evaluated on a public wood
defect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of
77.5\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON
BM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to
17.3\% of the original implementation, and incurs only a 0.5 percentage point
drop in mAP. These results demonstrate that CFIS-YOLO is a practical and
effective solution for real-world wood defect detection in resource-constrained
environments.

</details>

### [138] [Context-Aware Palmprint Recognition via a Relative Similarity Metric](https://arxiv.org/abs/2504.11306)
*Trinnhallen Brisley,Aryan Gandhi,Joseph Magen*

Main category: cs.CV

TLDR: 提出了一种基于相对相似性度量（RSM）的新方法，用于增强掌纹识别的鲁棒性和区分性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖直接成对相似性度量（如余弦或欧氏距离），无法捕捉数据集中相似性的相对关系。

Method: 通过评估相似性分数在所有身份中的相对一致性，减少误报和漏报，并在CCNet架构上实现。

Result: 在Tongji数据集上达到0.000036%的等错误率（EER），优于先前方法。

Conclusion: 将关系结构引入掌纹匹配过程显著提升了性能。

Abstract: We propose a new approach to matching mechanism for palmprint recognition by
introducing a Relative Similarity Metric (RSM) that enhances the robustness and
discriminability of existing matching frameworks. While conventional systems
rely on direct pairwise similarity measures, such as cosine or Euclidean
distances, these metrics fail to capture how a pairwise similarity compares
within the context of the entire dataset. Our method addresses this by
evaluating the relative consistency of similarity scores across up to all
identities, allowing for better suppression of false positives and negatives.
Applied atop the CCNet architecture, our method achieves a new state-of-the-art
0.000036% Equal Error Rate (EER) on the Tongji dataset, outperforming previous
methods and demonstrating the efficacy of incorporating relational structure
into the palmprint matching process.

</details>

### [139] [Uncertainty Estimation for Trust Attribution to Speed-of-Sound Reconstruction with Variational Networks](https://arxiv.org/abs/2504.11307)
*Sonia Laguna,Lin Zhang,Can Deniz Bezek,Monika Farkas,Dieter Schweizer,Rahel A. Kubik-Huch,Orcun Goksel*

Main category: cs.CV

TLDR: 该论文提出了一种基于不确定性的自动帧选择方法，用于改进声速（SoS）重建的准确性，并应用于乳腺癌的鉴别诊断。


<details>
  <summary>Details</summary>
Motivation: 声速成像是组织生物力学特性的潜在诊断标志物，但数据帧可能因噪声而损坏，影响重建质量。通过利用重建中的不确定性，可以自动选择最可信的数据帧。

Method: 采用蒙特卡洛Dropout和贝叶斯变分推断进行不确定性估计，并基于不确定性标准从多次采集中选择最可信的帧。

Result: 在乳腺癌鉴别诊断中，基于不确定性的帧选择方法在AUC上优于未考虑不确定性的基线方法（76%和80% vs. 64%）。

Conclusion: 不确定性估计可用于从多次采集中选择最优帧，提升诊断决策的准确性。

Abstract: Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its
imaging can provide a promising biomarker for diagnosis. Reconstructing SoS
images from ultrasound acquisitions can be cast as a limited-angle
computed-tomography problem, with Variational Networks being a promising
model-based deep learning solution. Some acquired data frames may, however, get
corrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows,
which in turn negatively affects the resulting SoS reconstructions. We propose
to use the uncertainty in SoS reconstructions to attribute trust to each
individual acquired frame. Given multiple acquisitions, we then use an
uncertainty based automatic selection among these retrospectively, to improve
diagnostic decisions. We investigate uncertainty estimation based on Monte
Carlo Dropout and Bayesian Variational Inference. We assess our automatic frame
selection method for differential diagnosis of breast cancer, distinguishing
between benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions
classified as BI-RADS~4, which represents suspicious cases for probable
malignancy. The most trustworthy frame among four acquisitions of each lesion
was identified using uncertainty based criteria. Selecting a frame informed by
uncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout
and Bayesian Variational Inference, respectively, superior to any
uncertainty-uninformed baselines with the best one achieving 64%. A novel use
of uncertainty estimation is proposed for selecting one of multiple data
acquisitions for further processing and decision making.

</details>

### [140] [Big Brother is Watching: Proactive Deepfake Detection via Learnable Hidden Face](https://arxiv.org/abs/2504.11309)
*Hongbo Li,Shangchao Yang,Ruiyang Xia,Lin Yuan,Xinbo Gao*

Main category: cs.CV

TLDR: 论文提出了一种基于可学习隐藏人脸的新型检测框架，结合半脆弱可逆隐写网络，主动防御深度伪造技术。


<details>
  <summary>Details</summary>
Motivation: 被动检测方法难以泛化应对多种伪造技术和数据集，主动防御技术成为研究重点。本文旨在填补被动检测与主动防御之间的空白。

Method: 利用半脆弱可逆隐写网络，将秘密模板图像嵌入宿主图像中，通过反向隐写过程检测恶意伪造。秘密模板在训练中优化为中性面部外观，并采用自混合机制和鲁棒性学习策略。

Result: 在多个数据集上的实验表明，该方法优于现有的被动和主动检测方法。

Conclusion: 提出的主动防御框架有效解决了深度伪造检测问题，具有优越性和鲁棒性。

Abstract: As deepfake technologies continue to advance, passive detection methods
struggle to generalize with various forgery manipulations and datasets.
Proactive defense techniques have been actively studied with the primary aim of
preventing deepfake operation effectively working. In this paper, we aim to
bridge the gap between passive detection and proactive defense, and seek to
solve the detection problem utilizing a proactive methodology. Inspired by
several watermarking-based forensic methods, we explore a novel detection
framework based on the concept of ``hiding a learnable face within a face''.
Specifically, relying on a semi-fragile invertible steganography network, a
secret template image is embedded into a host image imperceptibly, acting as an
indicator monitoring for any malicious image forgery when being restored by the
inverse steganography process. Instead of being manually specified, the secret
template is optimized during training to resemble a neutral facial appearance,
just like a ``big brother'' hidden in the image to be protected. By
incorporating a self-blending mechanism and robustness learning strategy with a
simulative transmission channel, a robust detector is built to accurately
distinguish if the steganographic image is maliciously tampered or benignly
processed. Finally, extensive experiments conducted on multiple datasets
demonstrate the superiority of the proposed approach over competing passive and
proactive detection methods.

</details>

### [141] [Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection](https://arxiv.org/abs/2504.11310)
*Dayong Liu,Qingrui Zhang,Zeyang Meng*

Main category: cs.CV

TLDR: 提出了一种基于YOLOv5和点云3D投影的智能驾驶车辆多目标跟踪与检测方法，通过图像增强和点云技术实现高精度跟踪。


<details>
  <summary>Details</summary>
Motivation: 多目标跟踪与检测任务中，准确关联目标并形成稳定轨迹是一个复杂问题，尤其在智能驾驶场景中。

Method: 使用Retinex算法增强图像，基于YOLOv5构建检测模型，结合点云3D投影技术推断目标位置变化。

Result: 实验显示，该方法在智能驾驶车辆前多目标跟踪中MOTA值大于30，性能优越。

Conclusion: 该方法有效提升了多目标跟踪与检测的精度和稳定性，适用于智能驾驶场景。

Abstract: In multi-target tracking and detection tasks, it is necessary to continuously
track multiple targets, such as vehicles, pedestrians, etc. To achieve this
goal, the system must be able to continuously acquire and process image frames
containing these targets. These consecutive frame images enable the algorithm
to update the position and state of the target in real-time in each frame of
the image. How to accurately associate the detected target with the target in
the previous or next frame to form a stable trajectory is a complex problem.
Therefore, a multi object tracking and detection method for intelligent driving
vehicles based on YOLOv5 and point cloud 3D projection is proposed. Using
Retinex algorithm to enhance the image of the environment in front of the
vehicle, remove light interference in the image, and build an intelligent
detection model based on YOLOv5 network structure. The enhanced image is input
into the model, and multiple targets in front of the vehicle are identified
through feature extraction and target localization. By combining point cloud 3D
projection technology, the correlation between the position changes of adjacent
frame images in the projection coordinate system can be inferred. By
sequentially projecting the multi-target recognition results of multiple
consecutive frame images into the 3D laser point cloud environment, effective
tracking of the motion trajectories of all targets in front of the vehicle can
be achieved. The experimental results show that the application of this method
for intelligent driving vehicle front multi-target tracking and detection
yields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its
superior tracking and detection performance.

</details>

### [142] [PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild](https://arxiv.org/abs/2504.11326)
*Henghui Ding,Chang Liu,Nikhila Ravi,Shuting He,Yunchao Wei,Song Bai,Philip Torr,Kehuan Song,Xinglin Xie,Kexin Zhang,Licheng Jiao,Lingling Li,Shuyuan Yang,Xuqiang Cao,Linnan Zhao,Jiaxuan Zhao,Fang Liu,Mengjiao Wang,Junpei Zhang,Xu Liu,Yuting Yang,Mengru Ma,Hao Fang,Runmin Cong,Xiankai Lu,Zhiyang Che,Wei Zhan,Tianming Liang,Haichao Jiang,Wei-Shi Zheng,Jian-Fang Hu,Haobo Yuan,Xiangtai Li,Tao Zhang,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TLDR: 报告总结了CVPR 2025中Pixel-level Video Understanding in the Wild (PVUW)挑战赛的成果、方法和未来方向，包含MOSE和MeViS两个赛道。


<details>
  <summary>Details</summary>
Motivation: 推动复杂场景视频分割技术的发展，通过新数据集更好地反映现实场景。

Method: 挑战赛分为两个赛道：MOSE（复杂场景视频对象分割）和MeViS（基于语言和运动的视频分割），并引入更具挑战性的数据集。

Result: 通过详细评估和分析，展示了当前最先进技术和新兴趋势。

Conclusion: 挑战赛为复杂视频分割领域提供了宝贵见解，并指明了未来研究方向。

Abstract: This report provides a comprehensive overview of the 4th Pixel-level Video
Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.
It summarizes the challenge outcomes, participating methodologies, and future
research directions. The challenge features two tracks: MOSE, which focuses on
complex scene video object segmentation, and MeViS, which targets
motion-guided, language-based video segmentation. Both tracks introduce new,
more challenging datasets designed to better reflect real-world scenarios.
Through detailed evaluation and analysis, the challenge offers valuable
insights into the current state-of-the-art and emerging trends in complex video
segmentation. More information can be found on the workshop website:
https://pvuw.github.io/.

</details>

### [143] [Seedream 3.0 Technical Report](https://arxiv.org/abs/2504.11346)
*Yu Gao,Lixue Gong,Qiushan Guo,Xiaoxia Hou,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xuanda Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Xin Xia,Xuefeng Xiao,Zhonghua Zhai,Xinyu Zhang,Qi Zhang,Yuwei Zhang,Shijia Zhao,Jianchao Yang,Weilin Huang*

Main category: cs.CV

TLDR: Seedream 3.0是一个高性能的中英双语图像生成基础模型，通过数据增强、训练优化和后训练改进，显著提升了图像生成的质量、分辨率和速度。


<details>
  <summary>Details</summary>
Motivation: 解决Seedream 2.0在复杂提示对齐、细粒度排版生成、视觉美学和保真度不足以及分辨率受限等方面的挑战。

Method: 采用缺陷感知训练范式、双轴协作数据采样框架、混合分辨率训练、跨模态RoPE、表示对齐损失等技术，并在后训练阶段使用多样化美学描述和VLM奖励模型。

Result: 实现了4到8倍的加速，同时保持图像质量；显著提升了复杂中文文本渲染能力，支持原生2K高分辨率输出。

Conclusion: Seedream 3.0在图像生成能力、速度和分辨率方面均有显著提升，尤其在专业排版生成和高分辨率图像生成上表现优异。

Abstract: We present Seedream 3.0, a high-performance Chinese-English bilingual image
generation foundation model. We develop several technical improvements to
address existing challenges in Seedream 2.0, including alignment with
complicated prompts, fine-grained typography generation, suboptimal visual
aesthetics and fidelity, and limited image resolutions. Specifically, the
advancements of Seedream 3.0 stem from improvements across the entire pipeline,
from data construction to model deployment. At the data stratum, we double the
dataset using a defect-aware training paradigm and a dual-axis collaborative
data-sampling framework. Furthermore, we adopt several effective techniques
such as mixed-resolution training, cross-modality RoPE, representation
alignment loss, and resolution-aware timestep sampling in the pre-training
phase. During the post-training stage, we utilize diversified aesthetic
captions in SFT, and a VLM-based reward model with scaling, thereby achieving
outputs that well align with human preferences. Furthermore, Seedream 3.0
pioneers a novel acceleration paradigm. By employing consistent noise
expectation and importance-aware timestep sampling, we achieve a 4 to 8 times
speedup while maintaining image quality. Seedream 3.0 demonstrates significant
improvements over Seedream 2.0: it enhances overall capabilities, in particular
for text-rendering in complicated Chinese characters which is important to
professional typography generation. In addition, it provides native
high-resolution output (up to 2K), allowing it to generate images with high
visual quality.

</details>

### [144] [DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation](https://arxiv.org/abs/2504.11347)
*Soyoung Yoo,Namwoo Kang*

Main category: cs.CV

TLDR: 本文提出了一种基于生成AI的合成设计-性能数据集生成框架DeepWheel，用于填补车辆轮毂设计领域的数据空白，包含6000多张照片级图像和900个结构分析模型。


<details>
  <summary>Details</summary>
Motivation: 车辆轮毂设计领域缺乏大规模、高质量的数据集，限制了数据驱动设计的应用。

Method: 使用Stable Diffusion生成2D渲染图像，通过2.5D深度估计重建3D几何，进行结构仿真获取性能数据，并应用拓扑优化扩展设计多样性。

Result: 生成了包含6000多张图像和900个3D模型的DeepWheel数据集，支持代理模型训练、逆向设计和设计空间探索。

Conclusion: 该框架为复杂设计领域提供了数据支持，数据集已公开并适用于非商业用途。

Abstract: Data-driven design is emerging as a powerful strategy to accelerate
engineering innovation. However, its application to vehicle wheel design
remains limited due to the lack of large-scale, high-quality datasets that
include 3D geometry and physical performance metrics. To address this gap, this
study proposes a synthetic design-performance dataset generation framework
using generative AI. The proposed framework first generates 2D rendered images
using Stable Diffusion, and then reconstructs the 3D geometry through 2.5D
depth estimation. Structural simulations are subsequently performed to extract
engineering performance data. To further expand the design and performance
space, topology optimization is applied, enabling the generation of a more
diverse set of wheel designs. The final dataset, named DeepWheel, consists of
over 6,000 photo-realistic images and 900 structurally analyzed 3D models. This
multi-modal dataset serves as a valuable resource for surrogate model training,
data-driven inverse design, and design space exploration. The proposed
methodology is also applicable to other complex design domains. The dataset is
released under the Creative Commons Attribution-NonCommercial 4.0
International(CC BY-NC 4.0) and is available on the
https://www.smartdesignlab.org/datasets

</details>

### [145] [Explicit and Implicit Representations in AI-based 3D Reconstruction for Radiology: A systematic literature review](https://arxiv.org/abs/2504.11349)
*Yuezhe Yang,Boyu Yang,Yaqian Wang,Yang He,Xingbo Dong,Zhe Jin*

Main category: cs.CV

TLDR: 本文综述了基于AI的放射影像3D重建技术，分为显式和隐式方法，并探讨了评估指标、数据集及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 提高放射影像3D重建的精度和效率，减少患者辐射暴露，改善临床诊断。

Method: 将AI算法分为显式（点、体积、高斯表示）和隐式（隐式先验嵌入、神经辐射场）两类，并分析其原理。

Result: 总结了当前技术、评估指标和数据集，提出了发展现状和挑战。

Conclusion: AI在3D重建中潜力巨大，但仍需解决关键挑战以推动未来发展。

Abstract: The demand for high-quality medical imaging in clinical practice and assisted
diagnosis has made 3D reconstruction in radiological imaging a key research
focus. Artificial intelligence (AI) has emerged as a promising approach to
enhancing reconstruction accuracy while reducing acquisition and processing
time, thereby minimizing patient radiation exposure and discomfort and
ultimately benefiting clinical diagnosis. This review explores state-of-the-art
AI-based 3D reconstruction algorithms in radiological imaging, categorizing
them into explicit and implicit approaches based on their underlying
principles. Explicit methods include point-based, volume-based, and Gaussian
representations, while implicit methods encompass implicit prior embedding and
neural radiance fields. Additionally, we examine commonly used evaluation
metrics and benchmark datasets. Finally, we discuss the current state of
development, key challenges, and future research directions in this evolving
field. Our project available on: https://github.com/Bean-Young/AI4Med.

</details>

### [146] [A Decade of Wheat Mapping for Lebanon](https://arxiv.org/abs/2504.11366)
*Hasan Wehbi,Hasan Nasrallah,Mohamad Hasan Zahweh,Zeinab Takach,Veera Ganesh Yalla,Ali J. Ghandour*

Main category: cs.CV

TLDR: 本文提出了一种改进的小麦田地分割方法，结合时空视觉变换器和高效参数微调技术，用于卫星图像中的小麦田地精确制图。


<details>
  <summary>Details</summary>
Motivation: 小麦是全球粮食安全的重要组成部分，准确制图有助于政策制定者和农业组织做出更明智的决策。

Method: 采用时空视觉变换器（TSViT）结合参数高效微调（PEFT）和基于FTW框架的后处理流程，解决现有方法中的田地边界模糊问题。

Result: 实验表明，该方法在田地边界划分和精度上表现优异，适用于农业监测和历史趋势分析。

Conclusion: 该方法为小麦田地精确制图提供了新工具，支持作物监测和产量估算等后续研究。

Abstract: Wheat accounts for approximately 20% of the world's caloric intake, making it
a vital component of global food security. Given this importance, mapping wheat
fields plays a crucial role in enabling various stakeholders, including policy
makers, researchers, and agricultural organizations, to make informed decisions
regarding food security, supply chain management, and resource allocation. In
this paper, we tackle the problem of accurately mapping wheat fields out of
satellite images by introducing an improved pipeline for winter wheat
segmentation, as well as presenting a case study on a decade-long analysis of
wheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer
(TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing
pipeline based on the Fields of The World (FTW) framework. Our proposed
pipeline addresses key challenges encountered in existing approaches, such as
the clustering of small agricultural parcels in a single large field. By
merging wheat segmentation with precise field boundary extraction, our method
produces geometrically coherent and semantically rich maps that enable us to
perform in-depth analysis such as tracking crop rotation pattern over years.
Extensive evaluations demonstrate improved boundary delineation and field-level
precision, establishing the potential of the proposed framework in operational
agricultural monitoring and historical trend analysis. By allowing for accurate
mapping of wheat fields, this work lays the foundation for a range of critical
studies and future advances, including crop monitoring and yield estimation.

</details>

### [147] [From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation](https://arxiv.org/abs/2504.11368)
*Jingkun Chen,Haoran Duan,Xiao Zhang,Boyan Gao,Tao Tan,Vicente Grau,Jungong Han*

Main category: cs.CV

TLDR: 提出了一种结合医生注视数据和视觉语言模型的教师-学生框架，用于医学图像分割，显著提升了性能并保持了临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要高成本的像素级标注，而弱监督方法（如医生注视数据和视觉语言模型）各有局限性，无法单独满足需求。

Method: 通过教师-学生框架整合注视数据和语言监督，采用多尺度特征对齐、置信加权一致性约束和自适应掩码三种策略。

Result: 在Kvasir-SEG、NCI-ISBI和ISIC数据集上Dice分数分别达到80.78%、80.53%和84.22%，比基线提升3-5%。

Conclusion: 结合人类视觉注意力和AI生成的语义上下文，克服了单一弱监督信号的局限性，推动了高效标注的医学AI系统发展。

Abstract: Medical image segmentation remains challenging due to the high cost of
pixel-level annotations for training. In the context of weak supervision,
clinician gaze data captures regions of diagnostic interest; however, its
sparsity limits its use for segmentation. In contrast, vision-language models
(VLMs) provide semantic context through textual descriptions but lack the
explanation precision required. Recognizing that neither source alone suffices,
we propose a teacher-student framework that integrates both gaze and language
supervision, leveraging their complementary strengths. Our key insight is that
gaze data indicates where clinicians focus during diagnosis, while VLMs explain
why those regions are significant. To implement this, the teacher model first
learns from gaze points enhanced by VLM-generated descriptions of lesion
morphology, establishing a foundation for guiding the student model. The
teacher then directs the student through three strategies: (1) Multi-scale
feature alignment to fuse visual cues with textual semantics; (2)
Confidence-weighted consistency constraints to focus on reliable predictions;
(3) Adaptive masking to limit error propagation in uncertain areas. Experiments
on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves
Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over
gaze baselines without increasing the annotation burden. By preserving
correlations among predictions, gaze data, and lesion descriptions, our
framework also maintains clinical interpretability. This work illustrates how
integrating human visual attention with AI-generated semantic context can
effectively overcome the limitations of individual weak supervision signals,
thereby advancing the development of deployable, annotation-efficient medical
AI systems. Code is available at: https://github.com/jingkunchen/FGI.git.

</details>

### [148] [Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model](https://arxiv.org/abs/2504.11379)
*Liu Yang,Huiyu Duan,Yucheng Zhu,Xiaohong Liu,Lu Liu,Zitong Xu,Guangji Ma,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet*

Main category: cs.CV

TLDR: 论文提出了Any2Omni数据集和Omni²模型，用于解决360度全景图像生成和编辑的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于360度全景图像（ODIs）的捕获成本高且需要专业设备，其生成和编辑变得重要，但现有2D图像方法难以满足需求。

Method: 构建了包含60,000+训练数据的Any2Omni数据集，并提出了Omni²模型，支持多种生成和编辑任务。

Result: 实验证明Omni²模型在ODI生成和编辑任务中表现优越且有效。

Conclusion: Any2Omni数据集和Omni²模型为ODI生成和编辑提供了全面解决方案。

Abstract: $360^{\circ}$ omnidirectional images (ODIs) have gained considerable
attention recently, and are widely used in various virtual reality (VR) and
augmented reality (AR) applications. However, capturing such images is
expensive and requires specialized equipment, making ODI synthesis increasingly
important. While common 2D image generation and editing methods are rapidly
advancing, these models struggle to deliver satisfactory results when
generating or editing ODIs due to the unique format and broad 360$^{\circ}$
Field-of-View (FoV) of ODIs. To bridge this gap, we construct
\textbf{\textit{Any2Omni}}, the first comprehensive ODI generation-editing
dataset comprises 60,000+ training data covering diverse input conditions and
up to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an
\textbf{\underline{Omni}} model for \textbf{\underline{Omni}}-directional image
generation and editing (\textbf{\textit{Omni$^2$}}), with the capability of
handling various ODI generation and editing tasks under diverse input
conditions using one model. Extensive experiments demonstrate the superiority
and effectiveness of the proposed Omni$^2$ model for both the ODI generation
and editing tasks.

</details>

### [149] [Multi-level Cellular Automata for FLIM networks](https://arxiv.org/abs/2504.11406)
*Felipe Crispim Salvagnini,Jancarlo F. Gomes,Cid A. N. Santos,Silvio Jamil F. Guimarães,Alexandre X. Falcão*

Main category: cs.CV

TLDR: 论文提出了一种结合FLIM和CA的方法，用于解决深度学习显著目标检测中数据稀缺和计算资源有限的问题，通过专家知识初始化CA状态，无需用户交互。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境（如发展中国家）中，深度学习显著目标检测面临标注数据不足和复杂网络架构的挑战。结合现代与经典技术可以保持性能并实现实际应用。

Method: 使用FLIM方法设计卷积编码器，通过用户绘制的标记学习滤波器，结合自适应解码器构建轻量网络。利用FLIM网络初始化CA状态，形成多级CA框架。

Result: 在两种医学数据集上的实验表明，该方法与现有深度学习显著目标检测模型相比具有竞争力。

Conclusion: 通过结合FLIM和CA，该方法在数据稀缺和资源有限的情况下实现了高性能，为实际应用提供了可行方案。

Abstract: The necessity of abundant annotated data and complex network architectures
presents a significant challenge in deep-learning Salient Object Detection
(deep SOD) and across the broader deep-learning landscape. This challenge is
particularly acute in medical applications in developing countries with limited
computational resources. Combining modern and classical techniques offers a
path to maintaining competitive performance while enabling practical
applications. Feature Learning from Image Markers (FLIM) methodology empowers
experts to design convolutional encoders through user-drawn markers, with
filters learned directly from these annotations. Recent findings demonstrate
that coupling a FLIM encoder with an adaptive decoder creates a flyweight
network suitable for SOD, requiring significantly fewer parameters than
lightweight models and eliminating the need for backpropagation. Cellular
Automata (CA) methods have proven successful in data-scarce scenarios but
require proper initialization -- typically through user input, priors, or
randomness. We propose a practical intersection of these approaches: using FLIM
networks to initialize CA states with expert knowledge without requiring user
interaction for each image. By decoding features from each level of a FLIM
network, we can initialize multiple CAs simultaneously, creating a multi-level
framework. Our method leverages the hierarchical knowledge encoded across
different network layers, merging multiple saliency maps into a high-quality
final output that functions as a CA ensemble. Benchmarks across two challenging
medical datasets demonstrate the competitiveness of our multi-level CA approach
compared to established models in the deep SOD literature.

</details>

### [150] [Robustness and sex differences in skin cancer detection: logistic regression vs CNNs](https://arxiv.org/abs/2504.11415)
*Nikolette Pedersen,Regitze Sydendal,Andreas Wulff,Ralf Raumanns,Eike Petersen,Veronika Cheplygina*

Main category: cs.CV

TLDR: 该研究探讨了皮肤癌检测中的性别偏见，使用逻辑回归（LR）和预训练的ResNet-50模型，发现CNN对男性患者的准确性显著高于女性患者。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决深度学习在皮肤癌检测中的可重复性和偏见问题，特别是性别偏见。

Method: 方法包括使用PAD-UFES-20数据集，训练LR（基于手工特征）和ResNet-50模型，并在不同性别组成的数据集上评估模型鲁棒性。

Result: 结果显示LR和CNN对性别分布均表现鲁棒，但CNN对男性患者的准确性和AUROC显著更高。

Conclusion: 结论强调了研究偏见对医学机器学习的重要性，并提供了可复现的数据和脚本。

Abstract: Deep learning has been reported to achieve high performances in the detection
of skin cancer, yet many challenges regarding the reproducibility of results
and biases remain. This study is a replication (different data, same analysis)
of a study on Alzheimer's disease [28] which studied robustness of logistic
regression (LR) and convolutional neural networks (CNN) across patient sexes.
We explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset
with LR trained on handcrafted features reflecting dermatological guidelines
(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We
evaluate these models in alignment with [28]: across multiple training datasets
with varied sex composition to determine their robustness. Our results show
that both the LR and the CNN were robust to the sex distributions, but the
results also revealed that the CNN had a significantly higher accuracy (ACC)
and area under the receiver operating characteristics (AUROC) for male patients
than for female patients. We hope these findings to contribute to the growing
field of investigating potential bias in popular medical machine learning
methods. The data and relevant scripts to reproduce our results can be found in
our Github.

</details>

### [151] [Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps](https://arxiv.org/abs/2504.11416)
*Panagiotis Agrafiotis,Begüm Demir*

Main category: cs.CV

TLDR: 提出了一种结合SfM-MVS和深度学习的方法Swin-BathyUNet，用于高精度海底地形建模，解决了传统方法的数据缺失和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 浅海区域的高精度地形数据对环境和人类活动至关重要，但现有方法（如SDB和SfM-MVS）存在数据缺失、噪声或高成本问题。

Method: 结合SfM-MVS的高保真3D重建和深度学习（Swin-BathyUNet），利用光谱分析和自注意力机制改进地形预测。

Result: 在地中海和波罗的海的测试中，该方法显著提高了地形数据的精度、覆盖范围和噪声抑制能力。

Conclusion: Swin-BathyUNet不仅填补了SfM-MVS的数据缺失，还可作为独立解决方案，为海底地形建模提供了高效且准确的新方法。

Abstract: Accurate, detailed, and high-frequent bathymetry is crucial for shallow
seabed areas facing intense climatological and anthropogenic pressures. Current
methods utilizing airborne or satellite optical imagery to derive bathymetry
primarily rely on either SfM-MVS with refraction correction or Spectrally
Derived Bathymetry (SDB). However, SDB methods often require extensive manual
fieldwork or costly reference data, while SfM-MVS approaches face challenges
even after refraction correction. These include depth data gaps and noise in
environments with homogeneous visual textures, which hinder the creation of
accurate and complete Digital Surface Models (DSMs) of the seabed. To address
these challenges, this work introduces a methodology that combines the
high-fidelity 3D reconstruction capabilities of the SfM-MVS methods with
state-of-the-art refraction correction techniques, along with the spectral
analysis capabilities of a new deep learning-based method for bathymetry
prediction. This integration enables a synergistic approach where SfM-MVS
derived DSMs with data gaps are used as training data to generate complete
bathymetric maps. In this context, we propose Swin-BathyUNet that combines
U-Net with Swin Transformer self-attention layers and a cross-attention
mechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve
bathymetric accuracy by capturing long-range spatial relationships and can also
function as a standalone solution for standard SDB with various training depth
data, independent of the SfM-MVS output. Experimental results in two completely
different test sites in the Mediterranean and Baltic Seas demonstrate the
effectiveness of the proposed approach through extensive experiments that
demonstrate improvements in bathymetric accuracy, detail, coverage, and noise
reduction in the predicted DSM. The code is available at
https://github.com/pagraf/Swin-BathyUNet.

</details>

### [152] [Leveraging Point Transformers for Detecting Anatomical Landmarks in Digital Dentistry](https://arxiv.org/abs/2504.11418)
*Tibor Kubík,Oldřich Kodym,Petr Šilling,Kateřina Trávníčková,Tomáš Mojžiš,Jan Matula*

Main category: cs.CV

TLDR: 论文提出了一种基于点云学习和Transformer架构的方法，用于自动检测口腔扫描中的关键解剖标志，解决了数据集小、解剖变异大等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着口腔扫描设备的普及，自动检测关键解剖标志（如牙尖、牙龈边界等）的需求增加，但面临数据集小、解剖变异大等挑战。

Method: 采用Point Transformer v3模块提取几何和解剖特征，结合轻量级解码器和基于图的非最小值抑制技术预测标志点。

Result: 实验结果表明该方法在3DTeethLand挑战中表现良好，并提供了对学习特征可解释性的见解。

Conclusion: 该方法为口腔扫描中的自动标志检测提供了有效解决方案，并展示了Transformer架构在此领域的潜力。

Abstract: The increasing availability of intraoral scanning devices has heightened
their importance in modern clinical orthodontics. Clinicians utilize advanced
Computer-Aided Design techniques to create patient-specific treatment plans
that include laboriously identifying crucial landmarks such as cusps,
mesial-distal locations, facial axis points, and tooth-gingiva boundaries.
Detecting such landmarks automatically presents challenges, including limited
dataset sizes, significant anatomical variability among subjects, and the
geometric nature of the data. We present our experiments from the 3DTeethLand
Grand Challenge at MICCAI 2024. Our method leverages recent advancements in
point cloud learning through transformer architectures. We designed a Point
Transformer v3 inspired module to capture meaningful geometric and anatomical
features, which are processed by a lightweight decoder to predict per-point
distances, further processed by graph-based non-minima suppression. We report
promising results and discuss insights on learned feature interpretability.

</details>

### [153] [ADT: Tuning Diffusion Models with Adversarial Supervision](https://arxiv.org/abs/2504.11423)
*Dazhong Shen,Guanglu Song,Yi Zhang,Bingqi Ma,Lujundong Li,Dongzhi Jiang,Zhuofan Zong,Yu Liu*

Main category: cs.CV

TLDR: 论文提出了一种名为Adversarial Diffusion Tuning (ADT)的微调框架，通过对抗监督优化推理过程，以解决扩散模型训练与推理之间的分布偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在训练和推理过程中存在分布偏差，导致预测偏差和累积误差，影响生成图像的质量。

Method: ADT通过对抗监督和Siamese网络判别器优化推理过程，结合图像到图像采样策略和原始扩散损失，防止判别器被欺骗。

Result: 在Stable Diffusion模型上的实验表明，ADT显著改善了分布对齐和图像质量。

Conclusion: ADT是一种有效的方法，能够优化扩散模型的推理过程，提升生成图像的质量和分布对齐效果。

Abstract: Diffusion models have achieved outstanding image generation by reversing a
forward noising process to approximate true data distributions. During
training, these models predict diffusion scores from noised versions of true
samples in a single forward pass, while inference requires iterative denoising
starting from white noise. This training-inference divergences hinder the
alignment between inference and training data distributions, due to potential
prediction biases and cumulative error accumulation. To address this problem,
we propose an intuitive but effective fine-tuning framework, called Adversarial
Diffusion Tuning (ADT), by stimulating the inference process during
optimization and aligning the final outputs with training data by adversarial
supervision. Specifically, to achieve robust adversarial training, ADT features
a siamese-network discriminator with a fixed pre-trained backbone and
lightweight trainable parameters, incorporates an image-to-image sampling
strategy to smooth discriminative difficulties, and preserves the original
diffusion loss to prevent discriminator hacking. In addition, we carefully
constrain the backward-flowing path for back-propagating gradients along the
inference path without incurring memory overload or gradient explosion.
Finally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),
demonstrate that ADT significantly improves both distribution alignment and
image quality.

</details>

### [154] [NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors](https://arxiv.org/abs/2504.11427)
*Yanrui Bin,Wenbo Hu,Haoyuan Wang,Xinya Chen,Bing Wang*

Main category: cs.CV

TLDR: 论文提出NormalCrafter方法，利用视频扩散模型的时序先验，结合语义特征正则化（SFR）和两阶段训练协议，实现视频中高保真且时序一致的表面法线估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对静态图像，视频中的时序一致性法线估计仍具挑战性。

Method: 提出NormalCrafter，利用视频扩散模型的时序先验；引入SFR对齐扩散特征与语义线索；采用两阶段训练协议结合潜在空间和像素空间学习。

Result: 实验表明，该方法能生成时序一致且细节丰富的法线序列。

Conclusion: NormalCrafter在视频法线估计中表现出色，兼顾时序一致性和细节保留。

Abstract: Surface normal estimation serves as a cornerstone for a spectrum of computer
vision applications. While numerous efforts have been devoted to static image
scenarios, ensuring temporal coherence in video-based normal estimation remains
a formidable challenge. Instead of merely augmenting existing methods with
temporal components, we present NormalCrafter to leverage the inherent temporal
priors of video diffusion models. To secure high-fidelity normal estimation
across sequences, we propose Semantic Feature Regularization (SFR), which
aligns diffusion features with semantic cues, encouraging the model to
concentrate on the intrinsic semantics of the scene. Moreover, we introduce a
two-stage training protocol that leverages both latent and pixel space learning
to preserve spatial accuracy while maintaining long temporal context. Extensive
evaluations demonstrate the efficacy of our method, showcasing a superior
performance in generating temporally consistent normal sequences with intricate
details from diverse videos.

</details>

### [155] [Enhancing Out-of-Distribution Detection with Extended Logit Normalization](https://arxiv.org/abs/2504.11434)
*Yifan Ding,Xixi Liu,Jonas Unger,Gabriel Eilertsen*

Main category: cs.CV

TLDR: 论文提出了一种名为ELogitNorm的新方法，通过改进LogitNorm，显著提升了多种后验OOD检测方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法通常针对特定后验检测技术设计，缺乏通用性。LogitNorm在某些后验OOD检测方法中效果不佳，需要改进。

Method: 提出ELogitNorm，一种无需超参数的新方法，通过引入特征距离感知改进LogitNorm。

Result: 实验表明，ELogitNorm在OOD检测上优于现有方法，同时保持ID分类准确性。

Conclusion: ELogitNorm是一种通用且高效的OOD检测方法，显著提升了检测性能。

Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of
machine learning models. Recent advances have explored improved classification
losses and representation learning strategies to enhance OOD detection.
However, these methods are often tailored to specific post-hoc detection
techniques, limiting their generalizability. In this work, we identify a
critical issue in Logit Normalization (LogitNorm), which inhibits its
effectiveness in improving certain post-hoc OOD detection methods. To address
this, we propose Extended Logit Normalization ($\textbf{ELogitNorm}$), a novel
hyperparameter-free formulation that significantly benefits a wide range of
post-hoc detection methods. By incorporating feature distance-awareness to
LogitNorm, $\textbf{ELogitNorm}$ shows more robust OOD separability and
in-distribution (ID) confidence calibration than its predecessor. Extensive
experiments across standard benchmarks demonstrate that our approach
outperforms state-of-the-art training-time methods in OOD detection while
maintaining strong ID classification accuracy.

</details>

### [156] [Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion](https://arxiv.org/abs/2504.11447)
*An Zhaol,Shengyuan Zhang,Ling Yang,Zejian Li,Jiale Wu,Haoran Xu,AnYang Wei,Perry Pengyun GU Lingyun Sun*

Main category: cs.CV

TLDR: 提出Distillation-DPO框架，结合偏好学习和扩散蒸馏，提升3D LiDAR场景补全的质量和速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在3D LiDAR场景补全中因采样速度慢受限，现有方法如分数蒸馏会降低性能，而直接策略优化（DPO）虽能提升性能但需偏好数据。

Method: 通过学生模型生成配对补全场景，利用LiDAR评估指标构建偏好样本对，优化学生模型的分数函数差异。

Result: 实验显示，Distillation-DPO在质量上优于现有方法，且速度提升5倍以上。

Conclusion: 首次将偏好学习引入扩散蒸馏，为偏好对齐蒸馏提供了新思路。

Abstract: The application of diffusion models in 3D LiDAR scene completion is limited
due to diffusion's slow sampling speed. Score distillation accelerates
diffusion sampling but with performance degradation, while post-training with
direct policy optimization (DPO) boosts performance using preference data. This
paper proposes Distillation-DPO, a novel diffusion distillation framework for
LiDAR scene completion with preference aligment. First, the student model
generates paired completion scenes with different initial noises. Second, using
LiDAR scene evaluation metrics as preference, we construct winning and losing
sample pairs. Such construction is reasonable, since most LiDAR scene metrics
are informative but non-differentiable to be optimized directly. Third,
Distillation-DPO optimizes the student model by exploiting the difference in
score functions between the teacher and student models on the paired completion
scenes. Such procedure is repeated until convergence. Extensive experiments
demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion
models, Distillation-DPO achieves higher-quality scene completion while
accelerating the completion speed by more than 5-fold. Our method is the first
to explore adopting preference learning in distillation to the best of our
knowledge and provide insights into preference-aligned distillation. Our code
is public available on https://github.com/happyw1nd/DistillationDPO.

</details>

### [157] [PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond](https://arxiv.org/abs/2504.11451)
*Minghua Liu,Mikaela Angelina Uy,Donglai Xiang,Hao Su,Sanja Fidler,Nicholas Sharp,Jun Gao*

Main category: cs.CV

TLDR: PartField是一种基于部分的3D特征学习方法，无需预定义模板或文本名称，适用于多种模态的开放世界3D形状。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在运行时和鲁棒性上的不足，同时支持开放世界的3D形状分析。

Method: 通过对比学习从标记数据集和大型无监督数据集中提取2D和3D部分提案，生成连续特征场。

Result: PartField比现有方法准确率提高20%，且运行速度快几个数量级。

Conclusion: PartField不仅支持单形状部分分解，还能实现跨形状的一致性任务，如共分割和对应关系。

Abstract: We propose PartField, a feedforward approach for learning part-based 3D
features, which captures the general concept of parts and their hierarchy
without relying on predefined templates or text-based names, and can be applied
to open-world 3D shapes across various modalities. PartField requires only a 3D
feedforward pass at inference time, significantly improving runtime and
robustness compared to prior approaches. Our model is trained by distilling 2D
and 3D part proposals from a mix of labeled datasets and image segmentations on
large unsupervised datasets, via a contrastive learning formulation. It
produces a continuous feature field which can be clustered to yield a
hierarchical part decomposition. Comparisons show that PartField is up to 20%
more accurate and often orders of magnitude faster than other recent
class-agnostic part-segmentation methods. Beyond single-shape part
decomposition, consistency in the learned field emerges across shapes, enabling
tasks such as co-segmentation and correspondence, which we demonstrate in
several applications of these general-purpose, hierarchical, and consistent 3D
feature fields. Check our Webpage!
https://research.nvidia.com/labs/toronto-ai/partfield-release/

</details>

### [158] [SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL](https://arxiv.org/abs/2504.11455)
*Junke Wang,Zhi Tian,Xun Wang,Xinyu Zhang,Weilin Huang,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TLDR: SimpleAR是一个简单的自回归视觉生成框架，无需复杂架构修改，通过训练和推理优化，生成高分辨率图像并取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 探索自回归视觉生成的潜力，通过优化训练和推理方法，展示其在生成高分辨率图像和文本到图像任务中的竞争力。

Method: 采用自回归框架，结合监督微调（SFT）和Group Relative Policy Optimization（GRPO）训练，以及推理加速技术（如vLLM）。

Result: 模型仅需0.5B参数即可生成1024x1024高保真图像，在GenEval和DPG上表现优异；推理时间优化至约14秒。

Conclusion: SimpleAR展示了自回归视觉生成的潜力，开源代码以鼓励更多研究参与。

Abstract: This work presents SimpleAR, a vanilla autoregressive visual generation
framework without complex architecure modifications. Through careful
exploration of training and inference optimization, we demonstrate that: 1)
with only 0.5B parameters, our model can generate 1024x1024 resolution images
with high fidelity, and achieve competitive results on challenging
text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both
supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)
training could lead to significant improvements on generation aesthectics and
prompt alignment; and 3) when optimized with inference acceleraton techniques
like vLLM, the time for SimpleAR to generate an 1024x1024 image could be
reduced to around 14 seconds. By sharing these findings and open-sourcing the
code, we hope to reveal the potential of autoregressive visual generation and
encourage more participation in this research field. Code is available at
https://github.com/wdrink/SimpleAR.

</details>

### [159] [Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception](https://arxiv.org/abs/2504.11457)
*Ziqi Pang,Xin Xu,Yu-Xiong Wang*

Main category: cs.CV

TLDR: 论文分析了生成扩散模型在判别任务中的直接应用问题，提出改进方法以提升感知质量，并在多个任务中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型在判别任务中存在中间采样误差容忍与判别任务严格准确性要求之间的差距，尤其是在多模态任务中。

Method: 通过分析去噪过程中感知质量的变化，提出定制学习目标和扩散定制的数据增强方法，并利用生成过程的交互性。

Result: 改进后的方法在深度估计、参考图像分割和通用感知任务中达到最优性能。

Conclusion: 通过调整生成扩散过程与感知任务的匹配，无需架构改变即可显著提升模型性能。

Abstract: With the success of image generation, generative diffusion models are
increasingly adopted for discriminative tasks, as pixel generation provides a
unified perception interface. However, directly repurposing the generative
denoising process for discriminative objectives reveals critical gaps rarely
addressed previously. Generative models tolerate intermediate sampling errors
if the final distribution remains plausible, but discriminative tasks require
rigorous accuracy throughout, as evidenced in challenging multi-modal tasks
like referring image segmentation. Motivated by this gap, we analyze and
enhance alignment between generative diffusion processes and perception tasks,
focusing on how perception quality evolves during denoising. We find: (1)
earlier denoising steps contribute disproportionately to perception quality,
prompting us to propose tailored learning objectives reflecting varying
timestep contributions; (2) later denoising steps show unexpected perception
degradation, highlighting sensitivity to training-denoising distribution
shifts, addressed by our diffusion-tailored data augmentation; and (3)
generative processes uniquely enable interactivity, serving as controllable
user interfaces adaptable to correctional prompts in multi-round interactions.
Our insights significantly improve diffusion-based perception models without
architectural changes, achieving state-of-the-art performance on depth
estimation, referring image segmentation, and generalist perception tasks. Code
available at https://github.com/ziqipang/ADDP.

</details>

<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [160] [Network Alignment](https://arxiv.org/abs/2504.11367)
*Rui Tang,Ziyun Yong,Shuyu Jiang,Xingshu Chen,Yaofang Liu,Yi-Cheng Zhang,Gui-Quan Sun,Wei Wang*

Main category: physics.soc-ph

TLDR: 该综述总结了网络对齐研究的最新进展，分析了不同领域（如社交网络、生物信息学等）的网络对齐特点和方法，并探讨了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 揭示跨网络实体关系对理解复杂系统结构和行为至关重要，同时推动理论研究和实际应用。

Method: 综述了基于结构一致性、网络嵌入和图神经网络的方法，并讨论了不同网络条件下的对齐方法。

Result: 详细分析了各种方法的实现原理、流程和性能差异，总结了不同领域的进展。

Conclusion: 网络对齐研究仍面临挑战，未来需解决术语统一性和跨领域协作等问题。

Abstract: Complex networks are frequently employed to model physical or virtual complex
systems. When certain entities exist across multiple systems simultaneously,
unveiling their corresponding relationships across the networks becomes
crucial. This problem, known as network alignment, holds significant
importance. It enhances our understanding of complex system structures and
behaviours, facilitates the validation and extension of theoretical physics
research about studying complex systems, and fosters diverse practical
applications across various fields. However, due to variations in the
structure, characteristics, and properties of complex networks across different
fields, the study of network alignment is often isolated within each domain,
with even the terminologies and concepts lacking uniformity. This review
comprehensively summarizes the latest advancements in network alignment
research, focusing on analyzing network alignment characteristics and progress
in various domains such as social network analysis, bioinformatics,
computational linguistics and privacy protection. It provides a detailed
analysis of various methods' implementation principles, processes, and
performance differences, including structure consistency-based methods, network
embedding-based methods, and graph neural network-based (GNN-based) methods.
Additionally, the methods for network alignment under different conditions,
such as in attributed networks, heterogeneous networks, directed networks, and
dynamic networks, are presented. Furthermore, the challenges and the open
issues for future studies are also discussed.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [161] [GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA](https://arxiv.org/abs/2504.10490)
*Gabriel Bo,Marc Bernardino,Justin Gu*

Main category: cs.LG

TLDR: 论文探讨了在预训练的GPT-2模型中集成可学习和可解释模块（如KAN和GAT）以提升多任务学习准确性的潜力，但发现优化的LoRA增强Transformer表现最佳。


<details>
  <summary>Details</summary>
Motivation: 受KAN和GAT在CoT模型中的应用及与简单架构（如MLP）的争议启发，研究旨在通过改进模块提升多任务学习性能。

Method: 通过LoRA微调超参数、L2正则化增强标准自注意力Transformer，并开发了Graph LoRA和Hybrid-KAN LoRA两种变体。

Result: 优化的LoRA增强Transformer在SST测试集上达到55.249%准确率，CFIMDB开发集99.18%，检测准确率89.9%，CHRF得分42.097。

Conclusion: LoRA参数适应是情感分析、检测和生成任务中最有效的策略。

Abstract: We explore the potential of integrating learnable and interpretable
modules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based
representations--within a pre-trained GPT-2 model to enhance multi-task
learning accuracy. Motivated by the recent surge in using KAN and graph
attention (GAT) architectures in chain-of-thought (CoT) models and debates over
their benefits compared to simpler architectures like MLPs, we begin by
enhancing a standard self-attention transformer using Low-Rank Adaptation
(LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This
approach yields significant improvements. To further boost interpretability and
richer representations, we develop two variants that attempt to improve the
standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However,
systematic evaluations reveal that neither variant outperforms the optimized
LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set,
99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On
sonnet generation, we get a CHRF score of 42.097. These findings highlight that
efficient parameter adaptation via LoRA remains the most effective strategy for
our tasks: sentiment analysis, paraphrase detection, and sonnet generation.

</details>

### [162] [How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients](https://arxiv.org/abs/2504.10766)
*Ming Li,Yanhong Li,Ziyue Li,Tianyi Zhou*

Main category: cs.LG

TLDR: 本文通过谱分析研究了不同质量数据对大型语言模型（LLM）后训练的影响，发现梯度奇异值分解（SVD）的谱特性可以解释和统一数据评估指标。高质量数据通常具有较低的核范数和较高的有效秩，其中有效秩在捕捉数据质量差异方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索不同质量数据（低/高质量指令和推理数据）对LLM后训练的影响，填补了数据质量与训练动态关系的研究空白。

Method: 采用谱分析方法，分析层间梯度的奇异值分解（SVD）特性，研究数据质量与梯度结构的关系。

Result: 高质量数据通常具有较低的核范数和较高的有效秩；推理数据的有效秩显著高于指令数据，表明复杂任务中梯度结构更丰富。同一模型家族内的模型梯度模式相似，不同家族则差异显著。

Conclusion: 研究为数据质量对训练稳定性的影响提供了统一视角，为后训练中的数据探索策略提供了新见解。

Abstract: As the post-training of large language models (LLMs) advances from
instruction-following to complex reasoning tasks, understanding how different
data affect finetuning dynamics remains largely unexplored. In this paper, we
present a spectral analysis of layer-wise gradients induced by low/high-quality
instruction and reasoning data for LLM post-training. Our analysis reveals that
widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and
Reward, can be explained and unified by spectral properties computed from
gradients' singular value decomposition (SVD). Specifically, higher-quality
data are usually associated with lower nuclear norms and higher effective
ranks. Notably, effective rank exhibits better robustness and resolution than
nuclear norm in capturing subtle quality differences. For example, reasoning
data achieves substantially higher effective ranks than instruction data,
implying richer gradient structures on more complex tasks. Our experiments also
highlight that models within the same family share similar gradient patterns
regardless of their sizes, whereas different model families diverge
significantly. Providing a unified view on the effects of data quality across
instruction and reasoning data, this work illuminates the interplay between
data quality and training stability, shedding novel insights into developing
better data exploration strategies for post-training.

</details>

### [163] [Looking beyond the next token](https://arxiv.org/abs/2504.11336)
*Abitha Thankaraj,Yiding Jiang,J. Zico Kolter,Yonatan Bisk*

Main category: cs.LG

TLDR: 论文提出了一种通过重新排列和处理训练数据序列的方法（Trelawney），以更准确地模仿真实数据生成过程，无需改变模型架构或训练基础设施。该方法在规划、算法推理和故事生成等任务中表现优异，并能自然生成长期目标。


<details>
  <summary>Details</summary>
Motivation: 现有的因果语言模型训练假设每个标记可以从上下文中准确预测，这与人类自然的写作和推理过程（目标通常先于具体论证或措辞）不匹配。论文旨在通过数据序列处理解决这一问题。

Method: 提出Trelawney方法，通过重新排列和处理训练数据序列，使模型更接近真实数据生成过程，无需改变架构或训练基础设施。

Result: Trelawney在规划、算法推理和故事生成等任务中提升了性能，并能自然生成长期目标。此外，该方法可能为语言模型带来新能力。

Conclusion: 通过数据序列处理而非架构调整，Trelawney有效解决了语言模型与人类写作推理的差异，提升了性能并扩展了潜在能力。

Abstract: The structure of causal language model training assumes that each token can
be accurately predicted from the previous context. This contrasts with humans'
natural writing and reasoning process, where goals are typically known before
the exact argument or phrasings. While this mismatch has been well studied in
the literature, the working assumption has been that architectural changes are
needed to address this mismatch. We argue that rearranging and processing the
training data sequences can allow models to more accurately imitate the true
data-generating process, and does not require any other changes to the
architecture or training infrastructure. We demonstrate that this technique,
Trelawney, and the inference algorithms derived from it allow us to improve
performance on several key benchmarks that span planning, algorithmic
reasoning, and story generation tasks. Finally, our method naturally enables
the generation of long-term goals at no additional cost. We investigate how
using the model's goal-generation capability can further improve planning and
reasoning. Additionally, we believe Trelawney could potentially open doors to
new capabilities beyond the current language modeling paradigm.

</details>

### [164] [A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce](https://arxiv.org/abs/2504.11343)
*Wei Xiong,Jiarui Yao,Yuhui Xu,Bo Pang,Lei Wang,Doyen Sahoo,Junnan Li,Nan Jiang,Tong Zhang,Caiming Xiong,Hanze Dong*

Main category: cs.LG

TLDR: GRPO是一种用于微调大型语言模型的强化学习方法，但其有效性来源不明。研究发现，简单的拒绝采样基线RAFT表现与GRPO和PPO相当，GRPO的优势主要来自丢弃完全错误的样本。基于此，提出了Reinforce-Rej方法，进一步提升了效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索GRPO方法在微调大型语言模型中的有效性来源，并寻找更简单高效的替代方案。

Method: 通过分析GRPO的核心组件，提出RAFT作为基线，并进一步设计Reinforce-Rej方法，结合样本过滤策略。

Result: RAFT表现与GRPO和PPO相当，Reinforce-Rej在KL效率和稳定性上有所提升。

Conclusion: 建议未来研究关注更原则性的负样本利用方法，RAFT可作为稳健基线，Reinforce-Rej为轻量级替代方案。

Abstract: Reinforcement learning (RL) has become a prevailing approach for fine-tuning
large language models (LLMs) on complex reasoning tasks. Among recent methods,
GRPO stands out for its empirical success in training models such as
DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In
this work, we revisit GRPO from a reinforce-like algorithm perspective and
analyze its core components. Surprisingly, we find that a simple rejection
sampling baseline, RAFT, which trains only on positively rewarded samples,
yields competitive performance than GRPO and PPO. Our ablation studies reveal
that GRPO's main advantage arises from discarding prompts with entirely
incorrect responses, rather than from its reward normalization. Motivated by
this insight, we propose Reinforce-Rej, a minimal extension of policy gradient
that filters both entirely incorrect and entirely correct samples.
Reinforce-Rej improves KL efficiency and stability, serving as a lightweight
yet effective alternative to more complex RL algorithms. We advocate RAFT as a
robust and interpretable baseline, and suggest that future advances should
focus on more principled designs for incorporating negative samples, rather
than relying on them indiscriminately. Our findings provide guidance for future
work in reward-based LLM post-training.

</details>

### [165] [Teaching Large Language Models to Reason through Learning and Forgetting](https://arxiv.org/abs/2504.11364)
*Tianwei Ni,Allen Nie,Sapana Chaudhary,Yao Liu,Huzefa Rangwala,Rasool Fakoor*

Main category: cs.LG

TLDR: 通过将搜索能力直接集成到模型中，并通过微调使用成功和失败的推理路径，显著提升了模型解决复杂数学和推理问题的能力，同时大幅降低了推理时间。


<details>
  <summary>Details</summary>
Motivation: 推理时搜索虽然能提升模型能力，但显著增加了计算成本和推理时间。为了解决这一问题，研究提出了一种更高效的方法。

Method: 通过微调模型，结合成功和失败的推理路径，并使用较小的学习率以防止搜索能力退化。

Result: 在Game-of-24和Countdown基准测试中，该方法不仅优于标准微调和推理时搜索基线，还将推理时间减少了180倍。

Conclusion: 该方法有效提升了模型性能并显著降低了推理时间，为解决复杂推理问题提供了更高效的途径。

Abstract: Leveraging inference-time search in large language models has proven
effective in further enhancing a trained model's capability to solve complex
mathematical and reasoning problems. However, this approach significantly
increases computational costs and inference time, as the model must generate
and evaluate multiple candidate solutions to identify a viable reasoning path.
To address this, we propose an effective approach that integrates search
capabilities directly into the model by fine-tuning it using both successful
(learning) and failed reasoning paths (forgetting) derived from diverse search
methods. While fine-tuning the model with these data might seem
straightforward, we identify a critical issue: the model's search capability
tends to degrade rapidly if fine-tuning is performed naively. We show that this
degradation can be substantially mitigated by employing a smaller learning
rate. Extensive experiments on the challenging Game-of-24 and Countdown
mathematical reasoning benchmarks show that our approach not only outperforms
both standard fine-tuning and inference-time search baselines but also
significantly reduces inference time by 180$\times$.

</details>

### [166] [DataDecide: How to Predict Best Pretraining Data with Small Experiments](https://arxiv.org/abs/2504.11393)
*Ian Magnusson,Nguyen Tai,Ben Bogin,David Heineman,Jena D. Hwang,Luca Soldaini,Akshita Bhagia,Jiacheng Liu,Dirk Groeneveld,Oyvind Tafjord,Noah A. Smith,Pang Wei Koh,Jesse Dodge*

Main category: cs.LG

TLDR: 论文探讨了如何通过小规模实验选择数据集以降低大模型预训练成本，并发布了DataDecide套件支持研究。研究发现小规模模型排名可有效预测大规模模型性能，且连续似然指标在小实验中可高效预测基准表现。


<details>
  <summary>Details</summary>
Motivation: 降低大语言模型预训练成本，通过小规模实验选择最优数据集。

Method: 在25个不同来源、去重和过滤的语料库上进行控制性预训练实验，模型规模达1B参数，使用3个随机种子。

Result: 小规模模型（如150M参数）排名能准确预测大规模模型（1B）性能（约80%正确）。连续似然指标在小实验中可高效预测基准表现。

Conclusion: 小规模实验可有效指导大规模模型数据选择，DataDecide为未来扩展法律研究提供了工具。

Abstract: Because large language models are expensive to pretrain on different
datasets, using smaller-scale experiments to decide on data is crucial for
reducing costs. Which benchmarks and methods of making decisions from observed
performance at small scale most accurately predict the datasets that yield the
best large models? To empower open exploration of this question, we release
models, data, and evaluations in DataDecide -- the most extensive open suite of
models over differences in data and scale. We conduct controlled pretraining
experiments across 25 corpora with differing sources, deduplication, and
filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random
seeds. We find that the ranking of models at a single, small size (e.g., 150M
parameters) is a strong baseline for predicting best models at our larger
target scale (1B) (~80% of com parisons correct). No scaling law methods among
8 baselines exceed the compute-decision frontier of single-scale predictions,
but DataDecide can measure improvement in future scaling laws. We also identify
that using continuous likelihood metrics as proxies in small experiments makes
benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable
at the target 1B scale with just 0.01% of the compute.

</details>

### [167] [LEMUR Neural Network Dataset: Towards Seamless AutoML](https://arxiv.org/abs/2504.10552)
*Arash Torabi Goodarzi,Roman Kochnev,Waleed Khalid,Furui Qin,Tolgay Atinc Uzun,Yashkumar Sanjaybhai Dhameliya,Yash Kanubhai Kathiriya,Zofia Antonina Bentyn,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TLDR: LEMUR是一个开源的神经网络模型数据集，支持多样化的架构和任务，旨在为AutoML任务提供结构化模型表示和性能数据。


<details>
  <summary>Details</summary>
Motivation: 高质量数据集对神经网络发展至关重要，LEMUR旨在填补神经网络模型数据集的空白，支持AutoML和模型分析。

Method: LEMUR基于Python和PyTorch构建，提供结构化模型代码和性能数据，集成了Optuna框架进行优化和分析，并支持边缘设备部署。

Result: LEMUR为研究人员和从业者提供了丰富的工具和API，支持模型评估、优化和实验，适用于资源受限环境。

Conclusion: LEMUR作为一个开源项目，将促进神经网络模型的开发、测试和分析，为AutoML和LLM实验提供支持。

Abstract: Neural networks are fundamental in artificial intelligence, driving progress
in computer vision and natural language processing. High-quality datasets are
crucial for their development, and there is growing interest in datasets
composed of neural networks themselves to support benchmarking, automated
machine learning (AutoML), and model analysis. We introduce LEMUR, an open
source dataset of neural network models with well-structured code for diverse
architectures across tasks such as object detection, image classification,
segmentation, and natural language processing. LEMUR is primarily designed to
enable fine-tuning of large language models (LLMs) for AutoML tasks, providing
a rich source of structured model representations and associated performance
data. Leveraging Python and PyTorch, LEMUR enables seamless extension to new
datasets and models while maintaining consistency. It integrates an
Optuna-powered framework for evaluation, hyperparameter optimization,
statistical analysis, and graphical insights. LEMUR provides an extension that
enables models to run efficiently on edge devices, facilitating deployment in
resource-constrained environments. Providing tools for model evaluation,
preprocessing, and database management, LEMUR supports researchers and
practitioners in developing, testing, and analyzing neural networks.
Additionally, it offers an API that delivers comprehensive information about
neural network models and their complete performance statistics with a single
request, which can be used in experiments with code-generating large language
models. The LEMUR will be released as an open source project under the MIT
license upon acceptance of the paper.

</details>

### [168] [Power-scaled Bayesian Inference with Score-based Generative mModels](https://arxiv.org/abs/2504.10807)
*Huseyin Tuna Erdinc,Yunlin Zeng,Abhinav Prakash Gahlot,Felix J. Herrmann*

Main category: cs.LG

TLDR: 提出一种基于分数的生成算法，用于在贝叶斯推断框架中采样功率缩放先验和似然，无需重新训练即可灵活控制先验与似然的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何在贝叶斯推断中灵活调节先验与似然的影响，以优化后验样本的生成。

Method: 提出一种基于分数的生成算法，支持对先验和似然进行功率缩放，并通过实验分析不同功率参数设置的影响。

Result: 实验表明，适度增加似然功率可提高后验样本与条件数据的匹配度，而降低先验功率可增加样本的结构多样性。

Conclusion: 该方法在贝叶斯推断中实现了对先验与似然影响的灵活控制，为后验优化提供了实用工具。

Abstract: We propose a score-based generative algorithm for sampling from power-scaled
priors and likelihoods within the Bayesian inference framework. Our algorithm
enables flexible control over prior-likelihood influence without requiring
retraining for different power-scaling configurations. Specifically, we focus
on synthesizing seismic velocity models conditioned on imaged seismic. Our
method enables sensitivity analysis by sampling from intermediate power
posteriors, allowing us to assess the relative influence of the prior and
likelihood on samples of the posterior distribution. Through a comprehensive
set of experiments, we evaluate the effects of varying the power parameter in
different settings: applying it solely to the prior, to the likelihood of a
Bayesian formulation, and to both simultaneously. The results show that
increasing the power of the likelihood up to a certain threshold improves the
fidelity of posterior samples to the conditioning data (e.g., seismic images),
while decreasing the prior power promotes greater structural diversity among
samples. Moreover, we find that moderate scaling of the likelihood leads to a
reduced shot data residual, confirming its utility in posterior refinement.

</details>

### [169] [Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations](https://arxiv.org/abs/2504.10833)
*Shubham Kumar,Dwip Dalal,Narendra Ahuja*

Main category: cs.LG

TLDR: 论文提出了一种新的评估方法Surrogate Faithfulness（SF），用于改进无监督概念解释方法（U-CBEMs）的忠实性，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有U-CBEMs的忠实性评估方法存在局限性，尤其是忽略了概念的空间分布，影响了评估的准确性。

Method: 引入SF方法，包括空间感知的代理模型和两种新的忠实性指标，并生成Optimal Faithful（OF）解释。

Result: 实验表明，SF方法显著提高了U-CBEMs的忠实性（30%或更高），且OF解释在域外数据和对抗样本上表现更优。

Conclusion: SF方法有效解决了现有U-CBEMs的评估问题，并显著提升了解释的忠实性和鲁棒性。

Abstract: Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a
promising tool for generating semantic explanations of the decision-making
processes in deep neural networks, having applications in both model
improvement and understanding. It is vital that the explanation is accurate, or
faithful, to the model, yet we identify several limitations of prior
faithfulness metrics that inhibit an accurate evaluation; most notably, prior
metrics involve only the set of concepts present, ignoring how they may be
spatially distributed. We address these limitations with Surrogate Faithfulness
(SF), an evaluation method that introduces a spatially-aware surrogate and two
novel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)
explanations, where concepts are found that maximize faithfulness. Our
experiments show that (1) adding spatial-awareness to prior U-CBEMs increases
faithfulness in all cases; (2) OF produces significantly more faithful
explanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's
learned concepts generalize well to out-of-domain data and are more robust to
adversarial examples, where prior U-CBEMs struggle.

</details>

### [170] [Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset](https://arxiv.org/abs/2504.11022)
*Joana Reuss,Jan Macdonald,Simon Becker,Konrad Schultka,Lorenz Richter,Marco Körner*

Main category: cs.LG

TLDR: 论文评估了迁移学习和元学习算法在真实世界作物分类任务中的表现，发现MAML类算法在特定场景下精度略高但计算成本更高，且跨地区知识迁移存在挑战。


<details>
  <summary>Details</summary>
Motivation: 解决作物类型数据空间不平衡问题，评估迁移学习和元学习算法在真实场景中的有效性。

Method: 在EuroCropsML数据集上对比了迁移学习和多种元学习算法（如MAML、ANIL、TIML）的性能。

Result: MAML类算法在爱沙尼亚作物分类任务中精度略高，但计算成本增加；跨地区（如爱沙尼亚和葡萄牙）知识迁移效果较差。

Conclusion: 需权衡精度与计算资源，跨地区知识迁移仍具挑战性，研究提供了首个真实场景下的作物分类算法基准。

Abstract: Spatial imbalances in crop type data pose significant challenges for accurate
classification in remote sensing applications. Algorithms aiming at
transferring knowledge from data-rich to data-scarce tasks have thus surged in
popularity. However, despite their effectiveness in previous evaluations, their
performance in challenging real-world applications is unclear and needs to be
evaluated. This study benchmarks transfer learning and several meta-learning
algorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML),
Almost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the
real-world EuroCropsML time series dataset, which combines farmer-reported crop
data with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal.
Our findings indicate that MAML-based meta-learning algorithms achieve slightly
higher accuracy compared to simpler transfer learning methods when applied to
crop type classification tasks in Estonia after pre-training on data from
Latvia. However, this improvement comes at the cost of increased computational
demands and training time. Moreover, we find that the transfer of knowledge
between geographically disparate regions, such as Estonia and Portugal, poses
significant challenges to all investigated algorithms. These insights
underscore the trade-offs between accuracy and computational resource
requirements in selecting machine learning methods for real-world crop type
classification tasks and highlight the difficulties of transferring knowledge
between different regions of the Earth. To facilitate future research in this
domain, we present the first comprehensive benchmark for evaluating transfer
and meta-learning methods for crop type classification under real-world
conditions. The corresponding code is publicly available at
https://github.com/dida-do/eurocrops-meta-learning.

</details>

### [171] [InfoClus: Informative Clustering of High-dimensional Data Embeddings](https://arxiv.org/abs/2504.11089)
*Fuyin Lai,Edith Heiter,Guillaume Bied,Jefrey Lijffijt*

Main category: cs.LG

TLDR: 提出了一种名为InfoClus的新方法，通过分区和稀疏解释来帮助理解低维嵌入数据，结合信息论优化分区和解释。


<details>
  <summary>Details</summary>
Motivation: 高维数据通过降维可视化后难以解释，需要一种方法帮助探索和解释低维嵌入。

Method: 引入分区与解释的概念，提出目标函数量化解释的信息量和复杂度，并通过贪婪搜索优化分区和解释。

Result: 在三个数据集上的定性和定量分析表明，InfoClus优于现有方法（RVX和VERA），并能自动为降维散点图分析提供良好起点。

Conclusion: InfoClus在解释低维嵌入数据方面具有独特优势，为数据分析提供了有效工具。

Abstract: Developing an understanding of high-dimensional data can be facilitated by
visualizing that data using dimensionality reduction. However, the
low-dimensional embeddings are often difficult to interpret. To facilitate the
exploration and interpretation of low-dimensional embeddings, we introduce a
new concept named partitioning with explanations. The idea is to partition the
data shown through the embedding into groups, each of which is given a sparse
explanation using the original high-dimensional attributes. We introduce an
objective function that quantifies how much we can learn through observing the
explanations of the data partitioning, using information theory, and also how
complex the explanations are. Through parameterization of the complexity, we
can tune the solutions towards the desired granularity. We propose InfoClus,
which optimizes the partitioning and explanations jointly, through greedy
search constrained over a hierarchical clustering. We conduct a qualitative and
quantitative analysis of InfoClus on three data sets. We contrast the results
on the Cytometry data with published manual analysis results, and compare with
two other recent methods for explaining embeddings (RVX and VERA). These
comparisons highlight that InfoClus has distinct advantages over existing
procedures and methods. We find that InfoClus can automatically create good
starting points for the analysis of dimensionality-reduction-based scatter
plots.

</details>

### [172] [Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay](https://arxiv.org/abs/2504.11118)
*Henrik Krauss,Takehisa Yairi*

Main category: cs.LG

TLDR: 论文提出了一种通过游戏数据揭示人类隐蔽注意力模式的新方法，利用强化学习的离线注意力技术，生成注意力图并与眼动追踪数据模型对比。


<details>
  <summary>Details</summary>
Motivation: 研究旨在从游戏数据中揭示人类隐蔽注意力模式，无需额外数据（如脑活动记录），并理解人类与智能体在注意力上的差异。

Method: 提出CTR注意力网络，从人类和RL智能体的游戏数据中生成稀疏但任务相关的注意力图，并与基于眼动追踪的TIOA模型对比。

Result: 人类CTR注意力图更接近TIOA模型，显示人类注意力集中在玩家和附近对手，而智能体则广泛关注所有对象。

Conclusion: CTR网络能有效从游戏数据中揭示人类隐蔽注意力模式，为开发具有人类注意力特征的RL智能体提供了基础。

Abstract: This study introduces a novel method for revealing human covert attention
patterns using gameplay data alone, utilizing offline attention techniques from
reinforcement learning (RL). We propose the contextualized, task-relevant (CTR)
attention network, which generates attention maps from both human and RL agent
gameplay in Atari environments. These maps are sparse yet retain the necessary
information for the current player's decision making. We compare the
CTR-derived attention maps with a temporally integrated overt attention (TIOA)
model based on eye-tracking data, serving as a point of comparison and
discussion. Visual inspection reveals distinct attention patterns: human CTR
maps focus on the player and rather nearby opponents, occasionally shifting
between stronger focus and broader views - sometimes even attending to empty
space ahead. In contrast, agent maps maintain a consistent broad focus on most
objects, including distant ones and the player. Quantitative analysis further
demonstrates that human CTR maps align more closely with TIOA than agent maps
do. Our findings indicate that the CTR attention network can effectively reveal
human covert attention patterns from gameplay alone, without the need for
additional data like brain activity recordings. This work contributes to
understanding human-agent attention differences and enables the development of
RL agents augmented with human covert attention.

</details>

### [173] [R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning](https://arxiv.org/abs/2504.11195)
*Lijun Sheng,Jian Liang,Zilei Wang,Ran He*

Main category: cs.LG

TLDR: 提出了一种名为R-TPT的方法，通过在推理阶段调整提示来防御对抗攻击，无需标记数据且灵活性强。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法依赖标记数据和训练阶段的对抗微调，缺乏灵活性，而R-TPT旨在解决这些问题。

Method: R-TPT通过优化边际熵目标并引入基于可靠性的加权集成策略，增强对抗攻击的防御能力。

Result: 在多种攻击和基准测试中，R-TPT表现出色，有效提升了防御能力。

Conclusion: R-TPT是一种无需标记数据且灵活的防御方法，显著提升了对抗攻击的鲁棒性。

Abstract: Vision-language models (VLMs), such as CLIP, have gained significant
popularity as foundation models, with numerous fine-tuning methods developed to
enhance performance on downstream tasks. However, due to their inherent
vulnerability and the common practice of selecting from a limited set of
open-source models, VLMs suffer from a higher risk of adversarial attacks than
traditional vision models. Existing defense techniques typically rely on
adversarial fine-tuning during training, which requires labeled data and lacks
of flexibility for downstream tasks. To address these limitations, we propose
robust test-time prompt tuning (R-TPT), which mitigates the impact of
adversarial attacks during the inference stage. We first reformulate the
classic marginal entropy objective by eliminating the term that introduces
conflicts under adversarial conditions, retaining only the pointwise entropy
minimization. Furthermore, we introduce a plug-and-play reliability-based
weighted ensembling strategy, which aggregates useful information from reliable
augmented views to strengthen the defense. R-TPT enhances defense against
adversarial attacks without requiring labeled training data while offering high
flexibility for inference tasks. Extensive experiments on widely used
benchmarks with various attacks demonstrate the effectiveness of R-TPT. The
code is available in https://github.com/TomSheng21/R-TPT.

</details>

### [174] [Mamba-Based Ensemble learning for White Blood Cell Classification](https://arxiv.org/abs/2504.11438)
*Lewis Clifton,Xin Tian,Duangdao Palasuwan,Phandee Watanaboonyongcharoen,Ponlapat Rojnuckarin,Nantheera Anantrasirichai*

Main category: cs.LG

TLDR: 论文提出了一种基于Mamba模型和集成学习的新框架，用于改进白细胞分类，解决了数据不平衡和计算资源限制的问题，并引入了新的数据集Chula-WBC-8。


<details>
  <summary>Details</summary>
Motivation: 手动白细胞分类耗时且易出错，现有深度学习方法虽有效但面临数据不平衡和计算资源限制的挑战。

Method: 结合Mamba模型（线性复杂度）和集成学习，提出了一种可扩展的分类框架。

Result: 验证了Mamba模型在白细胞分类中的有效性，显著提升了分类效率且不损失准确性。

Conclusion: Mamba模型为资源受限环境中的白细胞分类提供了高效解决方案，并展示了其潜力。

Abstract: White blood cell (WBC) classification assists in assessing immune health and
diagnosing various diseases, yet manual classification is labor-intensive and
prone to inconsistencies. Recent advancements in deep learning have shown
promise over traditional methods; however, challenges such as data imbalance
and the computational demands of modern technologies, such as Transformer-based
models which do not scale well with input size, limit their practical
application. This paper introduces a novel framework that leverages Mamba
models integrated with ensemble learning to improve WBC classification. Mamba
models, known for their linear complexity, provide a scalable alternative to
Transformer-based approaches, making them suitable for deployment in
resource-constrained environments. Additionally, we introduce a new WBC
dataset, Chula-WBC-8, for benchmarking. Our approach not only validates the
effectiveness of Mamba models in this domain but also demonstrates their
potential to significantly enhance classification efficiency without
compromising accuracy. The source code can be found at
https://github.com/LewisClifton/Mamba-WBC-Classification.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [175] [ArxivBench: Can LLMs Assist Researchers in Conducting Research?](https://arxiv.org/abs/2504.10496)
*Ning Li,Jingran Zhang,Justin Cui*

Main category: cs.IR

TLDR: arXivBench 是一个用于评估大语言模型（LLM）在 arXiv 平台上生成相关研究论文和准确链接能力的基准测试。研究发现，不同学科和子领域的准确性差异显著，Claude-3.5-Sonnet 表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决 LLM 生成内容中事实错误的问题，评估其在学术研究中的可靠性。

Method: 引入 arXivBench 基准，测试 LLM 在 arXiv 八个主要学科和计算机科学五个子领域的表现。

Result: LLM 的准确性因学科而异，Claude-3.5-Sonnet 表现最优，人工智能子领域准确性普遍较高。

Conclusion: arXivBench 为评估 LLM 生成科学内容的可靠性提供了标准化工具，促进其在学术研究中的可信使用。

Abstract: Large language models (LLMs) have demonstrated remarkable effectiveness in
completing various tasks such as reasoning, translation, and question
answering. However the issue of factual incorrect content in LLM-generated
responses remains a persistent challenge. In this study, we evaluate both
proprietary and open-source LLMs on their ability to respond with relevant
research papers and accurate links to articles hosted on the arXiv platform,
based on high level prompts. To facilitate this evaluation, we introduce
arXivBench, a benchmark specifically designed to assess LLM performance across
eight major subject categories on arXiv and five subfields within computer
science, one of the most popular categories among them. Our findings reveal a
concerning accuracy of LLM-generated responses depending on the subject, with
some subjects experiencing significantly lower accuracy than others. Notably,
Claude-3.5-Sonnet exhibits a substantial advantage in generating both relevant
and accurate responses. And interestingly, most LLMs achieve a much higher
accuracy in the Artificial Intelligence sub-field than other sub-fields. This
benchmark provides a standardized tool for evaluating the reliability of
LLM-generated scientific responses, promoting more dependable use of LLMs in
academic and research environments. Our code is open-sourced at
https://github.com/arxivBenchLLM/arXivBench and our dataset is available on
huggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench.

</details>

### [176] [Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey](https://arxiv.org/abs/2504.10499)
*Zulun Zhu,Tiancheng Huang,Kai Wang,Junda Ye,Xinghe Chen,Siqiang Luo*

Main category: cs.IR

TLDR: 该论文综述了图结构在检索增强生成（RAG）中的多样化作用，分析了其在提升性能、数据库构建、算法、管道和任务中的角色，并指出了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理时存在事实性错误和幻觉问题，检索增强生成（RAG）通过从外部源检索信息来提升准确性。然而，图结构在RAG中的多样化作用缺乏统一综述，本文旨在填补这一空白。

Method: 论文通过分析图结构在RAG中的功能，详细探讨了其在数据库构建、算法、管道和任务中的具体角色，并对现有方法进行了比较。

Result: 研究发现图结构在RAG中具有重要作用，能够提升性能并支持复杂推理。同时，论文总结了现有方法的共性与差异。

Conclusion: 本文为图学习、数据库系统和自然语言处理领域的研究者提供了未来研究方向，并强调了图结构在RAG中的潜力。

Abstract: Large language models (LLMs) struggle with the factual error during inference
due to the lack of sufficient training data and the most updated knowledge,
leading to the hallucination problem. Retrieval-Augmented Generation (RAG) has
gained attention as a promising solution to address the limitation of LLMs, by
retrieving relevant information from external source to generate more accurate
answers to the questions. Given the pervasive presence of structured knowledge
in the external source, considerable strides in RAG have been made to employ
the techniques related to graphs and achieve more complex reasoning based on
the topological information between knowledge entities. However, there is
currently neither unified review examining the diverse roles of graphs in RAG,
nor a comprehensive resource to help researchers navigate and contribute to
this evolving field. This survey offers a novel perspective on the
functionality of graphs within RAG and their impact on enhancing performance
across a wide range of graph-structured data. It provides a detailed breakdown
of the roles that graphs play in RAG, covering database construction,
algorithms, pipelines, and tasks. Finally, it identifies current challenges and
outline future research directions, aiming to inspire further developments in
this field. Our graph-centered analysis highlights the commonalities and
differences in existing methods, setting the stage for future researchers in
areas such as graph learning, database systems, and natural language
processing.

</details>

### [177] [JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2504.10512)
*Minh-Anh Nguyen,Dung D. Le*

Main category: cs.IR

TLDR: JEPA4Rec是一个结合联合嵌入预测架构和语言建模的框架，用于解决序列推荐中的数据稀疏性和用户偏好理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 语言表示学习在序列推荐中表现出潜力，但仍面临数据稀疏性和对用户偏好理解不足的挑战。

Method: JEPA4Rec通过将物品表示为文本句子，使用双向Transformer编码器，并结合掩码预测和两阶段训练策略，学习通用物品嵌入。

Result: 在六个真实数据集上的实验表明，JEPA4Rec在跨域、跨平台和低资源场景中优于现有方法。

Conclusion: JEPA4Rec通过结合语言建模和联合嵌入预测，显著提升了推荐性能，减少了对大规模预训练数据的依赖。

Abstract: Language representation learning has emerged as a promising approach for
sequential recommendation, thanks to its ability to learn generalizable
representations. However, despite its advantages, this approach still struggles
with data sparsity and a limited understanding of common-sense user
preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a
framework that combines $\textbf{J}$oint $\textbf{E}$mbedding
$\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item
textual descriptions. JEPA4Rec captures semantically rich and transferable
representations, improving recommendation performance and reducing reliance on
large-scale pre-training data. Specifically, JEPA4Rec represents items as text
sentences by flattening descriptive information such as $\textit{title,
category}$, and other attributes. To encode these sentences, we employ a
bidirectional Transformer encoder with modified embedding layers tailored for
capturing item information in recommendation datasets. We apply masking to text
sentences and use them to predict the representations of the unmasked
sentences, helping the model learn generalizable item embeddings. To further
improve recommendation performance and language understanding, we employ a
two-stage training strategy incorporating self-supervised learning losses.
Experiments on six real-world datasets demonstrate that JEPA4Rec consistently
outperforms state-of-the-art methods, particularly in cross-domain,
cross-platform, and low-resource scenarios.

</details>

### [178] [CSPLADE: Learned Sparse Retrieval with Causal Language Models](https://arxiv.org/abs/2504.10816)
*Zhichao Xu,Aosong Feng,Yijun Tian,Haibo Ding,Lin Leee Cheong*

Main category: cs.IR

TLDR: 论文探讨了大规模语言模型（LLM）在稀疏检索（LSR）中的应用，提出了解决训练不稳定性和性能问题的技术，并分析了性能与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 密集检索（dense retrieval）存在不可解释性和索引体积大的问题，而稀疏检索（LSR）虽能利用经典倒排索引结构，但在大规模LLM上的应用尚未充分探索。

Method: 提出两种技术：(1) 轻量级适应训练阶段解决训练不稳定性；(2) 两种模型变体实现双向信息。

Result: 成功训练了8B规模的LSR模型，性能竞争性强且索引体积减小，并首次通过模型量化分析了性能与效率的权衡。

Conclusion: 研究为LLM在高效检索建模中的应用提供了新思路。

Abstract: In recent years, dense retrieval has been the focus of information retrieval
(IR) research. While effective, dense retrieval produces uninterpretable dense
vectors, and suffers from the drawback of large index size. Learned sparse
retrieval (LSR) has emerged as promising alternative, achieving competitive
retrieval performance while also being able to leverage the classical inverted
index data structure for efficient retrieval. However, limited works have
explored scaling LSR beyond BERT scale. In this work, we identify two
challenges in training large language models (LLM) for LSR: (1) training
instability during the early stage of contrastive training; (2) suboptimal
performance due to pre-trained LLM's unidirectional attention. To address these
challenges, we propose two corresponding techniques: (1) a lightweight
adaptation training phase to eliminate training instability; (2) two model
variants to enable bidirectional information. With these techniques, we are
able to train LSR models with 8B scale LLM, and achieve competitive retrieval
performance with reduced index size. Furthermore, we are among the first to
analyze the performance-efficiency tradeoff of LLM-based LSR model through the
lens of model quantization. Our findings provide insights into adapting LLMs
for efficient retrieval modeling.

</details>

### [179] [Human-Oriented Image Retrieval System (HORSE): A Neuro-Symbolic Approach to Optimizing Retrieval of Previewed Images](https://arxiv.org/abs/2504.10502)
*Abraham Itzhak Weinberg*

Main category: cs.IR

TLDR: HORSE是一种基于神经符号索引的新型图像检索方法，结合认知科学与计算技术，优化检索效率与用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像搜索引擎在自然语言描述检索中的低效问题，提升与人类视觉感知和记忆的匹配度。

Method: 采用神经符号框架，结合神经网络与符号推理的优势，设计并实现HORSE系统。

Result: HORSE显著优化了图像检索过程，提供更直观高效的解决方案。

Conclusion: HORSE在设计与知识管理等领域有广泛应用潜力，未来需进一步优化系统指标与功能。

Abstract: Image retrieval remains a challenging task due to the complex interaction
between human visual perception, memory, and computational processes. Current
image search engines often struggle to efficiently retrieve images based on
natural language descriptions, as they rely on time-consuming preprocessing,
tagging, and machine learning pipelines. This paper introduces the
Human-Oriented Retrieval Search Engine for Images (HORSE), a novel approach
that leverages neuro-symbolic indexing to improve image retrieval by focusing
on human-oriented indexing. By integrating cognitive science insights with
advanced computational techniques, HORSE enhances the retrieval process, making
it more aligned with how humans perceive, store, and recall visual information.
The neuro-symbolic framework combines the strengths of neural networks and
symbolic reasoning, mitigating their individual limitations. The proposed
system optimizes image retrieval, offering a more intuitive and efficient
solution for users. We discuss the design and implementation of HORSE,
highlight its potential applications in fields such as design error detection
and knowledge management, and suggest future directions for research to further
refine the system's metrics and capabilities.

</details>

<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [180] [Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification](https://arxiv.org/abs/2504.10916)
*Zhenyu Yang,Haiming Zhu,Rihui Zhang,Haipeng Zhang,Jianliang Wang,Chunhao Wang,Minbin Chen,Fang-Fang Yin*

Main category: physics.med-ph

TLDR: RE-ViT结合了放射组学特征与ViT架构，通过早期融合提升医学图像分类性能，在多个数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: ViT在医学图像分析中因数据密集和缺乏领域特定归纳偏差而受限，而放射组学虽可解释但扩展性差。RE-ViT旨在结合两者优势。

Method: 将图像分块，提取放射组学特征并与像素嵌入融合，经归一化和位置编码后输入ViT编码器，使用[CLS]标记分类。

Result: 在BUSI、ChestXray2017和Retinal OCT数据集上，RE-ViT的AUC分别为0.950、0.989和0.986，优于其他模型。

Conclusion: RE-ViT成功整合放射组学与ViT，提升了多模态医学图像分类的性能和泛化能力。

Abstract: Background: Deep learning has significantly advanced medical image analysis,
with Vision Transformers (ViTs) offering a powerful alternative to
convolutional models by modeling long-range dependencies through
self-attention. However, ViTs are inherently data-intensive and lack
domain-specific inductive biases, limiting their applicability in medical
imaging. In contrast, radiomics provides interpretable, handcrafted descriptors
of tissue heterogeneity but suffers from limited scalability and integration
into end-to-end learning frameworks. In this work, we propose the
Radiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features
with data-driven visual embeddings within a ViT backbone.
  Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and
patch-wise ViT embeddings through early fusion, enhancing robustness and
performance in medical image classification.
  Methods: Following the standard ViT pipeline, images were divided into
patches. For each patch, handcrafted radiomic features were extracted and fused
with linearly projected pixel embeddings. The fused representations were
normalized, positionally encoded, and passed to the ViT encoder. A learnable
[CLS] token aggregated patch-level information for classification. We evaluated
RE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal
OCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was
benchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.
  Results: RE-ViT achieved state-of-the-art results: on BUSI,
AUC=0.950+/-0.011; on ChestXray2017, AUC=0.989+/-0.004; on Retinal OCT,
AUC=0.986+/-0.001, which outperforms other comparison models.
  Conclusions: The RE-ViT framework effectively integrates radiomics with ViT
architectures, demonstrating improved performance and generalizability across
multimodal medical image classification tasks.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [181] [Toward Super Agent System with Hybrid AI Routers](https://arxiv.org/abs/2504.10519)
*Yuhang Yao,Haixin Wang,Yibo Chen,Jiawen Wang,Min Chang Jordan Ren,Bosheng Ding,Salman Avestimehr,Chaoyang He*

Main category: cs.AI

TLDR: 本文提出了一种超级代理系统设计，通过意图识别和任务路由实现高效、低成本的AI代理服务，并探讨了本地与云端混合模式的应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足多样化的用户需求并实现大规模部署，需要优化AI代理系统以提高效率和降低成本。

Method: 系统通过检测用户意图，将请求路由至专用任务代理或自动生成工作流，并采用本地与云端混合模式动态选择模型。

Result: 设计了一种超级代理系统，支持本地与云端协作，为未来无缝集成到日常生活奠定了基础。

Conclusion: 通过多模态模型和边缘硬件的进步，超级代理有望在本地完成大部分计算，仅在需要时与云端协作，实现广泛应用的愿景。

Abstract: AI Agents powered by Large Language Models are transforming the world through
enormous applications. A super agent has the potential to fulfill diverse user
needs, such as summarization, coding, and research, by accurately understanding
user intent and leveraging the appropriate tools to solve tasks. However, to
make such an agent viable for real-world deployment and accessible at scale,
significant optimizations are required to ensure high efficiency and low cost.
This paper presents a design of the Super Agent System. Upon receiving a user
prompt, the system first detects the intent of the user, then routes the
request to specialized task agents with the necessary tools or automatically
generates agentic workflows. In practice, most applications directly serve as
AI assistants on edge devices such as phones and robots. As different language
models vary in capability and cloud-based models often entail high
computational costs, latency, and privacy concerns, we then explore the hybrid
mode where the router dynamically selects between local and cloud models based
on task complexity. Finally, we introduce the blueprint of an on-device super
agent enhanced with cloud. With advances in multi-modality models and edge
hardware, we envision that most computations can be handled locally, with cloud
collaboration only as needed. Such architecture paves the way for super agents
to be seamlessly integrated into everyday life in the near future.

</details>

### [182] [ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search](https://arxiv.org/abs/2504.10893)
*Yize Zhang,Tianshu Wang,Sirui Chen,Kun Wang,Xingyu Zeng,Hongyu Lin,Xianpei Han,Le Sun,Chaochao Lu*

Main category: cs.AI

TLDR: ARise框架通过结合风险评估和动态检索增强生成，显著提升了开放知识密集型复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在开放知识密集型复杂推理任务中表现有限，现有方法存在错误传播和验证瓶颈问题。

Method: 提出ARise框架，整合风险评估与动态检索增强生成，采用蒙特卡洛树搜索优化多分支推理计划。

Result: ARise在知识增强推理任务中性能提升23.10%，在检索增强推理模型中提升25.37%。

Conclusion: ARise有效解决了现有方法的局限性，显著提升了复杂推理任务的性能。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities and
are receiving increasing attention to enhance their reasoning through scaling
test--time compute. However, their application in open--ended,
knowledge--intensive, complex reasoning scenarios is still limited.
Reasoning--oriented methods struggle to generalize to open--ended scenarios due
to implicit assumptions of complete world knowledge. Meanwhile,
knowledge--augmented reasoning (KAR) methods fail to address two core
challenges: 1) error propagation, where errors in early steps cascade through
the chain, and 2) verification bottleneck, where the explore--exploit tradeoff
arises in multi--branch decision processes. To overcome these limitations, we
introduce ARise, a novel framework that integrates risk assessment of
intermediate reasoning states with dynamic retrieval--augmented generation
(RAG) within a Monte Carlo tree search paradigm. This approach enables
effective construction and optimization of reasoning plans across multiple
maintained hypothesis branches. Experimental results show that ARise
significantly outperforms the state--of--the--art KAR methods by up to 23.10%,
and the latest RAG-equipped large reasoning models by up to 25.37%.

</details>

### [183] [Enhancing multimodal analogical reasoning with Logic Augmented Generation](https://arxiv.org/abs/2504.11190)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.AI

TLDR: 论文提出了一种逻辑增强生成（LAG）框架，利用语义知识图结合提示启发式方法提取隐式类比连接，提升多模态数据的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多任务中表现出色，但自动从自然语言中提取隐式知识仍具挑战性，尤其是机器缺乏物理世界的直接经验。

Method: 采用逻辑增强生成（LAG）框架，通过语义知识图显式表示文本，并结合提示启发式方法生成隐式知识图三元组。

Result: 在四个数据集的三个隐喻检测与理解任务中，该方法超越现有基线，甚至在视觉隐喻理解上优于人类，推理过程更具解释性。

Conclusion: 该方法在多模态数据推理中表现优异，但仍存在领域特定隐喻理解的局限性，并提出了对隐喻标注和评估方法的改进建议。

Abstract: Recent advances in Large Language Models have demonstrated their capabilities
across a variety of tasks. However, automatically extracting implicit knowledge
from natural language remains a significant challenge, as machines lack active
experience with the physical world. Given this scenario, semantic knowledge
graphs can serve as conceptual spaces that guide the automated text generation
reasoning process to achieve more efficient and explainable results. In this
paper, we apply a logic-augmented generation (LAG) framework that leverages the
explicit representation of a text through a semantic knowledge graph and
applies it in combination with prompt heuristics to elicit implicit analogical
connections. This method generates extended knowledge graph triples
representing implicit meaning, enabling systems to reason on unlabeled
multimodal data regardless of the domain. We validate our work through three
metaphor detection and understanding tasks across four datasets, as they
require deep analogical reasoning capabilities. The results show that this
integrated approach surpasses current baselines, performs better than humans in
understanding visual metaphors, and enables more explainable reasoning
processes, though still has inherent limitations in metaphor understanding,
especially for domain-specific metaphors. Furthermore, we propose a thorough
error analysis, discussing issues with metaphorical annotations and current
evaluation methods.

</details>

### [184] [Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs](https://arxiv.org/abs/2504.11239)
*Chang Yang,Ruiyu Wang,Junzhe Jiang,Qi Jiang,Qinggang Zhang,Yanchen Deng,Shuxin Li,Shuyue Hu,Bo Li,Florian T. Pokorny,Xiao Huang,Xinrun Wang*

Main category: cs.AI

TLDR: NPPC是一个不可破解、自动验证且通用的基准测试，用于评估大语言模型的推理能力，通过三个模块（npgym、npsolver、npeval）实现，实验显示其能显著降低先进模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试容易被快速破解或过时，需要一种不可破解且能自动验证的基准来评估LLMs的推理能力。

Method: 提出NPPC基准，包含三个模块：npgym生成复杂问题实例，npsolver评估模型性能，npeval分析结果。

Result: NPPC将先进LLMs的性能降至10%以下，DeepSeek-R1表现最佳，且发现token数量和aha时刻随问题难度变化。

Conclusion: NPPC是首个不可破解的推理基准，为LLMs迈向AGI提供了可靠的测试平台。

Abstract: Reasoning is the fundamental capability of large language models (LLMs). Due
to the rapid progress of LLMs, there are two main issues of current benchmarks:
i) these benchmarks can be crushed in a short time (less than 1 year), and ii)
these benchmarks may be easily hacked. To handle these issues, we propose the
ever-scalingness for building the benchmarks which are uncrushable, unhackable,
auto-verifiable and general. This paper presents Nondeterministic
Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark
for LLMs. Specifically, the NPPC has three main modules: i) npgym, which
provides a unified interface of 25 well-known NP-complete problems and can
generate any number of instances with any levels of complexities, ii) npsolver:
which provides a unified interface to evaluate the problem instances with both
online and offline models via APIs and local deployments, respectively, and
iii) npeval: which provides the comprehensive and ready-to-use tools to analyze
the performances of LLMs over different problems, the number of tokens, the aha
moments, the reasoning errors and the solution errors. Extensive experiments
over widely-used LLMs demonstrate: i) NPPC can successfully decrease the
performances of advanced LLMs' performances to below 10%, demonstrating that
NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the
most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and
o1/o3-mini in most NP-complete problems considered, and iii) the numbers of
tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and
DeepSeek-R1, are observed first to increase and then decrease when the problem
instances become more and more difficult. We believe that NPPC is the first
ever-scaling reasoning benchmark, serving as the uncrushable and unhackable
testbed for LLMs toward artificial general intelligence (AGI).

</details>

### [185] [Towards Automated Safety Requirements Derivation Using Agent-based RAG](https://arxiv.org/abs/2504.11243)
*Balahari Vignesh Balu,Florian Geissler,Francesco Carella,Joao-Vitor Zacchi,Josef Jiru,Nuria Mata,Reinhard Stolle*

Main category: cs.AI

TLDR: 论文提出了一种基于代理的检索增强生成方法，用于自动驾驶车辆的安全需求推导，解决了传统方法在复杂查询和领域知识不足时的性能问题。


<details>
  <summary>Details</summary>
Motivation: 传统预训练LLMs在安全分析中缺乏领域知识，现有RAG方法在处理复杂查询时性能下降，尤其在安全相关应用中表现不佳。

Method: 采用基于代理的RAG方法，结合汽车标准文档和Apollo案例研究，测试安全需求问题数据集。

Result: 代理方法比默认RAG方法检索的信息更相关，性能更优。

Conclusion: 基于代理的RAG方法在安全需求推导中更具优势，适用于复杂查询和领域知识需求高的场景。

Abstract: We study the automated derivation of safety requirements in a self-driving
vehicle use case, leveraging LLMs in combination with agent-based
retrieval-augmented generation. Conventional approaches that utilise
pre-trained LLMs to assist in safety analyses typically lack domain-specific
knowledge. Existing RAG approaches address this issue, yet their performance
deteriorates when handling complex queries and it becomes increasingly harder
to retrieve the most relevant information. This is particularly relevant for
safety-relevant applications. In this paper, we propose the use of agent-based
RAG to derive safety requirements and show that the retrieved information is
more relevant to the queries. We implement an agent-based approach on a
document pool of automotive standards and the Apollo case study, as a
representative example of an automated driving perception system. Our solution
is tested on a data set of safety requirement questions and answers, extracted
from the Apollo data. Evaluating a set of selected RAG metrics, we present and
discuss advantages of a agent-based approach compared to default RAG methods.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [186] [Integrating electrocardiogram and fundus images for early detection of cardiovascular diseases](https://arxiv.org/abs/2504.10493)
*K. A. Muthukumar,Dhruva Nandi,Priya Ranjan,Krithika Ramachandran,Shiny PJ,Anirban Ghosh,Ashwini M,Aiswaryah Radhakrishnan,V. E. Dhandapani,Rajiv Janardhanan*

Main category: eess.IV

TLDR: 提出了一种结合心电图（ECG）和视网膜眼底图像的新方法，用于早期心血管疾病（CVD）诊断和优先级分类，初步测试准确率达84%。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要健康问题，需要先进的诊断技术。视网膜血管网络与心血管系统相关，结合ECG数据可提供更全面的诊断视角。

Method: 使用快速傅里叶变换（FFT）处理ECG和眼底图像数据，计算两者的Earth Mover's Distance（EMD），将特征输入神经网络分类器。

Result: 初步测试显示分类准确率为84%，表明该方法在CVD诊断中具有潜力。

Conclusion: 该方法有望在资源有限的医疗环境中提升CVD诊断效果，未来将进一步优化和验证。

Abstract: Cardiovascular diseases (CVD) are a predominant health concern globally,
emphasizing the need for advanced diagnostic techniques. In our research, we
present an avant-garde methodology that synergistically integrates ECG readings
and retinal fundus images to facilitate the early disease tagging as well as
triaging of the CVDs in the order of disease priority. Recognizing the
intricate vascular network of the retina as a reflection of the cardiovascular
system, alongwith the dynamic cardiac insights from ECG, we sought to provide a
holistic diagnostic perspective. Initially, a Fast Fourier Transform (FFT) was
applied to both the ECG and fundus images, transforming the data into the
frequency domain. Subsequently, the Earth Mover's Distance (EMD) was computed
for the frequency-domain features of both modalities. These EMD values were
then concatenated, forming a comprehensive feature set that was fed into a
Neural Network classifier. This approach, leveraging the FFT's spectral
insights and EMD's capability to capture nuanced data differences, offers a
robust representation for CVD classification. Preliminary tests yielded a
commendable accuracy of 84 percent, underscoring the potential of this combined
diagnostic strategy. As we continue our research, we anticipate refining and
validating the model further to enhance its clinical applicability in resource
limited healthcare ecosystems prevalent across the Indian sub-continent and
also the world at large.

</details>

### [187] [PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with SAM2](https://arxiv.org/abs/2504.10526)
*Mingyang Zhu,Yinting Liu,Mingyu Li,Jiacheng Wang*

Main category: eess.IV

TLDR: PathSeqSAM是一种新颖的病理图像分割方法，通过将2D切片视为视频帧并利用SAM2的记忆机制，结合距离感知注意力和LoRA域适应，显著提升了分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立处理2D切片，忽略了跨切片信息，限制了分割性能。

Method: 将2D切片视为视频帧，利用SAM2的记忆机制，引入距离感知注意力处理切片间物理距离，并使用LoRA进行域适应。

Result: 在KPI Challenge 2024数据集上，PathSeqSAM在肾小球分割任务中表现出色，尤其在需要跨切片上下文的情况下。

Conclusion: PathSeqSAM通过整合跨切片信息，显著提升了病理图像分割的质量，代码已开源。

Abstract: Current methods for pathology image segmentation typically treat 2D slices
independently, ignoring valuable cross-slice information. We present
PathSeqSAM, a novel approach that treats 2D pathology slices as sequential
video frames using SAM2's memory mechanisms. Our method introduces a
distance-aware attention mechanism that accounts for variable physical
distances between slices and employs LoRA for domain adaptation. Evaluated on
the KPI Challenge 2024 dataset for glomeruli segmentation, PathSeqSAM
demonstrates improved segmentation quality, particularly in challenging cases
that benefit from cross-slice context. We have publicly released our code at
https://github.com/JackyyyWang/PathSeqSAM.

</details>

### [188] [Efficient and Robust Remote Sensing Image Denoising Using Randomized Approximation of Geodesics' Gramian on the Manifold Underlying the Patch Space](https://arxiv.org/abs/2504.10820)
*Kelum Gajamannage,Dilhani I. Jayathilake,Maria Vasilyeva*

Main category: eess.IV

TLDR: 提出了一种无需额外训练样本的高效遥感图像去噪方法，通过低秩流形和随机近似技术实现。


<details>
  <summary>Details</summary>
Motivation: 遥感图像因环境和成像系统问题质量下降，现有去噪算法难以处理复杂纹理，神经网络方法资源消耗大。

Method: 将图像分块，利用低秩流形表示无噪版本，通过随机近似揭示流形，并分通道去噪后合并。

Result: 方法计算高效且鲁棒，无需额外训练样本。

Conclusion: 该方法为遥感图像去噪提供了一种资源友好的解决方案。

Abstract: Remote sensing images are widely utilized in many disciplines such as feature
recognition and scene semantic segmentation. However, due to environmental
factors and the issues of the imaging system, the image quality is often
degraded which may impair subsequent visual tasks. Even though denoising remote
sensing images plays an essential role before applications, the current
denoising algorithms fail to attain optimum performance since these images
possess complex features in the texture. Denoising frameworks based on
artificial neural networks have shown better performance; however, they require
exhaustive training with heterogeneous samples that extensively consume
resources like power, memory, computation, and latency. Thus, here we present a
computationally efficient and robust remote sensing image denoising method that
doesn't require additional training samples. This method partitions patches of
a remote-sensing image in which a low-rank manifold, representing the
noise-free version of the image, underlies the patch space. An efficient and
robust approach to revealing this manifold is a randomized approximation of the
singular value spectrum of the geodesics' Gramian matrix of the patch space.
The method asserts a unique emphasis on each color channel during denoising so
the three denoised channels are merged to produce the final image.

</details>

### [189] [AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent](https://arxiv.org/abs/2504.10978)
*Pu Wang,Zhihua Zhang,Dianjie Lu,Guijuan Zhang,Youshan Zhang,Zhuoran Zheng*

Main category: eess.IV

TLDR: AgentPolyp框架结合CLIP语义引导和动态图像增强，通过轻量级神经网络解决息肉图像噪声问题，提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 息肉图像常因光线、模糊和过曝等问题影响分割任务，需解决噪声导致的图像质量下降。

Method: 利用CLIP语义分析评估图像质量，通过强化学习动态应用多模态增强操作，结合反馈循环优化分割。

Result: 模块化架构支持多种增强算法和分割网络，适用于内窥镜设备部署。

Conclusion: AgentPolyp能有效提升息肉图像的分割鲁棒性，具有扩展性和实用性。

Abstract: Since human and environmental factors interfere, captured polyp images
usually suffer from issues such as dim lighting, blur, and overexposure, which
pose challenges for downstream polyp segmentation tasks. To address the
challenges of noise-induced degradation in polyp images, we present AgentPolyp,
a novel framework integrating CLIP-based semantic guidance and dynamic image
enhancement with a lightweight neural network for segmentation. The agent first
evaluates image quality using CLIP-driven semantic analysis (e.g., identifying
``low-contrast polyps with vascular textures") and adapts reinforcement
learning strategies to dynamically apply multi-modal enhancement operations
(e.g., denoising, contrast adjustment). A quality assessment feedback loop
optimizes pixel-level enhancement and segmentation focus in a collaborative
manner, ensuring robust preprocessing before neural network segmentation. This
modular architecture supports plug-and-play extensions for various enhancement
algorithms and segmentation networks, meeting deployment requirements for
endoscopic devices.

</details>

### [190] [Efficient Medical Image Restoration via Reliability Guided Learning in Frequency Domain](https://arxiv.org/abs/2504.11286)
*Pengcheng Zheng,Kecheng Chen,Jiaxin Huang,Bohao Chen,Ju Liu,Yazhou Ren,Xiaorong Pu*

Main category: eess.IV

TLDR: LRformer是一种基于Transformer的轻量级方法，通过频域中的可靠性引导学习，解决了医学图像恢复任务中的计算效率和结果可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像恢复任务在临床中有迫切需求，但现有深度学习方法在计算效率和结果可靠性上存在不足。

Method: 提出了Reliable Lesion-Semantic Prior Producer (RLPP)生成可靠先验，并通过Guided Frequency Cross-Attention (GFCA)在频域中高效处理。

Result: 实验结果表明，LRformer在多种任务中表现出色，兼具高效性和有效性。

Conclusion: LRformer通过频域学习和可靠性引导，显著提升了医学图像恢复的性能和效率。

Abstract: Medical image restoration tasks aim to recover high-quality images from
degraded observations, exhibiting emergent desires in many clinical scenarios,
such as low-dose CT image denoising, MRI super-resolution, and MRI artifact
removal. Despite the success achieved by existing deep learning-based
restoration methods with sophisticated modules, they struggle with rendering
computationally-efficient reconstruction results. Moreover, they usually ignore
the reliability of the restoration results, which is much more urgent in
medical systems. To alleviate these issues, we present LRformer, a Lightweight
Transformer-based method via Reliability-guided learning in the frequency
domain. Specifically, inspired by the uncertainty quantification in Bayesian
neural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer
(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling
operations to generate sufficiently-reliable priors by performing multiple
inferences on the foundational medical image segmentation model, MedSAM.
Additionally, instead of directly incorporating the priors in the spatial
domain, we decompose the cross-attention (CA) mechanism into real symmetric and
imaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in
the design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging
the conjugated symmetric property of FFT, GFCA reduces the computational
complexity of naive CA by nearly half. Extensive experimental results in
various tasks demonstrate the superiority of the proposed LRformer in both
effectiveness and efficiency.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [191] [Exposure to Content Written by Large Language Models Can Reduce Stigma Around Opioid Use Disorder in Online Communities](https://arxiv.org/abs/2504.10501)
*Shravika Mittal,Darshi Shah,Shin Won Do,Mai ElSherief,Tanushree Mitra,Munmun De Choudhury*

Main category: cs.SI

TLDR: 研究发现，大型语言模型（LLMs）生成的回复能有效减少与阿片类药物使用障碍（OUD）相关的污名化态度，尤其是在线社区中。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能在在线社区中减少OUD相关的污名化，促进健康公平和同理心对话。

Method: 通过预注册的随机对照实验，比较LLM生成、人工撰写或无回复对参与者污名化态度的影响。

Result: LLM生成的回复在单次和重复阅读实验中均显著降低了参与者对药物治疗（MAT）的污名化态度。

Conclusion: LLMs可作为教育干预工具，促进对OUD的包容性讨论和积极态度。

Abstract: Widespread stigma, both in the offline and online spaces, acts as a barrier
to harm reduction efforts in the context of opioid use disorder (OUD). This
stigma is prominently directed towards clinically approved medications for
addiction treatment (MAT), people with the condition, and the condition itself.
Given the potential of artificial intelligence based technologies in promoting
health equity, and facilitating empathic conversations, this work examines
whether large language models (LLMs) can help abate OUD-related stigma in
online communities. To answer this, we conducted a series of pre-registered
randomized controlled experiments, where participants read LLM-generated,
human-written, or no responses to help seeking OUD-related content in online
communities. The experiment was conducted under two setups, i.e., participants
read the responses either once (N = 2,141), or repeatedly for 14 days (N =
107). We found that participants reported the least stigmatized attitudes
toward MAT after consuming LLM-generated responses under both the setups. This
study offers insights into strategies that can foster inclusive online
discourse on OUD, e.g., based on our findings LLMs can be used as an
education-based intervention to promote positive attitudes and increase
people's propensity toward MAT.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [192] [Cryo-em images are intrinsically low dimensional](https://arxiv.org/abs/2504.11249)
*Luke Evans,Octavian-Vlad Murad,Lars Dingeldein,Pilar Cossio,Roberto Covino,Marina Meila*

Main category: q-bio.QM

TLDR: 论文通过流形学习技术分析CryoSBI的潜在表示，揭示了高维数据本质上是低维平滑流形，并建立了潜在结构与物理参数的联系。


<details>
  <summary>Details</summary>
Motivation: 探索CryoSBI潜在表示的几何结构，以更好地理解生物分子构象的推断过程。

Method: 应用流形学习技术（如Diffusion Maps）分析模拟和实验数据中的潜在表示。

Result: 发现数据本质上是低维平滑流形，模拟数据覆盖实验数据，并建立了潜在结构与物理参数的联系。

Conclusion: 验证了CryoSBI方法的有效性，为未来推断策略的改进提供了基于流形几何的新机会。

Abstract: Simulation-based inference provides a powerful framework for cryo-electron
microscopy, employing neural networks in methods like CryoSBI to infer
biomolecular conformations via learned latent representations. This latent
space represents a rich opportunity, encoding valuable information about the
physical system and the inference process. Harnessing this potential hinges on
understanding the underlying geometric structure of these representations. We
investigate this structure by applying manifold learning techniques to CryoSBI
representations of hemagglutinin (simulated and experimental). We reveal that
these high-dimensional data inherently populate low-dimensional, smooth
manifolds, with simulated data effectively covering the experimental
counterpart. By characterizing the manifold's geometry using Diffusion Maps and
identifying its principal axes of variation via coordinate interpretation
methods, we establish a direct link between the latent structure and key
physical parameters. Discovering this intrinsic low-dimensionality and
interpretable geometric organization not only validates the CryoSBI approach
but enables us to learn more from the data structure and provides opportunities
for improving future inference strategies by exploiting this revealed manifold
geometry.

</details>

<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [193] [Visual anemometry of natural vegetation from their leaf motion](https://arxiv.org/abs/2504.10584)
*Roni H. Goldshmid,John O. Dabiri,John E. Sader*

Main category: physics.flu-dyn

TLDR: 该论文提出了一种基于植被叶片运动的远程风速测量方法，适用于低到中风速范围，为气象预测和气候模型提供了新的低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 高分辨率近地面风速数据对气象预测、野火控制和航空安全至关重要，但现有方法成本高或复杂。

Method: 通过分析叶片运动与风速的关系，提出公式$U_{wind}\approx740\sqrt{{\mu}U_{leaf}/{\rho}D}$，并验证其准确性。

Result: 实验室和野外测试验证了该方法的有效性，适用于多种植被类型。

Conclusion: 该方法为全球范围内的低成本远程风速测量提供了新途径。

Abstract: High-resolution, near-ground wind-speed data are critical for improving the
accuracy of weather predictions and climate models,$^{1-3}$ supporting wildfire
control efforts,$^{4-7}$ and ensuring the safe passage of airplanes during
takeoff and landing maneouvers.$^{8,9}$ Quantitative wind speed anemometry
generally employs on-site instrumentation for accurate single-position data or
sophisticated remote techniques such as Doppler radar for quantitative field
measurements. It is widely recognized that the wind-induced motion of
vegetation depends in a complex manner on their structure and mechanical
properties, obviating their use in quantitative anemometry.$^{10-14}$ We
analyze measurements on a host of different vegetation showing that leaf motion
can be decoupled from the leaf's branch and support structure, at
low-to-moderate wind speed, $U_{wind}$. This wind speed range is characterized
by a leaf Reynolds number, enabling the development of a remote, quantitative
anemometry method based on the formula,
$U_{wind}\approx740\sqrt{{\mu}U_{leaf}/{\rho}D}$, that relies only on the leaf
size $D$, its measured fluctuating (RMS) speed $U_{leaf}$, the air viscosity
$\mu$, and its mass density $\rho$. This formula is corroborated by a
first-principles model and validated using a host of laboratory and field tests
on diverse vegetation types, ranging from oak, olive, and magnolia trees
through to camphor and bullgrass. The findings of this study open the door to a
new paradigm in anemometry, using natural vegetation to enable remote and rapid
quantitative field measurements at global locations with minimal cost.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [194] [VideoPanda: Video Panoramic Diffusion with Multi-view Attention](https://arxiv.org/abs/2504.11389)
*Kevin Xie,Amirmojtaba Sabour,Jiahui Huang,Despoina Paschalidou,Greg Klar,Umar Iqbal,Sanja Fidler,Xiaohui Zeng*

Main category: cs.GR

TLDR: VideoPanda是一种基于文本或单视图视频生成360度全景视频的新方法，通过多视角注意力层增强视频扩散模型，实现一致的多视角视频合成。


<details>
  <summary>Details</summary>
Motivation: 高分辨率全景视频对虚拟现实沉浸体验至关重要，但采集困难，需专业设备和复杂设置。

Method: 利用多视角注意力层增强视频扩散模型，支持文本或单视图视频条件生成，训练时随机子采样时长和视角以降低计算负担。

Result: 在真实和合成视频数据集上评估，VideoPanda生成的全景视频比现有方法更真实、连贯。

Conclusion: VideoPanda为全景视频生成提供了一种高效、高质量的方法，支持长视频自回归生成。

Abstract: High resolution panoramic video content is paramount for immersive
experiences in Virtual Reality, but is non-trivial to collect as it requires
specialized equipment and intricate camera setups. In this work, we introduce
VideoPanda, a novel approach for synthesizing 360$^\circ$ videos conditioned on
text or single-view video data. VideoPanda leverages multi-view attention
layers to augment a video diffusion model, enabling it to generate consistent
multi-view videos that can be combined into immersive panoramic content.
VideoPanda is trained jointly using two conditions: text-only and single-view
video, and supports autoregressive generation of long-videos. To overcome the
computational burden of multi-view video generation, we randomly subsample the
duration and camera views used during training and show that the model is able
to gracefully generalize to generating more frames during inference. Extensive
evaluations on both real-world and synthetic video datasets demonstrate that
VideoPanda generates more realistic and coherent 360$^\circ$ panoramas across
all input conditions compared to existing methods. Visit the project website at
https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [195] [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/abs/2504.11257)
*Xinyi Liu,Xiaoyi Zhang,Ziyun Zhang,Yan Lu*

Main category: cs.HC

TLDR: 论文提出了一种基于视觉的GUI指令定位方法，通过数据合成管道UI-E2I-Synth生成训练数据，并引入新基准UI-I2E-Bench，模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有GUI指令定位任务中数据稀缺和标注成本高的问题，同时应对元素比例、类型不平衡和隐式指令等挑战。

Method: 使用GPT-4o生成复杂指令数据集UI-E2I-Synth，提出新基准UI-I2E-Bench，并训练模型进行GUI指令定位。

Result: 模型在GUI指令定位任务中表现优异，验证了数据合成管道的有效性。

Conclusion: 提出的数据合成方法和基准为未来GUI定位研究提供了实用工具和方向。

Abstract: Recent advancements in Large Vision-Language Models are accelerating the
development of Graphical User Interface (GUI) agents that utilize human-like
vision perception capabilities to enhance productivity on digital devices.
Compared to approaches predicated on GUI metadata, which are platform-dependent
and vulnerable to implementation variations, vision-based approaches offer
broader applicability. In this vision-based paradigm, the GUI instruction
grounding, which maps user instruction to the location of corresponding element
on the given screenshot, remains a critical challenge, particularly due to
limited public training dataset and resource-intensive manual instruction data
annotation.In this paper, we delve into unexplored challenges in this task
including element-to-screen ratio, unbalanced element type, and implicit
instruction. To address these challenges, we introduce a large-scale data
synthesis pipeline UI-E2I-Synth for generating varying complex instruction
datasets using GPT-4o instead of human annotators. Furthermore, we propose a
new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to
address the limitations of existing benchmarks by incorporating diverse
annotation aspects. Our model, trained on the synthesized data, achieves
superior performance in GUI instruction grounding, demonstrating the
advancements of proposed data synthesis pipeline. The proposed benchmark,
accompanied by extensive analyses, provides practical insights for future
research in GUI grounding. We will release corresponding artifacts at
https://colmon46.github.io/i2e-bench-leaderboard/

</details>

### [196] [The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to Fine-Print Injections](https://arxiv.org/abs/2504.11281)
*Chaoran Chen,Zhiping Zhang,Bingcan Guo,Shang Ma,Ibrahim Khalilov,Simret A Gebreegziabher,Yanfang Ye,Ziang Xiao,Yaxing Yao,Tianshi Li,Toby Jia-Jun Li*

Main category: cs.HC

TLDR: 论文研究了基于大型语言模型（LLM）的GUI代理在隐私和安全方面的风险，提出了六种攻击类型，并通过实验验证了其脆弱性，同时提出了防御策略。


<details>
  <summary>Details</summary>
Motivation: GUI代理在处理敏感用户数据时可能面临隐私和安全风险，攻击者可能通过恶意内容操控代理行为或泄露信息。

Method: 通过实验研究，测试了六种攻击类型对六种先进GUI代理的影响，并分析了234个对抗性网页和39名人类参与者的数据。

Result: GUI代理对上下文嵌入的威胁高度脆弱，人类用户也难以完全避免这些攻击。

Conclusion: 需要设计隐私感知的代理，并提出了实用的防御策略以提高GUI代理的安全性和可靠性。

Abstract: A Large Language Model (LLM) powered GUI agent is a specialized autonomous
system that performs tasks on the user's behalf according to high-level
instructions. It does so by perceiving and interpreting the graphical user
interfaces (GUIs) of relevant apps, often visually, inferring necessary
sequences of actions, and then interacting with GUIs by executing the actions
such as clicking, typing, and tapping. To complete real-world tasks, such as
filling forms or booking services, GUI agents often need to process and act on
sensitive user data. However, this autonomy introduces new privacy and security
risks. Adversaries can inject malicious content into the GUIs that alters agent
behaviors or induces unintended disclosures of private information. These
attacks often exploit the discrepancy between visual saliency for agents and
human users, or the agent's limited ability to detect violations of contextual
integrity in task automation. In this paper, we characterized six types of such
attacks, and conducted an experimental study to test these attacks with six
state-of-the-art GUI agents, 234 adversarial webpages, and 39 human
participants. Our findings suggest that GUI agents are highly vulnerable,
particularly to contextually embedded threats. Moreover, human users are also
susceptible to many of these attacks, indicating that simple human oversight
may not reliably prevent failures. This misalignment highlights the need for
privacy-aware agent design. We propose practical defense strategies to inform
the development of safer and more reliable GUI agents.

</details>

### [197] [Interactivity x Explainability: Toward Understanding How Interactivity Can Improve Computer Vision Explanations](https://arxiv.org/abs/2504.10745)
*Indu Panigrahi,Sunnie S. Y. Kim,Amna Liaqat,Rohan Jinturkar,Olga Russakovsky,Ruth Fong,Parastoo Abtahi*

Main category: cs.HC

TLDR: 研究探讨了交互性如何改进计算机视觉模型的解释方法，发现交互性虽提升用户控制与理解，但也带来新挑战，并提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 静态解释方法存在信息过载、语义与像素级信息脱节及探索机会有限等问题，交互性被视为解决这些问题的机制。

Method: 通过鸟类识别任务研究（N=24），评估三种交互式解释方法（热图、概念和原型）的效果。

Result: 交互性增强用户控制、快速定位信息并扩展理解，但也引入新挑战。

Conclusion: 提出设计建议，如默认视图选择、独立输入控制和受限输出空间，以优化交互式解释。

Abstract: Explanations for computer vision models are important tools for interpreting
how the underlying models work. However, they are often presented in static
formats, which pose challenges for users, including information overload, a gap
between semantic and pixel-level information, and limited opportunities for
exploration. We investigate interactivity as a mechanism for tackling these
issues in three common explanation types: heatmap-based, concept-based, and
prototype-based explanations. We conducted a study (N=24), using a bird
identification task, involving participants with diverse technical and domain
expertise. We found that while interactivity enhances user control, facilitates
rapid convergence to relevant information, and allows users to expand their
understanding of the model and explanation, it also introduces new challenges.
To address these, we provide design recommendations for interactive computer
vision explanations, including carefully selected default views, independent
input controls, and constrained output spaces.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [198] [ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping](https://arxiv.org/abs/2504.10857)
*Shun Iwase,Zubair Irshad,Katherine Liu,Vitor Guizilini,Robert Lee,Takuya Ikeda,Ayako Amma,Koichi Nishiwaki,Kris Kitani,Rares Ambrus,Sergey Zakharov*

Main category: cs.RO

TLDR: ZeroGrasp是一个实时3D重建和抓取姿态预测框架，通过建模场景几何关系和遮挡推理提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从部分信息预测抓取姿态，未建模场景几何，导致运动不优甚至碰撞。

Method: ZeroGrasp同时进行3D重建和抓取姿态预测，结合遮挡推理和空间关系建模。

Result: 在GraspNet-1B基准和真实机器人实验中达到SOTA性能，并能泛化到新物体。

Conclusion: ZeroGrasp通过合成数据和几何建模显著提升了抓取性能。

Abstract: Robotic grasping is a cornerstone capability of embodied systems. Many
methods directly output grasps from partial information without modeling the
geometry of the scene, leading to suboptimal motion and even collisions. To
address these issues, we introduce ZeroGrasp, a novel framework that
simultaneously performs 3D reconstruction and grasp pose prediction in near
real-time. A key insight of our method is that occlusion reasoning and modeling
the spatial relationships between objects is beneficial for both accurate
reconstruction and grasping. We couple our method with a novel large-scale
synthetic dataset, which comprises 1M photo-realistic images, high-resolution
3D reconstructions and 11.3B physically-valid grasp pose annotations for 12K
objects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the
GraspNet-1B benchmark as well as through real-world robot experiments.
ZeroGrasp achieves state-of-the-art performance and generalizes to novel
real-world objects by leveraging synthetic data.

</details>

### [199] [Acquisition of high-quality images for camera calibration in robotics applications via speech prompts](https://arxiv.org/abs/2504.11031)
*Timm Linder,Kadir Yilmaz,David B. Adrian,Bastian Leibe*

Main category: cs.RO

TLDR: 提出一种基于语音命令的相机标定图像采集技术，提高标定过程的鲁棒性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有相机标定方法依赖高质量的图像输入，避免运动模糊和卷帘效应，传统触发方式（如遥控器）或后处理过滤模糊帧效率较低。

Method: 利用先进的语音转文本模型，通过精确时间戳捕捉触发词，实现语音控制的图像采集。

Result: 实验表明该方法快速高效，成功标定复杂多相机系统，提升用户体验。

Conclusion: 语音控制的标定图像采集技术是一种更鲁棒和用户友好的解决方案。

Abstract: Accurate intrinsic and extrinsic camera calibration can be an important
prerequisite for robotic applications that rely on vision as input. While there
is ongoing research on enabling camera calibration using natural images, many
systems in practice still rely on using designated calibration targets with
e.g. checkerboard patterns or April tag grids. Once calibration images from
different perspectives have been acquired and feature descriptors detected,
those are typically used in an optimization process to minimize the geometric
reprojection error. For this optimization to converge, input images need to be
of sufficient quality and particularly sharpness; they should neither contain
motion blur nor rolling-shutter artifacts that can arise when the calibration
board was not static during image capture. In this work, we present a novel
calibration image acquisition technique controlled via voice commands recorded
with a clip-on microphone, that can be more robust and user-friendly than e.g.
triggering capture with a remote control, or filtering out blurry frames from a
video sequence in postprocessing. To achieve this, we use a state-of-the-art
speech-to-text transcription model with accurate per-word timestamping to
capture trigger words with precise temporal alignment. Our experiments show
that the proposed method improves user experience by being fast and efficient,
allowing us to successfully calibrate complex multi-camera setups.

</details>

### [200] [Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks](https://arxiv.org/abs/2504.11247)
*Fikrican Özgür,René Zurbrügg,Suryansh Kumar*

Main category: cs.RO

TLDR: 论文提出了一种名为“Next-Future”的新回放策略，用于多目标强化学习，显著提高了样本效率和准确性，尤其在复杂机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: Hindsight Experience Replay (HER) 虽然高效，但其启发式回放方法缺乏理论框架，限制了性能。

Method: 提出“Next-Future”策略，专注于单步过渡的奖励，优化多目标MDP学习。

Result: 在八项机器人任务中，七项样本效率提升，六项成功率提高，实际实验验证了策略的可行性。

Conclusion: “Next-Future”策略在多目标强化学习中表现优越，适用于复杂机器人任务。

Abstract: Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art
algorithm for achieving sample-efficient multi-goal reinforcement learning (RL)
in robotic manipulation tasks with binary rewards. HER facilitates learning
from failed attempts by replaying trajectories with redefined goals. However,
it relies on a heuristic-based replay method that lacks a principled framework.
To address this limitation, we introduce a novel replay strategy,
"Next-Future", which focuses on rewarding single-step transitions. This
approach significantly enhances sample efficiency and accuracy in learning
multi-goal Markov decision processes (MDPs), particularly under stringent
accuracy requirements -- a critical aspect for performing complex and precise
robotic-arm tasks. We demonstrate the efficacy of our method by highlighting
how single-step learning enables improved value approximation within the
multi-goal RL framework. The performance of the proposed replay strategy is
evaluated across eight challenging robotic manipulation tasks, using ten random
seeds for training. Our results indicate substantial improvements in sample
efficiency for seven out of eight tasks and higher success rates in six tasks.
Furthermore, real-world experiments validate the practical feasibility of the
learned policies, demonstrating the potential of "Next-Future" in solving
complex robotic-arm tasks.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [201] [Will AI shape the way we speak? The emerging sociolinguistic influence of synthetic voices](https://arxiv.org/abs/2504.10650)
*Éva Székely,Jūra Miniota,Míša,Hejná*

Main category: cs.CY

TLDR: 论文探讨了语音交互界面对人类交流的影响，尤其是通过语音传递的社会身份信息，并呼吁跨学科研究其潜在社会影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解语音AI如何通过语音特征（如口音、语调）影响用户的社会身份认知和语言模式。

Method: 通过分析语音交互的动态特性（如声学韵律趋同和语言适应）来探讨其影响。

Result: 语音AI的交互性可能比被动媒体更显著地影响用户的语言习惯和社会身份表达。

Conclusion: 结论认为语音AI的社会影响值得关注，需要跨学科研究以更好地理解其潜在影响。

Abstract: The growing prevalence of conversational voice interfaces, powered by
developments in both speech and language technologies, raises important
questions about their influence on human communication. While written
communication can signal identity through lexical and stylistic choices,
voice-based interactions inherently amplify socioindexical elements - such as
accent, intonation, and speech style - which more prominently convey social
identity and group affiliation. There is evidence that even passive media such
as television is likely to influence the audience's linguistic patterns. Unlike
passive media, conversational AI is interactive, creating a more immersive and
reciprocal dynamic that holds a greater potential to impact how individuals
speak in everyday interactions. Such heightened influence can be expected to
arise from phenomena such as acoustic-prosodic entrainment and linguistic
accommodation, which occur naturally during interaction and enable users to
adapt their speech patterns in response to the system. While this phenomenon is
still emerging, its potential societal impact could provide organisations,
movements, and brands with a subtle yet powerful avenue for shaping and
controlling public perception and social identity. We argue that the
socioindexical influence of AI-generated speech warrants attention and should
become a focus of interdisciplinary research, leveraging new and existing
methodologies and technologies to better understand its implications.

</details>

### [202] [Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment](https://arxiv.org/abs/2504.10886)
*Jiseon Kim,Jea Kwon,Luiz Felipe Vecchietti,Alice Oh,Meeyoung Cha*

Main category: cs.CY

TLDR: 研究探讨大型语言模型（LLM）在道德困境中的决策与人类判断的一致性，发现其决策因角色设定差异显著，且政治倾向影响较大。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在现实应用中道德决策的可靠性及其与人类判断的差异。

Method: 通过道德机器实验，分析不同社会人口学角色下LLM与人类决策的对比。

Result: LLM的道德决策因角色设定差异显著，政治倾向对其影响尤为突出。

Conclusion: 需谨慎评估LLM在涉及道德决策应用中的伦理风险。

Abstract: Deploying large language models (LLMs) with agency in real-world applications
raises critical questions about how these models will behave. In particular,
how will their decisions align with humans when faced with moral dilemmas? This
study examines the alignment between LLM-driven decisions and human judgment in
various contexts of the moral machine experiment, including personas reflecting
different sociodemographics. We find that the moral decisions of LLMs vary
substantially by persona, showing greater shifts in moral decisions for
critical tasks than humans. Our data also indicate an interesting partisan
sorting phenomenon, where political persona predominates the direction and
degree of LLM decisions. We discuss the ethical implications and risks
associated with deploying these models in applications that involve moral
decisions.

</details>